Boston <- read.csv("C:/Users/Brando/OneDrive/Documents/SchoolWork/Machine Learning/Homework2/Boston.csv")
str(Boston)
# 3 create column of booleans for High Crime areas
Boston["crim1"] <- NA
Boston$crim1 <- ifelse(Boston$crim > median(Boston$crim),1,0)
# 4 - Explore the data to find potential associationgs
# Perform the following for all variables to check which are viable for comparison.
summary(Boston$zn[Boston$crim1 == 1])
summary(Boston$zn[Boston$crim1 == 0])
# 5 - Set a seed for replayability
set.seed(1234)
# 6 - Normalize attribute data
normalize <- function(x) {return ((x - min(x)) / (max(x) - min(x)))}
Boston_n <- data.frame(Boston[1],lapply(Boston[2:15], normalize),Boston[16])
# 7 - Randomize order of data
Boston_n <- Boston_n[sample(1:nrow(Boston_n)), ]
# 8 - Split Data Sets
Boston_Train <- Boston_n[1:406,]
Boston_Test <- Boston_n[407:506,]
# 9 - Perform kNN and find a good value of k
library(class)
library(gmodels)
Boston_Train_Labels <- Boston_n[1:406, 16]
Boston_Test_Labels <- Boston_n[407:506,16]
# Repeat this next step with different values of k until a good one is found
Boston_Test_Predictions <- knn(train = Boston_Train, test = Boston_Test, cl = Boston_Train_Labels, k = 5)
CrossTable(x = Boston_Test_Labels, y = Boston_Test_Predictions, prop.chisq=FALSE)

# 10 - Perform a Naive Bayes Classification on the data
# install.packages("e1071")
library(e1071)

# Turns out that the data appears to not need to be discretized
# If column 2 is left in the classifier can predict with 100% accuracy a factor based solely on column 2
# Data frames here must be altered to match appropraite data types. Specifically factors and matrices. Discovering this was pain.
Boston_Classifier <- naiveBayes(as.matrix(Boston_Train[3:15]),as.factor(Boston_Train_Labels))
Boston_Bayes_Predictions <- predict(Boston_Classifier, as.matrix(Boston_Test[3:15]))
CrossTable(x = as.factor(Boston_Test_Labels), y = Boston_Bayes_Predictions, prop.chisq=FALSE, prop.t = FALSE, dnn = c('actual','predicted'))
