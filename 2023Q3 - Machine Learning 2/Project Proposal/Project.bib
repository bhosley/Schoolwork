
@article{johnson2019,
	title = {Survey on deep learning with class imbalance},
	volume = {6},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-019-0192-5},
	doi = {10.1186/s40537-019-0192-5},
	abstract = {The purpose of this study is to examine existing deep learning techniques for addressing class imbalanced data. Effective classification with imbalanced data is an important area of research, as high class imbalance is naturally inherent in many real-world applications, e.g., fraud detection and cancer detection. Moreover, highly imbalanced data poses added difficulty, as most learners will exhibit bias towards the majority class, and in extreme cases, may ignore the minority class altogether. Class imbalance has been studied thoroughly over the last two decades using traditional machine learning models, i.e. non-deep learning. Despite recent advances in deep learning, along with its increasing popularity, very little empirical work in the area of deep learning with class imbalance exists. Having achieved record-breaking performance results in several complex domains, investigating the use of deep neural networks for problems containing high levels of class imbalance is of great interest. Available studies regarding class imbalance and deep learning are surveyed in order to better understand the efficacy of deep learning when applied to class imbalanced data. This survey discusses the implementation details and experimental results for each study, and offers additional insight into their strengths and weaknesses. Several areas of focus include: data complexity, architectures tested, performance interpretation, ease of use, big data application, and generalization to other domains. We have found that research in this area is very limited, that most existing work focuses on computer vision tasks with convolutional neural networks, and that the effects of big data are rarely considered. Several traditional methods for class imbalance, e.g. data sampling and cost-sensitive learning, prove to be applicable in deep learning, while more advanced methods that exploit neural network feature learning abilities show promising results. The survey concludes with a discussion that highlights various gaps in deep learning from class imbalanced data for the purpose of guiding future research.},
	pages = {27},
	number = {1},
	journaltitle = {Journal of Big Data},
	shortjournal = {J Big Data},
	author = {Johnson, Justin M. and Khoshgoftaar, Taghi M.},
	urldate = {2023-04-06},
	date = {2019-03-19},
	langid = {english},
	keywords = {Big data, Class imbalance, Deep learning, Deep neural networks},
	file = {Johnson_Khoshgoftaar_2019_Survey on deep learning with class imbalance.pdf:/Users/brandonhosley/Zotero/storage/F9LEJAPQ/Johnson_Khoshgoftaar_2019_Survey on deep learning with class imbalance.pdf:application/pdf},
}

@online{cdc2022,
	title = {{CDC} - {BRFSS} Annual Survey Data},
	url = {https://www.cdc.gov/brfss/annual_data/annual_data.htm},
	abstract = {{BRFSS} has a long history in behavioral and chronic disease surveillance. Fifteen states participated in the first {BRFSS}, conducted in 1984.},
	author = {{CDC}},
	urldate = {2023-05-15},
	date = {2022-07-26},
	langid = {english},
	file = {Snapshot:/Users/brandonhosley/Zotero/storage/DW3INDXR/annual_data.html:text/html},
}

@online{fajobi2023,
	title = {Diabetes Prediction},
	url = {https://www.kaggle.com/code/solafajobi/diabetes-perfect-prediction/notebook},
	author = {Fajobi, Sola},
	urldate = {2023-05-15},
	date = {2023-02-14},
	file = {Notebook:/Users/brandonhosley/Zotero/storage/AHT52V5P/notebook.html:text/html},
}

@article{xie2019,
	title = {Building Risk Prediction Models for Type 2 Diabetes Using Machine Learning Techniques},
	volume = {16},
	issn = {1545-1151},
	url = {https://www.cdc.gov/pcd/issues/2019/19_0109.htm},
	doi = {10.5888/pcd16.190109},
	abstract = {Preventing Chronic Disease ({PCD}) is a peer-reviewed electronic journal established by the National Center for Chronic Disease Prevention and Health Promotion. {PCD} provides an open exchange of information and knowledge among researchers, practitioners, policy makers, and others who strive to improve the health of the public through chronic disease prevention.},
	journaltitle = {Preventing Chronic Disease},
	shortjournal = {Prev. Chronic Dis.},
	author = {Xie, Zidian},
	urldate = {2023-05-15},
	date = {2019},
	langid = {english},
	file = {Snapshot:/Users/brandonhosley/Zotero/storage/7S47TINQ/19_0109.html:text/html;Xie_2019_Building Risk Prediction Models for Type 2 Diabetes Using Machine Learning.pdf:/Users/brandonhosley/Zotero/storage/HVECFDCU/Xie_2019_Building Risk Prediction Models for Type 2 Diabetes Using Machine Learning.pdf:application/pdf},
}

@article{chawla2002,
	title = {{SMOTE}: Synthetic Minority Over-sampling Technique},
	volume = {16},
	issn = {1076-9757},
	url = {http://arxiv.org/abs/1106.1813},
	doi = {10.1613/jair.953},
	shorttitle = {{SMOTE}},
	abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in {ROC} space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in {ROC} space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve ({AUC}) and the {ROC} convex hull strategy.},
	pages = {321--357},
	journaltitle = {Journal of Artificial Intelligence Research},
	shortjournal = {jair},
	author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
	urldate = {2023-05-15},
	date = {2002-06-01},
	eprinttype = {arxiv},
	eprint = {1106.1813 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/brandonhosley/Zotero/storage/JDVKS5C9/1106.html:text/html;Chawla et al_2002_SMOTE.pdf:/Users/brandonhosley/Zotero/storage/B8N8HUHD/Chawla et al_2002_SMOTE.pdf:application/pdf},
}

@article{liu2009,
	title = {Exploratory undersampling for class-imbalance learning},
	volume = {39},
	issn = {1941-0492},
	doi = {10.1109/TSMCB.2008.2007853},
	abstract = {Undersampling is a popular method in dealing with class-imbalance problems, which uses only a subset of the majority class and thus is very efficient. The main deficiency is that many majority class examples are ignored. We propose two algorithms to overcome this deficiency. {EasyEnsemble} samples several subsets from the majority class, trains a learner using each of them, and combines the outputs of those learners. {BalanceCascade} trains the learners sequentially, where in each step, the majority class examples that are correctly classified by the current trained learners are removed from further consideration. Experimental results show that both methods have higher Area Under the {ROC} Curve, F-measure, and G-mean values than many existing class-imbalance learning methods. Moreover, they have approximately the same training time as that of undersampling when the same number of weak classifiers is used, which is significantly faster than other methods.},
	pages = {539--550},
	number = {2},
	journaltitle = {{IEEE} transactions on systems, man, and cybernetics. Part B, Cybernetics: a publication of the {IEEE} Systems, Man, and Cybernetics Society},
	shortjournal = {{IEEE} Trans Syst Man Cybern B Cybern},
	author = {Liu, Xu-Ying and Wu, Jianxin and Zhou, Zhi-Hua},
	date = {2009-04},
	pmid = {19095540},
	file = {Liu et al_2009_Exploratory undersampling for class-imbalance learning.pdf:/Users/brandonhosley/Zotero/storage/I65QX2WM/Liu et al_2009_Exploratory undersampling for class-imbalance learning.pdf:application/pdf},
}

@article{elkan2001,
	title = {The Foundations of Cost-Sensitive Learning},
	volume = {1},
	abstract = {This paper revisits the problem of optimal learning and decision-making when different misclassification errors incur different penalties. We characterize precisely but intuitively when a cost matrix is reasonable, and we show how to avoid the mistake of defining a cost matrix that is economically incoherent. For the two-class case, we prove a theorem that shows how to change the proportion of negative examples in a training set in order to make optimal cost-sensitive classification decisions using a classifier learned by a standard non-costsensitive learning method. However, we then argue that changing the balance of negative and positive training examples has little effect on the classifiers produced by standard Bayesian and decision tree learning methods. Accordingly, the recommended way of applying one of these methods in a domain with differing misclassification costs is to learn a classifier from the training set as given, and then to compute optimal decisions ...},
	journaltitle = {Proceedings of the Seventeenth International Conference on Artificial Intelligence: 4-10 August 2001; Seattle},
	shortjournal = {Proceedings of the Seventeenth International Conference on Artificial Intelligence: 4-10 August 2001; Seattle},
	author = {Elkan, Charles},
	date = {2001-05-12},
	file = {Elkan_2001_The Foundations of Cost-Sensitive Learning.pdf:/Users/brandonhosley/Zotero/storage/A4ZU79DU/Elkan_2001_The Foundations of Cost-Sensitive Learning.pdf:application/pdf},
}

@article{freund1997,
	title = {A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting},
	volume = {55},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S002200009791504X},
	doi = {10.1006/jcss.1997.1504},
	abstract = {In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone–Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.},
	pages = {119--139},
	number = {1},
	journaltitle = {Journal of Computer and System Sciences},
	shortjournal = {Journal of Computer and System Sciences},
	author = {Freund, Yoav and Schapire, Robert E},
	urldate = {2023-05-15},
	date = {1997-08-01},
	langid = {english},
	file = {Freund_Schapire_1997_A Decision-Theoretic Generalization of On-Line Learning and an Application to.pdf:/Users/brandonhosley/Zotero/storage/GY8RNJUA/Freund_Schapire_1997_A Decision-Theoretic Generalization of On-Line Learning and an Application to.pdf:application/pdf;ScienceDirect Snapshot:/Users/brandonhosley/Zotero/storage/JW8X5CM3/S002200009791504X.html:text/html},
}

@article{chen2004,
	title = {Using Random Forest to Learn Imbalanced Data},
	abstract = {In this paper we propose two ways to deal with the imbalanced data classiﬁcation problem using random forest. One is based on cost sensitive learning, and the other is based on a sampling technique. Performance metrics such as precision and recall, false positive rate and false negative rate, F-measure and weighted accuracy are computed. Both methods are shown to improve the prediction accuracy of the minority class, and have favorable performance compared to the existing algorithms.},
	author = {Chen, Chao and Liaw, Andy and Breiman, Leo},
	date = {2004-07-01},
	langid = {english},
	file = {Chen - Using Random Forest to Learn Imbalanced Data.pdf:/Users/brandonhosley/Zotero/storage/BUB6HQ67/Chen - Using Random Forest to Learn Imbalanced Data.pdf:application/pdf},
}

@inproceedings{gao2015,
	title = {Online One-Class {SVMs} with Active-Set Optimization for Data Streams},
	doi = {10.1109/ICMLA.2015.101},
	abstract = {A great advantage of support vector machines ({SVMs}) is its capability to learn decision borders, represented by a set of particular data points called margin support vectors. The real-time or nearly real-time online learning and detection from data streams poses stringent time and space constraints for the learner. We consider solving online one-class {SVMs} with an active-set method for quadratic programming ({QP}). At each iteration, the problem size is the size of the estimated support vectors so far. Active-set programming has the nice property that the solution of a previous problem can serve as a warm start of the next and computation time can thereby be greatly reduced. In general, finding a good warm-start point is difficult. We propose a method to find a good warm start by exploiting the structure of the {SVM} optimization problem.},
	eventtitle = {2015 {IEEE} 14th International Conference on Machine Learning and Applications ({ICMLA})},
	pages = {116--121},
	booktitle = {2015 {IEEE} 14th International Conference on Machine Learning and Applications ({ICMLA})},
	author = {Gao, Katelyn},
	date = {2015-12},
	keywords = {Kernel, Quadratic programming, Real-time systems, Standards, Support vector machines, Training},
	file = {IEEE Xplore Abstract Record:/Users/brandonhosley/Zotero/storage/AFZLYY6N/7424295.html:text/html},
}

@misc{cui2019,
	title = {Class-Balanced Loss Based on Effective Number of Samples},
	url = {http://arxiv.org/abs/1901.05555},
	doi = {10.48550/arXiv.1901.05555},
	abstract = {With the rapid increase of large-scale, real-world datasets, it becomes critical to address the problem of long-tailed data distribution (i.e., a few classes account for most of the data, while most classes are under-represented). Existing solutions typically adopt class re-balancing strategies such as re-sampling and re-weighting based on the number of observations for each class. In this work, we argue that as the number of samples increases, the additional benefit of a newly added data point will diminish. We introduce a novel theoretical framework to measure data overlap by associating with each sample a small neighboring region rather than a single point. The effective number of samples is defined as the volume of samples and can be calculated by a simple formula \$(1-{\textbackslash}beta{\textasciicircum}\{n\})/(1-{\textbackslash}beta)\$, where \$n\$ is the number of samples and \${\textbackslash}beta {\textbackslash}in [0,1)\$ is a hyperparameter. We design a re-weighting scheme that uses the effective number of samples for each class to re-balance the loss, thereby yielding a class-balanced loss. Comprehensive experiments are conducted on artificially induced long-tailed {CIFAR} datasets and large-scale datasets including {ImageNet} and {iNaturalist}. Our results show that when trained with the proposed class-balanced loss, the network is able to achieve significant performance gains on long-tailed datasets.},
	number = {{arXiv}:1901.05555},
	publisher = {{arXiv}},
	author = {Cui, Yin and Jia, Menglin and Lin, Tsung-Yi and Song, Yang and Belongie, Serge},
	urldate = {2023-05-15},
	date = {2019-01-16},
	eprinttype = {arxiv},
	eprint = {1901.05555 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/brandonhosley/Zotero/storage/AMKPLNNR/1901.html:text/html;Cui et al_2019_Class-Balanced Loss Based on Effective Number of Samples.pdf:/Users/brandonhosley/Zotero/storage/XWY6FG8V/Cui et al_2019_Class-Balanced Loss Based on Effective Number of Samples.pdf:application/pdf},
}

@misc{khan2017,
	title = {Cost Sensitive Learning of Deep Feature Representations from Imbalanced Data},
	url = {http://arxiv.org/abs/1508.03422},
	doi = {10.48550/arXiv.1508.03422},
	abstract = {Class imbalance is a common problem in the case of real-world object detection and classification tasks. Data of some classes is abundant making them an over-represented majority, and data of other classes is scarce, making them an under-represented minority. This imbalance makes it challenging for a classifier to appropriately learn the discriminating boundaries of the majority and minority classes. In this work, we propose a cost sensitive deep neural network which can automatically learn robust feature representations for both the majority and minority classes. During training, our learning procedure jointly optimizes the class dependent costs and the neural network parameters. The proposed approach is applicable to both binary and multi-class problems without any modification. Moreover, as opposed to data level approaches, we do not alter the original data distribution which results in a lower computational cost during the training process. We report the results of our experiments on six major image classification datasets and show that the proposed approach significantly outperforms the baseline algorithms. Comparisons with popular data sampling techniques and cost sensitive classifiers demonstrate the superior performance of our proposed method.},
	number = {{arXiv}:1508.03422},
	publisher = {{arXiv}},
	author = {Khan, Salman H. and Hayat, Munawar and Bennamoun, Mohammed and Sohel, Ferdous and Togneri, Roberto},
	urldate = {2023-05-15},
	date = {2017-03-23},
	eprinttype = {arxiv},
	eprint = {1508.03422 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/brandonhosley/Zotero/storage/SCMT65U4/1508.html:text/html;Khan et al_2017_Cost Sensitive Learning of Deep Feature Representations from Imbalanced Data.pdf:/Users/brandonhosley/Zotero/storage/NGTB88Z5/Khan et al_2017_Cost Sensitive Learning of Deep Feature Representations from Imbalanced Data.pdf:application/pdf},
}

@incollection{wang2018,
	title = {Bootstrap Methods: The Classical Theory and Recent Development},
	isbn = {978-1-118-44511-2},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat04579.pub2},
	shorttitle = {Bootstrap Methods},
	abstract = {Bootstrap is a resampling method for statistical inference. Under fairly general conditions, the technique can be used to approximate sampling distributions of almost any statistics, by recycling data from the observed sample, that is, resampling. In this article, we review the theoretical tenets of bootstrapping, focusing primarily on the fundamental property of consistency, while showing examples where lack of consistency can lead to failures of the method. We also describe residual and pairs bootstrap methods in linear models, as well as their applications in low- and high-dimensional problems. Finally, we discuss a modified bootstrap procedure in big data situations.},
	pages = {1--12},
	booktitle = {Wiley {StatsRef}: Statistics Reference Online},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Wang, Honglang and Tu, Wanzhu},
	urldate = {2023-05-15},
	date = {2018},
	langid = {english},
	doi = {10.1002/9781118445112.stat04579.pub2},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat04579.pub2},
	keywords = {high dimensional data, resampling, statistical inference},
	file = {Snapshot:/Users/brandonhosley/Zotero/storage/2I8QVPVR/9781118445112.stat04579.html:text/html},
}

@article{kaur2019,
	title = {A Systematic Review on Imbalanced Data Challenges in Machine Learning: Applications and Solutions},
	volume = {52},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3343440},
	doi = {10.1145/3343440},
	shorttitle = {A Systematic Review on Imbalanced Data Challenges in Machine Learning},
	abstract = {In machine learning, the data imbalance imposes challenges to perform data analytics in almost all areas of real-world research. The raw primary data often suffers from the skewed perspective of data distribution of one class over the other as in the case of computer vision, information security, marketing, and medical science. The goal of this article is to present a comparative analysis of the approaches from the reference of data pre-processing, algorithmic and hybrid paradigms for contemporary imbalance data analysis techniques, and their comparative study in lieu of different data distribution and their application areas.},
	pages = {79:1--79:36},
	number = {4},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Kaur, Harsurinder and Pannu, Husanbir Singh and Malhi, Avleen Kaur},
	urldate = {2023-05-15},
	date = {2019-08-30},
	keywords = {data analysis, Data imbalance, machine learning, sampling},
}

@article{batista2004,
	title = {A study of the behavior of several methods for balancing machine learning training data},
	volume = {6},
	issn = {1931-0145},
	url = {https://doi.org/10.1145/1007730.1007735},
	doi = {10.1145/1007730.1007735},
	abstract = {There are several aspects that might influence the performance achieved by existing learning systems. It has been reported that one of these aspects is related to class imbalance in which examples in training data belonging to one class heavily outnumber the examples in the other class. In this situation, which is found in real world data describing an infrequent but important event, the learning system may have difficulties to learn the concept related to the minority class. In this work we perform a broad experimental evaluation involving ten methods, three of them proposed by the authors, to deal with the class imbalance problem in thirteen {UCI} data sets. Our experiments provide evidence that class imbalance does not systematically hinder the performance of learning systems. In fact, the problem seems to be related to learning with too few minority class examples in the presence of other complicating factors, such as class overlapping. Two of our proposed methods deal with these conditions directly, allying a known over-sampling method with data cleaning methods in order to produce better-defined class clusters. Our comparative experiments show that, in general, over-sampling methods provide more accurate results than under-sampling methods considering the area under the {ROC} curve ({AUC}). This result seems to contradict results previously published in the literature. Two of our proposed methods, Smote + Tomek and Smote + {ENN}, presented very good results for data sets with a small number of positive examples. Moreover, Random over-sampling, a very simple over-sampling method, is very competitive to more complex over-sampling methods. Since the over-sampling methods provided very good performance results, we also measured the syntactic complexity of the decision trees induced from over-sampled data. Our results show that these trees are usually more complex then the ones induced from original data. Random over-sampling usually produced the smallest increase in the mean number of induced rules and Smote + {ENN} the smallest increase in the mean number of conditions per rule, when compared among the investigated over-sampling methods.},
	pages = {20--29},
	number = {1},
	journaltitle = {{ACM} {SIGKDD} Explorations Newsletter},
	shortjournal = {{SIGKDD} Explor. Newsl.},
	author = {Batista, Gustavo E. A. P. A. and Prati, Ronaldo C. and Monard, Maria Carolina},
	urldate = {2023-05-15},
	date = {2004-06-01},
}

@inproceedings{kubat1997,
	title = {Addressing the Curse of Imbalanced Training Sets: One-Sided Selection},
	url = {https://www.semanticscholar.org/paper/Addressing-the-Curse-of-Imbalanced-Training-Sets%3A-Kub%C3%A1t-Matwin/ebc3914181d76c817f0e35f788b7c4c0f80abb07},
	shorttitle = {Addressing the Curse of Imbalanced Training Sets},
	abstract = {Adding examples of the majority class to the training set can have a detrimental e(cid:11)ect on the learner's behavior: noisy or otherwise unreliable examples from the majority class can overwhelm the minority class. The paper discusses criteria to evaluate the utility of classi(cid:12)ers induced from such imbalanced training sets, gives explanation of the poor behavior of some learners under these circumstances, and suggests as a solution a simple technique called one-sided selection of examples},
	eventtitle = {International Conference on Machine Learning},
	author = {Kubát, M. and Matwin, S.},
	urldate = {2023-05-16},
	date = {1997},
	file = {Kubát_Matwin_1997_Addressing the Curse of Imbalanced Training Sets.pdf:/Users/brandonhosley/Zotero/storage/6ER7428M/Kubát_Matwin_1997_Addressing the Curse of Imbalanced Training Sets.pdf:application/pdf},
}

@inproceedings{tomek1976,
	title = {Two Modifications of {CNN}},
	volume = {{SMC}-6},
	url = {http://ieeexplore.ieee.org/document/4309452/},
	doi = {10.1109/TSMC.1976.4309452},
	abstract = {Semantic Scholar extracted view of "Two Modifications of {CNN}" by I. Tomek},
	pages = {769--772},
	booktitle = {{IEEE} Transactions on Systems, Man, and Cybernetics},
	author = {Tomek, Ivan},
	urldate = {2023-05-17},
	date = {1976-11},
	note = {{ISSN}: 0018-9472, 2168-2909
Issue: 11
Journal Abbreviation: {IEEE} Trans. Syst., Man, Cybern.},
}

@incollection{han2005,
	location = {Berlin, Heidelberg},
	title = {Borderline-{SMOTE}: A New Over-Sampling Method in Imbalanced Data Sets Learning},
	volume = {3644},
	isbn = {978-3-540-28226-6 978-3-540-31902-3},
	url = {http://link.springer.com/10.1007/11538059_91},
	shorttitle = {Borderline-{SMOTE}},
	abstract = {In recent years, mining with imbalanced data sets receives more and more attentions in both theoretical and practical aspects. This paper introduces the importance of imbalanced data sets and their broad application domains in data mining, and then summarizes the evaluation metrics and the existing methods to evaluate and solve the imbalance problem. Synthetic minority oversampling technique ({SMOTE}) is one of the over-sampling methods addressing this problem. Based on {SMOTE} method, this paper presents two new minority over-sampling methods, borderline-{SMOTE}1 and borderline-{SMOTE}2, in which only the minority examples near the borderline are over-sampled. For the minority class, experiments show that our approaches achieve better {TP} rate and F-value than {SMOTE} and random over-sampling methods.},
	pages = {878--887},
	booktitle = {Advances in Intelligent Computing},
	publisher = {Springer Berlin Heidelberg},
	author = {Han, Hui and Wang, Wen-Yuan and Mao, Bing-Huan},
	editor = {Huang, De-Shuang and Zhang, Xiao-Ping and Huang, Guang-Bin},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	urldate = {2023-05-17},
	date = {2005},
	langid = {english},
	doi = {10.1007/11538059_91},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Han et al. - 2005 - Borderline-SMOTE A New Over-Sampling Method in Im.pdf:/Users/brandonhosley/Zotero/storage/GC8Y9AKY/Han et al. - 2005 - Borderline-SMOTE A New Over-Sampling Method in Im.pdf:application/pdf},
}

@article{nguyen2011,
	title = {Borderline over-sampling for imbalanced data classification},
	volume = {3},
	issn = {1755-3210, 1755-3229},
	url = {http://www.inderscience.com/link.php?id=39875},
	doi = {10.1504/IJKESDP.2011.039875},
	abstract = {Traditional classiﬁcation algorithms, in many times, perform poorly on imbalanced data sets in which some classes are heavily outnumbered by the remaining classes. For this kind of data, minority class instances, which are usually much more of interest, are often misclassiﬁed. The paper proposes a method to deal with them by changing class distribution through oversampling at the borderline between the minority class and the majority class of the data set. A Support Vector Machines ({SVMs}) classiﬁer then is trained to predict new unknown instances. Compared to other over-sampling methods, the proposed method focuses only on the minority class instances lying around the borderline due to the fact that this area is most crucial for establishing the decision boundary. Furthermore, new instances will be generated in such a manner that minority class area will be expanded further toward the side of the majority class at the places where there appear few majority class instances. Experimental results show that the proposed method can achieve better performance than some other over-sampling methods, especially with data sets having low degree of overlap due to its ability of expanding minority class area in such cases.},
	pages = {4},
	number = {1},
	journaltitle = {International Journal of Knowledge Engineering and Soft Data Paradigms},
	shortjournal = {{IJKESDP}},
	author = {Nguyen, Hien M. and Cooper, Eric W. and Kamei, Katsuari},
	urldate = {2023-05-17},
	date = {2011},
	langid = {english},
	file = {Nguyen et al. - 2011 - Borderline over-sampling for imbalanced data class.pdf:/Users/brandonhosley/Zotero/storage/GCUF8XHX/Nguyen et al. - 2011 - Borderline over-sampling for imbalanced data class.pdf:application/pdf},
}
@misc{Cats-vs-Dogs, url={https://www.kaggle.com/datasets/shaunthesheep/microsoft-catsvsdogs-dataset}, abstractNote={image dataset for binary classification.}, language={en} }
@misc{Fashion MNIST, url={https://www.kaggle.com/datasets/zalando-research/fashionmnist}, abstractNote={An MNIST-like dataset of 70,000 28x28 labeled fashion images}, language={en} }
@misc{Music Genre Classification, url={https://www.kaggle.com/datasets/purumalgi/music-genre-classification}, abstractNote={Optimizing multi-class log loss to generalize well on unseen data}, language={en} }
@misc{Toy Dataset, url={https://www.kaggle.com/datasets/carlolepelaars/toy-dataset}, abstractNote={A dataset to play around with!}, language={en} }


