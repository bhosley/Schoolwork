\documentclass[]{article}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{framed}
\usepackage[hypcap=false]{caption}
\usepackage{subcaption}
\usepackage{forest}
\usepackage{multicol}
\graphicspath{ {./images/} }
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{hyperref}
\hypersetup{
	hidelinks,
	colorlinks = True,
	urlcolor = blue,
	citecolor = red
	}

\title{CSC 535: Deep Learning \\ Final Project: \\ }
\author{Brandon Hosley}
\date{\today}

\begin{document}
	\maketitle
	\clearpage
	
\begin{abstract}
	
\end{abstract}
	
\section{Introduction} 

\begin{wrapfigure}{r}{0.5\textwidth}
	\begin{center}
		\includegraphics[width=0.48\textwidth]{Oerke-fig1}
	\end{center}
	\caption{Oerke \emph{et al.}\cite{Oerke2006}}
\end{wrapfigure}
Whether they are weeds invading cultivated land or potentially harmful species growing in pasture, control of unwanted flora is a paramount issue within agriculture.
Weeds are shown to be the greatest factor in decreased crop yield when compared to pests, and disease
\cite{Oerke2006}\cite{Rao2000}\cite{Gianessi2007}.
Kraehmer and Baur \cite{WeedAnatomy2013}
suggest that global crop loss and weed control measures each cost in the range of billions of dollars.
Specifically, global costs are estimated at 
\$40 Billion \cite{Monaco2002} for crop loss and 
\$30 Billion \cite{Lawes2008} for weed prevention.

A multitude of mitigation methods are available; 
the primary methods are manual removal and application of herbicides.
The easier and more cost effective of the two is a general application of herbicide to cultivated land.
The industrial scale preference for general application of herbicides raises significant concerns regarding negative environmental or health effects \cite{Sopena2009};
at this time organic weed control is more costly and less effective \cite{Rood2002}.

Computer vision offers a means to improve both of these solutions.
Herbicides could be applied directly to the intended target rather than the entire field resulting in a dramatic decrease in the amount of herbicide used and introduced into the environment.
Examples of current and proposed organic methods include:
mechanical removal\cite{Chicouene2007}, 
mulching\cite{Riley2004},
solarization\cite{Sahile2005}\cite{Candido2011},
selective watering, 
hot water\cite{Pinel2000}, 
and lenses\cite{Johnson1989}.
Automated targeting applied to such methods would improve their effectiveness and 
may make them cost effective for industrial scale use.

\subsection{Problem}

To effectively apply computer vision to this task several smaller tasks must be accomplished. Common to any of the aforementioned weed control methods, plants must be located and distinguished. Location of plants comprises a particular kind of object detection task. Distinguishing plants is essentially a standard classification task. 

For many image classification tasks there are a substantial number of classes; for a plant classification model this may still be the case. However, a plant classifier applied to agricultural purposes may be given the advantage of merely distinguishing between desired and undesired plants. Within a monoculture application the model gets the potential further advantage of having to positively identify one type of plant. Additionally, within this application the effect of false negatives (crop labeled as weed) may act as a proxy for the seedling thinning process anyway.

\begin{wrapfigure}{l}{0.5\textwidth}
	\begin{center}
		\includegraphics[width=0.48\textwidth]{Mimosa_pudica_cotyledon}
	\end{center}
	\caption{\emph{M. Pudica} showing both cotyledon and first true-leaf.\cite{MPudicaPic}}
\end{wrapfigure}
A particularly challenging variable in plant recognition is classification at various stages of growth. 
The appearance of seedling features is often quite different from their mature appearances, especially when only cotyledon leaves are present before true leaves begin to sprout.
A model's usability will be greatly extended if it can distinguish plants throughout a grow season.
There is a significant advantage gained by earlier recognition of weeds.
If recognized before reproductive maturity they can be prevented from local-source propagation, greatly reducing the population growth of the species within the area of interest.
Further benefit is gained by identifying weeds even earlier as it prevents the competition for resources that they will otherwise provide.

Any area of cultivation is one of interest. 
Often the cultivated fields most threatened by unwanted species are those furthest from large scale human development, far from robust infrastructure. 
With this potential limitation in mind and for the environmental benefit of reduced power consumption a smaller more efficient model will be preferred. 

For this project we seek to determine if a transfer model may be trained such that 
it has the flexibility to be applied to different types of fields and cultures, 
has the ability to distinguish species as early in the growing process as possible, and the model is small and efficient enough to be deployed to edge or low-powered computing devices.

\subsection{Related Work: Datasets}

Numerous options for plant image datasets are available, most with a specific type of learning task in mind.

Sudars et al. \cite{Sudars2020} 
produced an image dataset of 8 weed species and 6 crop species. 
The images in the dataset are taken from a wider angle and bear an appearance similar to what a camera would capture in an uncontrolled environment. 
Many of the pictures contain multiple plants and offer a great opportunity to test object detection as well as recognition. 
Although the authors specified which crop and weed species were examined in their write-up, the data presented only annotates each plant as a weed or crop. 
Without the ability to distinguish between different types of crops the data as presented is insufficient to train monoculture specific models.

\begin{wrapfigure}{r}{0.4\textwidth}
	\begin{center}
		\includegraphics[width=0.48\textwidth]{Hughes2016Example}
	\end{center}
	\caption{Examples of images from Hughes \emph{et al.}\cite{Hughes2016}}
\end{wrapfigure}
Hughes et al. \cite{Hughes2016}
provide a dataset of plant images in which the species is annotated, and a large variety is given to each species. 
The dataset also includes health status and disease diagnosis of the plants included.
The intention of their work is to increase the availability of diagnostic tools to small scale or independent farmers.
The disease focus of this dataset would contribute a good diversification of samples for classification purposes.
The limitation of this dataset is that all images are single leaves placed upon a gray background.
Conveniently this data is also packaged as an option in Tensorflow datasets as Plant Village.

Giselsson et al. \cite{Giselsson2017}
provide the data set used in this project.
The most important factors that went into choosing this dataset were the researchers labeling each example by species of plant and that the examples provided are plants in various stages of growth. 
The background of the images was the growing medium which may work as an approximation of the in situ growing medium.

\subsection{Related Work: Classification}

The Kaggle notebooks of user
\href{https://www.kaggle.com/gaborfodor/seedlings-pretrained-keras-models/data
}{Gaborfodor} provided the methodology for employing the Xception model used in this project.
The notebook did not cite any sources, but appeared to use an implementation of transductive transfer learning as described in Paul, Rottenseiner, and Heipke \cite{Paul2015}.

Huixian\cite{Huixian2020} provides an effective method for classification of plants based on support vector machines. However, their method relies completely on leaf profile, as a preprocessing mask removes all other details. This may have a negative effect on identifying plants based on cotyledons.

\section{Analysis}

As stated above, the dataset chosen is one provided by Giselsson \emph{et al.}\cite{Giselsson2017} and is available from the Aarhus University 
\href{https://vision.eng.au.dk/plant-seedlings-dataset/}{website}.
The test environment was \href{https://colab.research.google.com/}{Google's Colab.}

\subsection{Data Exploration}

\begin{wrapfigure}{l}{0.5\textwidth}
	\begin{center}
		\includegraphics[width=0.45\textwidth]{DataDistribution}
	\end{center}
	\caption{Data distribution by species.}	
	\begin{center}
		\includegraphics[width=0.45\textwidth]{SampleDistribution}
	\end{center}
	\caption{Distribution of samples in each dataset.}
\end{wrapfigure}

The dataset contains images of over 960 unique plants belonging to 12 species at several growth stages. 
The images are RBG with a resolution of approximatly 10 pixels per mm.
The images are annotated by species.
The database was recorded at Aarhus University's Flakkebjerg Research station in  collaboration with the University of Southern Denmark.

Upon initial examination the distribution of data between different species is highly uneven.
To resolve this problem SMOTE\cite{Bowyer2011} or ADASYN\cite{Bai2008}  was going to be applied to oversample the data.
The additional data caused the kernel to run out of memory and repeatedly crash.

The 5539 sample size alone was large enough to repeatedly crash the environment.
Successive attempts to pre-process a smaller data set eventually led to the final volume of 3000 samples. 
This number pulls 250 samples from each category, less than the minimum present in any one category. 

Variance was reintroduced via imbalance between the training, validation, and testing sets.

Examination of a random selection of images from each classification does not show any immediately obvious differences. Background interference and measuring implements do not appear to be unique or consistent in any one class.
\begin{figure}[H]
	\includegraphics[width=\linewidth]{ImageGrid}
	\caption{Example images pulled from each category.}
\end{figure}

\clearpage

\subsection{Model Exploration}

All of the candidate models investigated for transfer learning here come with initial weights based on the 'Imagenet' dataset.

\subsubsection{Xception}

\begin{wrapfigure}{r}{0.5\textwidth}
	\vspace{-6em}
	\begin{center}
		\includegraphics[width=0.49\textwidth]{XceptionConfusionMatrixTest}
	\end{center}
	\caption{Xception model with logistic regression.}	
\end{wrapfigure}

The Xception model is currently the highest performing model available in the Keras application library\cite{Chollet2016}.
A base model of the Xception network was used to classify the plant images. 
The resulting classification to the original 1000 categories of ImageNet were used to fit a logistic regression model providing.
The logistic model was fit using L-BFGS-B â€“ Software for Large-scale Bound-constrained Optimization\cite{Zhu1997}.

This model is achieved very fast with little training and it is quite effective.
This will be the baseline by which we compare the next two models.

\subsubsection{MobileNetv2}

MobileNet\cite{Howard2017} and MobileNetv2\cite{Sandler2019} 
are a set of models intended for mobile applications.
They are designed to be efficient and light-weight.
This makes MobileNetv2, as the more refined implementation,
a perfect candidate for our application.  

First the same logistic regression transductive learning technique is applied.
This results in about a 40\% testing Accuracy.

After adding 4 densely connected layers, two with 50\% droupout and training for 30 epochs, the model performs much better. Providing around 85\% accuracy. Despite numerous adjustments we were not able to get much better performance without significant modification.

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{MobileWithRegressionValidation}
		\caption{MobileNetv2 with logistic regression.}	
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{MobileNetV2SlowConfusion}
		\caption{MobileNetv2 with additional layers.}	
	\end{subfigure}
\end{figure}

\subsubsection{NASNetMobile}

\begin{wrapfigure}{r}{0.4\textwidth}
	\begin{center}
		\includegraphics[width=0.5\textwidth]{NasNetMobileConfusionMatrix}
	\end{center}
	\caption{NASNetMobile with additional layers.}	
\end{wrapfigure}

NASNet is a family of pre-trained models that come trained on ImageNet but are intended for transfer learning.
NAS (Neural Architecture Search) refers to the model's creators searching for the highest performing cell on one dataset and building a model based on reproductions of that cell\cite{Zoph2018}. 
One implementation called NASNetMobile, is selected for its light weight.

The implementation of NASNet presented here used two densely connected layers with 50\% dropout and was trained for 10 epochs. It gave a much better performance of 94\% accuracy on the test set. 

\section{Conclusion}

The NASNetMobile based model works fairly well, outperforming the MobileNet model significantly.
The NASNetMobile model may provide a very effective base for transfer learning when developing a classifier for mobile agricultural solutions.

The Giselsson \emph{et al.}\cite{Giselsson2017} dataset provides a good idea of what kind of images should be gathered when providing a locality specific dataset. 


\clearpage
\bibliographystyle{IEEEtran}
\bibliography{\jobname}
\end{document}

\begin{wrapfigure}{r}{0.5\textwidth}
	\begin{center}
		\includegraphics[width=0.45\textwidth]{}
	\end{center}
	\caption{}	
\end{wrapfigure}