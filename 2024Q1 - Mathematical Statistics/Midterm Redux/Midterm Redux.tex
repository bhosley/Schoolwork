\documentclass[12pt,letterpaper]{exam}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[width=8.50in, height=11.00in, left=0.50in, right=0.50in, top=0.50in, bottom=0.50in]{geometry}

\usepackage{libertine}
\usepackage{multicol}
\usepackage[shortlabels]{enumitem}

\usepackage{booktabs}
\usepackage[table]{xcolor}

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bbm}

\usepackage{hyperref}
\usepackage{graphicx}
%\usepackage{wrapfig}
%\usepackage{capt-of}
%\usepackage{tikz}
%\usepackage{pgfplots}
%\usetikzlibrary{shapes,arrows,positioning,patterns}
%\usepackage{pythonhighlight}

\newcommand\chapter{ 2 }
%\renewcommand{\thequestion}{\textbf{\chapter.\arabic{question}}}
%\renewcommand{\questionlabel}{\thequestion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\class}{  } % This is the name of the course 
\newcommand{\assignmentname}{Midterm (Quiz \# \chapter) Redux} % 
\newcommand{\authorname}{Hosley, Brandon} % 
\newcommand{\workdate}{\today} % 
\printanswers % this includes the solutions sections
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}
\pagestyle{plain}
\thispagestyle{empty}
\noindent
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
\begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} r @{\extracolsep{10pt}} l}
	\textbf{\class} & \textbf{\authorname}  &\\ %Your name here instead, obviously 
	\textbf{\assignmentname } & \textbf{\workdate} & \\
\end{tabular*}\\ 
\rule{\textwidth}{2pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% HEADER %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{questions}
	\question[20] 
	Let \(X_1,X_2,\ldots,X_n\) be a random sample from a \(GAM(\alpha,\beta)\) with \(\alpha\) known.
	Show \(\hat{\beta}=\frac{\bar{x}}{\alpha}\) is UMVUE for \(\beta\).
	\begin{solution}
		For \(GAM(\alpha,\beta)\) we know that \(E[X] = \alpha\beta\), and thus,
		\begin{align*}
			\hat{\beta} 
			= \frac{E[X]}{\alpha}
			= \frac{1}{n\alpha} \sum_{i=1}^{n} x_i
			= \frac{\bar{x}}{\alpha}
		\end{align*}
		This shows that \(\frac{\bar{x}}{\alpha}\) is unbiased as an estimator for \(\hat{\beta}\).
		% Does this answer the assumptions of the Cramer Rao Theorem?
		Next, we can verify that this is UMVUE if it meets the Cram\'{e}r-Rao lower bound.
		As it happens, it is easier to use the attainment theorem. First calculate the likelihood function,
		\begin{align*}
			\mathcal{L}(\theta=\beta|x)
			= \prod_{i=1}^{n} \frac{1}{\Gamma(\alpha)\beta^\alpha}x_i^{\alpha-1}e^{-x_i/\beta}
			=  \frac{1}{\Gamma(\alpha)^n\beta^{n\alpha}}  \prod_{i=1}^{n}  x_i^{\alpha-1}e^{-x_i/\beta}.
		\end{align*}
		Then the log-likelihood function,
		\begin{align*}
			\ell(\beta|x) = \ln \mathcal{L}(\beta|x)
			&= - n\ln\Gamma(\alpha) - n\alpha\ln(\beta) + \sum_{i=1}^{n} (\alpha-1)\ln x_i -\frac{x_i}{\beta}   \\
			&= - n\ln\Gamma(\alpha) - n\alpha\ln(\beta) - \frac{n\bar{x}}{\beta} + (\alpha-1)\sum_{i=1}^{n} \ln x_i  \\
			&= - n\ln\Gamma(\alpha) - n\alpha\ln(\beta) - \frac{n\bar{x}}{\beta} + (\alpha-1)\ln n\bar{x}.
		\end{align*}
		Then take the derivative, and rearrange,
		\begin{align*}
			\frac{\partial}{\partial\beta} \ell(\beta|x)
			= \frac{n\alpha}{\beta} - \frac{n\bar{x}}{\beta^2}
			= \frac{n}{\beta^2}\left[-\bar{x} + \alpha\beta\right]
			= \frac{n\alpha}{\beta^2}\left[-\frac{\bar{x}}{\alpha} + \beta\right].
		\end{align*}
		Finally, from this we get that \(-\frac{\bar{x}}{\alpha}\) is the UMVUE for \(-\beta\), 
		equivalently the result to the result above.
		
		Thus we conclude that \(\frac{\bar{x}}{\alpha}\) is UMVUE for \(\hat{\beta}\).
		
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\clearpage
	
	\question[20] 
	Let \(X_1,X_2,\ldots,X_n\) be a random sample from the continuous distribution with pdf 
	\( f(x|\theta) = \frac{2\theta^2}{x^3} \mathbbm{1}_{(\theta,\infty)}(x) \) where 
	\( 0 < \theta < \infty \).
	\begin{parts}
		\part
		Show that \(X_{(1)}\) is sufficient for \(\theta\).
		\part
		When \(0<\theta<\infty\), the probability distribution of \(X_{(1)}\) is complete.
		In this case, find the best unbiased estimator of \(\theta\).
	\end{parts}
	\begin{solution}
		\begin{parts}
			\part
			First we reorganize the pdf,
			\begin{align*}
				f(x|\theta)
				= \frac{2\theta^2}{x^3} \mathbbm{1}_{(\theta,\infty)}(x) 
				= \frac{2\theta^2}{x^3} \mathbbm{1}_{(\theta,\infty)}(x_{(1)}) 
				= \frac{2\theta^2}{x^3} \mathbbm{1}_{(0,x_{(1)})}(\theta) 
				= \underbrace{2\theta^2\mathbbm{1}_{(0,x_{(1)})}(\theta)}_{g(T(x)|\theta)}  \underbrace{\frac{1}{x^3}}_{h(x)}
			\end{align*}
			Using the factorization theorem we can identify a \(g(T(x)|\theta)\) only requiring \(T(X) = X_{(1)}\)
			and conclude that \(X_{(1)}\) is a sufficient statistic.
			\part
			
		\end{parts}
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		

\end{questions}
\end{document}
