\documentclass[12pt,letterpaper]{exam}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{alphabeta}
\usepackage[width=8.50in, height=11.00in, left=0.50in, right=0.50in, top=0.50in, bottom=0.50in]{geometry}

\usepackage{libertine}
\usepackage{multicol}
\usepackage[shortlabels]{enumitem}

\usepackage{booktabs}
\usepackage[table]{xcolor}

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bbm}

\usepackage{hyperref}
\usepackage{graphicx}
%\usepackage{wrapfig}
%\usepackage{capt-of}
%\usepackage{tikz}
%\usepackage{pgfplots}
%\usetikzlibrary{shapes,arrows,positioning,patterns}
%\usepackage{pythonhighlight}

\newcommand\chapter{8}
\renewcommand{\thequestion}{\textbf{\chapter.\arabic{question}}}
\renewcommand{\questionlabel}{\thequestion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\class}{  } % This is the name of the course 
\newcommand{\assignmentname}{Homework \# \chapter} % 
\newcommand{\authorname}{Hosley, Brandon} % 
\newcommand{\workdate}{\today} % 
\printanswers % this includes the solutions sections
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\pagestyle{plain}
\thispagestyle{empty}
\noindent

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
\begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} r @{\extracolsep{10pt}} l}
	\textbf{\class} & \textbf{\authorname}  &\\ %Your name here instead, obviously
	\textbf{\assignmentname } & \textbf{\workdate} & \\
\end{tabular*}\\
\rule{\textwidth}{2pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% HEADER %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{questions}

	\question 
	In 1,000 tosses of a coin, 560 heads and 440 tails appear. 
	Is it reasonable to assume that the coin is fair? Justify your answer.
	
	\begin{solution}
		If the coin is 'fair' we typically assume that the \(p=1-p\) and therefore \(p=0.5\).
		Further, we expect \(X\sim \text{bin}(1000,0.5)\), and
		
		\( E[X] = np = 500 \),
		
		\( Var[X] = np(1-p) = 250\).
		
		Then, noting that as \(n\rightarrow\infty\) the binomial approaches normal distribution.
		We can utilize the normal distribution to approximate the probability of this coin's result.
		\begin{align*}
			z &= \frac{x-\mu}{\sigma} \\
			&= \frac{60}{\sqrt{250}} \\
			&= 3.7947,
		\intertext{thus,}
			P(Z\geq z)  &= P(Z\geq 3.7947) .\\
		\end{align*}
		With this value we can calculate an approximate probability;
		of the first 4 tables that I consulted, the highest \(Z\) score was \(3.69\)
		which returns a value of \(0.0001\).
		Thus, even without an exact answer we can determine that it is highly unlikely that this
		particular coin is fair (\(p=0.5\)).
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\setcounter{question}{3-1}
	
	\question 
	Here, the LRT alluded to in Example 8.2.9 will be derived. 
	Suppose that we observe \(m\) iid Bernoulli(\(\theta\)) random variables, denoted \(Y_1,\ldots,Y_m\). 
	Show that the LRT of \(H_0: \theta \leq \theta_0\) versus \(H_1:\theta > \theta_0\) will reject \(H_0\) if \(\sum_{i=1}^{m} Y_i > b\).
	
	\begin{solution}
		Equivalently
		\(\lambda(y)<c\)
		\[\lambda(y) = \frac{\sup_{\theta<\theta_0}L(\theta|y)}{\sup_\Theta L(\theta|y)}\]
		
		Let \(y = \sum_{i=1}^{m} Y_i\). Then, we know that \(y\sim\text{Binom}(\theta,m)\)
		which can help us shortcut to \(\hat{p} = \frac{y}{m}\).
		
		We can find this the long way by
		\begin{align*}
			L(p|y) &= \prod_{i=1}^{m} p^{y_i} (1-p)^{1-y_i} \\
			&= p^{m\bar{y}} (1-p)^{m(1-\bar{y})}
		\intertext{take log likelihood to ease the solving process,}
			\ell(p|y) &= m\bar{y} \ln(p) + m(1-\bar{y})\ln(1-p) 
		\intertext{then we set the derivative of this equal to 0,}
			0 &=: \frac{m\bar{y}}{p} - \frac{m(1-\bar{y})}{1-p}
		\intertext{equivalently,}
			pm(1-\bar{y}) &= (1-p)m\bar{y} \\
			p-\bar{y}p &= \bar{y}-\bar{y}p \\
			p &= \bar{y} \\
			p &= \frac{\sum_{i=1}^my_i}{m} = \frac{y}{m} .
		\end{align*}
		This is the \(\sup_\Theta L(\theta|y)\). The \(\sup_{\theta<\theta_0}L(\theta|y)\) is \(\min(\theta_0,\frac{y}{m})\).
		
		With this we can revisit \(\lambda\)
		\begin{align*}
			\lambda(y) 
			&= \frac{\sup_{\theta<\theta_0}L(\theta|y)}{\sup_\Theta L(\theta|y)} \\
			&= \begin{cases}
				\frac{\theta_0^y(1-\theta_0)^{m-y}}{(y/m)^y(1-y/m)^{m-y}} & \frac{y}{m} > \theta_0 \\
				1 & \frac{y}{m} \leq \theta_0
			\end{cases}
		\end{align*}
		Next, we would like to determine what the behavior of this function is, which will be easier (as above) in log,
		\begin{align*}
			\frac{d}{dy}\ln\lambda(y)
			&= \frac{d}{dy} y\ln\theta_0 + (m-y)\ln(1-\theta_0) - y\ln\frac{y}{m} - (m-y)\ln(1-\frac{y}{m}) \\
			&= \ln\theta_0 - \ln(1-\theta_0) - \frac{1}{y} \,y + \ln(1-y/m) + (m-y)\frac{1}{m-y} \\
			&= \ln\left( \frac{\theta_0}{1-\theta_0}(1-y/m) \right)
		\end{align*}
		by inspection of the final expression we can see that \(\lambda\) is a decreasing function in \(y\)
		and thus \(\lambda(y)<c \). 
		
		For greater context, one may need to review example 8.2.9; but this implies that for \(\lambda(y)<c \)
		there exists a \(b\) for which \(\sum_{i=1}^{m} Y_i > b\), and is therefor an equivalent rejection region.
		
	\end{solution}
	\clearpage
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\setcounter{question}{12-1}
	
	\question 
	For sample size \(n = 1, 4, 16, 64, 100\) from a normal population with mean \(\mu\) and known variance \(\sigma^2\), 
	plot the power function following the LRTs. Take \(\alpha = 0.05\)
	\begin{parts}
		\part \(H_0: \mu \le 0\) versus \(H_1:\mu > 0\)
		\part \(H_0: \mu = 0\) versus \(H_1:\mu \ne 0\)
	\end{parts}
	
	\begin{solution}
		\begin{parts}
			\part
			Borrowing the normal power function from Casella and Berger example 8.3.3
			\begin{align*}
				\beta(\theta)
				&= P\left(Z>c-\frac{\theta_0-\theta}{\sigma/\sqrt{n}}\right) \\
				&= P\left(\frac{\bar{X}-\theta}{\sigma/\sqrt{n}}>1.645-\frac{\theta_0-\theta}{\sigma}\sqrt{n}\right)
			\end{align*}
				I have approximated the power functions in Desmos, requiring some liberties on the axes, 
				but hopefully showing the appropriate changes as \(n\) increases.
			\begin{center}
				\includegraphics[width=0.7\linewidth]{"Screenshot 2024-02-21 at 18.50.40"}
			\end{center}
			
			\part
			Borrowing the normal power function from Casella and Berger example 8.3.3
			\begin{align*}
				\beta(\theta)
				&= P\left(Z<\left| c+\frac{\theta_0-\theta}{\sigma/\sqrt{n}} \right|\right) \\
				&= P\left(Z<\left| 1.96+\frac{\theta_0-\theta}{\sigma/\sqrt{n}} \right|\right)
			\end{align*}
			
		\end{parts}
		
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	%\setcounter{question}{14-1}
	%\question For a random sample $X_1, ..., X_n$ of Bernoulli($p$) variables, it is desired to test $$H_0: p = .49 \quad\text{versus}\quad H_1: p = .51$$
	%Use the Central Limit Theorem ot determine, approximately, the sample size needed so that the two probability of error are both about .01. Use a test function that rejects $H_0$ if $\sum_{i=1}^{n} X_i$ is large.
	%\begin{solution}
	%	Here is the next solution
	%\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\clearpage
	\setcounter{question}{15-1}
	
	\question 
	Show that for a random sample $X_1, ..., X_n$ from a $N(0,\sigma^2)$ population, 
	the most powerful test of $H_0: \sigma = \sigma_0$ versus $H_1: \sigma = \sigma_1$, 
	where $\sigma_0 < \sigma_1$, is given by
	\begin{align*}
		\phi\left(\Sigma X_i^2\right) =
		\begin{cases}
			1 &, \Sigma X_i^2 > c \\
			0 &, \Sigma X_i^2 \le c.
		\end{cases}
	\end{align*}
	For a given value of $\alpha$, the size of the Type I Error, show how the value of $c$ is explicitly determined.

	\begin{solution}
		First, using Neyman-Pearson;
		\begin{align*}
			\lambda(x)
			&= \frac{\mathcal{L}(\sigma_1|x)}{\mathcal{L}(\sigma_1|x)}
			= \frac{(2\pi\sigma_1^2)^{-n/2}e^{\sum_{i=1}^{n}x_i^2/(2\sigma_1^2)}}
				{(2\pi\sigma_0^2)^{-n/2}e^{\sum_{i=1}^{n}x_i^2/(2\sigma_0^2)}}
			= \frac{\sigma_0^n}{\sigma_1^n} \exp\left\{\frac{1}{2}\left(\frac{1}{\sigma_0^2}-\frac{1}{\sigma_1^2}\right) \sum_{i=1}^nx_i^2 \right\}.
		\end{align*}
		Then for some \(k\),
		\begin{align*}
			k &< \frac{\sigma_0^n}{\sigma_1^n} \exp\left\{\frac{1}{2}\left(\frac{1}{\sigma_0^2}-\frac{1}{\sigma_1^2}\right) \sum_{i=1}^nx_i^2 \right\} \\
			k\frac{\sigma_1^n}{\sigma_0^n} &< \exp\left\{\frac{1}{2}\left(\frac{1}{\sigma_0^2}-\frac{1}{\sigma_1^2}\right) \sum_{i=1}^nx_i^2 \right\} \\
			\ln(k\sigma_1^n/\sigma_0^n) &< \frac{1}{2}\left(\frac{1}{\sigma_0^2}-\frac{1}{\sigma_1^2}\right)\sum_{i=1}^nx_i^2 \\
			2\ln(k\sigma_1^n/\sigma_0^n) &< \left(\frac{1}{\sigma_0^2}-\frac{1}{\sigma_1^2}\right)\sum_{i=1}^nx_i^2 \\
			\frac{2\ln(k\sigma_1^n/\sigma_0^n)}{\left(\frac{1}{\sigma_0^2}-\frac{1}{\sigma_1^2}\right)} &< \sum_{i=1}^nx_i^2. \\
		\end{align*}
		Thus,
		\begin{align*}
			\alpha = P\left(\sum_{i=1}^nx_i^2 > c\right)
		\intertext{then, if we standard normal the normal distribution we can leverage a \(\chi^2\) distribution,}
			\alpha &= P\left(\sum_{i=1}^n\frac{x_i}{\sigma_0^2} > \frac{c}{\sigma_0^2}\right) \\
			&= P\left(\chi_n^2 > \frac{c}{\sigma_0^2}\right).
		\end{align*}
		Thus the \(c\) corresponds to the value of \(\sigma_0^2\chi_n^2\).
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\clearpage
	\setcounter{question}{19-1}
	
	\question 
	The random variable $X$ has pdf $f(x) = e^{-x}, x> 0$. 
	One observation is obtained on the random variable $Y = X^\theta$, 
	and a test of $H_0: \theta = 1$ versus $H_1: \theta = 2$ needs to be constructed. 
	Find the UMP level $\alpha = .10$ test and compute the Type II Error probability.

	\begin{solution}
		First, we perform a transform of variable
		\begin{align*}
			f(y) 
			= f(g^{-1}(x)) 
			= e^{-y^{-1/\theta}}\,\frac{d}{dy} e^{-y^{-1/\theta}}
			= -\frac{1}{\theta}y^{1/\theta-1} e^{-y^{-1/\theta}}
		\end{align*}
		
		Second, we determine the direction of MLR
		\begin{align*}
			\frac{f(y|H_1)}{f(y|H_0)}
			= \frac{-\frac{1}{2}y^{-1/2}e^{y^{1/2}}}{-1(1)e^{-y}} 
			= \frac{1}{2}y^{-1/2}e^{y-y^{1/2}}.
		\end{align*}
		Graphing the function shows us that the function is decreasing on \(y<1\) and increasing on \(y>1\).
		\begin{center}
			\includegraphics[width=0.5\linewidth]{"Screenshot 2024-02-21 at 13.06.55"}
		\end{center}
		This implies two rejection regions.

		
		Third, we set the two rejection regions equal to each other and to sum to \(\alpha\).
		\(y\leq c_0\) and \(y\geq c_1\)
		\begin{align*}
			\beta(\theta)
			&= P(y\leq c_0) + P(y\geq c_1)
			= \alpha
			= 0.10
		\intertext{or equivalently,}
			\frac{\alpha}{2}
			= 0.05
			&= P(y\leq c_0)
			= P(y\geq c_1) \\
			&= \int_{0}^{c_0} e^{-y} \,dy
			= 1 - \int_{0}^{c_1} e^{-y} \,dy.
		\end{align*}
		Given to Wolfram's solver we get,
		\(c_0 \approx 0.512933\) and 
		\(c_1 \approx 2.99573\).
		
		Finally, we can calculate the Type II error by integrating over the acceptance region.
		\begin{align*}
			\int_{c_0}^{c_1} f(y|H_1)
			&= \int_{c_0}^{c_1} \frac{1}{2}y^{-1/2}e^{-y^{1/2}} \,dy
		\intertext{once again leveraging a solver,}
			&\approx 0.620196.
		\end{align*}
		
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\clearpage
	\setcounter{question}{20-1}
	
	\question 
	Let $X$ be a random variable whose pmf under $H_0$ and $H_1$ is given by
	$$
	\begin{array}{cccccccc}
		x & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\ \hline
		f(x|H_0) & .01 & .01 & .01 & .01 & .01 & .01 & .94 \\
		f(x|H_1) & .06 & .05 & .04 & .03 & .02 & .01 & .79 \\
	\end{array}
	$$
	Use the Neyman-Pearson Lemma to find the most powerful test for $H_0$ versus $H_1$ with size $\alpha = .04$. 
	Compute the probability of Type II Error for this test.

	\begin{solution}
		First we calculate the likelihood ratios between the two hypotheses,
		\begin{align*}
			\begin{array}{cccccccc}
				x & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\ \hline 
				\frac{f(x|H_1)}{f(x|H_0)} & 6 & 5 & 4 & 3 & 2 & 1 & .84 \\
			\end{array}
		\end{align*}
		From this we can observe that \(f(x)\) is decreasing on \(x\), 
		and thus we are looking for a \(c\) such that
		\begin{align*}
			P(X\leq c|H_0) &= \alpha \\
			\sum_{i=1}^{c} f(x|H_0) &= \alpha \\
		\end{align*}
		which can be brute-forced to determine that \(c=4\).
		
		Then the probability of a Type II error is
		\begin{align*}
			\sum_{i=c+1}^{n} f(x|H_1) = 0.82 \,.
		\end{align*}
	
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\setcounter{question}{23-1}
	
	\question 
	Suppose $X$ is one observation from a population with beta($\theta$, 1) pdf.
	\begin{parts}
		\part For testing $H_0: \theta \le 1$ versus $H_1: \theta > 1$, 
			find the size and sketch the power function of the test that rejects $H_0$ if $X > \frac{1}{2}$.
		\part Find the most powerful level $\alpha$ test for $H_0: \theta =1$ versus $H_1: \theta = 2$.
		\part Is there a UMP test of $H_0: \theta \le 1$ versus $H_1: \theta > 1$? If so, find it. If not, prove so.
	\end{parts}
	
	\begin{solution}
		\begin{parts}
			\part \phantom{} \vspace{-3em}
			\begin{align*}
				\beta(\theta)
				= P_\theta(X>\frac{1}{2})
				= \int_{1/2}^{1} \frac{\Gamma(\theta+1)}{\Gamma(\theta)\Gamma(1)} x^{\theta-1} (1-x)^{1-1} \,dx
				= \int_{1/2}^{1} \theta x^{\theta-1} \,dx
				= \left. x^\theta \right|_{1/2}^{1} 
				= 1 - \frac{1}{2^\theta}
			\end{align*}
			\begin{center}
				\includegraphics[width=0.4\linewidth]{"Screenshot 2024-02-21 at 23.23.38"}
			\end{center}
			As can be seen here this function is increasing, on \(\theta\) 
			as a result, the \(\sup_{\theta<1}\beta(\theta) = 0.5\).
			
			\part
			Utilizing the simplified pdf for beta\((\theta,1)\) from above;
			\begin{align*}
				\lambda(x)
				= \frac{f(x|x=2)}{f(x|\theta=1)}
				= \frac{2x^{2-1}}{1x^{1-1}}
				= 2x
				\Rightarrow x>\frac{k}{2}
			\end{align*}
			Then we can evaluate the \(\alpha\),
			\begin{align*}
				\alpha 
				= P(X>\frac{k}{2})
				= 1-\left(\frac{k}{2}\right)^{\theta_0}
				= 1-\frac{k}{2}.
			\end{align*}
			
			\part
			For the UMP we can first note that beta\((\theta,1)\)
			is a member of the exponential family with and exponentiation component of
			\(ln(x)(\theta-1)\).
			The \(\theta\) component is non decreasing, 
			thus we have a monotone, non-decreasing function and by extension, an MLR.
			
			Using the Karlin-Rubin theorem this allows us to conclude that the UMP
			can be calculated using \(X > \frac{k}{2}\)
			where \(\alpha = 1-\frac{k}{2}\)
			and \(\frac{k}{2} = 1-\alpha\);
			thus the UMP for a size \(\alpha\) test is \(X>1-\alpha\).
			
		\end{parts}
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\setcounter{question}{28-1}
	
	\question 
	Let $f(x|\theta)$ be the logistic location pdf
	$$
	f(x|\theta)
	= \frac{ e^{(x-\theta)} }{ (1 + e^{(x-\theta)})^2 }, \quad -\infty < x < \infty, \quad -\infty < \theta < \infty
	$$
	\begin{parts}
		\part Show that this familiy has an MLR.
		\part Based on one observation, $X$, find the most powerful size $\alpha$ test of $H_0: \theta = 0$ versus $H_1: \theta =1$. 
			For $\alpha$ = .2, find the size of the Type II Error.
		\part  Show that the test in part (b) is UMP size $\alpha$ for testing $H_0: \theta \le 0$ versus $H_1: \theta > 0$. What can be said about UMP tests in general for the logistic location family?
	\end{parts}
	
	\begin{solution}
		\begin{parts}
			\part
			
			\begin{align*}
				\frac{f(x|\theta_2)}{f(x|\theta_1)}
				= \frac{ \frac{e^{(x-\theta_2)}}{(1+e^{(x-\theta_2)})^2} }{ \frac{e^{(x-\theta_1)}}{(1+e^{(x-\theta_1)})^2} }
				= e^{\theta_1-\theta_2}  \frac{(1+e^{x-\theta_1})^2}{(1+e^{x-\theta_2})^2} 
			\end{align*}
			Then, to evaluate the behavior of the function in relation to \(x\)
			we can examine,
			\begin{align*}
				\frac{d}{dx}\frac{1+e^{x-\theta_1}}{1+e^{x-\theta_1}}
				= \frac{e^{x-\theta_1}-e^{x-\theta_2}}{(e^{x-\theta_2})^2}
			\end{align*}
			assuming fixed \(\theta\) such that \(\theta_2>\theta_1\)
			we can see that the function is increasing on \(x\)
			and conclude that the family has MLR.
			
			\part
			Using the CDF in lieu of the integral (as that would be significantly more difficult),
			\begin{align*}
				\alpha &=  1- \frac{1}{1+e^c} \\
				0.2 &=  1- \frac{1}{1+e^c} \\
				c &\approx 1.386 
			\end{align*}
			Then evaluating the CDF over \(H_1\) using this \(c\) we get
			\[\frac{e^{0.386}}{1+e^{0.386}} \approx 0.59532\]
			
			\part
			The above satisfies the assumptions of the Karlin-Rubin Theorem
			and thus the test is UMP for the corresponding size.
			
		\end{parts}
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\setcounter{question}{37-1}
	
	\question 
	Let $X_1, ..., X_n$ be a random sample from a n($\theta, \sigma^2$) population. 
	Consider testing $$H_0: \theta \le \theta_0 \quad \text{versus} \quad H_1: \theta > \theta_0.$$
	\begin{parts}
		\part If $\sigma^2$ is known, show that the test that rejects $H_0$ when
		$$\bar{X} > \theta_0 + z_\alpha \sqrt{\sigma^2/n}$$
		is a test of size $\alpha$. Show that the test can be derived as an LRT.
		\part Show that the test in part (a) is a UMP test
		\part If $\sigma^2$ is unknown, show that the test that rejects $H_0$ when
		$$\bar{X} > \theta_0 + t_{n-1,\alpha} \sqrt{S^2/n}$$ is a test of size $\alpha$. Show that the test can be derived as an LRT.
	\end{parts}
	
	\begin{solution}
		\begin{parts}
			\part
			First, we construct an LRT,
			\begin{align*}
				\lambda(x)
				&= \frac{
					(2\pi\sigma^2)^{-n/2} e^{\sum_{i=1}^n (x_i-\theta_0)^2/2\sigma^2}
					}{
					(2\pi\sigma^2)^{-n/2} e^{\sum_{i=1}^n (x_i-\bar{x})^2/2\sigma^2}
					} \\
				&= \exp\left\{ \sum_{i=1}^n (x_i-\theta_0)^2/2\sigma^2 - \sum_{i=1}^n (x_i-\bar{x})^2/2\sigma^2 \right\} \\
				&= \exp\left\{ \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i-\theta_0)^2 - (x_i-\bar{x})^2 \right\} \\
				&= \exp\left\{ \frac{1}{2\sigma^2} \sum_{i=1}^n x_i^2-2\theta_0x_i+\theta_0^2 -x_i^2 + 2\bar{x}x_i -\bar{x}^2 \right\} \\
				&= \exp\left\{ \frac{n}{2\sigma^2} \left( \theta_0^2 - 2\theta_0\bar{x} + 2\bar{x}^2 - \bar{x}^2 \right) \right\} \\
				&= \exp\left\{ \frac{n}{2\sigma^2} \left( \theta_0^2 - 2\theta_0\bar{x} + \bar{x}^2 \right) \right\} \\
				&= \exp\left\{ \frac{n}{2\sigma^2} \left( \theta_0 - \bar{x} \right)^2 \right\}
				:< c
			\end{align*}
			for some \(c\).
			Solving for \(\bar{x}\),
			\begin{align*}
				\exp\left\{ \frac{n}{2\sigma^2} \left( \theta_0 - \bar{x} \right)^2 \right\} &< c \\
				\frac{n}{2\sigma^2} \left( \theta_0 - \bar{x} \right)^2 &< \ln(c) \\
				\left( \theta_0 - \bar{x} \right)^2 &< \frac{2\sigma^2\ln(c)}{n} \\
				\theta_0 - \bar{x} &< \sqrt{\frac{2\sigma^2\ln(c)}{n}} \\
				 \bar{x} &> \theta_0 - \sqrt{\frac{2\sigma^2\ln(c)}{n}} .
			\end{align*}
			Then,
			\begin{align*}
				\alpha
				&= P\left( \bar{X}> \theta_0 - \sqrt{\frac{2\sigma^2\ln(c)}{n}} \right) \\
				&= P\left( \bar{X} - \theta_0 >  - \sqrt{\frac{2\sigma^2\ln(c)}{n}} \right) \\
				&= P\left( (\bar{X} - \theta_0)\sqrt{n} >  - \sqrt{2\sigma^2\ln(c)} \right) \\
				&= P\left( \frac{(\bar{X} - \theta_0)\sqrt{n}}{\sqrt{\sigma^2}} >  - \sqrt{2\ln(c)} \right) \\
				&= P\left( \frac{(\bar{X} - \theta_0)}{\sigma/\sqrt{n}} >  c^\prime \right)
			\end{align*}
			The left hand side of this inequality is equivalent to the \(z\) value of a standard normal distribution,
			by setting this \(c^\prime\) as such we can achieve the rejection region stated in the problem,
			\begin{align*}
				P\left( \frac{(\bar{X} - \theta_0)}{\sigma/\sqrt{n}} > z_\alpha \right)
				= P\left( {(\bar{X} - \theta_0)} > z_\alpha\sigma/\sqrt{n} \right)
				= P\left( {\bar{X} } > \theta_0+ z_\alpha\sqrt{\sigma^2/n} \right)
			\end{align*}
						
			\part
			First we check for an MLR,
			\begin{align*}
				\frac{f(x|\theta_2)}{f(x|\theta_1)}
				&= \frac{
					(2\pi\sigma^2)^{-n/2} e^{(x-\theta_2)^2/2\sigma^2}
				}{
					(2\pi\sigma^2)^{-n/2} e^{(x-\theta_1)^2/2\sigma^2}
				} \\
				&= \exp\left\{ \frac{1}{2\sigma^2} \left((x-\theta_1)^2 - (x-\theta_2)^2 \right)\right\} \\
				&= \exp\left\{ \frac{1}{2\sigma^2} \left(\theta_1^2 - 2x\theta_1 - \theta_2^2 + 2x\theta_2 \right)\right\} \\
				&= \exp\left\{ \frac{1}{2\sigma^2} \left(\theta_1^2 - \theta_2^2 + 2x(\theta_2 - \theta_1) \right)\right\}
			\end{align*}
			on inspection we can see that when \(\theta_2>\theta_1\)
			the \(2x(\theta_2 - \theta_1)\) is increasing in \(x\)
			and by extension, this entire function is increasing in \(x\),
			and we conclude that the function has an MLR.
			
			This meets the requirement for the Karlin-Rubin Theorem,
			thus we further conclude that the test in part a
			is a UMP level \(\alpha\) test for the provided hypotheses.
			
			\part
			It is intuitive that when the \(\sigma^2\) is unknown that the test would look very similar to the previous
			test with a \(z\) distribution, but in this case replaced with a \(t\).
			
			Like in part (a) we wish to use MLE's established for normal distributions shown in other parts of the class.
			{\footnotesize\begin{align*} 
				\lambda(x)
				= \frac{
					(2\pi\sigma^2)^{-n/2} e^{\sum_{i=1}^n (x_i-\theta_0)^2/2\sigma^2}
				}{
					(2\pi S^2)^{-n/2} e^{\sum_{i=1}^n (x_i-\bar{x})^2/2S^2}
				} 
				= \frac{
					(\sigma^2)^{-n/2} e^{\sum_{i=1}^n (x_i-\theta_0)^2/2\sigma^2}
				}{
					(S^2)^{-n/2} e^{\sum_{i=1}^n (x_i-\bar{x})^2/2S^2}
				} 
				= \left( \frac{S^2}{\sigma^2} \right)^{n/2} :< c
			\end{align*}}
			for some \(c\).
			
			\begin{align*}
				\lambda(x) = \left( \frac{S^2}{\sigma^2} \right)^{n/2} &< c \\
				\frac{S^2}{\sigma^2} &< c^{n/2} \\
				\sigma^2 &> \frac{S^2}{c^{n/2}} \\
				\sigma &> \sqrt{\frac{S^2}{c^{n/2}} } \\
				\frac{1}{n}\sum_{i=1}^n (x_i - \sigma_0) &> \sqrt{\frac{S^2}{c^{n/2}} } \\
				\frac{1}{n}\sum_{i=1}^n (x_i) &> \sigma_0 + \sqrt{\frac{S^2}{c^{n/2}} } \\
				\bar{x} &> \sigma_0 + \sqrt{\frac{S^2}{c^{n/2}} } \\
			\end{align*}
			
			
			\textcolor{red}{.. Ran out of steam at this point and was already behind on turning it in...}
		\end{parts}
	\end{solution}

\end{questions}
\end{document}
