\documentclass[12pt,letterpaper]{exam}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{alphabeta}
\usepackage[width=8.50in, height=11.00in, left=0.50in, right=0.50in, top=0.50in, bottom=0.50in]{geometry}

\usepackage{libertine}
\usepackage{multicol}
\usepackage[shortlabels]{enumitem}

\usepackage{booktabs}
\usepackage[table]{xcolor}

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bbm}

\usepackage{hyperref}
\usepackage{graphicx}
%\usepackage{wrapfig}
%\usepackage{capt-of}
%\usepackage{tikz}
%\usepackage{pgfplots}
%\usetikzlibrary{shapes,arrows,positioning,patterns}
%\usepackage{pythonhighlight}

\newcommand\chapter{8}
\renewcommand{\thequestion}{\textbf{\chapter.\arabic{question}}}
\renewcommand{\questionlabel}{\thequestion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\class}{  } % This is the name of the course 
\newcommand{\assignmentname}{Homework \# \chapter} % 
\newcommand{\authorname}{Hosley, Brandon} % 
\newcommand{\workdate}{\today} % 
\printanswers % this includes the solutions sections
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\pagestyle{plain}
\thispagestyle{empty}
\noindent

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
\begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} r @{\extracolsep{10pt}} l}
	\textbf{\class} & \textbf{\authorname}  &\\ %Your name here instead, obviously
	\textbf{\assignmentname } & \textbf{\workdate} & \\
\end{tabular*}\\
\rule{\textwidth}{2pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% HEADER %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{questions}

	\question 
	In 1,000 tosses of a coin, 560 heads and 440 tails appear. 
	Is it reasonable to assume that the coin is fair? Justify your answer.
	
	\begin{solution}
		If the coin is 'fair' we typically assume that the \(p=1-p\) and therefore \(p=0.5\).
		Further, we expect \(X\sim \text{bin}(1000,0.5)\), and
		
		\( E[X] = np = 500 \),
		
		\( Var[X] = np(1-p) = 250\).
		
		Then, noting that as \(n\rightarrow\infty\) the binomial approaches normal distribution.
		We can utilize the normal distribution to approximate the probability of this coin's result.
		\begin{align*}
			z &= \frac{x-\mu}{\sigma} \\
			&= \frac{60}{\sqrt{250}} \\
			&= 3.7947,
		\intertext{thus,}
			P(Z\geq z)  &= P(Z\geq 3.7947) .\\
		\end{align*}
		With this value we can calculate an approximate probability;
		of the first 4 tables that I consulted, the highest \(Z\) score was \(3.69\)
		which returns a value of \(0.0001\).
		Thus, even without an exact answer we can determine that it is highly unlikely that this
		particular coin is fair (\(p=0.5\)).
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\setcounter{question}{3-1}
	
	\question 
	Here, the LRT alluded to in Example 8.2.9 will be derived. 
	Suppose that we observe \(m\) iid Bernoulli(\(\theta\)) random variables, denoted \(Y_1,\ldots,Y_m\). 
	Show that the LRT of \(H_0: \theta \leq \theta_0\) versus \(H_1:\theta > \theta_0\) will reject \(H_0\) if \(\sum_{i=1}^{m} Y_i > b\).
	
	\begin{solution}
		Equivalently
		\(\lambda(y)<c\)
		\[\lambda(y) = \frac{\sup_{\theta<\theta_0}L(\theta|y)}{\sup_\Theta L(\theta|y)}\]
		
		Let \(y = \sum_{i=1}^{m} Y_i\). Then, we know that \(y\sim\text{Binom}(\theta,m)\)
		which can help us shortcut to \(\hat{p} = \frac{y}{m}\).
		
		We can find this the long way by
		\begin{align*}
			L(p|y) &= \prod_{i=1}^{m} p^{y_i} (1-p)^{1-y_i} \\
			&= p^{m\bar{y}} (1-p)^{m(1-\bar{y})}
		\intertext{take log likelihood to ease the solving process,}
			\ell(p|y) &= m\bar{y} \ln(p) + m(1-\bar{y})\ln(1-p) 
		\intertext{then we set the derivative of this equal to 0,}
			0 &=: \frac{m\bar{y}}{p} - \frac{m(1-\bar{y})}{1-p}
		\intertext{equivalently,}
			pm(1-\bar{y}) &= (1-p)m\bar{y} \\
			p-\bar{y}p &= \bar{y}-\bar{y}p \\
			p &= \bar{y} \\
			p &= \frac{\sum_{i=1}^my_i}{m} = \frac{y}{m} .
		\end{align*}
		This is the \(\sup_\Theta L(\theta|y)\). The \(\sup_{\theta<\theta_0}L(\theta|y)\) is \(\min(\theta_0,\frac{y}{m})\).
		
		With this we can revisit \(\lambda\)
		\begin{align*}
			\lambda(y) 
			&= \frac{\sup_{\theta<\theta_0}L(\theta|y)}{\sup_\Theta L(\theta|y)} \\
			&= \begin{cases}
				\frac{\theta_0^y(1-\theta_0)^{m-y}}{(y/m)^y(1-y/m)^{m-y}} & \frac{y}{m} > \theta_0 \\
				1 & \frac{y}{m} \leq \theta_0
			\end{cases}
		\end{align*}
		Next, we would like to determine what the behavior of this function is, which will be easier (as above) in log,
		\begin{align*}
			\frac{d}{dy}\ln\lambda(y)
			&= \frac{d}{dy} y\ln\theta_0 + (m-y)\ln(1-\theta_0) - y\ln\frac{y}{m} - (m-y)\ln(1-\frac{y}{m}) \\
			&= \ln\theta_0 - \ln(1-\theta_0) - \frac{1}{y} \,y + \ln(1-y/m) + (m-y)\frac{1}{m-y} \\
			&= \ln\left( \frac{\theta_0}{1-\theta_0}(1-y/m) \right)
		\end{align*}
		by inspection of the final expression we can see that \(\lambda\) is a decreasing function in \(y\)
		and thus \(\lambda(y)<c \). 
		
		For greater context, one may need to review example 8.2.9; but this implies that for \(\lambda(y)<c \)
		there exists a \(b\) for which \(\sum_{i=1}^{m} Y_i > b\), and is therefor an equivalent rejection region.
		
	\end{solution}
	\clearpage
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\setcounter{question}{12-1}
	
	\question 
	For sample size \(n = 1, 4, 16, 64, 100\) from a normal population with mean \(\mu\) and known variance \(\sigma^2\), 
	plot the power function following the LRTs. Take \(\alpha = 0.05\)
	\begin{parts}
		\part \(H_0: \mu \le 0\) versus \(H_1:\mu > 0\)
		\part \(H_0: \mu = 0\) versus \(H_1:\mu \ne 0\)
	\end{parts}
	
	\begin{solution}
		\begin{parts}
			\part
			Borrowing the normal power function from Casella and Berger example 8.3.3
			\begin{align*}
				\beta(\theta)
				&= P\left(Z>c-\frac{\theta_0-\theta}{\sigma/\sqrt{n}}\right) \\
				&= P\left(\frac{\bar{X}-\theta}{\sigma/\sqrt{n}}>1.645-\frac{\theta_0-\theta}{\sigma}\sqrt{n}\right)
			\end{align*}
				I have approximated the power functions in Desmos, requiring some liberties on the axes, 
				but hopefully showing the appropriate changes as \(n\) increases.
			\begin{center}
				\includegraphics[width=0.7\linewidth]{"Screenshot 2024-02-21 at 18.50.40"}
			\end{center}
			
			\part
			Borrowing the normal power function from Casella and Berger example 8.3.3
			\begin{align*}
				\beta(\theta)
				&= P\left(Z<\left| c+\frac{\theta_0-\theta}{\sigma/\sqrt{n}} \right|\right) \\
				&= P\left(Z<\left| 1.96+\frac{\theta_0-\theta}{\sigma/\sqrt{n}} \right|\right)
			\end{align*}
			
		\end{parts}
		
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	%\setcounter{question}{14-1}
	%\question For a random sample $X_1, ..., X_n$ of Bernoulli($p$) variables, it is desired to test $$H_0: p = .49 \quad\text{versus}\quad H_1: p = .51$$
	%Use the Central Limit Theorem ot determine, approximately, the sample size needed so that the two probability of error are both about .01. Use a test function that rejects $H_0$ if $\sum_{i=1}^{n} X_i$ is large.
	%\begin{solution}
	%	Here is the next solution
	%\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\clearpage
	\setcounter{question}{15-1}
	
	\question 
	Show that for a random sample $X_1, ..., X_n$ from a $N(0,\sigma^2)$ population, 
	the most powerful test of $H_0: \sigma = \sigma_0$ versus $H_1: \sigma = \sigma_1$, 
	where $\sigma_0 < \sigma_1$, is given by
	\begin{align*}
		\phi\left(\Sigma X_i^2\right) =
		\begin{cases}
			1 &, \Sigma X_i^2 > c \\
			0 &, \Sigma X_i^2 \le c.
		\end{cases}
	\end{align*}
	For a given value of $\alpha$, the size of the Type I Error, show how the value of $c$ is explicitly determined.

	\begin{solution}
		First, using Neyman-Pearson;
		\begin{align*}
			\lambda(x)
			&= \frac{\mathcal{L}(\sigma_1|x)}{\mathcal{L}(\sigma_1|x)}
			= \frac{(2\pi\sigma_1^2)^{-n/2}e^{\sum_{i=1}^{n}x_i^2/(2\sigma_1^2)}}
				{(2\pi\sigma_0^2)^{-n/2}e^{\sum_{i=1}^{n}x_i^2/(2\sigma_0^2)}}
			= \frac{\sigma_0^n}{\sigma_1^n} \exp\left\{\frac{1}{2}\left(\frac{1}{\sigma_0^2}-\frac{1}{\sigma_1^2}\right) \sum_{i=1}^nx_i^2 \right\}.
		\end{align*}
		Then for some \(k\),
		\begin{align*}
			k &< \frac{\sigma_0^n}{\sigma_1^n} \exp\left\{\frac{1}{2}\left(\frac{1}{\sigma_0^2}-\frac{1}{\sigma_1^2}\right) \sum_{i=1}^nx_i^2 \right\} \\
			k\frac{\sigma_1^n}{\sigma_0^n} &< \exp\left\{\frac{1}{2}\left(\frac{1}{\sigma_0^2}-\frac{1}{\sigma_1^2}\right) \sum_{i=1}^nx_i^2 \right\} \\
			\ln(k\sigma_1^n/\sigma_0^n) &< \frac{1}{2}\left(\frac{1}{\sigma_0^2}-\frac{1}{\sigma_1^2}\right)\sum_{i=1}^nx_i^2 \\
			2\ln(k\sigma_1^n/\sigma_0^n) &< \left(\frac{1}{\sigma_0^2}-\frac{1}{\sigma_1^2}\right)\sum_{i=1}^nx_i^2 \\
			\frac{2\ln(k\sigma_1^n/\sigma_0^n)}{\left(\frac{1}{\sigma_0^2}-\frac{1}{\sigma_1^2}\right)} &< \sum_{i=1}^nx_i^2. \\
		\end{align*}
		Thus,
		\begin{align*}
			\alpha = P\left(\sum_{i=1}^nx_i^2 > c\right)
		\intertext{then, if we standard normal the normal distribution we can leverage a \(\chi^2\) distribution,}
			\alpha &= P\left(\sum_{i=1}^n\frac{x_i}{\sigma_0^2} > \frac{c}{\sigma_0^2}\right) \\
			&= P\left(\chi_n^2 > \frac{c}{\sigma_0^2}\right).
		\end{align*}
		Thus the \(c\) corresponds to the value of \(\sigma_0^2\chi_n^2\).
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\clearpage
	\setcounter{question}{19-1}
	
	\question 
	The random variable $X$ has pdf $f(x) = e^{-x}, x> 0$. 
	One observation is obtained on the random variable $Y = X^\theta$, 
	and a test of $H_0: \theta = 1$ versus $H_1: \theta = 2$ needs to be constructed. 
	Find the UMP level $\alpha = .10$ test and compute the Type II Error probability.

	\begin{solution}
		First, we perform a transform of variable
		\begin{align*}
			f(y) 
			= f(g^{-1}(x)) 
			= e^{-y^{-1/\theta}}\,\frac{d}{dy} e^{-y^{-1/\theta}}
			= -\frac{1}{\theta}y^{1/\theta-1} e^{-y^{-1/\theta}}
		\end{align*}
		
		Second, we determine the direction of MLR
		\begin{align*}
			\frac{f(y|H_1)}{f(y|H_0)}
			= \frac{-\frac{1}{2}y^{-1/2}e^{y^{1/2}}}{-1(1)e^{-y}} 
			= \frac{1}{2}y^{-1/2}e^{y-y^{1/2}}.
		\end{align*}
		Graphing the function shows us that the function is decreasing on \(y<1\) and increasing on \(y>1\).
		\begin{center}
			\includegraphics[width=0.5\linewidth]{"Screenshot 2024-02-21 at 13.06.55"}
		\end{center}
		This implies two rejection regions.

		
		Third, we set the two rejection regions equal to each other and to sum to \(\alpha\).
		\(y\leq c_0\) and \(y\geq c_1\)
		\begin{align*}
			\beta(\theta)
			&= P(y\leq c_0) + P(y\geq c_1)
			= \alpha
			= 0.10
		\intertext{or equivalently,}
			\frac{\alpha}{2}
			= 0.05
			&= P(y\leq c_0)
			= P(y\geq c_1) \\
			&= \int_{0}^{c_0} e^{-y} \,dy
			= 1 - \int_{0}^{c_1} e^{-y} \,dy.
		\end{align*}
		Given to Wolfram's solver we get,
		\(c_0 \approx 0.512933\) and 
		\(c_1 \approx 2.99573\).
		
		Finally, we can calculate the Type II error by integrating over the acceptance region.
		\begin{align*}
			\int_{c_0}^{c_1} f(y|H_1)
			&= \int_{c_0}^{c_1} \frac{1}{2}y^{-1/2}e^{-y^{1/2}} \,dy
		\intertext{once again leveraging a solver,}
			&\approx 0.620196.
		\end{align*}
		
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\clearpage
	\setcounter{question}{20-1}
	
	\question 
	Let $X$ be a random variable whose pmf under $H_0$ and $H_1$ is given by
	$$
	\begin{array}{cccccccc}
		x & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\ \hline
		f(x|H_0) & .01 & .01 & .01 & .01 & .01 & .01 & .94 \\
		f(x|H_1) & .06 & .05 & .04 & .03 & .02 & .01 & .79 \\
	\end{array}
	$$
	Use the Neyman-Pearson Lemma to find the most powerful test for $H_0$ versus $H_1$ with size $\alpha = .04$. 
	Compute the probability of Type II Error for this test.

	\begin{solution}
		First we calculate the likelihood ratios between the two hypotheses,
		\begin{align*}
			\begin{array}{cccccccc}
				x & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\ \hline 
				\frac{f(x|H_1)}{f(x|H_0)} & 6 & 5 & 4 & 3 & 2 & 1 & .84 \\
			\end{array}
		\end{align*}
		From this we can observe that \(f(x)\) is decreasing on \(x\), 
		and thus we are looking for a \(c\) such that
		\begin{align*}
			P(X\leq c|H_0) &= \alpha \\
			\sum_{i=1}^{c} f(x|H_0) &= \alpha \\
		\end{align*}
		which can be brute-forced to determine that \(c=4\).
		
		Then the probability of a Type II error is
		\begin{align*}
			\sum_{i=c+1}^{n} f(x|H_1) = 0.82 \,.
		\end{align*}
	
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\setcounter{question}{23-1}
	
	\question 
	Suppose $X$ is one observation from a population with beta($\theta$, 1) pdf.
	\begin{parts}
		\part For testing $H_0: \theta \le 1$ versus $H_1: \theta > 1$, 
			find the size and sketch the power function of the test that rejects $H_0$ if $X > \frac{1}{2}$.
		\part Find the most powerful level $\alpha$ test fo $H_0: \theta =1$ versus $H_1: \theta = 2$.
		\part Is there a UMP test of $H_0: \theta \le 1$ versus $H_1: \theta > 1$? If so, find it. If not, prove so.
	\end{parts}
	
	\begin{solution}
		Here is the next solution
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\setcounter{question}{28-1}
	
	\question 
	Let $f(x|\theta)$ be the logistic location pdf
	$$
	f(x|\theta)
	= \frac{ e^{(x-\theta)} }{ (1 + e^{(x-\theta)})^2 }, \quad -\infty < x < \infty, \quad -\infty < \theta < \infty
	$$
	\begin{parts}
		\part Show that this familiy has an MLR.
		\part Based on one observation, $X$, find the most powerful size $\alpha$ test of $H_0: \theta = 0$ versus $H_1: \theta =1$. 
			For $\alpha$ = .2, find the size of the Type II Error.
		\part  Show that the test in part (b) is UMP size $\alpha$ for testing $H_0: \theta \le 0$ versus $H_1: \theta > 0$. What can be said about UMP tests in general for the logistic location family?
	\end{parts}
	
	\begin{solution}
		Here is the next solution
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\setcounter{question}{37-1}
	
	\question 
	Let $X_1, ..., X_n$ be a random sample from a n($\theta, \sigma^2$) population. 
	Consider testing $$H_0: \theta \le \theta_0 \quad \text{versus} \quad H_1: \theta > \theta_0.$$
	\begin{parts}
		\part If $\sigma^2$ is known, show that the test that rejects $H_0$ when
		$$\bar{X} > \theta_0 + z_\alpha \sqrt{\sigma^2/n}$$
		is a test of size $\alpha$. Show that the test can be derived as an LRT.
		\part Show that the test in part (a) is a UMP test
		\part If $\sigma^2$ is unknown, show that the test that rejects $H_0$ when
		$$\bar{X} > \theta_0 + z_{n-1,\alpha} \sqrt{S^2/n}$$ is a test of size $\alpha$. Show that the test can be derived as an LRT.
	\end{parts}
	
	\begin{solution}
		Here is the next solution
	\end{solution}

\end{questions}
\end{document}
