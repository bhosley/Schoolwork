\documentclass[12pt,letterpaper]{exam}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{alphabeta}
\usepackage[width=8.50in, height=11.00in, left=0.50in, right=0.50in, top=0.50in, bottom=0.50in]{geometry}

\usepackage{libertine}
\usepackage{multicol}
\usepackage[shortlabels]{enumitem}

\usepackage{booktabs}
\usepackage[table]{xcolor}

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bbm}

\usepackage{hyperref}
\usepackage{graphicx}
%\usepackage{wrapfig}
%\usepackage{capt-of}
%\usepackage{tikz}
%\usepackage{pgfplots}
%\usetikzlibrary{shapes,arrows,positioning,patterns}
%\usepackage{pythonhighlight}

\newcommand\chapter{7}
\renewcommand{\thequestion}{\textbf{\chapter.\arabic{question}}}
\renewcommand{\questionlabel}{\thequestion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\class}{  } % This is the name of the course 
\newcommand{\assignmentname}{Homework \# \chapter} % 
\newcommand{\authorname}{Hosley, Brandon} % 
\newcommand{\workdate}{\today} % 
\printanswers % this includes the solutions sections
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\pagestyle{plain}
\thispagestyle{empty}
\noindent

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
\begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} r @{\extracolsep{10pt}} l}
	\textbf{\class} & \textbf{\authorname}  &\\ %Your name here instead, obviously
	\textbf{\assignmentname } & \textbf{\workdate} & \\
\end{tabular*}\\
\rule{\textwidth}{2pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% HEADER %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{questions}

\question One observation is taken on a discrete random variable $X$ with pmf $f(x|\theta)$, where $\theta \in \{1,2,3\}$. Find the MLE of $\theta$.
	$$\begin{array}{cccc}
	\hline
	x & f(x|1) & f(x|2) & f(x|3) \\
	\hline
	0 & 1/3 & 1/4 & 0 \\
	1 & 1/3 & 1/4 & 0 \\
	2 & 0 & 1/4 & 1/4 \\
	3 & 1/6 & 1/4 & 1/2 \\
	4 & 1/6 & 0 &1/4 \\
	\hline
	\end{array}$$

	\begin{solution}
		The MLE given a single sample of \(x\) is;
		\begin{align*}
			\hat{\theta} = \arg\max\mathcal{L}(\theta|x=0) &= 1 \\
			\hat{\theta} = \arg\max\mathcal{L}(\theta|x=1) &= 1 \\
			\hat{\theta} = \arg\max\mathcal{L}(\theta|x=2) &= \{2,3\} \\
			\hat{\theta} = \arg\max\mathcal{L}(\theta|x=3) &= 3 \\
			\hat{\theta} = \arg\max\mathcal{L}(\theta|x=4) &= 3 .
		\end{align*}
	\end{solution}

\setcounter{question}{6-1}
\question Let $X_1,...,X_n$ be a random sample from the pdf $$f(x|\theta) = \theta x^{-2}, \quad 0 < \theta \le x < \infty.$$
	\begin{parts}
		\part What is a sufficient statistic for $\theta$?
		\part Find the MLE of $\theta$.
		\part Find the method of moments estimator of $\theta$.
	\end{parts}

	\begin{solution}
		\begin{parts}
			\part \(x_{(1)}\) is sufficient for \(\theta\)
			\part
				\begin{align*}
					\arg\max\mathcal{L}(\theta|x) 
					= \prod_{i=1}^{n} \theta x_i^{-2}
					= \theta^n \prod_{i=1}^{n} x_i^{-2},
				\end{align*}
				By inspection that this is a monotonically increasing function in \(\theta\), 
				thus \(\hat{\theta} = \max\theta = x_{(1)}\)
			\part The Method of Moments
				\begin{align*}
					E[x] 
					= \int_{\theta}^{\infty} x\,\theta x^{-2}\,dx
					= \theta \int_{\theta}^{\infty} x^{-1}\,dx
					= \theta \left. \ln x \right|_{\theta}^{\infty}
					= \infty
				\end{align*}
				thus, the M.o.M. estimator is undefined.
		\end{parts}
	\end{solution}

\setcounter{question}{9-1}
\question Let $X_1,...,X_n$ be iid with pdf $$f(x|\theta) = \frac{1}{\theta}, \quad 0 < x < \theta, \quad \theta > 0.$$
Estimate $\theta$ using both the method of moments and maximum likelihood. Calculate the means and variances of the two estimators. Which one should be preferred and why?
	\begin{solution}
		This pdf is a uniform distribution, which shortens the method of moments estimator calculations;
		\begin{align*}
			E[f(x|\theta)] &= \frac{0+\theta}{2}
		\intertext{allowing us to calculate}
			\theta &= 2\bar{X},
		\end{align*}
		and,
		\begin{align*}
			\text{Var}[\theta] 
			= \text{Var}[2\bar{X}] 
			= 4\text{Var}\left[\frac{1}{n}\sum_{i=1}^n x_i\right] 
			= \frac{4}{n^2} \sum_{i=1}^n \text{Var}[x_i] 
			= \frac{4}{n^2}  n\left(\frac{(\theta-0)^2}{12}\right) 
			= \frac{\theta^2}{3n}.
		\end{align*}
		For the MLE we start with,
		\begin{align*}
			\arg\max\mathcal{L}(\theta|x)
			= \prod_{i=1}^n \frac{1}{\theta} \mathbbm{1}_{(0,\theta)}(x)
			=  \frac{1}{\theta^n} \prod_{i=1}^n\mathbbm{1}_{(-\infty,\theta)}(x_{(n)}) \,
			\mathbbm{1}_{(0,\infty)}(x_{(1)})
			\equiv \frac{1}{\theta^n} \prod_{i=1}^n\mathbbm{1}_{(x_{(n)},\infty,)}(\theta)
		\end{align*}
		From this we can see that the \(\hat{\theta}\) corresponds with the lowest possible \(\theta\)
		which is bounded by \(x_{(n)}\).
		To calculate the variance of this estimator, we start with the pdf of an order statistic,
		\begin{align*}
			nf(x)F(x)^{x-1}
			= n\frac{1}{\theta}\left(\frac{x}{\theta}\right)^{n-1}
			= \frac{n}{\theta^n}x^{n-1}
		\end{align*}
		From this we can calculate
		\begin{align*}
			E[x_{(n)}]
			= \int_{0}^{\theta} x \frac{n}{\theta^n}x^{n-1} \,dx
			= \frac{n}{\theta^n} \int_{0}^{\theta} x^{n} \,dx
			= \frac{n}{\theta^n} \left[\frac{x^{n+1}}{n+1}\right]_{0}^{\theta}
			= \frac{n}{\theta^n}\frac{\theta^{n+1}}{n+1}
			= \frac{n\theta}{n+1}
		\end{align*}
		and for the variance, the second moment,
		\begin{align*}
			E[x_{(n)}^2]
			= \int_{0}^{\theta} x^2 \frac{n}{\theta^n}x^{n-1} \,dx
			= \frac{n}{\theta^n} \int_{0}^{\theta} x^{n+1} \,dx
			= \frac{n}{\theta^n} \left[\frac{x^{n+2}}{n+2}\right]_{0}^{\theta}
			= \frac{n}{\theta^n}\frac{\theta^{n+2}}{n+2}
			= \frac{n\theta^2}{n+2}.
		\end{align*}
		Which allows us to calculate,
		\begin{align*}
			\text{Var}[x_{(n)}] &
			= \frac{n\theta^2}{n+2} - \left( \frac{n\theta}{n+1} \right)^2
			= \frac{n\theta^2(n+1)^2 - n^2\theta^2(n+2)}{(n+1)^2(n+2)} \\&
			= \frac{n\theta^2 \big((n+1)^2 - n(n+2)\big)}{(n+1)^2(n+2)}
			= \frac{n\theta^2}{(n+1)^2(n+2)}.
		\end{align*}
		Assuming that lower variance makes a better estimator, then the MLE estimator is better, as the variance
		is lower for all \(n\).
	\end{solution}

\setcounter{question}{10-1}
\question The independent random variables $X_1,...,X_n$ have the common distribution
$$P(X_i \le x | \alpha, \beta) =
\begin{cases}
0 & , x< 0\\
(x/\beta)^\alpha & , 0 \le x \le \beta \\
1 & ,x > \beta
\end{cases}$$
where the parameters $\alpha$ and $\beta$ are positive.

	\begin{parts}
		\part Find a two-dimensional sufficient statistic for $(\alpha, \beta)$.
		\part Find the MLEs of $\alpha$ and $\beta$.
		\part The length (in millimeters) of cuckoos' eggs found in hedge sparrow nests can be modeled with this distribution. For the data
		$$22.0, 23.9, 20.9, 23.8, 25.0, 24.0, 21.7, 23.8, 22.8, 23.1, 23.1, 23.5, 23.0, 23.0$$
		Find the MLEs of $\alpha$ and $\beta$.
	\end{parts}

	\begin{solution}
		\begin{parts}
			\part
			From the provided CDF we can calculate the pdf as \(\frac{\alpha x^{\alpha-1}}{\beta^\alpha}\).
			Then, 
			\begin{align*}
				f(x|\alpha,\beta)
				= \prod_{i=1}^n \frac{\alpha x_i^{\alpha-1}}{\beta^\alpha} \mathbbm{1}_{(0,\beta)}(x)
				= \frac{\alpha^n}{\beta^{n\alpha}} \prod_{i=1}^n x_i^{\alpha-1} 
					\mathbbm{1}_{(0,\infty)}(x_{(1)}) \mathbbm{1}_{(-\infty,\beta)}(x_{(n)})
				\equiv \prod_{i=1}^n x_i^{\alpha-1} \mathbbm{1}_{(-\infty,\beta)}(x_{(n)}),
			\end{align*}
			leveraging factorization we get sufficient statistics \((\prod_{i=1}^n x_i, x_{(n)})\).
			
			\part
			Let \(\beta=x_{(n)}\) as above, then looking at \(\alpha\),
			\begin{align*}
				\frac{d}{d\alpha} \ell(\alpha,\beta|x)
				&= \frac{d}{d\alpha}\ \frac{\alpha^n}{\beta^{n\alpha}} \prod_{i=1}^n x_i^{\alpha-1} \\
				&\equiv \frac{d}{d\alpha}\ n\ln\alpha - n\alpha\ln\beta + (\alpha-1)\ln\prod_{i=1}^n x_i
				= \frac{n}{\alpha} - n\ln\beta + \ln\prod_{i=1}^n x_i.
			\end{align*}
			Taking the derivative of the previous result,
			\begin{align*}
				\frac{d}{d\alpha}\ \left(\frac{n}{\alpha} - n\ln\beta + \ln\prod_{i=1}^n x_i\right)
				= -\frac{n}{\alpha^2},
			\end{align*}
			tells us that this is a maxima.
			Then, setting the first derivative to \(0\),
			\begin{align*}
				0 &= \frac{n}{\alpha} - n\ln\beta + \ln\prod_{i=1}^n x_i \\
				\frac{n}{\alpha} &= n\ln\beta - \ln\prod_{i=1}^n x_i \\
				\alpha &= \frac{n}{n\ln\beta - \ln\prod_{i=1}^n x_i}.
			\end{align*}
			
			Finally, the MLEs are,
			\begin{align*}
				\hat\alpha &= \frac{n}{n\ln\beta - \ln\prod_{i=1}^n x_i} \\
				\hat\beta &= x_{(n)}
			\end{align*}
			
			\part
			\(\beta = 25.0\) and
			\(\alpha = \frac{14}{14\ln(25)- 43.9527} = 12.59489\)
			
			
		\end{parts}
	\end{solution}

\setcounter{question}{12-1}
\question  Let $X_1,...,X_n$ be a random sample from the population with pmf
$$P_\theta (X = x) = \theta^x (1-\theta)^{1-x}, \quad x = 0 \text{ or } 1, \quad 0 \le \theta \le \frac{1}{2}.$$
	\begin{parts}
	\part Find the method of moments estimator and MLE of $\theta$.
	\part Find the mean squared errors of each of the estimators.
	\part Which estimator is preferred? Justify your choice
	\end{parts}

	\begin{solution}
		\begin{parts}
			\part
			Recognizing this as a Bernoulli(\(p=\theta\)) will make some of the calculations shorter.
			\\ \textbf{Method of Moments:}\\
			\begin{align*}
				E[x] = p = \theta = \bar{X} = \frac{1}{n}\sum_{i=1}^{n}x_i
			\end{align*}
			\\ \textbf{MLE:}\\
			\begin{align*}
				\ell(\theta|x) = \sum_{i=1}^n x_i\ln\theta + (1-x_i)\ln(1-\theta) \\
				\frac{d}{d\theta} \ell(\theta|x) = n\frac{\bar{X}}{\theta} - n\frac{1-\bar{X}}{1-\theta} \\
				\frac{d^2}{d\theta^2} \ell(\theta|x) = -n\frac{\bar{X}}{\theta^2} - n\frac{1-\bar{X}}{(1-\theta)^2} 
			\end{align*}
			The second derivative is \(<0\) for all values \(x\) and \(\theta\) within their respective domains.
			Thus we use the first derivative to find the MLE,
			\begin{align*}
				\frac{d}{d\theta} \ell(\theta|x) = 0 &= n\frac{\bar{X}}{\theta} - n\frac{1-\bar{X}}{1-\theta} \\
				\frac{\bar{X}}{\theta} &= \frac{1-\bar{X}}{1-\theta} \\
				\bar{X}(1-\theta) &= \theta(1-\bar{X}) \\
				\bar{X} = \theta
			\end{align*}
			However, with the upper bound assigned to \(\theta\) it would be more appropriately \(\min(\bar{X},1/2)\) in both cases.
			
			\part
			\textbf{Method of Moments MSE:}\\
			Using the \(\text{Var}_\theta W + (E_\theta W-\theta)^2\) starting point of MSE,
			\begin{align*}
				\text{Var}_\theta \bar{X}
				= \text{Var}_\theta \frac{1}{n}\sum_{i=1}^n x_i
				= \frac{\text{Var}_\theta x_i}{n}
				= \frac{\theta(1-\theta)}{n}
			\end{align*}
			and 
			\begin{align*}
				E_\theta[\bar{X}] = \theta
			\end{align*}
			gives
			\begin{align*}
				\text{Var}_\theta W + (E_\theta W-\theta)^2
				= \frac{\theta(1-\theta)}{n} (\theta-\theta)^2
				= \frac{\theta(1-\theta)}{n}
			\end{align*}
			\\\textbf{MLE MSE:}\\
			In this context we have a two part function, when \(\bar{x}<1/2\) 
			the MSE is equal to that found in the M.o.M. calculation.
			\begin{align*}
				\text{Var}_\theta W + (E_\theta W-\theta)^2
				= 0 + (E[1/2]-\theta)^2
				=(1/2-\theta)^2
			\intertext{therefore,}
				\text{MLEMSE}
				= \frac{\theta(1-\theta)}{n} \mathbbm{1}_{(0,\frac{1}{2})}(\bar{x})
				+ (1/2-\theta)^2 \mathbbm{1}_{(\frac{1}{2},1)}(\bar{x})
			\end{align*}
			
			\part
			If the answers calculated in part b are correct, then the M.o.M. MLE is would have lower variance
			in all cases, thus the better choice.
			
		\end{parts}
	\end{solution}

\setcounter{question}{38-1}
\question For each of the following distributions, let $X_1,...,X_n$ be a random sample. Is there a function of $\theta$, say $g(\theta)$, for which there exists an unbiased estimator whose variance attains the Cramer-Rao lower bound? If so, find it. If not, show why not.
\begin{parts}
\part $f(x|\theta) = \theta x^{\theta -1 }, \quad 0 < x < 1, \quad \theta > 0$
\part $f(x|\theta) = \frac{ \log(\theta) }{ \theta - 1} \theta^x, \quad 0 < x < 1, \quad \theta > 1$
\end{parts}
	\begin{solution}
		Using the attainment theorem;
		\begin{parts}
			\part
			\begin{align*}
				\frac{\partial}{\partial\theta} \log\mathcal{L}(\theta|x)
				= \frac{\partial}{\partial\theta} \ln \prod_{i=1}^n \theta x_i^{\theta-1}
				= \frac{\partial}{\partial\theta} \ln \prod_{i=1}^n \exp\{ \ln\theta + (\theta-1)\ln x_i\} \\
				= \frac{\partial}{\partial\theta} n\ln\theta + (\theta-1)\sum_{i=1}^n\ln x_i
				= \frac{n}{\theta} + \sum_{i=1}^n\ln x_i
				=  - \sum_{i=1}^n\ln x_i - \frac{n}{\theta}
			\end{align*}
			Thus \(-\sum_{i=1}^n\ln x_i\) is an estimator of \(\tau(\theta) = \frac{n}{\theta}\) 
			that attains the Cramer-Rao lower bound.
			
			To show that this estimator is unbiased:
			
			\part
			\begin{align*}
				\frac{\partial}{\partial\theta} \log\mathcal{L}(\theta|x)
				&= \frac{\partial}{\partial\theta} \ln \prod_{i=1}^n \frac{ \ln(\theta) }{ \theta - 1} \theta^x_i
				= \frac{\partial}{\partial\theta} \ln \prod_{i=1}^n \ln(\theta) \exp\left\{x_i\ln\theta - \ln(\theta-1)\right\} \\
				&= \frac{\partial}{\partial\theta} \sum_{i=1}^n \ln\ln(\theta) + x_i\ln\theta - \ln(\theta-1)
				= \sum_{i=1}^n \frac{1}{\ln\theta} + \frac{x_i}{\theta} + \frac{1}{\theta-1} \\
				&= \frac{n\bar{x}}{\theta} + \frac{n}{\ln\theta} + \frac{n}{\theta-1}
				= \frac{n}{\theta}\left[\bar{x} + \frac{\theta}{\ln\theta} + \frac{\theta}{\theta-1}\right]
			\end{align*}
			Thus, \(\bar{x}\) is an estimator that attains the CRLB for 
			\(\tau(\theta) = \frac{\theta}{\ln\theta} + \frac{\theta}{\theta-1}\)
			where \(a(\theta) = \frac{n}{\theta}\).
			
			To show that this estimator is unbiased:
			
		\end{parts}
	\end{solution}
	
\clearpage

\setcounter{question}{40-1}
\question Let $X_1,...,X_n$ be iid Bernoulli($p$). Show that the variance of $\bar{X}$ attains the Cramer-Rao Lower bound, and hence $\bar{X}$ is the best unbiased estimator of $p$.
	\begin{solution}
		First we calculate Var\(\bar{X}\),
		\begin{align*}
			\text{Var}[\bar{X}]
			= \text{Var}\left[\frac{1}{n}\sum_{i=1}^{n}x_i\right]
			= \frac{1}{n}\sum_{i=1}^{n}\text{Var}[x_i]
			= \frac{p(1-p)}{n}.
		\end{align*}
		Next the numerator of the book's CRLB definition
		\begin{align*}
			\left(\frac{d}{dp}\bar{X}\right)^2
			=\left(\frac{d}{dp}p\right)^2
			=1,
		\end{align*}
		and the denominator, 
		\begin{align*}
			nE\left[\frac{d}{dp}\ell(p|x)^2\right] &
			= nE\left[\left(\frac{d}{dp}x\ln p + (1-x)\ln(1-p)\right)^2\right]
			= nE\left[\left(\frac{x}{p} - \frac{1-x}{1-p}\right)^2\right] \\&
			= nE\left[\left(\frac{x-xp-p+xp}{p(1-p)}\right)^2\right]
			= nE\left[\left(\frac{x-p}{p(1-p)}\right)^2\right]
			= nE\left[\frac{x^2-2px+p^2}{p^2(1-p)^2}\right] \\&
			= n\frac{E[x^2]-2pE[x]+p^2}{p^2(1-p)^2}
			= n\frac{p(1-p)+p^2-2p^2+p^2}{p^2(1-p)^2}
			= n\frac{p(1-p)}{p^2(1-p)^2}
			= \frac{n}{p(1-p)}.
		\end{align*}
		Thus the Cramer-Rao lower bound is
		\(\frac{1}{\frac{n}{p(1-p)}} = \frac{p(1-p)}{n}\),
		and hence Var\([\bar{X}]\) is a best unbiased estimator.
	\end{solution}

\setcounter{question}{46-1}
\question Let $X_1, X_2,$ and $X_3$ be a random sample of size 3 from a uniform($\theta, 2\theta$) distribution, where $\theta >0$.
\begin{parts}
\part Find the method of moments estimator of $\theta$
\part Find the MLE, $\hat{\theta}$, and find a constant $k$ such that $E_\theta (k \hat{\theta}) = \theta$
\part Which of the two estimators can be improved by using sufficiency? How?
\part Find the method of moments estimate and the MLE of $\theta$ based on the data $$1.29, 0.86, 1.33,$$ three observations of average berry sizes (in centimeters) of wine grapes.
\end{parts}
	\begin{solution}
		\begin{parts}
			\part The method of moments estimator for \(\theta\):
			\begin{align*}
				E[\bar{X}] &= \frac{\theta+2\theta}{2} = \frac{3}{2}\theta \\
				\tilde{\theta} &= \frac{2}{3}\bar{x}
			\end{align*}
			
			\part The MLE for \(\theta\)
			\begin{align*}
				\mathcal{L}(\theta|x) 
				= \prod_{i=1}^3 \frac{1}{\theta} \mathbbm{1}_{(\theta,2\theta)}(x)
				= \frac{1}{\theta^3} \mathbbm{1}_{(\frac{x_{(3)}}{2},x_{(1)})}(x)
			\end{align*}
			\(\theta\) is monotonically decreasing, and thus the maximum likelihood is where \(\theta\) is minimal,
			which will be at the lower bound \(\frac{x_{(3)}}{2}\). 
			It will either equal this value or approach this value based on whether or not 
			\(\theta\) is included within the distribution.
			
			\begin{align*}
				\theta
				&= E_\theta\left[ k\frac{x_{(3)}}{2} \right]
			\end{align*}
			
			First, we find the pdf of the order statistic,
			\[\frac{(x-\theta)^2}{\theta^3}\]
			then, the expected value from that,
			\begin{align*}
				\int_{\theta}^{\theta} x \frac{(x-\theta)^2}{\theta^3} \,dx
				&= \int_{\theta}^{\theta} \frac{x^3}{\theta^3} -\frac{2x^2\theta}{\theta^3} +\frac{x\theta^2}{\theta^3} \,dx
				= \left. \frac{x^4}{4\theta^3} -\frac{2x^3\theta}{3\theta^3} +\frac{x^2\theta^2}{2\theta^3} \right|\int_{\theta}^{\theta} \\
				&= \frac{15\theta^4}{4\theta^3} -\frac{18\theta^4}{3\theta^3} +\frac{3\theta^4}{2\theta^3}
				= \theta\left(1+\frac{1}{3}-\frac{1}{4}-\frac{1}{2}\right)
				= \theta\frac{7}{8}
			\end{align*}
			which allows us to determine that \(k=\frac{8}{7}\).
			
			\part
			The \(k\) in the previous problem suggests a bias, however, the method of moments statistic is not sufficient for
			a uniform distribution, and thus would be the one that might be improved.
			
			\part
			\begin{align*}
				\tilde{\theta} &= 2/9(1.29 + 0.86 + 1.33) = 0.7733\bar{3} \\
				\hat{\theta} &= \frac{1.33}{2} = 0.665
			\end{align*}
			
			
		\end{parts}
	\end{solution}

\setcounter{question}{49-1}
\question Let $X_1,...,X_n$ be iid exponential($\lambda$).
\begin{parts}
\part Find an unbiased estimator of $\lambda$ based only on $Y = \min \{X_1,...,X_n\}$
\part Find a better estimator than the one in part (a). Prove that it is better.
\part The following data are high-stress failure times (in hours) of Kevlar/epoxy spherical vessels used in a sustained pressure environment on the space shuttle:
$$ 50.1, 70.1, 137.0, 166.9, 170.5, 152.8, 80.5, 123.5, 112.6, 148.5, 160.0, 125.4.$$
Failure tiems are often modeled with the exponential distribution. Estimate the mean failure time using the estimators from parts (a) and (b).
\end{parts}
	\begin{solution}
		\begin{parts}
			\part
			First we find the pdf of \(Y\)
			\begin{align*}
				\frac{n}{\lambda}e^{-y/\lambda}\left(1-(1-e^-y/\lambda)\right)^{n-1}
				= \frac{n}{\lambda}e^{-ny/\lambda}
			\end{align*}
			Thus, \(Y\sim Exp(\lambda/n)\) which allows us to determine, that an unbiased estimate is
			\begin{align*}
				E[Y] = \lambda/n \\
				E[nY] = \hat\lambda
			\end{align*}
						
			\part
			Using the exponential family statistical sufficiency we can find that \(\sum_{i=1}^{n}x_i\) is sufficient for \(\lambda\).
			Additionally, the \(\theta\) part of the exponential is \(-n/\lambda\), which is an open set in \(\mathbb{R}\); 
			thus the statistic is also complete.
		
			Then, because this is an exponential distribution we know that,
			\begin{align*}
				E[\bar x] &= \lambda
				\intertext{and equivalently,}
				E\left[\sum_{i=1}^{n}x_i\right] &= n\lambda.
			\end{align*}
			We can leverage theorem 7.3.23, where \(T\) is our sum, a complete sufficient statistic for \(\lambda\)
			then define \(\phi(T)\) as \(\frac{1}{n}T\), 
			and find that \(E[\bar x]\) is a best unbiased estimator for \(\lambda\).
			
			\part
			\begin{align*}
				\text{Part A }\hat{\lambda} &= 601.2 \\
				\text{Part B }\hat{\lambda} &= 124.817
			\end{align*}
			
		\end{parts}
	\end{solution}

\end{questions}
\end{document}
