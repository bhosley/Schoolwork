\documentclass[]{article}
\usepackage{graphicx}
\usepackage{caption}
\graphicspath{ {./images/} }

% Minted
\usepackage[cache=false]{minted}
	\usemintedstyle{vs}
	\usepackage{xcolor}
		\definecolor{light-gray}{gray}{0.97}

% Subsubsection header run along with text.	
\usepackage{titlesec}
\titleformat{\subsubsection}[runin]% runin puts it in the same paragraph
	{\normalfont\bfseries}% formatting commands to apply to the whole heading
	{\thesubsection}% the label and number
	{0.5em}% space between label/number and subsection title
	{}% formatting commands applied just to subsection title
	[]% punctuation or other commands following subsection title

\usepackage{enumitem}
\usepackage{hyperref}

\usepackage[normalem]{ulem}

\title{Big Data Analytics: Exercise 5-1}
\author{Brandon Hosley}
\date{\today}

\begin{document}
\maketitle

\section*{Pre-Assignment}
\begin{minted}[breaklines,bgcolor=light-gray]{shell-session}
from pyspark.ml.linalg import Vectors
from pyspark.ml.classification import LogisticRegression
training = spark.createDataFrame([
	(1.0, Vectors.dense([0.0, 1.1, 0.1])),
	(0.0, Vectors.dense([2.0, 1.0, -1.0])),
	(0.0, Vectors.dense([2.0, 1.3, 1.0])),
	(1.0, Vectors.dense([0.0, 1.2, -0.5]))], ["label", "features"])
\end{minted}

\section*{Assignment 1}
\emph{ Do the exercise in section 2.2.1.2 and 2.2.1.3 }

\begin{minted}[breaklines,bgcolor=light-gray]{shell-session}
lr = LogisticRegression(maxIter=10, regParam=0.01)
print("LogisticRegression parameters:\n" + lr.explainParams() + "\n")
model1 = lr.fit(training)

print("Model 1 was fit using parameters: ")
print(model1.extractParamMap())
\end{minted}
\includegraphics[width=\linewidth]{image1} %\vspace{-1.5em}


\section*{Assignment 2}
\emph{ Do the exercise in section 2.2.1.4 and 2.2.1.5 }

\begin{minted}[breaklines,bgcolor=light-gray]{shell-session}
paramMap = {lr.maxIter: 20}
paramMap[lr.maxIter] = 30 # Specify 1 Param, overwriting the original maxIter.
paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55}) # Specify multiple Params.

paramMap2 = {lr.probabilityCol: "myProbability"} # Change output column name
paramMapCombined = paramMap.copy()
paramMapCombined.update(paramMap2)

model2 = lr.fit(training, paramMapCombined)
print("Model 2 was fit using parameters: ")
print(model2.extractParamMap())
\end{minted}
\includegraphics[width=\linewidth]{image2} %\vspace{-1.5em}


\section*{Assignment 3}
\emph{ Do the exercise in section 2.2.1.7 and 2.2.1.8 }

\begin{minted}[breaklines,bgcolor=light-gray]{shell-session}
test = spark.createDataFrame([
	(1.0, Vectors.dense([-1.0, 1.5, 1.3])),
	(0.0, Vectors.dense([3.0, 2.0, -0.1])),
	(1.0, Vectors.dense([0.0, 2.2, -1.5]))], ["label", "features"])

prediction = model2.transform(test)
result = prediction.select("features", "label", "myProbability", "prediction").collect()

for row in result:
	print("features=%s, label=%s -> prob=%s, prediction=%s"
		% (row.features, row.label, row.myProbability, row.prediction))
\end{minted}
\includegraphics[width=\linewidth]{image3} %\vspace{-1.5em}


\section*{Assignment 4}

\emph{ Do the exercise to learn ML pipeline in }
\href{https://spark.apache.org/docs/2.4.0/ml-pipeline.html#example-pipeline
}{Apache Docs}.
 

\begin{minted}[breaklines,bgcolor=light-gray]{shell-session}
from pyspark.ml import Pipeline
from pyspark.ml.feature import HashingTF, Tokenizer

training = spark.createDataFrame([
	(0, "a b c d e spark", 1.0),
	(1, "b d", 0.0),
	(2, "spark f g h", 1.0),
	(3, "hadoop mapreduce", 0.0)
], ["id", "text", "label"])

tokenizer = Tokenizer(inputCol="text", outputCol="words")
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")
lr = LogisticRegression(maxIter=10, regParam=0.001)
pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])
\end{minted}
\includegraphics[width=\linewidth]{image4.1} %\vspace{-1.5em}


\begin{minted}[breaklines,bgcolor=light-gray]{shell-session}
model = pipeline.fit(training)

test = spark.createDataFrame([
	(4, "spark i j k"),
	(5, "l m n"),
	(6, "spark hadoop spark"),
	(7, "apache hadoop")
], ["id", "text"])

prediction = model.transform(test)
selected = prediction.select("id", "text", "probability", "prediction")
for row in selected.collect():
	rid, text, prob, prediction = row
	print("(%d, %s) --> prob=%s, prediction=%f" % (rid, text, str(prob), prediction))
\end{minted}
\includegraphics[width=\linewidth]{image4.2} %\vspace{-1.5em}


\clearpage


\section*{Assignment 5}
\emph{ Learn a model using the ML pipeline and then test it and show the results. We will use 
	\hyperref{CIKM2020 AnalytiCup COVID-19 Retweet}{} 
	Prediction Challenge dataset. The CIKM is one of 
	the best conferences in Machine Learning world.
 }

\begin{minted}[breaklines,bgcolor=light-gray,fontsize=\scriptsize]{python}
from pyspark.ml.linalg import Vectors
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
from pyspark.ml.feature import HashingTF, Tokenizer, VectorAssembler, VectorIndexer
from pyspark.sql.functions import split

schema = 'TweetId LONG, Username STRING, Timestamp TIMESTAMP, Followers INT, Friends INT, Retweets INT, Favorites INT, Entities STRING, Sentiment STRING, Mentions STRING, Hashtags STRING, URLs STRING'

df = spark.read.schema(schema) \
  .option("delimiter", "\t") \
  .option("timestampFormat","EEE MMM dd HH:mm:ss Z yyyy") \
  .csv("/user/data/CSC534BDA/COVID19-Retweet/TweetsCOV19-train.tsv")
df.printSchema()

sent_col = split(df['Sentiment'], ' ')
df = df.withColumn('Positivity', sent_col.getItem(0).cast('INT'))
df = df.withColumn('Negativity', sent_col.getItem(1).cast('INT'))

(trainingData, testData) = df.randomSplit([0.8, 0.2])

tokenizer = Tokenizer(inputCol="Entities", outputCol="words")
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="Words")
assembler = VectorAssembler(outputCol='features', \
  inputCols=['Followers', 'Friends', 'Favorites', 'Positivity', 'Negativity'])

estimator = LogisticRegression(labelCol='Retweets', featuresCol='features', maxIter=10, regParam=0.001)

pipeline = Pipeline(stages=[tokenizer, hashingTF, assembler, lr])
model = pipeline.fit(trainingData)

# Test

predictions = model.transform(testData)
evaluator = RegressionEvaluator(predictionCol="prediction", \
  labelCol="Retweets",metricName="r2")
print("R Squared (R2) on test data = %g" % \
  evaluator.evaluate(predictions))
predictions.select("prediction","Retweets","features").show(5)
\end{minted}
\includegraphics[width=\linewidth]{image5.5} \vspace{1.5em}


For this model we achieve a
($R^2 = 0.251982$)
which, while still a weak performer, 
is a somewhat significant predictor. 

Unfortunately we were unable to utilize some of the other metrics as the hashing of the tokenized versions of both Entities and Mentions caused heap overflow.

While the small sample of the data set suggested that the retweets numbers fit a Tweedie distribution, the logistic regression ended up out-performing all of the other models tested.

\clearpage

\section*{Appendix:}
The following are all of the models that were used to explore the provided data before the final selection was made.

\begin{minted}[breaklines,bgcolor=light-gray]{shell-session}
from pyspark.ml.linalg import Vectors
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
from pyspark.ml.feature import HashingTF, Tokenizer

schema = 'TweetId LONG, Username STRING, Timestamp TIMESTAMP, Followers INT, Friends INT, Retweets INT, Favorites INT, Entities STRING, Sentiment STRING, Mentions STRING, Hashtags STRING, URLs STRING'

# Define the Schema 

df = spark.read.schema(schema) \
	.option("delimiter", "\t") \
	.option("timestampFormat","EEE MMM dd HH:mm:ss Z yyyy") \
	.csv("/user/data/CSC534BDA/COVID19-Retweet/TweetsCOV19-train.tsv")
df.printSchema()

# Fix Sentiment

sent_col = split(df['Sentiment'], ' ')
df = df.withColumn('Positivity', sent_col.getItem(0).cast('INT'))
df = df.withColumn('Negativity', sent_col.getItem(1).cast('INT'))

# Split Data

assembler = VectorAssembler( \
	outputCol='features', \
	inputCols=['Followers', 'Friends', 'Positivity', 'Negativity'])
features_df = assembler.transform(df)

(trainingData, testData) = features_df.randomSplit([0.8, 0.2])
\end{minted}
\includegraphics[width=\linewidth]{image5.1} %\vspace{-1.5em}


\begin{minted}[breaklines,bgcolor=light-gray]{shell-session}
from pyspark.ml.regression import GeneralizedLinearRegression

glr = GeneralizedLinearRegression(labelCol='Retweets', featuresCol='features', \
family="gaussian", link="identity", maxIter=10, regParam=0.3)
glr_model = glr.fit(trainingData)

glr_predictions = glr_model.transform(testData)
glr_evaluator = RegressionEvaluator(predictionCol="prediction", \
labelCol="Retweets",metricName="r2")
print("R Squared (R2) on test data = %g" % \
glr_evaluator.evaluate(glr_predictions))
glr_predictions.select("prediction","Retweets","features").show(5)
\end{minted}
\includegraphics[width=\linewidth]{image5.2} %\vspace{-1.5em}

Logistic regression was performed and did not give very good results. 
($R^2 = -0.0098882$)
Slightly worse than a fixed predictor. 
Next we will try a decision tree regression to compensate for the highly irregular distribution of retweets.

\begin{minted}[breaklines,bgcolor=light-gray]{shell-session}
from pyspark.ml.regression import DecisionTreeRegressor

dt_model = DecisionTreeRegressor(labelCol='Retweets', featuresCol='features').fit(trainingData)

dt_predictions = dt_model.transform(testData)
dt_evaluator = RegressionEvaluator(predictionCol="prediction", \
	labelCol="Retweets",metricName="r2")
print("R Squared (R2) on test data = %g" % \
	dt_evaluator.evaluate(dt_predictions))
dt_predictions.select("prediction","Retweets","features").show(5)
\end{minted}
\includegraphics[width=\linewidth]{image5.3} %\vspace{-1.5em}

This model performs worse than the linear regression.
($R^2 = -0.0693814$)


\begin{minted}[breaklines,bgcolor=light-gray]{shell-session}
from pyspark.ml.regression import IsotonicRegression

ir = IsotonicRegression(labelCol='Retweets', featuresCol='features', regParam=0.3)
ir_model = ir.fit(trainingData)

ir_predictions = ir_model.transform(testData)
ir_evaluator = RegressionEvaluator(predictionCol="prediction", \
	labelCol="Retweets",metricName="r2")
print("R Squared (R2) on test data = %g" % \
	ir_evaluator.evaluate(ir_predictions))
ir_predictions.select("prediction","Retweets","features").show(5)
\end{minted}
\includegraphics[width=\linewidth]{image5.4} %\vspace{-1.5em}

The piece-wise fit of an isotonic regression provides 
us with the first model to perform better than a fixed model 
($R^2 = 0.0224688$)
but is still essentially an insignificant predictor.


\end{document}


\begin{minted}[breaklines,bgcolor=light-gray]{shell-session}

\end{minted}
\includegraphics[width=\linewidth]{image1} %\vspace{-1.5em}


\includegraphics[width=\linewidth]{image1} %\vspace{-1.5em}
\begin{minted}[breaklines,bgcolor=light-gray,fontsize=\scriptsize]{shell-session}
\end{minted}
\mintinline[bgcolor=light-gray]{bash}{} \begin{enumerate}[before=\itshape,font=\normalfont,label=\alph*.]
\end{enumerate}