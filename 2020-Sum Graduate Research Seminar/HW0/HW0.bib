@article{Goldreich1987,
abstract = {We present a polynomial-time algorithm that, given ss a input the description of a game with incomplete information and any number of players, produces a protocol for playing the game that ‘leaks no partial information, provided the majority of the players is honest. Our algorithm automatically solves all the multi-party protocol problems addressed in complexity-based cryptography during the lsst 10 years. It actually is a completeness Itheorem for the cias of distributed protocols with honest majority. Such completeness theorem is optimal in the sense that, if the majority of the players is not honest, some protocol problems have no efficient solution.},
author = {Goldreich, O. and Micali, S. and Wigderson, A.},
doi = {10.1145/28395.28420},
file = {:C$\backslash$:/Users/brand/Downloads/How{\_}to{\_}play{\_}ANY{\_}mental{\_}game.pdf:pdf},
number = {January},
pages = {218--229},
title = {{How to play ANY mental game}},
year = {1987}
}
@inproceedings{Paillier1999,
author = {Paillier, Pascal},
booktitle = {International conference on the theory and applications of cryptographic techniques},
file = {:C$\backslash$:/Users/brand/Downloads/Paillier1999{\_}Chapter{\_}Public-KeyCryptosystemsBasedOn.pdf:pdf},
pages = {223--238},
publisher = {Springer},
title = {{Public-key cryptosystems based on composite degree residuosity classes}},
year = {1999}
}
@inproceedings{Lindell2000,
author = {Lindell, Yehuda and Pinkas, Benny},
booktitle = {Annual International Cryptology Conference},
file = {:C$\backslash$:/Users/brand/Downloads/Lindell-Pinkas2000{\_}Chapter{\_}PrivacyPreservingDataMining.pdf:pdf},
pages = {36--54},
publisher = {Springer},
title = {{Privacy preserving data mining}},
year = {2000}
}
@book{Goldreich2004,
abstract = {Focuses on the basic mathematical tools needed for cryptographic design: computational difficulty (one-way functions), pseudorandomness and zero-knowledge proofs.},
author = {Goldreich, Oded},
booktitle = {Foundations of Cryptography},
doi = {10.1017/cbo9780511721656},
file = {:C$\backslash$:/Users/brand/Downloads/Foundations{\_}of{\_}cryptography{\_}II{\_}Basic{\_}applications.pdf:pdf},
isbn = {9780511721656},
keywords = {0521791723},
number = {January 2004},
title = {{Foundations of Cryptography}},
year = {2004}
}
@article{Laur2006,
abstract = {We propose private protocols implementing the Kernel Adatron and Kernel Perceptron learning algorithms, give private classification protocols and private polynomial kernel computation protocols. The new protocols return their outputs - either the kernel value, the classifier or the classifications - in encrypted form so that they can be decrypted only by a common agreement by the protocol participants. We show how to use the encrypted classifications to privately estimate many properties of the data and the classifier. The new SVM classifiers are the first to be proven private according to the standard cryptographic definitions. Copyright 2006 ACM.},
author = {Laur, Sven and Lipmaa, Helger and Mielik{\"{a}}ihen, Taneli},
doi = {10.1145/1150402.1150477},
file = {:C$\backslash$:/Users/brand/Downloads/1150402.1150477.pdf:pdf},
isbn = {1595933395},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
keywords = {Kernel Methods,Privacy Preserving Data Mining},
pages = {618--624},
title = {{Cryptographically private support vector machines}},
volume = {2006},
year = {2006}
}
@article{Barni2009,
abstract = {Diagnostic and classification algorithms play an important role in data analysis, with applications in areas such as health care, fault diagnostics, or benchmarking. Branching programs (BP) is a popular representation model for describing the underlying classification/diagnostics algorithms. Typical application scenarios involve a client who provides data and a service provider (server) whose diagnostic program is run on client's data. Both parties need to keep their inputs private. We present new, more efficient privacy-protecting protocols for remote evaluation of such classification/diagnostic programs. In addition to efficiency improvements, we generalize previous solutions - we securely evaluate private linear branching programs (LBP), a useful generalization of BP that we introduce. We show practicality of our solutions: we apply our protocols to the privacy-preserving classification of medical ElectroCardioGram (ECG) signals and present implementation results. Finally, we discover and fix a subtle security weakness of the most recent remote diagnostic proposal, which allowed malicious clients to learn partial information about the program. {\textcopyright} 2009 Springer Berlin Heidelberg.},
author = {Barni, Mauro and Failla, Pierluigi and Kolesnikov, Vladimir and Lazzeretti, Riccardo and Sadeghi, Ahmad Reza and Schneider, Thomas},
doi = {10.1007/978-3-642-04444-1_26},
file = {:C$\backslash$:/Users/brand/Downloads/Secure{\_}Evaluation{\_}of{\_}Private{\_}Linear{\_}Branching{\_}Prog.pdf:pdf},
isbn = {3642044433},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {July 2015},
pages = {424--439},
title = {{Secure evaluation of private linear branching programs with medical applications}},
volume = {5789 LNCS},
year = {2009}
}
@article{Gentry2009,
abstract = {We propose the first fully homomorphic encryption scheme, solving a central open problem in cryptography. Such a scheme allows one to compute arbitrary functions over encrypted data without the decryption key {\{} i.e., given encryptions E(m1); : : : ;E(mt) of m1; : : : ;mt, one can e±ciently compute a compact ciphertext that encrypts f(m1; : : : ;mt) for any e±- ciently computable function f. This problem was posed by Rivest et al. in 1978. Fully homomorphic encryption has numerous applications. For example, it enables private queries to a search engine {\{} the user submits an encrypted query and the search engine computes a succinct encrypted answer without ever looking at the query in the clear. It also enables searching on encrypted data {\{} a user stores encrypted ¯les on a remote ¯le server and can later have the server retrieve only ¯les that (when decrypted) satisfy some boolean constraint, even though the server cannot decrypt the ¯les on its own. More broadly, fully homomorphic encryption improves the e±ciency of secure multiparty computation. Our construction begins with a somewhat homomorphic $\backslash$boostrappable" encryption scheme that works when the function f is the scheme's own decryption function. We then show how, through recursive self-embedding, bootstrappable encryption gives fully homo- morphic encryption. The construction makes use of hard problems on ideal lattices.}}}},
author = {Gentry, Craig},
doi = {10.1145/1536414.1536440},
file = {:C$\backslash$:/Users/brand/Downloads/craig-thesis.pdf:pdf},
journal = {Dissertation},
number = {September},
pages = {169},
title = {{A Fully Homomorphic Encryption Scheme}},
url = {http://cs.au.dk/{~}stm/local-cache/gentry-thesis.pdf},
year = {2009}
}
@inproceedings{Graepel2012,
author = {Graepel, Thore and Lauter, Kristin and Naehrig, Michael},
booktitle = {International Conference on Information Security and Cryptology},
file = {:C$\backslash$:/Users/brand/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Graepel, Lauter, Naehrig - 2012 - ML confidential Machine learning on encrypted data.pdf:pdf},
pages = {1--21},
publisher = {Springer},
title = {{ML confidential: Machine learning on encrypted data}},
year = {2012}
}
@inproceedings{Bost2015,
abstract = {Machine learning classification is used for numer- ous tasks nowadays, such as medical or genomics predictions, spam detection, face recognition, and financial predictions. Due to privacy concerns, in some of these applications, it is important that the data and the classifier remain confidential. In this work, we construct three major classification protocols that satisfy this privacy constraint: hyperplane decision, Na{\"{i}}ve Bayes, and decision trees. We also enable these protocols to be combined with AdaBoost. At the basis of these constructions is a new library of building blocks, which enables constructing a wide range of privacy-preserving classifiers; we demonstrate how this library can be used to construct other classifiers than the three mentioned above, such as a multiplexer and a face detection classifier. We implemented and evaluated our library and our classifiers. Our protocols are efficient, taking milliseconds to a few seconds to perform a classification when running on real medical datasets.},
author = {Bost, Raphael and Popa, Raluca Ada and Tu, Stephen and Goldwasser, Shafi},
booktitle = {NDSS},
doi = {10.14722/ndss.2015.23241},
file = {:C$\backslash$:/Users/brand/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bost et al. - 2015 - Machine learning classification over encrypted data.pdf:pdf},
pages = {14},
title = {{Machine Learning Classification over Encrypted Data}},
volume = {4324},
year = {2015}
}
@article{Hunt2016,
abstract = {Users of modern data-processing services such as tax preparation or genomic screening are forced to trust them with data that the users wish to keep secret. Ryoan protects secret data while it is processed by services that the data owner does not trust. Accomplishing this goal in a distributed setting is difficult because the user has no control over the service providers or the computational platform. Confining code to prevent it from leaking secrets is notoriously difficult, but Ryoan benefits from new hardware and a request-oriented data model. Ryoan provides a distributed sandbox, leveraging hardware enclaves (e.g., Intel's software guard extensions (SGX) [15]) to protect sandbox instances from potentially malicious computing platforms. The protected sandbox instances confine untrusted data-processing modules to prevent leakage of the user's input data. Ryoan is designed for a request-oriented data model, where confined modules only process input once and do not persist state about the input. We present the design and prototype implementation of Ryoan and evaluate it on a series of challenging problems including email filtering, heath analysis, image processing and machine translation.},
author = {Hunt, Tyler and Zhu, Zhiting and Xu, Yuanzhong and Peter, Simon and Witchel, Emmett},
file = {:C$\backslash$:/Users/brand/Downloads/3231594.pdf:pdf},
isbn = {9781931971331},
journal = {Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016},
number = {4},
pages = {533--549},
title = {{Ryoan: A distributed sandbox for untrusted computation on secret data}},
volume = {35},
year = {2016}
}
@article{Khedr2016,
abstract = {Homomorphic encryption (HE) systems enable computations on encrypted data, without decrypting and without knowledge of the secret key. In this work, we describe an optimized Ring Learning With Errors (RLWE) based implementation of a variant of the HE system recently proposed by Gentry, Sahai and Waters (GSW). Although this system was widely believed to be less efficient than its contemporaries, we demonstrate quite the opposite behavior for a large class of applications. We first highlight and carefully exploit the algebraic features of the system to achieve significant speedup over the state-of-the-art HE implementation, namely the IBM homomorphic encryption library (HElib). We introduce several optimizations on top of our HE implementation, and use the resulting scheme to construct a homomorphic Bayesian spam filter, secure multiple keyword search, and a homomorphic evaluator for binary decision trees. Our results show a factor of 10× improvement in performance (under the same security settings and CPU platforms) compared to IBM HElib for these applications. Our system is built to be easily portable to GPUs (unlike IBM HElib) which results in an additional speedup of up to a factor of 103.5× to offer an overall speedup of 1,035 ×.},
author = {Khedr, Alhassan and Gulak, Glenn and Vaikuntanathan, Vinod},
doi = {10.1109/TC.2015.2500576},
file = {:C$\backslash$:/Users/brand/Downloads/07328732.pdf:pdf},
issn = {00189340},
journal = {IEEE Transactions on Computers},
keywords = {Bayesian filter,FHE,GPU,Homomorphic encryption,Ring LWE,decision trees,implementation,secure search},
number = {9},
pages = {2848--2858},
publisher = {IEEE},
title = {{SHIELD: Scalable Homomorphic Implementation of Encrypted Data-Classifiers}},
volume = {65},
year = {2016}
}
@article{Liu2017,
abstract = {Machine learning models hosted in a cloud service are increasingly popular but risk privacy: Clients sending prediction requests to the service need to disclose potentially sensitive information. In this paper, we explore the problem of privacy-preserving predictions: After each prediction, the server learns nothing about clients' input and clients learn nothing about the model. We present MiniONN, the first approach for transforming an existing neural network to an oblivious neural network supporting privacy-preserving predictions with reasonable efficiency. Unlike prior work, MiniONN requires no change to how models are trained. To this end,we design oblivious protocols for commonly used operations in neural network prediction models.We show that MiniONN outperforms existing work in terms of response latency and message sizes. We demonstrate the wide applicability of MiniONN by transforming several typical neural network models trained from standard datasets.},
author = {Liu, Jian and Juuti, Mika and Lu, Yao and Asokan, N.},
doi = {10.1145/3133956.3134056},
file = {:C$\backslash$:/Users/brand/Downloads/3133956.3134056.pdf:pdf},
isbn = {9781450349468},
issn = {15437221},
journal = {Proceedings of the ACM Conference on Computer and Communications Security},
keywords = {Machine Learning,Neural Network Predictions,Privacy,Secure Twoparty Computation},
pages = {619--631},
title = {{Oblivious neural network predictions via MiniONN transformations}},
year = {2017}
}
@article{Song2017,
abstract = {Machine learning (ML) is becoming a commodity. Numerous ML frameworks and services are available to data holders who are not ML experts but want to train predictive models on their data. It is important that ML models trained on sensitive inputs (e.g., personal images or documents) not leak too much information about the training data. We consider a maliciousMLprovider who supplies model-training code to the data holder, does not observe the training, but then obtains white-or black-box access to the resulting model. In this setting, we design and implement practical algorithms, some of them very similar to standard ML techniques such as regularization and data augmentation, that "memorize" information about the training dataset in the model-yet the model is as accurate and predictive as a conventionally trained model. We then explain how the adversary can extract memorized information from the model. We evaluate our techniques on standard ML tasks for image classification (CIFAR10), face recognition (LFW and FaceScrub), and text analysis (20 Newsgroups and IMDB). In all cases, we show how our algorithms create models that have high predictive power yet allow accurate extraction of subsets of their training data.},
author = {Song, Congzheng and Ristenpart, Thomas and Shmatikov, Vitaly},
doi = {10.1145/3133956.3134077},
file = {:C$\backslash$:/Users/brand/Downloads/3133956.3134077.pdf:pdf},
isbn = {9781450349468},
issn = {15437221},
journal = {Proceedings of the ACM Conference on Computer and Communications Security},
keywords = {Machine Learning,Privacy},
pages = {587--601},
title = {{Machine learning models that remember too much}},
year = {2017}
}
@article{Li2018,
abstract = {For meeting diverse requirements of data analysis, the machine learning classifier has been provided as a tool to evaluate data in many applications. Due to privacy concerns of preventing disclosing sensitive information, data owners often suppress their data for an untrusted trainer to train a classifier. Some existing work proposed privacy-preserving solutions for learning algorithms, which allow a trainer to build a classifier over the data from a single owner. However, they cannot be directly used in the multi-owner setting where each owner is not totally trusted for each other. In this paper, we propose a novel privacy-preserving Naive Bayes learning scheme with multiple data sources. The proposed scheme enables a trainer to train a Naive Bayes classifier over the dataset provided jointly by different data owners, without the help of a trusted curator. The training result can achieve ϵ-differential privacy while the training will not break the privacy of each owner. We implement the prototype of the scheme and conduct corresponding experiment.},
author = {Li, Tong and Li, Jin and Liu, Zheli and Li, Ping and Jia, Chunfu},
doi = {10.1016/j.ins.2018.02.056},
file = {:C$\backslash$:/Users/brand/Downloads/1-s2.0-S0020025518301415-main.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Differential privacy,Naive Bayes classification,Privacy-preserving},
pages = {89--104},
publisher = {Elsevier Inc.},
title = {{Differentially private Naive Bayes learning over multiple data sources}},
url = {https://doi.org/10.1016/j.ins.2018.02.056},
volume = {444},
year = {2018}
}
@article{Gao2018,
abstract = {Naive Bayes (NB) is a simple but highly practical classifier, with a wide range of applications including spam filters, cancer diagnosis and face recognition, to name a few examples only. Consider a situation where a user requests a classification service from a NB classifier server, both the user and the server do not want to reveal their private data to each other. This paper focuses on constructing a privacy-preserving NB classifier that is resistant to an easy-to-perform, but difficult-to-detect attack, which we call the substitution-then-comparison (STC) attack. Without resorting to fully homomorphic encryptions, which has a high computational overhead, we propose a scheme which avoids information leakage under the STC attack. Our key technique involves the use of a “double-blinding” technique, and we show how to combine it with additively homomorphic encryptions and oblivious transfer to hide both parties' privacy. Furthermore, a completed evaluation shows that the construction is highly practical - most of the computations are in the server's offline phase, and the overhead of online computation and communication is small for both parties.},
author = {zhi Gao, Chong and Cheng, Qiong and He, Pei and Susilo, Willy and Li, Jin},
doi = {10.1016/j.ins.2018.02.058},
file = {:C$\backslash$:/Users/brand/Downloads/1-s2.0-S0020025518301543-main.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Naive Bayes classifier,Privacy-preserving,Substitution-then-comparison (STC) attack},
pages = {72--88},
publisher = {Elsevier Inc.},
title = {{Privacy-preserving Naive Bayes classifiers secure against the substitution-then-comparison attack}},
url = {https://doi.org/10.1016/j.ins.2018.02.058},
volume = {444},
year = {2018}
}
@article{SadeghRiazi2018,
abstract = {We present Chameleon, a novel hybrid (mixed-protocol) framework for secure function evaluation (SFE) which enables two parties to jointly compute a function without disclosing their private inputs. Chameleon combines the best aspects of generic SFE protocols with the ones that are based upon additive secret sharing. In particular, the framework performs linear operations in the ring Z2l using additively secret shared values and nonlinear operations using Yao's Garbled Circuits or the Goldreich-Micali-Wigderson protocol. Chameleon departs from the common assumption of additive or linear secret sharing models where three or more parties need to communicate in the online phase: the framework allows two parties with private inputs to communicate in the online phase under the assumption of a third node generating correlated randomness in an offline phase. Almost all of the heavy cryptographic operations are precomputed in an offline phase which substantially reduces the communication overhead. Chameleon is both scalable and significantly more efficient than the ABY framework (NDSS'15) it is based on. Our framework supports signed fixed-point numbers. In particular, Chameleon's vector dot product of signed fixed-point numbers improves the efficiency of mining and classification of encrypted data for algorithms based upon heavy matrix multiplications. Our evaluation of Chameleon on a 5 layer convolutional deep neural network shows 133x and 4.2x faster executions than Microsoft CryptoNets (ICML'16) and MiniONN (CCS'17), respectively.},
archivePrefix = {arXiv},
arxivId = {1801.03239},
author = {{Sadegh Riazi}, M. and Songhori, Ebrahim M. and Weinert, Christian and Schneider, Thomas and Tkachenko, Oleksandr and Koushanfar, Farinaz},
doi = {10.1145/3196494.3196522},
eprint = {1801.03239},
file = {:C$\backslash$:/Users/brand/Downloads/3196494.3196522.pdf:pdf},
isbn = {9781450355766},
journal = {ASIACCS 2018 - Proceedings of the 2018 ACM Asia Conference on Computer and Communications Security},
keywords = {Deep Neural Networks,Garbled Circuits,Machine Learning,Secret Sharing,Secure Computation},
pages = {707--721},
title = {{Chameleon: A hybrid secure computation framework for machine learning applications}},
year = {2018}
}
@article{Yasumura2019,
abstract = {Machine learning classification has a wide range of applications. In the big data era, a client may want to outsource classification tasks to reduce the computational burden at the client. Meanwhile, an entity may want to provide a classification model and classification services to such clients. However, applications such as medical diagnosis require sensitive data that both parties may not want to reveal. Fully homomorphic encryption (FHE) enables secure computation over encrypted data without decryption. By applying FHE, classification can be outsourced to a cloud without revealing any data. However, existing studies on classification over FHE do not achieve the scenario of outsourcing classification to a cloud while preserving the privacy of the classification model, client's data and result. In this work, we apply FHE to a na{\"{i}}ve Bayes classifier and, to the best of our knowledge, propose the first concrete secure classification protocol that satisfies the above scenario.},
author = {Yasumura, Yoshiko and Ishimaki, Yu and Yamana, Hayato},
doi = {10.1145/3366030.3366056},
file = {:C$\backslash$:/Users/brand/Downloads/3366030.3366056.pdf:pdf},
isbn = {9781450371797},
journal = {ACM International Conference Proceeding Series},
keywords = {Classification,Data privacy,Fully homomorphic encryption,Machine learning,Privacy-preservation},
title = {{Secure na{\"{i}}ve bayes classification protocol over encrypted data using fully homomorphic encryption}},
year = {2019}
}
