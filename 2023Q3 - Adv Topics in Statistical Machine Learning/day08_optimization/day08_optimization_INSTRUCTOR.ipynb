{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization (Part 1)\n",
    "\n",
    "In this activity, students will add Glorot weight initializtion (STEP 1), and Momentum (using velocity; STEP 2). Look for the locations to write this code after the (first) class definition for ANN (Students will add code in code cells 5 & 6)\n",
    "\n",
    "After implementing these code cells, students should run the notebook and review the performance of the models on XOR.   Note that the random seeds for initialization are not constrained, so multiple repetitions should be accomplished to see how the various configuraitons perform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBAL VARIABLES\n",
    "\n",
    "EPS = np.finfo(np.float32).eps  #minimum floating point number for numerical stability in calculations\n",
    "SHOW_EXAMPLES = False  #shows graphs and output for individually-trained models.\n",
    "REPS=20  #number of times to repeat model training experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug printing tool\n",
    "DEBUG = True\n",
    "def debug(*kargs):\n",
    "    if DEBUG:\n",
    "        print(*kargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the activation functions and their derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "\n",
    "def dSigmoid(x):  # derivative of sigmoid\n",
    "    s = sigmoid(x)\n",
    "    return np.multiply(s, (1-s))\n",
    "\n",
    "\n",
    "def relu(z):  # rectified linear unit activation\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "\n",
    "def dRelu(z):\n",
    "    \"\"\" \n",
    "    Derivative of Rectified Linear Unit\n",
    "\n",
    "    \"\"\"\n",
    "    return 1 * (z > 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Setting up the basic ANN class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    data = []\n",
    "    layers = []\n",
    "    inputWidth = 1\n",
    "    outputWidth = 1\n",
    "\n",
    "    class Layer:\n",
    "\n",
    "        \"\"\"class defining the elements of an ANN layer\"\"\"\n",
    "\n",
    "        @staticmethod\n",
    "        def gradl2norm(weight_vals):\n",
    "            \"\"\"returns the gradient of the l2 norm with respect to the weights\"\"\"\n",
    "            return weight_vals\n",
    "    \n",
    "        @staticmethod\n",
    "        def gradl1norm(weight_vals):\n",
    "            \"\"\"returns the gradient of the l1 norm with respect to the weights\"\"\"\n",
    "            return np.sign(weight_vals)\n",
    "\n",
    "        @staticmethod\n",
    "        def l2norm(vals):\n",
    "            \"\"\"returns the l2 norm of the vals\"\"\"\n",
    "            return np.linalg.norm(vals,ord=2)\n",
    "    \n",
    "        @staticmethod\n",
    "        def l1norm(vals):\n",
    "            \"\"\"returns the l1 norm of the vals\"\"\"\n",
    "            return np.linalg.norm(vals,ord=1)\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.w = []\n",
    "            self.b = []\n",
    "            self.lam = 0  #for weight regularization\n",
    "            self.weightRegFunction = self.l2norm  #placeholder for regularization function\n",
    "            self.weightRegGradFunction = self.gradl2norm   #placeholder for regularization function\n",
    "            self.vel_w=[] # for standard momentum of weight gradients\n",
    "            self.vel_b=[] # for standard momentum of bias gradients\n",
    "            self.nodecount = []\n",
    "            self.activation_fcn = []\n",
    "            self.activation_fcn_derivative = []\n",
    "            self.orderNumber = []\n",
    "            self.previous = None  # link to previous layer\n",
    "            self.next = None  # link to next layer\n",
    "            \n",
    "\n",
    "        def set_weights(self, w, b):\n",
    "            \"\"\"set the weights and bias for the layer.  Layer weights should have dimesion: (thislayer_nodecount, previouslayer_nodecount)\n",
    "            the dimension of the bias should be (thislayer_nodecount,1)\"\"\"\n",
    "            self.w = w\n",
    "            self.b = b\n",
    "            return self\n",
    "        \n",
    "            \n",
    "        def initialize_velocity(self):\n",
    "            assert self.w.size > 0\n",
    "            assert self.b.size > 0\n",
    "            self.vel_w = np.zeros(self.w.shape)  # same shape as hidden weight matrix [rows = to, columns = from]\n",
    "            self.vel_b = np.zeros(self.b.shape)  # same shape as hidden biases (column vector)\n",
    "        \n",
    "\n",
    "        def set_lambda(self, lam):\n",
    "            self.lam = lam\n",
    "\n",
    "        def set_weightRegFunction(self, fcn, d_fcn):\n",
    "            self.weightRegFunction = fcn\n",
    "            self.weightRegGradFunction = d_fcn\n",
    "            \n",
    "        def set_activation(self, activation_fcn):\n",
    "            self.activation_fcn = activation_fcn\n",
    "            return self\n",
    "\n",
    "        def set_activation_deriv(self, activation_fcn):\n",
    "            if activation_fcn == sigmoid:\n",
    "                self.activation_fcn_derivative = copy.deepcopy(dSigmoid)\n",
    "            elif activation_fcn == relu:\n",
    "                self.activation_fcn_derivative = copy.deepcopy(dRelu)\n",
    "            else:\n",
    "                self.activation_fcn_derivative = None\n",
    "                \n",
    "        def display_params(self):\n",
    "            \"\"\"displays the weights and biases of the layer (rows = to, colums = from)\"\"\"\n",
    "            for outNum in range(self.w.shape[0]):\n",
    "                print(self.w[outNum,:], \"  \", self.b[outNum])\n",
    "        \n",
    "\n",
    "        def compute_pre_activation(self, inputs):\n",
    "            net = np.dot(self.w, inputs) + self.b\n",
    "            return net\n",
    "\n",
    "        def compute_bias_gradient(self, gradient):\n",
    "            g = np.mean(gradient, axis=1)[:, np.newaxis]  # no regularization\n",
    "            return g\n",
    "\n",
    "        def compute_weight_gradient(self, inputs, gradient):\n",
    "            g = np.dot(gradient, inputs.T)\n",
    "            g = g/inputs.shape[1]   #divide by m (batchsize)\n",
    "            return g\n",
    "\n",
    "        def compute_activation(self, net):\n",
    "            return self.activation_fcn(net)\n",
    "\n",
    "        def compute_activation_derivative(self, net):\n",
    "            return self.activation_fcn_derivative(net)\n",
    "\n",
    "        def compute_activation_gradient(self, d_activation, gradient):\n",
    "            g = np.multiply(gradient, d_activation)\n",
    "            return g\n",
    "\n",
    "        def compute_forward(self, inputs):\n",
    "            \"\"\"Returns layer ouput from input (shape = [nodeCount, input]) of the weighted input plus bias\n",
    "            input shape must be [lastlayer_nodeCount, samples] or [featurecount, samplecount] \"\"\"\n",
    "            net = self.compute_pre_activation(self, inputs)\n",
    "            layer_out = self.compute_activation(net)\n",
    "            return layer_out\n",
    "\n",
    "\n",
    "        def regularization_grad_weights(self,vals):\n",
    "            \"\"\"computes the regularization cost for the current layer weights\"\"\"\n",
    "            mylam = self.lam\n",
    "            myregs = self.weightRegGradFunction(vals)\n",
    "            return mylam*myregs\n",
    "        \n",
    "        \n",
    "        def compute_layer_gradients(self, net, activation, gradient):\n",
    "            \"\"\" computes the loss gradient with respect to desired output of the layer\n",
    "            a set of desired targets is assumed to be matrix of shape [nodecount, samples]: SGD will have [nodecount,1]\n",
    "            hidden_inputs is assumed to be a matrix of shape [hiddenNodeCount, samples]\n",
    "            \n",
    "            This follows algorithm 6.4 line by line in the book!\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "            # f'(a(k))\n",
    "            d_activation = self.compute_activation_derivative(net)  # derivative of sigmoid:  shape = [NodeCount, samples]\n",
    "            \n",
    "            # g <- g * f'(a(k))\n",
    "            g_loss = self.compute_activation_gradient(d_activation, gradient)  # shape = [NodeCount, samples]  for outer layer\n",
    "            \n",
    "            # Delta_b(k) J = g (Take the mean across all samples (batch))\n",
    "            g_loss_b = self.compute_bias_gradient(g_loss)  # mean gradient with respect to BIAS, shape = [NodeCount, 1]\n",
    "            \n",
    "            # Delta w(k) J = g * h(k-1) +lam*regularizationGrad w.r.t weights\n",
    "            g_loss_w = self.compute_weight_gradient(activation, g_loss) + self.regularization_grad_weights(self.w) # [thisLayerNodecount,prevLayerOutputcount]  \n",
    "            #NOTE - regularization grad weights NOT WORKING YET\n",
    "            \n",
    "            # g <- W(k).T * g\n",
    "            g_loss_backprop = np.dot(self.w.T, g_loss)  # gradient to propagate back, shape = [hiddenNodeCount,samples]\n",
    "            \n",
    "            return g_loss_w, g_loss_b, g_loss_backprop\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.layers = []\n",
    "        self.inputWidth = 1\n",
    "        self.outputWidth = 1\n",
    "    \n",
    "        \n",
    "    def set_input_width(self, inputWidth):\n",
    "        \"\"\"defines the input layer width for the network\"\"\"\n",
    "        self.inputWidth = inputWidth\n",
    "\n",
    "    def add_layer(self, nodecount=1, activation_fcn=relu):\n",
    "        \"\"\"adds a layer to the neural network and returns the layer\"\"\"\n",
    "        oldLayerCount = len(self.layers)\n",
    "        thislayer = ANN.Layer()\n",
    "        thislayer.orderNumber = oldLayerCount + 1\n",
    "        if oldLayerCount > 0:  # other layers have been added already\n",
    "            lastLayer = self.layers[-1]\n",
    "            #lastLayer.display_params()\n",
    "            thislayer.previous = lastLayer\n",
    "            lastLayer.next = thislayer\n",
    "            layerInputSize = lastLayer.w.shape[0]\n",
    "        else:  # this will be the first layer\n",
    "            layerInputSize = self.inputWidth\n",
    "            \n",
    "        thislayer.w = np.zeros((nodecount, layerInputSize))  #[NODECOUNT,FROM]\n",
    "        thislayer.b = np.zeros((nodecount, 1 )) #[NODECOUNT,FROM]\n",
    "        thislayer.vel_w = np.zeros(thislayer.w.shape)  # same shape as hidden weight matrix [rows = to, columns = from]\n",
    "        thislayer.vel_b = np.zeros(thislayer.b.shape)  # same shape as hidden biases (column vector)\n",
    "\n",
    "        thislayer.activation_fcn = activation_fcn\n",
    "        thislayer.set_activation_deriv(activation_fcn)\n",
    "        self.outputWidth = nodecount\n",
    "        self.layers = self.layers + [thislayer]\n",
    "        return thislayer\n",
    "    \n",
    "    def initialize(self, glorot = False, seed = None):\n",
    "        \"\"\"initialize weights & biases & velocity: overwrites current network parameters\"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.initialize_weights(glorot=glorot, seed=seed)\n",
    "            layer.initialize_velocity()\n",
    "            \n",
    "    def setL1weightNormalization(self,lam=0):\n",
    "        for idx,layer in enumerate(self.layers):\n",
    "            layer.set_lambda(lam)\n",
    "            layer.set_weightRegFunction(layer.l1norm,layer.gradl1norm)\n",
    "            print(\" Set Layer \", idx,\" weightNorm to gradl1norm with lambda = \", lam)\n",
    "            \n",
    "\n",
    "    def setL2weightNormalization(self,lam=0):\n",
    "        for idx,layer in enumerate(self.layers):\n",
    "            layer.set_lambda(lam)\n",
    "            layer.set_weightRegFunction(layer.l2norm,layer.gradl2norm )\n",
    "            print(\" Set Layer \", idx,\" weightNorm to gradl2norm with lambda = \", lam)\n",
    "\n",
    "            \n",
    "    def summary(self):\n",
    "        \"\"\"displays a summary of the model\"\"\"\n",
    "        tot_train_parameters = 0\n",
    "        print(\"\\n\")\n",
    "        print(\"Layer     Inshape     Outshape     Param #     LambdaReg\")\n",
    "        print(\"==========================================================\")\n",
    "        for lnum, layer in enumerate(self.layers):\n",
    "            inshape = layer.w.shape[1]\n",
    "            weightCount = layer.w.shape[0]*layer.w.shape[1]  #assume fully connected\n",
    "            biasCount = layer.b.shape[0]\n",
    "            thislayerparams = weightCount+biasCount\n",
    "            tot_train_parameters += thislayerparams\n",
    "            lam = layer.lam\n",
    "            print(\"% 3d       % 3d         % 3d         %3d         %3f\" %(lnum,inshape,biasCount,thislayerparams,lam))\n",
    "        print(\"==========================================================\")\n",
    "        print(\"total trainable params: \",tot_train_parameters )\n",
    "        \n",
    "    def display_params(self):\n",
    "        \"\"\"displays the weights and biases of the network (rows = to, colums = from)\"\"\"\n",
    "        print(\"\\n\")\n",
    "        print(\"input width: \", self.inputWidth)\n",
    "        for lnum, layer in enumerate(self.layers):\n",
    "            print(\"Layer \",lnum)\n",
    "            layer.display_params()\n",
    "        print(\"output width: \", layer.w.shape[0])\n",
    "                \n",
    "                \n",
    "    def forwardPropagation(self, inputs):\n",
    "        \"\"\"Compute forward pass of two layer network\n",
    "        inputs are assumed to be (shape=[sampleCount,featureCount])\n",
    "        returns a matrix of raw outputs with one row of output per node (shape=[sampleCount, outputNodeCount])\n",
    "        Internal matrices are shaped for efficiency to avoid internal transposes (columns hold observations/samples) \"\"\"\n",
    "\n",
    "        # inputs and outputs will be transposed for efficiency during forwardPropagation and untransposed before returning\n",
    "\n",
    "        nets = []\n",
    "        activations = []\n",
    "        layer_input = inputs.T\n",
    "\n",
    "        for lnum, layer in enumerate(self.layers):\n",
    "            # inputs = inputs + inputs\n",
    "            layer_net = layer.compute_pre_activation(layer_input)\n",
    "            nets.append(layer_net)\n",
    "\n",
    "            layer_out = layer.compute_activation(layer_net)\n",
    "            activations.append(layer_out)\n",
    "\n",
    "            layer_input = layer_out\n",
    "        raw_output = layer_out.T\n",
    "        return raw_output, inputs, nets, activations\n",
    "\n",
    "    def backPropagation(self, inputs, desiredOutputs, learningRate, momentum=0):\n",
    "        w_grads = []\n",
    "        b_grads = []\n",
    "        # store nets and activations for each layer\n",
    "        raw_output, _, nets, activations = self.forwardPropagation(inputs)\n",
    "        layer_desired_out = desiredOutputs\n",
    "\n",
    "        # Note: This is only part of the gradient\n",
    "        layer_grad = desiredOutputs - raw_output\n",
    "        layer_grad = layer_grad.T  #in order to match expectation for last layer output\n",
    "        prev_layer_outputs = [inputs.T] + activations  #insert inputs onto activation stream for easy computations\n",
    "\n",
    "        #  computation of full gradient handled inside the loop below\n",
    "        for lnum, layer in reversed(list(enumerate(self.layers))):\n",
    "            #get the input to this layer\n",
    "            curr_layer_input=prev_layer_outputs[lnum]\n",
    "            #get the gradients for the layer    \n",
    "            w_grad, b_grad, loss_grad = layer.compute_layer_gradients(nets[lnum], curr_layer_input, layer_grad)    \n",
    "\n",
    "            layer.update_Layer(w_grad * learningRate, b_grad * learningRate, momentum=momentum)\n",
    "            layer_grad = loss_grad\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Compute predictions using forward propagation for single binary classification at threshold\n",
    "        X is a standard dataFrame without biases (shape=[observationCount,featureCount])\n",
    "        returns a standard column vector of binary predictions in {0,1}: (shape=[observationCount, 1])\"\"\"\n",
    "        raw_predictions, net_inputs, net_lst, activation_lst = self.forwardPropagation(X)\n",
    "        preds = raw_predictions > threshold\n",
    "        return preds\n",
    "\n",
    "    def compute_mse_loss(self, inputs, desired_targets):\n",
    "        \"\"\"computes the (scalar) loss using MSE of a set of targets and sigmoid outputs\n",
    "        inputs is assumed to be a matrix of shape [samples, features]\n",
    "         desired_targets is assumed to be a matrix of shape [samples, 1]\"\"\"\n",
    "        raw_outputs = self.forwardPropagation(inputs)[0]\n",
    "        error = desired_targets - raw_outputs\n",
    "        mse = np.dot(error.T, error) / error.size\n",
    "        return mse\n",
    "\n",
    "    def compute_loss(self, raw_outputs, desired_targets):\n",
    "        \"\"\"\n",
    "        computes the (scalar) loss using Binary Cross Entropy of a set raw ouputs and desired targets \n",
    "        raw_outputs and desired_targets are assumed to be arrays of shape [samples, 1]\n",
    "        \"\"\"\n",
    "        y = desired_targets\n",
    "        bce = -np.mean( (y*np.log(raw_outputs+EPS)) + ((1-y)*np.log(1-raw_outputs+EPS)) )\n",
    "        #using EPS to prevent log(0) numerical problems\n",
    "        return bce\n",
    "    \n",
    "    \n",
    "    def fit(self, tngInputs, tngTargets, valInputs, valTargets, learningRate, learningRateDecay,\n",
    "            batchsize = 1, momentum=0, valPatience=0, tolerance=1e-2, maxEpochs = 100, verbose = True):\n",
    "        \"\"\"fit model to map tngInputs to tngTargets. If valPatience > 0 then use early stopping on valInputs & valTargets\n",
    "        returns training loss history and val loss history \"\"\"\n",
    "        done = False\n",
    "        best_model_so_far = self  #store existing model\n",
    "\n",
    "        tng_loss_history = []\n",
    "        val_loss_history = []\n",
    "        if verbose:\n",
    "            print(\"Training Model...\")\n",
    "        epoch = 0\n",
    "        #get current tng performance\n",
    "        tng_out_raw, _, _, _ = self.forwardPropagation(tngInputs)\n",
    "        tngPreds = self.predict(tngInputs)\n",
    "        tngCorrect = tngTargets == tngPreds\n",
    "        curr_train_loss = self.compute_loss(tng_out_raw, tngTargets).item()\n",
    "        tng_loss_history.append(curr_train_loss)\n",
    "\n",
    "        #get current val performance\n",
    "        val_out_raw, _, _, _ = self.forwardPropagation(valInputs)\n",
    "        valPreds = self.predict(valInputs)\n",
    "        prev_val_loss = self.compute_loss(val_out_raw, valTargets).item()\n",
    "        val_loss_history.append(prev_val_loss)\n",
    "        val_epochs_nonimproved = 0\n",
    "        training_count = tngInputs.shape[0]\n",
    "        if batchsize>training_count: batchsize=training_count #prevent sampling beyond training size\n",
    "            \n",
    "        \n",
    "        while not done:\n",
    "            epoch+=1\n",
    "            if epoch>maxEpochs: \n",
    "                done = True\n",
    "            learningRate = learningRate * learningRateDecay\n",
    "            #eval training performance\n",
    "            tng_out_raw, _, _, _ = self.forwardPropagation(tngInputs)\n",
    "            tngPreds = self.predict(tngInputs)\n",
    "            tngCorrect = tngTargets == tngPreds\n",
    "            curr_train_loss = self.compute_loss(tng_out_raw, tngTargets).item()\n",
    "            tng_loss_history.append(curr_train_loss)\n",
    "            #evaluate validation performance\n",
    "            val_out_raw, _, _, _ = self.forwardPropagation(valInputs)\n",
    "            valPreds = self.predict(valInputs)\n",
    "            cur_val_loss = self.compute_loss(val_out_raw, valTargets).item()\n",
    "\n",
    "            if cur_val_loss < tolerance:  #regular stopping\n",
    "                done = True\n",
    "                if verbose:\n",
    "                    print(\" --- Regular Stopping due to val loss < tolerance; val loss:\", cur_val_loss)\n",
    "                break\n",
    "            \n",
    "            # run an epoch of backprop\n",
    "            #shuffle the indexes of the inputs & targets simultaneously\n",
    "            order=np.random.permutation(training_count)\n",
    "            #debug(\"range check \", np.arange(training_count))\n",
    "            #debug(\"tng count:\", training_count, \";   tng index order: \", order)\n",
    "            first_tng_index=0\n",
    "            last_tng_index = batchsize\n",
    "            \n",
    "            tinp = tngInputs[order]\n",
    "            ttar = tngTargets[order]\n",
    "            \n",
    "            while last_tng_index<=training_count:\n",
    "                #get a batch\n",
    "                batchIn = tinp[first_tng_index:last_tng_index,:]\n",
    "                batchTar = ttar[first_tng_index:last_tng_index,:]\n",
    "                #train on the batch using backprop\n",
    "                self.backPropagation(batchIn, batchTar, learningRate=learningRate, momentum=momentum)\n",
    "                first_tng_index+=batchsize\n",
    "                last_tng_index+=batchsize\n",
    "                #handle mis-aligned training set sizes\n",
    "                if first_tng_index < training_count-1 and last_tng_index>training_count-1:\n",
    "                    batchIn=tinp[first_tng_index:training_count,:]\n",
    "                    batchTar=ttar[first_tng_index:training_count,:]\n",
    "                    self.backPropagation(batchIn, batchTar, learningRate=learningRate, momentum=momentum)\n",
    "            \n",
    "\n",
    "            # Early Stopping via VAL loss improvement\n",
    "            # if validation loss has not improved in patience epochs then stop\n",
    "            if cur_val_loss < prev_val_loss:\n",
    "                val_epochs_nonimproved = 0\n",
    "                prev_val_loss = cur_val_loss\n",
    "                #TODO:  save best model info\n",
    "                best_model_so_far = self\n",
    "                tempTngHistory = tng_loss_history\n",
    "                tempValHistory = val_loss_history\n",
    "                \n",
    "            else:\n",
    "                val_epochs_nonimproved+=1\n",
    "                if valPatience > 0 and val_epochs_nonimproved > valPatience :\n",
    "                    if verbose:\n",
    "                        print(\" --- EARLY STOPPING ACTIVATED AT val_epochs_nonimproved =  \",val_epochs_nonimproved)\n",
    "                    #replace this model with best model so far (based on val loss)\n",
    "                    self = best_model_so_far\n",
    "                    val_loss_history = tempValHistory\n",
    "                    tng_loss_history = tempTngHistory\n",
    "                    done=True\n",
    "\n",
    "            val_loss_history.append(cur_val_loss)\n",
    "            \n",
    "        if verbose:\n",
    "            print(\"Training Complete!\")\n",
    "\n",
    "        return tng_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1 Student Coding - Glorot initialization:\n",
    "\n",
    "Add Glorot weight initialization in Layer subclass function ```def initialize_weights(self, glorot=False, seed=None )```\n",
    "\n",
    "Note that ```m``` (fan-in) and ```n``` (fan-out) are pre-defined in the code above the student code location and available for your use.\n",
    "\n",
    "The function to be implemented is to select the correct quantity of weights to set ```w``` using values drawn from a uniform distribution between [-sqrt(6) / sqrt(fan-in + fan-out), sqrt(6) / sqrt(fan-in + fan-out)]\n",
    "\n",
    "(Glorot is not necessarily the best or the state of the art... the point of this exercise is to see *how* to change the initializtion scheme)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extending the ANN class to allow for weight initialization and momentum in the Layer class\n",
    "class ANN(ANN):\n",
    "\n",
    "    # Extending the Layer class to glorot weight initialization\n",
    "    class Layer(ANN.Layer):\n",
    "\n",
    "        def initialize_weights(self, glorot=False, seed=None ):\n",
    "            assert self.w.size > 0\n",
    "            assert self.b.size > 0\n",
    "            \n",
    "            biasShape = self.b.shape\n",
    "            weightShape = self.w.shape\n",
    "            \n",
    "            #set biases \n",
    "            self.b = np.ones(self.b.shape)  #  biases (column vector of 1s)\n",
    "            if self.activation_fcn == relu:\n",
    "                self.b = self.b * 0.1  #for relu\n",
    "            else:\n",
    "                self.b = self.b * 0.0  # for sigmoid\n",
    "            \n",
    "            np.random.seed(seed)\n",
    "            if glorot:  #use glorot initialization\n",
    "                n = self.w.shape[0]\n",
    "                m = self.w.shape[1]\n",
    "                self.w = np.zeros(self.w.shape)   #placeholder\n",
    "                \n",
    "                ############# STUDENT CODE - ADD GLOROT INITIALIZATION OF WEIGHTS (NOT BIASES)##############\n",
    "                \n",
    "                #note:  the shape of self.w should be preserved... n-by-m\n",
    "                \n",
    "                # compute the upper and lower edges of the glorot-specified uniform distribution\n",
    "                u_edge = np.sqrt(6/(m+n))\n",
    "                \n",
    "                # set self.w to values drawn from a uniform distribution\n",
    "                # hint: use np.random.uniform(loweredge,upperedge,size) where size is the numpy shape of the weights (self.w)\n",
    "                self.w = np.random.uniform(low=-u_edge,high=u_edge,size=(self.w.shape))\n",
    "                \n",
    "                ###################################### END STUDENT CODE ####################################\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                #standard initialization\n",
    "                self.w = np.random.normal(size=(self.w.shape))  # hidden weight matrix [rows = to, columns = from]\n",
    "                \n",
    "            #asserting the correct shapes on self.b and self.w - if these fail the code may be incorrect\n",
    "            assert (self.b.shape == biasShape)\n",
    "            assert (self.w.shape == weightShape)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2 Student Coding - Momentum:\n",
    "\n",
    "Add momentum & velocity in Layer subclass function ```def update_Layer(self, weightUpdate, biasUpdate, momentum=0)```\n",
    "\n",
    "note that the ```momentum``` is passed in during the function call and the values for velocity (from the previous training iteration) are stored in\n",
    "```self.vel_w``` (velocity of the weight values) and ```self.vel_b```  (velocity of the bias values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Extending the ANN class to allow for momentum in the Layer class\n",
    "class ANN(ANN):\n",
    "\n",
    "    # extending the layer class \n",
    "    class Layer(ANN.Layer):\n",
    "       \n",
    "        def update_Layer(self, weightUpdate, biasUpdate, momentum=0.):\n",
    "            \"\"\"Update weights and biases. weightUpdate is shape [thisLayerNodecount,prevLayerOutputcount];\n",
    "            biasUpdate is shape [thisLayerNodecount,1]\n",
    "            \"\"\"\n",
    "            weightShape = self.w.shape\n",
    "            biasShape = self.b.shape\n",
    "            if momentum == 0:  #note this if-else statement not required if written as math eq with momentum & velocity >> more efficient!\n",
    "                self.w = self.w + weightUpdate\n",
    "                self.b = self.b + biasUpdate\n",
    "            else:  #note this if-else statement not required if written as math eq with momentum & velocity\n",
    "                # need to compute the new values for self.w and self.b using momentum\n",
    "                # note: momentum is passed in but velocity must be updated & stored in self.vel_w and self.vel_b\n",
    "                 \n",
    "                ############# STUDENT CODING - ADD VELOCITY TO WEIGHT & BIAS UPDATE ##################\n",
    "                \n",
    "                # compute & store the new velocity for the weights (self.vel_w)\n",
    "                self.vel_w = momentum*self.vel_w+weightUpdate\n",
    "                \n",
    "                # compute & store the new velocity for the biases (self.vel_b)\n",
    "                self.vel_b = momentum*self.vel_b+biasUpdate\n",
    "                \n",
    "                # compute & store the weights (self.w) using the previous weights and the new weight velocity (self.vel_w)\n",
    "                self.w = self.w + self.vel_w\n",
    "                \n",
    "                # compute & store the biases (self.b) using the previous biases and the new bias velocity (self.vel_b)\n",
    "                self.b = self.b + self.vel_b\n",
    "                \n",
    "                ############################ END STUDENT CODING  #####################################\n",
    "\n",
    "            #confirm the shapes are correct after momentum\n",
    "            assert (self.b.shape == biasShape)\n",
    "            assert (self.w.shape == weightShape)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions - Visualization\n",
    "\n",
    "For displaying graphical output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataplotter(featureData, labelData, title):\n",
    "    '''plot annotated points to show where the boolean inputs lie on the graph''' \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    xmin, xmax = np.min(featureData[:, 0]) - 0.1, np.max(featureData[:, 0]) + 0.1\n",
    "    ymin, ymax = np.min(featureData[:, 1]) - 0.1, np.max(featureData[:, 1]) + 0.1\n",
    "\n",
    "        \n",
    "#     ax.scatter(featureData[:, 0], featureData[:, 1], color='b', alpha=0.2)\n",
    "    for i, txt in enumerate(labelData):\n",
    "        ax.annotate(txt.item(), (featureData[i, 0], featureData[i, 1]))\n",
    "\n",
    "        # adjust the axis & add labels for the graph\n",
    "    plt.axis([xmin, xmax, ymin, ymax])\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def makeDecisionBoundaryBool2(model, featureData, labelData, title):\n",
    "    '''Build decision boundary figrue for 2-input, 1-output boolean logic functions\n",
    "    Note that this assumes a hard sigmoid was used and establishes a cutoff at 0.5\n",
    "    for predicting 0 or 1'''\n",
    "    cutoff = 0.5  # 0.5 for Sigmoid. 0.0 for TANH\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    xmin, xmax = np.min(featureData[:, 0]) - 0.1, np.max(featureData[:, 0]) + 0.1\n",
    "    ymin, ymax = np.min(featureData[:, 1]) - 0.1, np.max(featureData[:, 1]) + 0.1\n",
    "\n",
    "    # Create filled countour map to color both sides of the boundary\n",
    "    x = np.linspace(xmin, xmax, 200)\n",
    "    y = np.linspace(ymin, ymax, 200)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    grid = np.c_[X.ravel(), Y.ravel()]\n",
    "    preds = model.predict(grid)  # get predictions\n",
    "    z = preds.reshape(X.shape) > cutoff  # cutoff on predictions to return boolean output\n",
    "    plt.contourf(X, Y, z, cmap='YlOrBr')\n",
    "\n",
    "    # add annotated points to show where the boolean inputs lie on the graph\n",
    "    ax.scatter(featureData[:, 0], featureData[:, 1], color='b', alpha=0.5)\n",
    "    for i, txt in enumerate(labelData):\n",
    "        ax.annotate(txt.item(), (featureData[i, 0], featureData[i, 1]))\n",
    "\n",
    "        # adjust the axis & add labels for the graph\n",
    "    plt.axis([xmin, xmax, ymin, ymax])\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show2dFunctionOutput(model_function, featureData, labelData, title):\n",
    "    \"\"\"display results of arbitrary model function on 2-input (x1,x2) , 1-output (z) graphs\"\"\"\n",
    "    # cutoff = 0.5  # 0.5 for Sigmoid. 0.0 for TANH\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "    xmin, xmax = np.min(featureData[:, 0]) - 0.1, np.max(featureData[:, 0]) + 0.1\n",
    "    ymin, ymax = np.min(featureData[:, 1]) - 0.1, np.max(featureData[:, 1]) + 0.1\n",
    "\n",
    "    # Create filled countour map to color both sides of the boundary\n",
    "    x = np.linspace(xmin, xmax, 200)\n",
    "    y = np.linspace(ymin, ymax, 200)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    grid = np.c_[X.ravel(), Y.ravel()]\n",
    "    outputs, _, _, _ = model_function(grid)  # get predictions\n",
    "    z = outputs.reshape(X.shape)  # reshape predictions for 2d representation\n",
    "    plotlevels = np.linspace(0.,1., 25)  # split colors between 0 and 1\n",
    "    CS = plt.contourf(X, Y, z, levels = plotlevels, cmap='YlOrBr')\n",
    "\n",
    "    # add annotated points to show where the boolean inputs lie on the graph\n",
    "    ax.scatter(featureData[:, 0], featureData[:, 1], color='b', alpha=0.5)\n",
    "    for i, txt in enumerate(labelData):\n",
    "        ax.annotate(txt.item(), (featureData[i, 0], featureData[i, 1]))\n",
    "\n",
    "    \n",
    "    cbar = plt.colorbar(CS)\n",
    "    cbar.ax.set_ylabel('raw network output')\n",
    "    # adjust the axis & add labels for the graph\n",
    "    plt.axis([xmin, xmax, ymin, ymax])\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def showLossHistory(tng_loss_history=[],val_loss_history=[],semilog=True,plotname=\"\"):\n",
    "    plt.figure()\n",
    "    if semilog:\n",
    "        plt.semilogy(tng_loss_history,'r', label = \"Training Loss\")\n",
    "        plt.semilogy(val_loss_history, 'b', label = \"Validation Loss\")\n",
    "        plt.ylabel(\"Loss (Binary Cross-entropy) - semilog scale\")\n",
    "\n",
    "    else:    \n",
    "        plt.plot(tng_loss_history,'r', label = \"Training Loss\")\n",
    "        plt.plot(val_loss_history, 'b', label = \"Validation Loss\")\n",
    "        plt.ylabel(\"Loss (Binary Cross-entropy)\")\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.legend()\n",
    "    plt.title(plotname+\" Loss over iterations\")\n",
    "    plt.show() \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to produce datasets for logic gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_output_data(gate='XOR'):\n",
    "    \"\"\" Two dimensional inputs for logic gates\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gate : str\n",
    "        Must be either AND, OR, XOR\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : array-like, shape(samples, features)\n",
    "        Two dim input for logic gates\n",
    "\n",
    "    truth[gate] : array-like, shapes(samples, )\n",
    "        The truth value for this logic gate\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.array([[0., 0.],\n",
    "                  [0., 1.],\n",
    "                  [1., 0.],\n",
    "                  [1., 1.]])\n",
    "\n",
    "    truth = {\n",
    "        'AND': np.array([0, 0, 0, 1]),\n",
    "        'OR': np.array([0, 1, 1, 1]),\n",
    "        'XOR': np.array([0, 1, 1, 0])\n",
    "    }\n",
    "\n",
    "    return X, truth[gate][:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_input_output_data(gate='XOR')\n",
    "\n",
    "trainX = X\n",
    "trainY = Y\n",
    "valX = X\n",
    "valY = Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to instantiate an ANN from parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ANN_model(input_width = 2,\n",
    "                   layer_widths = [2,1],\n",
    "                   layer_activiations = [sigmoid,sigmoid],\n",
    "                   glorot = False,\n",
    "                   verbose = True):\n",
    "    model = ANN()\n",
    "    if verbose:\n",
    "        print(list(zip(layer_widths,layer_activiations)))\n",
    "        \n",
    "    model.set_input_width(input_width)\n",
    "    for lnum,(layerWidth,layerActivation) in enumerate(zip(layer_widths,layer_activiations)):\n",
    "        model.add_layer(nodecount = layerWidth, activation_fcn=layerActivation)\n",
    "    model.initialize(glorot=glorot)\n",
    "    return model\n",
    "        \n",
    " \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to train an ANN from parameters.  Allows for separate training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ANN_model(model=None,\n",
    "                    trainX = None, trainY = None,\n",
    "                    valX=None,valY=None,\n",
    "                    learning_rate=1.0,\n",
    "                    lr_decay=0.999,\n",
    "                    batchsize = 1,\n",
    "                    momentum = 0,\n",
    "                    valPatience=0,\n",
    "                    maxEpochs = 100,\n",
    "                    verbose = True,\n",
    "                    showgraphs = True,\n",
    "                    modelTitle = \"\"):\n",
    "    \n",
    "    \n",
    "\n",
    "    preds = model.predict(valX)\n",
    "    correct = valY == preds\n",
    "    if verbose:\n",
    "        textToPrint = modelTitle + \" BEFORE TRAINING (randomized weights)\"\n",
    "        print(textToPrint)\n",
    "        model.display_params()\n",
    "        \n",
    "    tng_loss_history,val_loss_history = model.fit(tngInputs=trainX, tngTargets=trainY, valInputs=valX, valTargets = valY,\n",
    "                                                  learningRate=learning_rate, learningRateDecay=lr_decay, batchsize=batchsize,\n",
    "                                                  momentum = momentum, valPatience=valPatience,\n",
    "                                                  tolerance=1e-1, maxEpochs=maxEpochs, verbose=verbose)\n",
    "\n",
    "    preds = model.predict(valX)\n",
    "    correct = valY == preds\n",
    "\n",
    "    if verbose:\n",
    "        textToPrint = modelTitle + \" AFTER TRAINING (learned model weights)\"\n",
    "        print(textToPrint)\n",
    "        model.display_params()\n",
    "\n",
    "    if showgraphs:\n",
    "        show2dFunctionOutput(model.forwardPropagation, X, Y, modelTitle + \" Raw Response of Network\")\n",
    "        makeDecisionBoundaryBool2(model, X, Y, modelTitle + \" XOR predictions from Network\")\n",
    "        showLossHistory(tng_loss_history,val_loss_history, modelTitle + \" Loss over Iterations\")\n",
    "\n",
    "    return tng_loss_history,val_loss_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to repeatedly train an ANN with model and training params. Reports results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_model_eval(model_params, train_function_params, reps = 1, progress_reporting=True):\n",
    "    ''' Repeatedly instantiate and fit a model multiple times and report results\n",
    "    params:\n",
    "    model_params:  parameters to pass to the model instantiation function (make_ANN_model())\n",
    "    train_function_params:  parameters to pass to the \n",
    "    '''\n",
    "    totalCorrect = 0\n",
    "    countPerfectVal = 0\n",
    "    print(\"running \"+str(reps)+\" iterations:\")\n",
    "\n",
    "    for idx in range(reps):\n",
    "        model = make_ANN_model(**model_params)  #activations at each layer)\n",
    "\n",
    "        tng_loss_history,val_loss_history= train_ANN_model(model,**train_function_params)\n",
    "\n",
    "        preds = model.predict(valX)\n",
    "        correct = valY == preds\n",
    "        totalCorrect=totalCorrect+correct\n",
    "        final_val_loss = round(val_loss_history[-1], 4)\n",
    "        countPerfectVal = countPerfectVal+(sum(correct)==4)\n",
    "        print(idx, end=\": \")\n",
    "        print(\"epoch count: \"+str(len(val_loss_history)), end = \"; \")\n",
    "        print(\"val loss: \" +str(final_val_loss), end=\"; \")\n",
    "        print(\"val preds correct: \"+str(sum(correct).item()))\n",
    "\n",
    "    correctness = sum(totalCorrect).item()/(reps*4)\n",
    "    \n",
    "    print(\"\\n\"+str(reps)+\n",
    "          \"x reps Correctness = \"+\n",
    "          str(correctness)+\n",
    "          \", Perfect Count = \"+str(countPerfectVal.item()))\n",
    "    return correctness,countPerfectVal.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Training the networks\n",
    "\n",
    "Here we initialize a network without, and with Glorot initialization.\n",
    "\n",
    "Students will need to implement the code in the ANN class before running the sections below.\n",
    "\n",
    "We will do this by running a set of 8 experiments over 3 levels with 2 factors per level\n",
    "Modelsize = {Small, Large}\n",
    "Glorot Weight Initialization = {False, True}\n",
    "Momentum = {False, True}\n",
    "\n",
    "The performance score (average correctness and number of perfectly correct classification scores on XOR)\n",
    "will be captured in the array listed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#placeholder array for experiment results\n",
    "# 3 levels with 2 factors per level Modelsize = {Small, Large} Glorot Weight Initialization = {False, True} Momentum = {False, True}\n",
    "model_experiment_results=np.zeros([2,2,2,2])\n",
    "#first index: Model size: 0 = small, 1 = large\n",
    "#second index: Glorot initialization: 0 = False, 1= True\n",
    "#third index: Momentum: 0 = False, 1= True\n",
    "#4th index contains performance:  0:  average correctness;  1:  number of perfectly correct classification scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple network *without* Glorot initialization\n",
    "this is a 2,1 fully connected sigmoid network.  works some of the time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallmodel_layer_widths = [2,1]\n",
    "smallmodel_layer_activations = [relu, sigmoid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the model multiple times to determine performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running 20 iterations:\n",
      "0: epoch count: 5002; val loss: 0.3538; val preds correct: 3\n",
      "1: epoch count: 115; val loss: 0.1; val preds correct: 4\n",
      "2: epoch count: 151; val loss: 0.1; val preds correct: 4\n",
      "3: epoch count: 230; val loss: 0.1009; val preds correct: 4\n",
      "4: epoch count: 5002; val loss: 0.4808; val preds correct: 3\n",
      "5: epoch count: 5002; val loss: 0.3543; val preds correct: 3\n",
      "6: epoch count: 5002; val loss: 0.3544; val preds correct: 3\n",
      "7: epoch count: 187; val loss: 0.1007; val preds correct: 4\n",
      "8: epoch count: 153; val loss: 0.1007; val preds correct: 4\n",
      "9: epoch count: 5002; val loss: 0.3538; val preds correct: 3\n",
      "10: epoch count: 5002; val loss: 0.4804; val preds correct: 3\n",
      "11: epoch count: 165; val loss: 0.1005; val preds correct: 4\n",
      "12: epoch count: 5002; val loss: 0.481; val preds correct: 3\n",
      "13: epoch count: 161; val loss: 0.1007; val preds correct: 4\n",
      "14: epoch count: 5002; val loss: 0.6931; val preds correct: 2\n",
      "15: epoch count: 145; val loss: 0.1001; val preds correct: 4\n",
      "16: epoch count: 5002; val loss: 0.4806; val preds correct: 3\n",
      "17: epoch count: 5002; val loss: 0.3541; val preds correct: 3\n",
      "18: epoch count: 5002; val loss: 0.4804; val preds correct: 3\n",
      "19: epoch count: 194; val loss: 0.1011; val preds correct: 4\n",
      "\n",
      "20x reps Correctness = 0.85, Perfect Count = 9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_params = {'input_width':2,            \n",
    "               'layer_widths':smallmodel_layer_widths,  #number of nodes in each layer\n",
    "               'layer_activiations':smallmodel_layer_activations,\n",
    "               'glorot':False,\n",
    "               'verbose':False}\n",
    "train_function_params = {\n",
    "    'trainX':trainX,'trainY':trainY,'valX':valX,'valY':valY,\n",
    "    'learning_rate':0.5,\n",
    "    'lr_decay':0.999,\n",
    "    'batchsize':1,\n",
    "    'momentum' : 0,\n",
    "    'valPatience' : 0,\n",
    "    'maxEpochs' : 5000,\n",
    "    'verbose' : False,\n",
    "    'showgraphs':False}\n",
    "model_experiment_results[0,0,0,:] = repeat_model_eval(model_params =model_params ,train_function_params = train_function_params , reps = REPS )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can show an example of model fitting if SHOW_EXAMPLES is set to True in the global variable section (top of jupyter notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_EXAMPLES:\n",
    "    model = make_ANN_model(input_width = 2,\n",
    "                           layer_widths = smallmodel_layer_widths,  #number of nodes in each layer\n",
    "                           layer_activiations = smallmodel_layer_activations,\n",
    "                           glorot=False)  #activations at each layer\n",
    "\n",
    "    print(\"empty model info\")\n",
    "    model.summary()\n",
    "\n",
    "    X, Y = get_input_output_data(gate='XOR')\n",
    "\n",
    "    trainX = X\n",
    "    trainY = Y\n",
    "    valX = X\n",
    "    valY = Y\n",
    "\n",
    "    tng_loss_history,val_loss_history= train_ANN_model(model,trainX,trainY,valX,valY,\n",
    "                    learning_rate=1.0,lr_decay=0.999, batchsize=1, \n",
    "                    momentum = 0, valPatience = 0, maxEpochs = 5000, modelTitle=\"Layers 2-1, no glorot, no momentum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple network with Glorot initialization\n",
    "this is a 2,1 fully connected sigmoid network.  works some of the time...\n",
    "\n",
    "but works more often once glorot initializations are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running 20 iterations:\n",
      "0: epoch count: 5002; val loss: 0.4807; val preds correct: 3\n",
      "1: epoch count: 5002; val loss: 0.3538; val preds correct: 3\n",
      "2: epoch count: 203; val loss: 0.1015; val preds correct: 4\n",
      "3: epoch count: 5002; val loss: 0.3541; val preds correct: 3\n",
      "4: epoch count: 5002; val loss: 0.4808; val preds correct: 3\n",
      "5: epoch count: 187; val loss: 0.1009; val preds correct: 4\n",
      "6: epoch count: 5002; val loss: 0.354; val preds correct: 3\n",
      "7: epoch count: 5002; val loss: 0.3542; val preds correct: 3\n",
      "8: epoch count: 180; val loss: 0.1009; val preds correct: 4\n",
      "9: epoch count: 5002; val loss: 0.4803; val preds correct: 3\n",
      "10: epoch count: 188; val loss: 0.1007; val preds correct: 4\n",
      "11: epoch count: 237; val loss: 0.1009; val preds correct: 4\n",
      "12: epoch count: 176; val loss: 0.1001; val preds correct: 4\n",
      "13: epoch count: 5002; val loss: 0.4804; val preds correct: 3\n",
      "14: epoch count: 211; val loss: 0.1002; val preds correct: 4\n",
      "15: epoch count: 5002; val loss: 0.4817; val preds correct: 3\n",
      "16: epoch count: 5002; val loss: 0.3539; val preds correct: 3\n",
      "17: epoch count: 5002; val loss: 0.3539; val preds correct: 3\n",
      "18: epoch count: 192; val loss: 0.1003; val preds correct: 4\n",
      "19: epoch count: 299; val loss: 0.1012; val preds correct: 4\n",
      "\n",
      "20x reps Correctness = 0.8625, Perfect Count = 9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_params = {'input_width':2,            \n",
    "               'layer_widths':smallmodel_layer_widths,  #number of nodes in each layer\n",
    "               'layer_activiations':smallmodel_layer_activations,\n",
    "               'glorot':True,\n",
    "               'verbose':False}\n",
    "train_function_params = {\n",
    "    'trainX':trainX,'trainY':trainY,'valX':valX,'valY':valY,\n",
    "    'learning_rate':0.5,\n",
    "    'lr_decay':0.999,\n",
    "    'batchsize':1,\n",
    "    'momentum' : 0,\n",
    "    'valPatience' : 0,\n",
    "    'maxEpochs' : 5000,\n",
    "    'verbose' : False,\n",
    "    'showgraphs':False}\n",
    "model_experiment_results[1,0,0,:]=repeat_model_eval(model_params =model_params ,train_function_params = train_function_params , reps = REPS )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_EXAMPLES:\n",
    "    model = make_ANN_model(input_width = 2,\n",
    "                           layer_widths = smallmodel_layer_widths,  #number of nodes in each layer\n",
    "                           layer_activiations = smallmodel_layer_activations,\n",
    "                           glorot=True)  #activations at each layer\n",
    "\n",
    "    print(\"empty model info\")\n",
    "    model.summary()\n",
    "\n",
    "    X, Y = get_input_output_data(gate='XOR')\n",
    "\n",
    "    trainX = X\n",
    "    trainY = Y\n",
    "    valX = X\n",
    "    valY = Y\n",
    "\n",
    "    tng_loss_history,val_loss_history=train_ANN_model(model,trainX,trainY,valX,valY,\n",
    "                                                      learning_rate=1.0,lr_decay=0.999, batchsize=1, \n",
    "                                                      momentum = 0, valPatience = 0, maxEpochs = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Network *without* Glorot initialization and with Momentum\n",
    "this is a 2,1 fully connected sigmoid network. works more often and probably trains quicker...\n",
    "\n",
    "you can vary the momentum parameter which should be in the interval \\[0,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running 20 iterations:\n",
      "0: epoch count: 5002; val loss: 0.4804; val preds correct: 3\n",
      "1: epoch count: 5002; val loss: 0.3529; val preds correct: 3\n",
      "2: epoch count: 5002; val loss: 0.3533; val preds correct: 3\n",
      "3: epoch count: 189; val loss: 0.1; val preds correct: 4\n",
      "4: epoch count: 5002; val loss: 0.4807; val preds correct: 3\n",
      "5: epoch count: 5002; val loss: 0.4803; val preds correct: 3\n",
      "6: epoch count: 5002; val loss: 0.3537; val preds correct: 3\n",
      "7: epoch count: 5002; val loss: 0.3536; val preds correct: 3\n",
      "8: epoch count: 5002; val loss: 0.4806; val preds correct: 3\n",
      "9: epoch count: 172; val loss: 0.1008; val preds correct: 4\n",
      "10: epoch count: 92; val loss: 0.1007; val preds correct: 4\n",
      "11: epoch count: 201; val loss: 0.1016; val preds correct: 4\n",
      "12: epoch count: 192; val loss: 0.1009; val preds correct: 4\n",
      "13: epoch count: 133; val loss: 0.1015; val preds correct: 4\n",
      "14: epoch count: 5002; val loss: 0.4805; val preds correct: 3\n",
      "15: epoch count: 114; val loss: 0.101; val preds correct: 4\n",
      "16: epoch count: 186; val loss: 0.101; val preds correct: 4\n",
      "17: epoch count: 5002; val loss: 0.3535; val preds correct: 3\n",
      "18: epoch count: 5002; val loss: 0.353; val preds correct: 3\n",
      "19: epoch count: 5002; val loss: 0.4803; val preds correct: 3\n",
      "\n",
      "20x reps Correctness = 0.85, Perfect Count = 8\n"
     ]
    }
   ],
   "source": [
    "momentum = 0.1  #value 0 to less than 1.   Too high overshoots!\n",
    "\n",
    "model_params = {'input_width':2,            \n",
    "               'layer_widths':smallmodel_layer_widths,  #number of nodes in each layer\n",
    "               'layer_activiations':smallmodel_layer_activations,\n",
    "               'glorot':False,\n",
    "               'verbose':False}\n",
    "train_function_params = {\n",
    "    'trainX':trainX,'trainY':trainY,'valX':valX,'valY':valY,\n",
    "    'learning_rate':0.5,\n",
    "    'lr_decay':0.999,\n",
    "    'batchsize':1,\n",
    "    'momentum' : momentum,\n",
    "    'valPatience' : 0,\n",
    "    'maxEpochs' : 5000,\n",
    "    'verbose' : False,\n",
    "    'showgraphs':False}\n",
    "model_experiment_results[0,1,0,:]=repeat_model_eval(model_params =model_params ,train_function_params = train_function_params , reps = REPS )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Network with Glorot initialization and Momentum\n",
    "this is a 2,1 fully connected sigmoid network. works more often and probably trains quicker...\n",
    "\n",
    "you can vary the momentum parameter which should be in the interval \\[0,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running 20 iterations:\n",
      "0: epoch count: 5002; val loss: 0.3534; val preds correct: 3\n",
      "1: epoch count: 5002; val loss: 0.4805; val preds correct: 3\n",
      "2: epoch count: 5002; val loss: 0.3533; val preds correct: 3\n",
      "3: epoch count: 5002; val loss: 0.4805; val preds correct: 3\n",
      "4: epoch count: 5002; val loss: 0.3539; val preds correct: 3\n",
      "5: epoch count: 143; val loss: 0.101; val preds correct: 4\n",
      "6: epoch count: 5002; val loss: 0.4801; val preds correct: 3\n",
      "7: epoch count: 5002; val loss: 0.3538; val preds correct: 3\n",
      "8: epoch count: 5002; val loss: 0.6931; val preds correct: 2\n",
      "9: epoch count: 5002; val loss: 0.4806; val preds correct: 3\n",
      "10: epoch count: 5002; val loss: 0.6931; val preds correct: 2\n",
      "11: epoch count: 5002; val loss: 0.3534; val preds correct: 3\n",
      "12: epoch count: 5002; val loss: 0.4807; val preds correct: 3\n",
      "13: epoch count: 197; val loss: 0.1004; val preds correct: 4\n",
      "14: epoch count: 5002; val loss: 0.3534; val preds correct: 3\n",
      "15: epoch count: 114; val loss: 0.1013; val preds correct: 4\n",
      "16: epoch count: 5002; val loss: 0.4807; val preds correct: 3\n",
      "17: epoch count: 159; val loss: 0.1011; val preds correct: 4\n",
      "18: epoch count: 5002; val loss: 0.4807; val preds correct: 3\n",
      "19: epoch count: 5002; val loss: 0.3534; val preds correct: 3\n",
      "\n",
      "20x reps Correctness = 0.775, Perfect Count = 4\n"
     ]
    }
   ],
   "source": [
    "momentum = 0.1  #value 0 to less than 1.   Too high overshoots!\n",
    "\n",
    "model_params = {'input_width':2,            \n",
    "               'layer_widths':smallmodel_layer_widths,  #number of nodes in each layer\n",
    "               'layer_activiations':smallmodel_layer_activations,\n",
    "               'glorot':True,\n",
    "               'verbose':False}\n",
    "train_function_params = {\n",
    "    'trainX':trainX,'trainY':trainY,'valX':valX,'valY':valY,\n",
    "    'learning_rate':0.5,\n",
    "    'lr_decay':0.999,\n",
    "    'batchsize':1,\n",
    "    'momentum' : momentum,\n",
    "    'valPatience' : 0,\n",
    "    'maxEpochs' : 5000,\n",
    "    'verbose' : False,\n",
    "    'showgraphs':False}\n",
    "model_experiment_results[1,1,0,:]=repeat_model_eval(model_params =model_params ,train_function_params = train_function_params , reps = REPS )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_EXAMPLES:\n",
    "    model = make_ANN_model(input_width = 2,\n",
    "                           layer_widths = smallmodel_layer_widths,  #number of nodes in each layer\n",
    "                           layer_activiations = smallmodel_layer_activations,\n",
    "                          glorot=True)  #activations at each layer\n",
    "\n",
    "\n",
    "    print(\"empty model info\")\n",
    "    model.summary()\n",
    "\n",
    "    X, Y = get_input_output_data(gate='XOR')\n",
    "\n",
    "    trainX = X\n",
    "    trainY = Y\n",
    "    valX = X\n",
    "    valY = Y\n",
    "\n",
    "    tng_loss_history,val_loss_history=train_ANN_model(model,trainX,trainY,valX,valY,\n",
    "                                                      learning_rate=1.0,lr_decay=0.999, batchsize=1,\n",
    "                                                      momentum = 0.7, valPatience = 0, maxEpochs = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher Capacity network\n",
    " a 5-3-1 network should learn easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "largemodel_layer_widths = [5,3,1]\n",
    "largemodel_layer_activations = [relu,relu,sigmoid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## First lets try the larger network without Glorot or momentum ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running 20 iterations:\n",
      "0: epoch count: 137; val loss: 0.1005; val preds correct: 4\n",
      "1: epoch count: 5002; val loss: 4.9356; val preds correct: 2\n",
      "2: epoch count: 110; val loss: 0.1002; val preds correct: 4\n",
      "3: epoch count: 5002; val loss: 0.6901; val preds correct: 3\n",
      "4: epoch count: 5002; val loss: 0.3106; val preds correct: 4\n",
      "5: epoch count: 195; val loss: 0.1004; val preds correct: 4\n",
      "6: epoch count: 30; val loss: 0.1022; val preds correct: 4\n",
      "7: epoch count: 62; val loss: 0.1002; val preds correct: 4\n",
      "8: epoch count: 57; val loss: 0.1037; val preds correct: 4\n",
      "9: epoch count: 72; val loss: 0.103; val preds correct: 4\n",
      "10: epoch count: 137; val loss: 0.1; val preds correct: 4\n",
      "11: epoch count: 147; val loss: 0.1; val preds correct: 4\n",
      "12: epoch count: 134; val loss: 0.1004; val preds correct: 4\n",
      "13: epoch count: 5002; val loss: 0.6931; val preds correct: 2\n",
      "14: epoch count: 5002; val loss: 0.1091; val preds correct: 4\n",
      "15: epoch count: 64; val loss: 0.1005; val preds correct: 4\n",
      "16: epoch count: 5002; val loss: 0.2058; val preds correct: 4\n",
      "17: epoch count: 27; val loss: 0.1016; val preds correct: 4\n",
      "18: epoch count: 5002; val loss: 0.4875; val preds correct: 3\n",
      "19: epoch count: 5002; val loss: 3.9384; val preds correct: 3\n",
      "\n",
      "20x reps Correctness = 0.9125, Perfect Count = 15\n"
     ]
    }
   ],
   "source": [
    "momentum = 0.1  #value 0 to less than 1.   Too high overshoots!\n",
    "\n",
    "model_params = {'input_width':2,            \n",
    "               'layer_widths':largemodel_layer_widths,  #number of nodes in each layer\n",
    "               'layer_activiations':largemodel_layer_activations,\n",
    "               'glorot':False,\n",
    "               'verbose':False}\n",
    "train_function_params = {\n",
    "    'trainX':trainX,'trainY':trainY,'valX':valX,'valY':valY,\n",
    "    'learning_rate':0.5,\n",
    "    'lr_decay':0.99,\n",
    "    'batchsize':1,\n",
    "    'momentum' : 0,\n",
    "    'valPatience' : 0,\n",
    "    'maxEpochs' : 5000,\n",
    "    'verbose' : False,\n",
    "    'showgraphs':False}\n",
    "model_experiment_results[0,0,1,:]=repeat_model_eval(model_params =model_params ,train_function_params = train_function_params , reps = REPS )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_EXAMPLES:\n",
    "    model = make_ANN_model(input_width = 2,\n",
    "                       layer_widths = largemodel_layer_widths,  #number of nodes in each layer\n",
    "                       layer_activiations = largemodel_layer_activations)  #activations at each layer\n",
    "\n",
    "    print(\"empty model info\")\n",
    "    model.summary()\n",
    "\n",
    "    X, Y = get_input_output_data(gate='XOR')\n",
    "\n",
    "    trainX = X\n",
    "    trainY = Y\n",
    "    valX = X\n",
    "    valY = Y\n",
    "\n",
    "    tng_loss_history,val_loss_history=train_ANN_model(model,trainX,trainY,valX,valY,\n",
    "                                                      learning_rate=0.5,lr_decay=1.0, batchsize=1, \n",
    "                                                      valPatience = 0, maxEpochs = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large model with Glorot but no momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running 20 iterations:\n",
      "0: epoch count: 5002; val loss: 0.1047; val preds correct: 4\n",
      "1: epoch count: 72; val loss: 0.1017; val preds correct: 4\n",
      "2: epoch count: 98; val loss: 0.1002; val preds correct: 4\n",
      "3: epoch count: 194; val loss: 0.1001; val preds correct: 4\n",
      "4: epoch count: 159; val loss: 0.1002; val preds correct: 4\n",
      "5: epoch count: 56; val loss: 0.1003; val preds correct: 4\n",
      "6: epoch count: 304; val loss: 0.1001; val preds correct: 4\n",
      "7: epoch count: 133; val loss: 0.1004; val preds correct: 4\n",
      "8: epoch count: 64; val loss: 0.1002; val preds correct: 4\n",
      "9: epoch count: 106; val loss: 0.1011; val preds correct: 4\n",
      "10: epoch count: 133; val loss: 0.1003; val preds correct: 4\n",
      "11: epoch count: 5002; val loss: 0.4882; val preds correct: 3\n",
      "12: epoch count: 5002; val loss: 0.3703; val preds correct: 3\n",
      "13: epoch count: 70; val loss: 0.1008; val preds correct: 4\n",
      "14: epoch count: 5002; val loss: 0.1257; val preds correct: 4\n",
      "15: epoch count: 253; val loss: 0.1; val preds correct: 4\n",
      "16: epoch count: 133; val loss: 0.1002; val preds correct: 4\n",
      "17: epoch count: 69; val loss: 0.1021; val preds correct: 4\n",
      "18: epoch count: 113; val loss: 0.1; val preds correct: 4\n",
      "19: epoch count: 119; val loss: 0.1004; val preds correct: 4\n",
      "\n",
      "20x reps Correctness = 0.975, Perfect Count = 18\n"
     ]
    }
   ],
   "source": [
    "momentum = 0  #value 0 to less than 1.   Too high overshoots!\n",
    "\n",
    "model_params = {'input_width':2,            \n",
    "               'layer_widths':largemodel_layer_widths,  #number of nodes in each layer\n",
    "               'layer_activiations':largemodel_layer_activations,\n",
    "               'glorot':True,\n",
    "               'verbose':False}\n",
    "train_function_params = {\n",
    "    'trainX':trainX,'trainY':trainY,'valX':valX,'valY':valY,\n",
    "    'learning_rate':0.5,\n",
    "    'lr_decay':0.99,\n",
    "    'batchsize':1,\n",
    "    'momentum' : momentum,\n",
    "    'valPatience' : 0,\n",
    "    'maxEpochs' : 5000,\n",
    "    'verbose' : False,\n",
    "    'showgraphs':False}\n",
    "model_experiment_results[1,0,1,:]=repeat_model_eval(model_params =model_params ,train_function_params = train_function_params , reps = REPS )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_EXAMPLES:\n",
    "    model = make_ANN_model(input_width = 2,\n",
    "                   layer_widths = largemodel_layer_widths,  #number of nodes in each layer\n",
    "                   layer_activiations = largemodel_layer_activations, #activations at each layer\n",
    "                   glorot=True)  \n",
    "\n",
    "    print(\"empty model info\")\n",
    "    model.summary()\n",
    "\n",
    "    X, Y = get_input_output_data(gate='XOR')\n",
    "\n",
    "    trainX = X\n",
    "    trainY = Y\n",
    "    valX = X\n",
    "    valY = Y\n",
    "\n",
    "    tng_loss_history,val_loss_history=train_ANN_model(model,trainX,trainY,valX,valY,\n",
    "                                                      learning_rate=0.5,lr_decay=1.0, batchsize=1, \n",
    "                                                      valPatience = 0, maxEpochs = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# and with the model using standard intialization (not Glorot) and with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running 20 iterations:\n",
      "0: epoch count: 73; val loss: 0.1006; val preds correct: 4\n",
      "1: epoch count: 183; val loss: 0.1004; val preds correct: 4\n",
      "2: epoch count: 116; val loss: 0.1002; val preds correct: 4\n",
      "3: epoch count: 5002; val loss: 0.4842; val preds correct: 3\n",
      "4: epoch count: 30; val loss: 0.1013; val preds correct: 4\n",
      "5: epoch count: 93; val loss: 0.1005; val preds correct: 4\n",
      "6: epoch count: 5002; val loss: 0.4832; val preds correct: 3\n",
      "7: epoch count: 5002; val loss: 0.3542; val preds correct: 3\n",
      "8: epoch count: 5002; val loss: 2.4348; val preds correct: 3\n",
      "9: epoch count: 5002; val loss: 0.6931; val preds correct: 2\n",
      "10: epoch count: 5002; val loss: 0.2089; val preds correct: 4\n",
      "11: epoch count: 34; val loss: 0.1036; val preds correct: 4\n",
      "12: epoch count: 5002; val loss: 0.1588; val preds correct: 4\n",
      "13: epoch count: 266; val loss: 0.1001; val preds correct: 4\n",
      "14: epoch count: 28; val loss: 0.101; val preds correct: 4\n",
      "15: epoch count: 5002; val loss: 0.1919; val preds correct: 4\n",
      "16: epoch count: 5002; val loss: 2.6587; val preds correct: 3\n",
      "17: epoch count: 25; val loss: 0.1002; val preds correct: 4\n",
      "18: epoch count: 5002; val loss: 0.3617; val preds correct: 3\n",
      "19: epoch count: 76; val loss: 0.1008; val preds correct: 4\n",
      "\n",
      "20x reps Correctness = 0.9, Perfect Count = 13\n"
     ]
    }
   ],
   "source": [
    "momentum = 0.1  #value 0 to less than 1.   Too high overshoots!\n",
    "\n",
    "model_params = {'input_width':2,            \n",
    "               'layer_widths':largemodel_layer_widths,  #number of nodes in each layer\n",
    "               'layer_activiations':largemodel_layer_activations,\n",
    "               'glorot':False,\n",
    "               'verbose':False}\n",
    "train_function_params = {\n",
    "    'trainX':trainX,'trainY':trainY,'valX':valX,'valY':valY,\n",
    "    'learning_rate':0.5,\n",
    "    'lr_decay':0.99,\n",
    "    'batchsize':1,\n",
    "    'momentum' : momentum,\n",
    "    'valPatience' : 0,\n",
    "    'maxEpochs' : 5000,\n",
    "    'verbose' : False,\n",
    "    'showgraphs':False}\n",
    "model_experiment_results[0,1,1,:]=repeat_model_eval(model_params =model_params ,train_function_params = train_function_params , reps = REPS )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_EXAMPLES:\n",
    "    model = make_ANN_model(input_width = 2,\n",
    "                       layer_widths = largemodel_layer_widths,  #number of nodes in each layer\n",
    "                       layer_activiations = largemodel_layer_activations, #activations at each layer\n",
    "                       glorot=False)  \n",
    "\n",
    "    print(\"empty model info\")\n",
    "    model.summary()\n",
    "\n",
    "    X, Y = get_input_output_data(gate='XOR')\n",
    "\n",
    "    trainX = X\n",
    "    trainY = Y\n",
    "    valX = X\n",
    "    valY = Y\n",
    "\n",
    "    tng_loss_history,val_loss_history=train_ANN_model(model,trainX,trainY,valX,valY,\n",
    "                                                      learning_rate=0.5,lr_decay=1.0, batchsize=1, \n",
    "                                                      valPatience = 0, momentum = 0.7,maxEpochs = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And finally, the large model initialized with glorot and trained with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running 20 iterations:\n",
      "0: epoch count: 133; val loss: 0.1; val preds correct: 4\n",
      "1: epoch count: 47; val loss: 0.1003; val preds correct: 4\n",
      "2: epoch count: 122; val loss: 0.1002; val preds correct: 4\n",
      "3: epoch count: 49; val loss: 0.1026; val preds correct: 4\n",
      "4: epoch count: 158; val loss: 0.1002; val preds correct: 4\n",
      "5: epoch count: 5002; val loss: 0.1199; val preds correct: 4\n",
      "6: epoch count: 5002; val loss: 0.3647; val preds correct: 3\n",
      "7: epoch count: 308; val loss: 0.1001; val preds correct: 4\n",
      "8: epoch count: 123; val loss: 0.1006; val preds correct: 4\n",
      "9: epoch count: 5002; val loss: 0.396; val preds correct: 3\n",
      "10: epoch count: 79; val loss: 0.1013; val preds correct: 4\n",
      "11: epoch count: 5002; val loss: 0.3677; val preds correct: 3\n",
      "12: epoch count: 43; val loss: 0.1016; val preds correct: 4\n",
      "13: epoch count: 5002; val loss: 0.2585; val preds correct: 4\n",
      "14: epoch count: 5002; val loss: 0.4876; val preds correct: 3\n",
      "15: epoch count: 169; val loss: 0.1002; val preds correct: 4\n",
      "16: epoch count: 153; val loss: 0.1001; val preds correct: 4\n",
      "17: epoch count: 56; val loss: 0.1003; val preds correct: 4\n",
      "18: epoch count: 52; val loss: 0.1003; val preds correct: 4\n",
      "19: epoch count: 39; val loss: 0.1006; val preds correct: 4\n",
      "\n",
      "20x reps Correctness = 0.95, Perfect Count = 16\n"
     ]
    }
   ],
   "source": [
    "momentum = 0.1  #value 0 to less than 1.   Too high overshoots!\n",
    "\n",
    "model_params = {'input_width':2,            \n",
    "               'layer_widths':largemodel_layer_widths,  #number of nodes in each layer\n",
    "               'layer_activiations':largemodel_layer_activations,\n",
    "               'glorot':True,\n",
    "               'verbose':False}\n",
    "train_function_params = {\n",
    "    'trainX':trainX,'trainY':trainY,'valX':valX,'valY':valY,\n",
    "    'learning_rate':0.5,\n",
    "    'lr_decay':0.99,\n",
    "    'batchsize':1,\n",
    "    'momentum' : momentum,\n",
    "    'valPatience' : 0,\n",
    "    'maxEpochs' : 5000,\n",
    "    'verbose' : False,\n",
    "    'showgraphs':False}\n",
    "model_experiment_results[1,1,1,:]=repeat_model_eval(model_params =model_params ,train_function_params = train_function_params , reps = REPS )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_EXAMPLES:\n",
    "    model = make_ANN_model(input_width = 2,\n",
    "                       layer_widths = largemodel_layer_widths,  #number of nodes in each layer\n",
    "                       layer_activiations = largemodel_layer_activations, #activations at each layer\n",
    "                       glorot=True)  \n",
    "\n",
    "    print(\"empty model info\")\n",
    "    model.summary()\n",
    "\n",
    "    X, Y = get_input_output_data(gate='XOR')\n",
    "\n",
    "    trainX = X\n",
    "    trainY = Y\n",
    "    valX = X\n",
    "    valY = Y\n",
    "\n",
    "    tng_loss_history,val_loss_history=train_ANN_model(model,trainX,trainY,valX,valY,\n",
    "                                                      learning_rate=0.5,lr_decay=1.0, batchsize=1, \n",
    "                                                      momentum = 0.7, valPatience = 0, maxEpochs = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Reporting for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sizeSmall</th>\n",
       "      <th>initGlorot</th>\n",
       "      <th>momentum</th>\n",
       "      <th>correctness</th>\n",
       "      <th>perfectCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9125</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8625</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9750</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7750</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sizeSmall  initGlorot  momentum  correctness  perfectCount\n",
       "0          0           0         0       0.8500           9.0\n",
       "1          0           0         1       0.9125          15.0\n",
       "2          0           1         0       0.8500           8.0\n",
       "3          0           1         1       0.9000          13.0\n",
       "4          1           0         0       0.8625           9.0\n",
       "5          1           0         1       0.9750          18.0\n",
       "6          1           1         0       0.7750           4.0\n",
       "7          1           1         1       0.9500          16.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resultsDF = pd.DataFrame()\n",
    "for s in [0,1]:\n",
    "    for i in [0,1]:\n",
    "        for m in [0,1]:\n",
    "            avg_correctness, perfect_count = model_experiment_results[s,i,m,:]\n",
    "            rowdata = {'sizeSmall': [s], 'initGlorot': [i], 'momentum': [m], 'correctness':[avg_correctness],'perfectCount':[perfect_count]}\n",
    "            rowDF = pd.DataFrame(rowdata)\n",
    "            resultsDF = pd.concat([resultsDF,rowDF ],ignore_index=True)\n",
    "            \n",
    "  \n",
    "display(resultsDF)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
