{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94427845-ab2b-4d4d-a2bf-019d9ce41e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# Original network adapted from karpathy\n",
    "# minesh.mathew@gmail.com\n",
    "# modified version of text generation example in keras;\n",
    "# trained in a many-to-many fashion using a time distributed dense layer\n",
    "\n",
    "####\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import matplotlib as mptl\n",
    "import pylab\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "#import keras\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "# from keras.layers import LSTM, TimeDistributedDense, SimpleRNN  #DEPRECATED TimeDistributedDense\n",
    "from keras.layers import LSTM, TimeDistributed, SimpleRNN\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "#import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44f3eeb2-34ea-4818-a4de-3169ccdc368e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.6.0\n",
      "Tensorflow devices:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Num GPUs Available:  1\n",
      "GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "#Test imports & GPU\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "print(\"Tensorflow devices: \", tf.config.list_physical_devices(\"GPU\"))\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"GPUs Available: \", tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0192c1c-f8aa-4c86-bf8f-0f206a703a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b08dca28-ab29-4719-9930-57696cd1b5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- GENERIC UTILITIES --------------------------\n",
    "def plot_confusion_matrix(cm, class_labels, title='Confusion matrix',\n",
    "                          filename = 'Confusion_Matrix.png', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix\n",
    "    :param cm: a confusion matrix generated by sklearn.metrics.confusion_matrix\n",
    "    :param class_labels: set of text labels\n",
    "    :param title: title of figure\n",
    "    :param cmap: color map for the confusion matrix\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    class_count = len(class_labels)\n",
    "    fig = plt.figure(title)\n",
    "    fig.set_size_inches(10, 8)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(class_count+1)\n",
    "    plt.xticks(tick_marks, class_labels, rotation=45)\n",
    "    plt.yticks(tick_marks, class_labels)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(filename, dpi=100)\n",
    "    np.set_printoptions(precision=2)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88d57769-06ca-43ab-8f7b-777b9fc0891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------- TEXT FILE PREPROCESSING -----------------\n",
    "\n",
    "def preprocess_text_file(filename, maxlen=40):\n",
    "    print('loading: ', filename)\n",
    "    text = open(filename).read().lower()\n",
    "    print('corpus length:', len(text))\n",
    "    chars = sorted(list(set(text)))  #returns unique characters from the text\n",
    "    print('total chars:', len(chars))\n",
    "    char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "    indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "    # split the corpus into sequences of length=maxlen\n",
    "    # input is a sequence of 40 chars and target is also a sequence of 40 chars shifted by one position\n",
    "    # for eg: if you maxlen=3 and the text corpus is abcdefghi, your input ---> target pairs will be\n",
    "    # [a,b,c] --> [b,c,d], [b,c,d]--->[c,d,e]....and so on\n",
    "    step = 1\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    #first generate sentences of characters\n",
    "    for i in range(0, len(text) - maxlen + 1, step):\n",
    "        sentences.append(text[i: i + maxlen])  # input seq is from i to i  + maxlen\n",
    "        next_chars.append(text[i + 1:i + 1 + maxlen])  # output seq is from i+1 to i+1+maxlen\n",
    "        # if i<10 :\n",
    "        # print (text[i: i + maxlen])\n",
    "        # print(text[i+1:i +1+ maxlen])\n",
    "    print('number of sequences:', len(sentences))\n",
    "    return text, chars, char_indices, indices_char, sentences, next_chars\n",
    "\n",
    "\n",
    "def save_processed_text(filename, text, chars, char_indices, indices_char, sentences, next_chars ):\n",
    "    #helper method for saving processed text in a compressed file\n",
    "    my_text = [text, chars, char_indices, indices_char, sentences, next_chars] #dont save the one-hot vectors\n",
    "    with gzip.open(filename, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "        pickle.dump(my_text, f)\n",
    "    # np.savez_compressed(filename,\n",
    "    #                     text = text,\n",
    "    #                     chars = chars,\n",
    "    #                     char_indices=char_indices,\n",
    "    #                     indices_char=indices_char,\n",
    "    #                     sentences=sentences,\n",
    "    #                     next_chars=next_chars)\n",
    "    #                     # allow_pickle=True, protocol=4)\n",
    "\n",
    "\n",
    "def load_processed_text(filename):\n",
    "    #helper method for saving processed text in a compressed file\n",
    "    with gzip.open(filename, 'rb') as f:  # Python 3: open(..., 'rb')\n",
    "        text, chars, char_indices, indices_char, sentences, next_chars = pickle.load(f)\n",
    "    # loaded = np.load(filename)\n",
    "    # text = loaded['text']\n",
    "    # chars = loaded['chars']\n",
    "    # char_indices= loaded['char_indices']\n",
    "    # indices_char= loaded['indices_char']\n",
    "    # sentences= loaded['sentences']\n",
    "    # next_chars= loaded['next_chars']\n",
    "    return text, chars, char_indices, indices_char, sentences, next_chars\n",
    "\n",
    "\n",
    "def vectorize_text(chars, char_indices, sentences, next_chars, maxlen=40):\n",
    "\n",
    "    # now generate dummy variables (1-hot vectors) for the sequences of characters\n",
    "    print('Vectorization processing... this could take a while...')\n",
    "    X = np.zeros((len(sentences), maxlen, len(chars)), dtype=bool)\n",
    "    y = np.zeros((len(sentences), maxlen, len(chars)),\n",
    "                 dtype=bool)  # y is also a sequence , or  a seq of 1 hot vectors\n",
    "    joblength = len(sentences)\n",
    "    tenpercent = joblength/10\n",
    "    nextpercent = tenpercent\n",
    "    print(\" part 1 of 2\")\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if i>nextpercent:\n",
    "            print(i, \" of \", joblength, \" completed\")\n",
    "            nextpercent += tenpercent\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, char_indices[char]] = 1  # X has dimension [sentence_count, sentence_length, char_count]\n",
    "    print(\" part 2 of 2\")\n",
    "    nextpercent = tenpercent\n",
    "\n",
    "    for i, sentence in enumerate(next_chars):\n",
    "        if i>nextpercent:\n",
    "            print(i, \" of \", joblength, \" completed\")\n",
    "            nextpercent += tenpercent\n",
    "        for t, char in enumerate(sentence):\n",
    "            y[i, t, char_indices[char]] = 1  # y has dimension [sentence_count, sentence_length, char_count]\n",
    "    print('vetorization completed')\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def generate_text(model, char_indices, indices_char, seed_string=\"brutus:\", generate_character_count=320):\n",
    "\n",
    "    print(\"seed string --> \", seed_string)\n",
    "    print('The generated text is: ')\n",
    "    sys.stdout.write(seed_string),\n",
    "    # x=np.zeros((1, len(seed_string), len(chars)))\n",
    "    for i in range(generate_character_count):\n",
    "        x = np.zeros((1, len(seed_string), len(chars)))\n",
    "        for t, char in enumerate(seed_string):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        # print (np.argmax(preds[7]))\n",
    "        next_index = np.argmax(preds[len(seed_string) - 1])\n",
    "\n",
    "        # next_index=np.argmax(preds[len(seed_string)-11])\n",
    "        # print (preds.shape)\n",
    "        # print (preds)\n",
    "        # next_index = sample(preds, 1) #diversity is 1\n",
    "        next_char = indices_char[next_index]\n",
    "        seed_string = seed_string + next_char\n",
    "\n",
    "        # print (seed_string)\n",
    "        # print ('##############')\n",
    "        # if i==40:\n",
    "        #    print ('####')\n",
    "        sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4351119c-b60c-4ea6-a73c-d5f2e8a0af75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- MODEL FILE I/O ---------------------------\n",
    "def save_model(model, save_dir=os.path.join(os.getcwd(), 'saved_models'),\n",
    "               model_file_name='keras_cifar10_trained_model.h5'):\n",
    "    \"\"\"\n",
    "    Save model and current weights\n",
    "    :param model: Keras model\n",
    "    :param save_dir: path name to save directory\n",
    "    :param model_file_name: filename for saved model\n",
    "    :return: nothing\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model_path = os.path.join(save_dir, model_file_name)\n",
    "    model.save(model_path)\n",
    "    print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "\n",
    "def load_model(save_dir, model_file_name):\n",
    "    # Load model and weights\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model_path = os.path.join(save_dir, model_file_name)\n",
    "    model = keras.models.load_model(model_path)\n",
    "    print('Loaded trained model from %s ' % model_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c827ebf-b93f-4926-aafe-4981c8c70bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- MODEL DEVELOPMENT ---------------------------\n",
    "def build_model(characters):\n",
    "    # build the model: 2 stacked LSTM\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    # model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, len(chars))))  # original one\n",
    "    model.add(LSTM(512, input_dim=len(characters), return_sequences=True))  # minesh witout specifying the input_length\n",
    "    model.add(LSTM(512, return_sequences=True))  # - original\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.add(TimeDistributedDense(len(chars)))   #Deprecated TimeDistributedDense\n",
    "    model.add(TimeDistributed(Dense(len(characters))))  # BJB:  is this really working??\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    print('model is made')\n",
    "    # train the model, output generated text after each iteration\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_net(model, x, y, training_iterations=6, maxlen=40, save_all_model_iterations=True):\n",
    "    for training_iteration in range(1, training_iterations+1):\n",
    "        print()\n",
    "        print('-' * 50)\n",
    "        print('Training Iteration (epoch) #:', training_iteration)\n",
    "        history = model.fit(x, y, batch_size=128, epochs=1, verbose=1)    #train 1 epoch at a time using previous weights\n",
    "        sleep(0.1)  # https://github.com/fchollet/keras/issues/2110\n",
    "\n",
    "        # saving models at the following iterations -- uncomment it if you want tos save weights and load it later\n",
    "        # if training_iteration==1 or training_iteration==3 or training_iteration==5 or training_iteration==10 or training_iteration==20 or training_iteration==30 or training_iteration==50 or training_iteration==60 :\n",
    "\n",
    "        # # save every training_iteration of weights\n",
    "        # model.save_weights('Karpathy_LSTM_weights_' + str(training_iteration) + '.h5', overwrite=True)\n",
    "        # start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "        save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "        current_model_file_name = 'LSTM_model_' + str(training_iteration) + '.h5'\n",
    "        if save_all_model_iterations:\n",
    "            save_model(model=model, save_dir=save_dir, model_file_name=current_model_file_name)\n",
    "        sys.stdout.flush()\n",
    "        print('loss is')\n",
    "        print(history.history['loss'][0])\n",
    "        print(history)\n",
    "        print()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e20d106b-4d68-4748-be8e-fbd4d4589628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- MODEL EVALUATION ---------------------------\n",
    "def test_model(model, observations, targets):\n",
    "    '''\n",
    "    STUDENT SHOULD WRITE THIS CODE\n",
    "    :param model: a trained RNN model which accepts a sequence and outputs a target class (0;1;2;3)\n",
    "    :param observations: a list of 40-character sequences to classify\n",
    "    :param targets: a list of the true classes of the 40-character sequences\n",
    "    :return: a sklearn confusion matrix\n",
    "    '''\n",
    "    #< put student code here to test the model >\n",
    "    actual_class_IDs = []\n",
    "    predicted_class_IDs = []\n",
    "    # generate & print confusion matrix to screen\n",
    "    cm = confusion_matrix(actual_class_IDs, predicted_class_IDs)\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdf360b5-558b-4694-a463-99d38f002e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading:  ./textdatasets/tinytesttext.txt\n",
      "corpus length: 5601\n",
      "total chars: 43\n",
      "number of sequences: 5562\n",
      "Vectorization processing... this could take a while...\n",
      " part 1 of 2\n",
      "557  of  5562  completed\n",
      "1113  of  5562  completed\n",
      "1669  of  5562  completed\n",
      "2225  of  5562  completed\n",
      "2782  of  5562  completed\n",
      "3338  of  5562  completed\n",
      "3894  of  5562  completed\n",
      "4450  of  5562  completed\n",
      "5006  of  5562  completed\n",
      " part 2 of 2\n",
      "557  of  5562  completed\n",
      "1113  of  5562  completed\n",
      "1669  of  5562  completed\n",
      "2225  of  5562  completed\n",
      "2782  of  5562  completed\n",
      "3338  of  5562  completed\n",
      "3894  of  5562  completed\n",
      "4450  of  5562  completed\n",
      "5006  of  5562  completed\n",
      "vetorization completed\n",
      "Build model...\n",
      "model is made\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, None, 512)         1138688   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 512)         2099200   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 43)          22059     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, None, 43)          0         \n",
      "=================================================================\n",
      "Total params: 3,259,947\n",
      "Trainable params: 3,259,947\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Iteration (epoch) #: 1\n",
      "44/44 [==============================] - 4s 15ms/step - loss: 3.2065\n",
      "Saved trained model at c:\\Users\\Brett\\Documents\\AFIT\\courses\\CSCE 823\\CSCE 823 2022\\06-Assignments\\HW4\\saved_models\\LSTM_model_1.h5 \n",
      "loss is\n",
      "3.206477403640747\n",
      "<keras.callbacks.History object at 0x000002677356D400>\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Iteration (epoch) #: 2\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 2.9615\n",
      "Saved trained model at c:\\Users\\Brett\\Documents\\AFIT\\courses\\CSCE 823\\CSCE 823 2022\\06-Assignments\\HW4\\saved_models\\LSTM_model_2.h5 \n",
      "loss is\n",
      "2.961491346359253\n",
      "<keras.callbacks.History object at 0x0000026703CEBC10>\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Iteration (epoch) #: 3\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 2.5715\n",
      "Saved trained model at c:\\Users\\Brett\\Documents\\AFIT\\courses\\CSCE 823\\CSCE 823 2022\\06-Assignments\\HW4\\saved_models\\LSTM_model_3.h5 \n",
      "loss is\n",
      "2.571470260620117\n",
      "<keras.callbacks.History object at 0x000002670565BE50>\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Iteration (epoch) #: 4\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 2.2675\n",
      "Saved trained model at c:\\Users\\Brett\\Documents\\AFIT\\courses\\CSCE 823\\CSCE 823 2022\\06-Assignments\\HW4\\saved_models\\LSTM_model_4.h5 \n",
      "loss is\n",
      "2.267500400543213\n",
      "<keras.callbacks.History object at 0x000002670569B940>\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Iteration (epoch) #: 5\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 1.9489\n",
      "Saved trained model at c:\\Users\\Brett\\Documents\\AFIT\\courses\\CSCE 823\\CSCE 823 2022\\06-Assignments\\HW4\\saved_models\\LSTM_model_5.h5 \n",
      "loss is\n",
      "1.9488897323608398\n",
      "<keras.callbacks.History object at 0x0000026779F20E80>\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Iteration (epoch) #: 6\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 1.5549\n",
      "Saved trained model at c:\\Users\\Brett\\Documents\\AFIT\\courses\\CSCE 823\\CSCE 823 2022\\06-Assignments\\HW4\\saved_models\\LSTM_model_6.h5 \n",
      "loss is\n",
      "1.554852843284607\n",
      "<keras.callbacks.History object at 0x00000267056411F0>\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Iteration (epoch) #: 7\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 1.1151\n",
      "Saved trained model at c:\\Users\\Brett\\Documents\\AFIT\\courses\\CSCE 823\\CSCE 823 2022\\06-Assignments\\HW4\\saved_models\\LSTM_model_7.h5 \n",
      "loss is\n",
      "1.1151454448699951\n",
      "<keras.callbacks.History object at 0x0000026705E4D9D0>\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Iteration (epoch) #: 8\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 0.7598\n",
      "Saved trained model at c:\\Users\\Brett\\Documents\\AFIT\\courses\\CSCE 823\\CSCE 823 2022\\06-Assignments\\HW4\\saved_models\\LSTM_model_8.h5 \n",
      "loss is\n",
      "0.7598181366920471\n",
      "<keras.callbacks.History object at 0x0000026705623910>\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Iteration (epoch) #: 9\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 0.5376\n",
      "Saved trained model at c:\\Users\\Brett\\Documents\\AFIT\\courses\\CSCE 823\\CSCE 823 2022\\06-Assignments\\HW4\\saved_models\\LSTM_model_9.h5 \n",
      "loss is\n",
      "0.5375617742538452\n",
      "<keras.callbacks.History object at 0x000002670563BD60>\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Iteration (epoch) #: 10\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 0.4273\n",
      "Saved trained model at c:\\Users\\Brett\\Documents\\AFIT\\courses\\CSCE 823\\CSCE 823 2022\\06-Assignments\\HW4\\saved_models\\LSTM_model_10.h5 \n",
      "loss is\n",
      "0.42725321650505066\n",
      "<keras.callbacks.History object at 0x0000026705E95220>\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Iteration (epoch) #: 11\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 0.3677\n",
      "Saved trained model at c:\\Users\\Brett\\Documents\\AFIT\\courses\\CSCE 823\\CSCE 823 2022\\06-Assignments\\HW4\\saved_models\\LSTM_model_11.h5 \n",
      "loss is\n",
      "0.36772170662879944\n",
      "<keras.callbacks.History object at 0x0000026703D01880>\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Iteration (epoch) #: 12\n",
      "44/44 [==============================] - 1s 14ms/step - loss: 0.3265\n",
      "Saved trained model at c:\\Users\\Brett\\Documents\\AFIT\\courses\\CSCE 823\\CSCE 823 2022\\06-Assignments\\HW4\\saved_models\\LSTM_model_12.h5 \n",
      "loss is\n",
      "0.3265213966369629\n",
      "<keras.callbacks.History object at 0x000002670A22E9A0>\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Iteration (epoch) #: 13\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 0.2985\n",
      "Saved trained model at c:\\Users\\Brett\\Documents\\AFIT\\courses\\CSCE 823\\CSCE 823 2022\\06-Assignments\\HW4\\saved_models\\LSTM_model_13.h5 \n",
      "loss is\n",
      "0.2984510362148285\n",
      "<keras.callbacks.History object at 0x000002670A23DCA0>\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Iteration (epoch) #: 14\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 0.2777\n",
      "Saved trained model at c:\\Users\\Brett\\Documents\\AFIT\\courses\\CSCE 823\\CSCE 823 2022\\06-Assignments\\HW4\\saved_models\\LSTM_model_14.h5 \n",
      "loss is\n",
      "0.2777194082736969\n",
      "<keras.callbacks.History object at 0x0000026705E3BA00>\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Iteration (epoch) #: 15\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 0.2615\n",
      "Saved trained model at c:\\Users\\Brett\\Documents\\AFIT\\courses\\CSCE 823\\CSCE 823 2022\\06-Assignments\\HW4\\saved_models\\LSTM_model_15.h5 \n",
      "loss is\n",
      "0.2614704370498657\n",
      "<keras.callbacks.History object at 0x000002670BD696A0>\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Iteration (epoch) #: 16\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 0.2491\n",
      "Saved trained model at c:\\Users\\Brett\\Documents\\AFIT\\courses\\CSCE 823\\CSCE 823 2022\\06-Assignments\\HW4\\saved_models\\LSTM_model_16.h5 \n",
      "loss is\n",
      "0.24905705451965332\n",
      "<keras.callbacks.History object at 0x0000026705E93250>\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Iteration (epoch) #: 17\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 0.2397\n",
      "Saved trained model at c:\\Users\\Brett\\Documents\\AFIT\\courses\\CSCE 823\\CSCE 823 2022\\06-Assignments\\HW4\\saved_models\\LSTM_model_17.h5 \n",
      "loss is\n",
      "0.2396782487630844\n",
      "<keras.callbacks.History object at 0x000002670BD60880>\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Iteration (epoch) #: 18\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 0.2307\n",
      "Saved trained model at c:\\Users\\Brett\\Documents\\AFIT\\courses\\CSCE 823\\CSCE 823 2022\\06-Assignments\\HW4\\saved_models\\LSTM_model_18.h5 \n",
      "loss is\n",
      "0.23069070279598236\n",
      "<keras.callbacks.History object at 0x0000026714D80EB0>\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Iteration (epoch) #: 19\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 0.2241\n",
      "Saved trained model at c:\\Users\\Brett\\Documents\\AFIT\\courses\\CSCE 823\\CSCE 823 2022\\06-Assignments\\HW4\\saved_models\\LSTM_model_19.h5 \n",
      "loss is\n",
      "0.22410066425800323\n",
      "<keras.callbacks.History object at 0x0000026714D701C0>\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Iteration (epoch) #: 20\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 0.2190\n",
      "Saved trained model at c:\\Users\\Brett\\Documents\\AFIT\\courses\\CSCE 823\\CSCE 823 2022\\06-Assignments\\HW4\\saved_models\\LSTM_model_20.h5 \n",
      "loss is\n",
      "0.21900542080402374\n",
      "<keras.callbacks.History object at 0x000002670BD91760>\n",
      "\n",
      "seed string -->  certainly\n",
      "The generated text is: \n",
      "certainly be confessed that the worst, the most tiresome,\n",
      "and the most dangerous of errors hitherto has been a dogmatist\n",
      "error--namely, plato's invention of pure spirit and the good in itself.\n",
      "but now when it "
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "#--------------------- Main Code -----------------------------\n",
    "\n",
    "\n",
    "# pick the filename you want to use, and comment out the rest\n",
    "# make sure you have this directory structure\n",
    "# QUICK CHECK w/SMALL TEXT\n",
    "raw_text_filename='./textdatasets/tinytesttext.txt'\n",
    "# CLASS DATASETS BELOW\n",
    "# raw_text_filename='./textdatasets/0_bible.txt'\n",
    "# raw_text_filename='./textdatasets/1_nietzsche.txt'\n",
    "# raw_text_filename='./textdatasets/2_shakespeare.txt'\n",
    "# raw_text_filename='./textdatasets/3_warpeace.txt'\n",
    "\n",
    "\n",
    "\n",
    "#raw_text_filename='./textdatasets/trumptweets.txt'\n",
    "\n",
    "processed_filename = raw_text_filename+'.pklz'  # save process will append a .pklz on the filename\n",
    "\n",
    "TEST_SAVE_LOAD_EQUAL = False\n",
    "if TEST_SAVE_LOAD_EQUAL:\n",
    "    text, chars, char_indices, indices_char, sentences, next_chars \\\n",
    "        = preprocess_text_file(raw_text_filename)\n",
    "    save_processed_text(processed_filename, text, chars, char_indices, indices_char, sentences, next_chars )\n",
    "    _text, _chars, _char_indices, _indices_char, _sentences, _next_chars = load_processed_text(processed_filename)\n",
    "    print(np.array_equal(_text, text))\n",
    "    print(np.array_equal(_chars, chars))\n",
    "    print(np.array_equal(_char_indices, char_indices))\n",
    "    print(np.array_equal(_indices_char, indices_char))\n",
    "    print(np.array_equal(_sentences, sentences))\n",
    "    print(np.array_equal(_next_chars, next_chars))\n",
    "    print(\"testing vectorization\")\n",
    "    X, y = vectorize_text(_chars, _char_indices, _sentences, _next_chars)\n",
    "\n",
    "PROCESS_RAW_TEXT = True  #set to True to process a previously unseen textfile - otherwise load a preprocessed file\n",
    "if PROCESS_RAW_TEXT:\n",
    "    text, chars, char_indices, indices_char, sentences, next_chars \\\n",
    "        = preprocess_text_file(raw_text_filename)\n",
    "    save_processed_text(processed_filename, text, chars, char_indices, indices_char, sentences, next_chars)\n",
    "else:  # instead, load previously processed text\n",
    "    text, chars, char_indices, indices_char, sentences, next_chars = load_processed_text(processed_filename)\n",
    "\n",
    "\n",
    "#vectorized form takes too much space to save... so process in real time\n",
    "X, y = vectorize_text(chars, char_indices, sentences, next_chars)\n",
    "\n",
    "\n",
    "TRAIN_MODE = True\n",
    "if TRAIN_MODE:\n",
    "    model = build_model(characters=chars)\n",
    "    model_epoch_training_iterations = 20    #the bigger your text corpus, the smaller you can make this\n",
    "    model = train_net(model=model, x=X, y=y,\n",
    "                      training_iterations=model_epoch_training_iterations,\n",
    "                      save_all_model_iterations=True)\n",
    "else:  # load a model from a file\n",
    "    # decide which iteration of the trained model you want to explore\n",
    "    model_training_iteration = 8\n",
    "    save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "    current_model_file_name = 'LSTM_model_' + str(model_training_iteration) + '.h5'\n",
    "    model = load_model(save_dir=save_dir, model_file_name=current_model_file_name)\n",
    "\n",
    "GENERATE_TEXT_MODE = True\n",
    "if GENERATE_TEXT_MODE:   # generate text mode\n",
    "    #decide which saved model to load\n",
    "    #make up a string of characters to start with\n",
    "    seed_string = \"certainly\"\n",
    "    # decide how many text characters you want to generate:\n",
    "    gen_char_count = 200\n",
    "    generate_text(model, char_indices, indices_char, seed_string, generate_character_count=gen_char_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f17f35c-1b4c-4022-9a59-32e70dc5d5af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd5103-7850-4e7d-8515-a2a07c51462f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "72634b88368ab24c21ef18650be59b2fd199dcd8032c106ede099bcd9f0f2c34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
