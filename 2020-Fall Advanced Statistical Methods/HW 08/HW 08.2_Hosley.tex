\documentclass[a4paper,man,natbib]{apa6}
\usepackage[english]{babel}

\usepackage[cache=false]{minted}
\usemintedstyle{vs}
\usepackage{xcolor}
\definecolor{bg}{rgb}{.95,.95,.95}

\graphicspath{ {./images/} }
\usepackage{graphicx}
\usepackage{caption}

\usepackage{setspace}
\usepackage{amsmath}


\title{Advanced Statistical Methods Homework 8 \\ Support Vector Machine}
\shorttitle{DAT 530 HW7}
\author{Brandon Hosley}
\date{\today}
\affiliation{University of Illinois - Springfield}
%\abstract{}

\begin{document}
\maketitle
\singlespacing

\section{Intro to the Algorithm}

Support Vector Machines are a type of classification algorithm that is most commonly used for binary classification problems. Classification is performed by calculating a boundary to act as a decision line. Typically the boundary is optimized by calculating it to be as far from the sample points as possible.

\subsection{Intuition}

The boundary can be thought of as a hyperplane of one dimension less than the number of factors being used. A dataset with two factors may be represented as a two-dimensional graph, the classifier will be a one-dimensional hyperplane (a line) dividing the data, and the classifier will optimized to be a far from each cluster as possible.

The maximum margin can be achieved by plotting the hyperplane in a way such that at least three of the closest points to the line are equidistant; with at least one point coming from each class. In the case of overlapping classes or classes too close and too dense the hyperplane is calculated such that a fixed margin will contain the fewest possible points.

\begin{center}
	\includegraphics[width=0.5\linewidth]{Margins}
\end{center}

In the case of a linear classifier of the classic $y=wx + b$ notation we get the following equation. $\lambda$ will represent the size of the margin.

\vspace{1em}
$ \left[ \frac{1}{n} \sum_{n}^{i=1} \text{max} (0,1-y_i(w \cdot x_i-b)) \right] + \lambda \|w\|^2 $
\vspace{1em}

\subsection{e1071}

The library used for this project will be e1071, which includes an SVM module. The type of SVM trained by e1071 is one in which the final model is a voting ensemble of $k(k-1)/2$ binary classifiers rather than a $k-1$ dimensional hyperplane. The ensemble method likely generalizes better as a library and is likely more robust under a large variety of user abilities and knowledge levels.

\clearpage

\section{Applying the Algorithm to Boston}
\subsection{Description of the Problem}

The \textcolor{red}{Boston} data set has been a staple of this course. As we have gained a significant familiarity with the dataset it seems like a good option for exercising SVM use. A primary consideration when choosing a dataset for demonstrating SVMs is the limitation of input and output data. Input data should be quantitative and may be either continuous or discrete. The output is binary. 

A common use of the \textcolor{red}{Boston} dataset, and what we will use it for in this assignment is using other attributes to predict crime in certain districts of the city. The data provided in \textcolor{red}{Boston} is Quantitative with the exception of 'chas' which is a binary variable representing whether or not the district borders the Charles River. It is provided as a dummy variable; for this exercise we will not be using it.

\subsection{Summary of the Dataset and Preliminaries}

Boston contains data gathered from a Census of the city of Boston Massachusetts taken in 1978. First we will load it into memory, load the e1071 library, and set a seed for repeatability. 

\begin{minted}[bgcolor=bg]{r}
library(MASS)
library(e1071)
set.seed(1234)
\end{minted}

\subsection{Training an SVM Model}

To begin we will make a copy of our data for preparation, remove the 'chas' variable, and normalize the data such that all variables will have a mean equal to zero and a standard deviation of one.

\begin{minted}[bgcolor=bg]{r}
b = Boston
# Remove Chas variable
b <- subset(b,select=-c(chas))
# Normalize the Data (mean=0, SD = 1)
b <- as.data.frame(scale(b))
\end{minted}

Next we will need to change the Crime statistic to be a factor, in this case we will only be predicting whether an area will be 'High Crime' or 'Low Crime' which specifically means whether the district will be in the upper 50\% of crime or in the lower.

\begin{minted}[bgcolor=bg]{r}
# Crim as a Factor(High = 1, Low = 0)
b$crim <- ifelse(b$crim > 0, 1, 0)
b$crim <- as.factor(b$crim)
\end{minted}

Now we will separate the data into a training set and a test set.

\begin{minted}[bgcolor=bg]{r}
# Separate a Training and Testing Set
ind = sort(sample(nrow(b), nrow(b)*0.8))
b_train <- b[ind,]
b_test <- b[-ind,]
\end{minted}

For the sake of exploring options we will train both a linear SVM and a polynomial SVM.

\begin{minted}[bgcolor=bg]{r}
# Training both a Linear and Polynomial Model
svm_lin=svm(crim~., data=b_train, kernel ="linear", cost=1, scale=FALSE)
svm_poly=svm(crim~., data=b_train, kernel ="polynomial", cost=1, 
				scale=FALSE)
\end{minted}

Let us take a look at the summaries of the models trained above.

\begin{minted}[bgcolor=bg]{r}
summary(svm_lin)
\end{minted}

\vspace{-0.5em}

\begin{minted}{bash}
Call:
svm(formula = crim ~ ., data = b_train, kernel = "linear", 
              cost = 1, scale = FALSE)

Parameters:
    SVM-Type:  C-classification 
  SVM-Kernel:  linear 
        cost:  1 
       gamma:  0.08333333 

Number of Support Vectors:  16
( 9 7 )

Number of Classes:  2 
Levels: 
0 1
\end{minted}

\begin{minted}[bgcolor=bg]{r}
summary(svm_poly)
\end{minted}

\vspace{-0.5em}

\begin{minted}{bash}
Call:
svm(formula = crim ~ ., data = b_train, kernel = "polynomial", 
              cost = 1, scale = FALSE)

Parameters:
    SVM-Type:  C-classification 
  SVM-Kernel:  polynomial 
        cost:  1 
      degree:  3 
       gamma:  0.08333333 
      coef.0:  0 

Number of Support Vectors:  38
( 24 14 )

Number of Classes:  2 
Levels: 
0 1
\end{minted}

\subsection{Summary of the Results}

The models both appear to have a reasonable number of support vectors.
Now we will need some predictions to test how effective the models are.

\begin{minted}[bgcolor=bg]{r}
# Calculate Predictions
pred_lin = predict(svm_lin, b_test)
pred_poly = predict(svm_poly, b_test)
\end{minted}

We can then compare these predictions to the ground truth of the test set.

\begin{minted}[bgcolor=bg]{r}
table(predict=pred_lin, truth=b_test$crim)
\end{minted}

\begin{minted}{bash}
       truth
predict  0  1
      0 77  0
      1  1 24
\end{minted}

\begin{minted}[bgcolor=bg]{r}
table(predict=pred_poly, truth=b_test$crim)
\end{minted}

\begin{minted}{bash}
       truth
predict  0  1
      0 77  0
      1  1 24
\end{minted}

The models performance is identical. In this case with all other hyper-parameters being equal the linear kernel SVM is a better choice. Linear models are simpler than Polynomials; in this case the linear model requires less than half as many support vectors; and in the end, the linear model and the polynomial model make the same mistake.

\end{document}

\begin{minted}[bgcolor=bg]{r}
\end{minted}