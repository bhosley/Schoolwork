\documentclass[xcolor={svgnames},aspectratio=169]{beamer}
\usepackage[english]{babel}
\usepackage{lmodern}
\usepackage{libertinus}
\usepackage{csquotes}
\usepackage{multicol}
\usepackage{caption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{comment}
% \usepackage[svgnames]{xcolor} % Must pass direct to beamer class

\usepackage{graphicx}
\graphicspath{{./images/}}
\usepackage[backend=biber,style=ieee,style=authortitle]{biblatex} %authortitle authoryear
\addbibresource{../2025Bibs/Prospectus.bib}
\usepackage{xpatch}
\xapptobibmacro{cite}{\setunit{\nametitledelim}\printfield{year}}{}{}


\title{Working Title}
% \subtitle{A Prospectus Defense}
\author{Capt. Brandon Hosley\inst{1}}
\institute[ENS]{
    \inst{1}
    Department of Operational Sciences\\
    Air Force Institute of Technology}
\date{\today}

\titlegraphic{
    \includegraphics[width=2cm]{afit_logo.png} \hfill
    \includegraphics[width=2cm]{en_logo.png}} 

\AtBeginSection[]{
  \begin{frame}
    \tableofcontents[currentsection, hideothersubsections]
  \end{frame}
}

% \usetheme{Madrid}
% \usecolortheme{beaver}

% \usetheme{CambridgeUS}
% \usecolortheme{seahorse}

\usetheme{Montpellier}
\usecolortheme{orchid}
\mode<presentation>

\begin{document}

% Title slide
\frame{\titlepage}
\begin{frame}
    \tableofcontents[subsectionstyle=hide]
\end{frame}

% Slide 1: Introduction and Objectives
\section{Introduction}

\begin{frame}{First Slide}
    %\textbf{Dissertation Theme:} 
\end{frame}
% Overview:
% Timeline?
% RL -> MARL -> HARL
% MARL/HARL Challenges

% Military Motivation
% Examples of Military Motivation

% Round back to through line (Jenn's "How does FL work?)
% Open Challenges

% Research Contributions

% Research Objectives

% For each Contribution:
%     Introduction
%         Lit review
%         Contribution
%     Methdology
%     Experimental Procedure
%     Results
%     Discussion

\section{Contribution 1}

\subsection{Introduction}

\begin{frame}
\end{frame}

\subsection{Research Questions}

\begin{frame}{Research Questions}
    \begin{enumerate}
        \item[RQ 1] {
            Can pretraining smaller teams of agents and then scaling to the target 
            team size via policy duplication and retraining improve training efficiency 
            without sacrificing final policy performance in MARL?}
        % \item[RQ 2] \textcolor{lightgray}{
        \item[RQ 2] {
            How does the effectiveness of this direct scaling strategy vary across 
            environments with different forms of agent heterogeneity 
            (e.g., behavioral vs. intrinsic)?}
    \end{enumerate}
\end{frame}

\begin{frame}{RQ 1 - Research Tasks}
    \begin{itemize}
        \item[RT 1.1] Design a curriculum-based upsampling strategy using policy duplication and retraining.
        \item[RT 1.2] Define an efficiency metric (agent-steps) that accounts for agent count and training time.
        \item[RT 1.3] Run full tabula rasa baselines for each target environment and team size.
        \item[RT 1.4] Evaluate training performance across various pretraining durations and target team sizes.
    \end{itemize}
\end{frame}

\begin{frame}{RQ 2 - Research Tasks}
    \begin{itemize}
        \item[RT 2.1] {Select environments that represent distinct forms of agent heterogeneity.}
        \item[RT 2.2] {Adapt observation structures to enable fixed policy architectures across team sizes.}
        \item[RT 2.3] {Evaluate the effect of heterogeneity type on the scalability and retraining benefit.}
    \end{itemize}
\end{frame}

\section{Contribution 2}


\subsection{Research Questions}

\begin{frame}{Contribution 2 - Research Questions}
    \begin{enumerate}
        \item[RQ 1] {
            How does the use of input-invariant neural architectures (e.g., permutation or 
            vector-length invariance) affect training efficiency and policy performance in 
            MARL settings?}
        \item[RQ 2] {
            Can shared policy networks with invariant input structures improve learning and 
            robustness in heterogeneous-agent teams with partially overlapping observation spaces?}
        \item[RQ 3] {
            Do input-invariant architectures improve agent resilience to team-size variation 
            or partial sensor failure during execution?}
    \end{enumerate}
\end{frame}

\begin{frame}{RQ 1 - Research Tasks}
    \begin{itemize}
        \item[RT 1.1] {
            Design an implement input-invariant policy architecture (e.g., 
            pooling layers, attention, permutation-invariant encodings).}
        \item[RT 1.2] {
            Evaluate performance against baseline architectures on standardized 
            MARL benchmarks (e.g., LBF, Waterworld).}
        \item[RT 1.3] {
            Compare training curves, sample efficiency, and final performance metrics 
            across architecture types.}
    \end{itemize}
\end{frame}

\begin{frame}{RQ 2 - Research Tasks}
    \begin{itemize}
        \item[RT 2.1] {
            Identify benchmark environment where agents have distinct but partially overlapping 
            observation features (e.g., different sensor arrays).}
        \item[RT 2.2] {
            Train shared policy networks across these agents and measure update 
            utilization and convergence speed.}
        \item[RT 2.3] {
            Compare against architectures with isolated policy networks to assess 
            benefits of shared learning.}
    \end{itemize}
\end{frame}

\begin{frame}{RQ 3 - Research Tasks}
    \begin{itemize}
        \item[RT 3.1] {
            Simulate runtime degradation by selectively masking agent inputs 
            (e.g., sensor failure).}
        \item[RT 3.2] {
            Simulate dynamic changes in team size by removing agents during evaluation.}
        \item[RT 3.3] {
            Measure policy stability, reward degradation, and recovery behavior under 
            these perturbations.}
    \end{itemize}
\end{frame}

\section{Contribution 3}


\subsection{Research Questions}

\begin{frame}{Contribution 3 - Research Questions}
    \begin{enumerate}
        \item[RQ 1] {
            Is it feasible to use tensor-based projections to grow policy network capacity 
            over time during training in multi-agent reinforcement learning?}
        \item[RQ 2] {
            Does progressively expanding the policy network architecture reduce total 
            training cost or improve final performance compared to fixed-size networks?}
    \end{enumerate}
\end{frame}




\begin{frame}{RQ 1 - Research Tasks}
    \begin{itemize}
        \item[RT 1.1] {
            Choose a method to project policy networks into higher-dimensional tensor 
            representations at scheduled points during training.}
        \item[RT 1.2] {
            Ensure that the projected networks retain comparable output behavior 
            post-projection.} %(e.g., bounded error on inference)
        \item[RT 1.3] {
            Integrate projection-based expansion into a curriculum training pipeline 
            and verify stability.}
    \end{itemize}
    \end{frame}


    \begin{frame}{RQ 2 - Research Tasks}
        \begin{itemize}
            \item[RT 2.1] {
                Compare projected curricula against fixed-size networks of both small and 
                large sizes across multiple MARL environments.}
            \item[RT 2.2] {
                Measure training cost using agent-steps, wall-clock time, and parameter updates.}
            \item[RT 2.3] {
                Evaluate final performance, sample efficiency, and learning curves to determine 
                net benefit of architectural growth.}
        \end{itemize}
        \end{frame}

\section{Conclusion}

% Begin Lit review

% Evolution of RL in Games

% \begin{frame}{Evolution of Reinforcement Learning in Games}
%     \textbf{Key References:}
%     \begin{itemize}
%         \item Go \footcite{silver2016}$^,$\footcite{silver2017}
%         \item Chess\footcite{silver2017a}
%         \item StarCraft II\footcite{vinyals2019}
%         \item DOTA\footcite{berner2019}
%         \item \textbf{Challenge:} High resource demands highlight the need for more efficient 
%             training methods.
%     \end{itemize}
% \end{frame}

% % Foundations of MARL
% \begin{frame}{Foundations of MARL}
%     \textbf{Key References:}
%     \begin{itemize}
%         \item Shoham et al.\footcite{shoham2007a}, Busonui et al.\footcite{busoniu2008}, 
%             and Cao et al.\footcite{cao2012}.
%         \item Early concepts in multi-agent learning, distributed coordination.
%         \item \textbf{Challenges:} Non-stationarity, credit assignment, 
%             \textcolor{DarkBlue}{scalability issues}.
%     \end{itemize}
% \end{frame}

% % Scalability Challenges in MARL
% \begin{frame}{Scalability Challenges in MARL}
%     \textbf{Key Insights}
%     \begin{itemize}
%         \item \textbf{Agent Complexity:} Action-state space explosion.
%             \footcite{lillicrap2019}$^,$~\footcite{baker2019}$^,$~\footcite{leibo2021}
%         \item \textbf{Environment Complexity:} Increased observation-action space.
%             $^{11,}$~\footcite{ye2020}$^,$~\footcite{shukla2022}$^,$~\footcite{liang2024}
%         \item \textbf{Robustness:} Sensitivity to disturbances and adversarial influences.
%             \footcite{gleave2021}$^,$~\footcite{li2019}$^,$~\footcite{spooner2020}$^,$~\footcite{guo2022}
%     \end{itemize}
%     \vspace{0.5em}
% \end{frame}
% %(Liu et al., 2024) liu2024

% % MARL Scalability Strategies

% \begin{frame}{MARL Scalability Strategies}
%     \textbf{Notable Strategies:}
%     \begin{itemize}
%         \item Value decomposition to handle dynamic populations.
%             \footcite{zhang2021}$^,$~\footcite{nguyen2020}
%         \item Hierarchical learning and role assignment.
%             \footcite{cui2022}
%         \item \textcolor{DarkBlue}{Curriculum learning} and transfer learning for adaptability.
%             \footcite{shukla2022}$^,$~\footcite{shi2023}
%     \end{itemize}
% \end{frame}

% TODO: Add HARL Slide

% \begin{frame}{Heterogeneous-Agent Reinforcement Learning (HARL)}
%     \textbf{Harl Extends MARL Challenges:}
%     \begin{itemize}
%         \item \textbf{Coordination Complexity:} Heterogeneous roles further complicate 
%             cooperation and strategic alignment.
%             ~\footcite{wakilpoor2020}$^,$~\footcite{yang2020a}$^,$~\footcite{gronauer2022}
%         \item \textbf{Scalability Issues:} Increased joint state-action space makes 
%             learning computationally expensive.
%             ~\footcite{leibo2021}$^,$~\footcite{rizk2019}
%         \item \textbf{Adaptability and Robustness:} Policies must generalize across 
%             changing agent compositions and dynamic environments.
%             ~\footcite{yang2021a}$^,$~\footcite{koster2020}
%     \end{itemize}
% \end{frame}

% % Distinction Between MARL and HARL Variants

% \begin{frame}{Defining MARL and HARL Variants}
%     \textbf{Proposed Distinctions:}
%     \begin{itemize}
%         \item \textbf{MARL (Multi-Agent Reinforcement Learning):}
%         \begin{itemize}
%             \item Agents may share identical structures or learn different behaviors.
%             \item Focuses on collaborative or competitive dynamics among agents.
%         \end{itemize}
%         \item \textbf{Heterodox-Agent RL (Behavioral Heterogeneity):}
%         \begin{itemize}
%             \item \emph{Structural Homogeneity:} Agents have identical observation and action spaces.
%             \item \emph{Behavioral Diversity:} Independent policies allow agents to learn unique behaviors.
%             \item \textit{Example:} Identical drones taking on distinct roles in a cooperative task.
%         \end{itemize}
%         \item \textbf{Heterogeneous-Agent RL (Intrinsic Heterogeneity):}
%         \begin{itemize}
%             \item \emph{Structural Heterogeneity:} Agents differ in observation or action spaces.
%             \item \emph{Behavioral Specialization:} Policies adapt to agents' intrinsic differences.
%             \item \textit{Example:} Combined arms.
%         \end{itemize}
%     \end{itemize}
% \end{frame}

% % Contextual Background


\begin{frame}
    \centering
    \Huge
    Questions?
\end{frame}

\section{References}

\renewcommand*{\bibfont}{\tiny}
\frame[allowframebreaks]{\printbibliography}

\end{document}
