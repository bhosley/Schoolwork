\documentclass[xcolor={svgnames},aspectratio=169]{beamer}
\usepackage[english]{babel}
\usepackage{lmodern}
\usepackage{libertinus}
\usepackage{csquotes}
\usepackage{multicol}
\usepackage{caption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{comment}
% \usepackage[svgnames]{xcolor} % Must pass direct to beamer class

\usepackage{graphicx}
\makeatletter
\def\input@path{{Chapters/}{other/}{Figures/}{Tables/}}
\makeatother
\graphicspath{{Figures}}
\usepackage[backend=biber,style=ieee,style=authortitle]{biblatex} %authortitle authoryear
\addbibresource{../2025Bibs/Prospectus.bib}
\usepackage{xpatch}
\xapptobibmacro{cite}{\setunit{\nametitledelim}\printfield{year}}{}{}


\title{Working Title}
% \subtitle{A Prospectus Defense}
\author{Capt. Brandon Hosley\inst{1}}
\institute[ENS]{
    \inst{1}
    Department of Operational Sciences\\
    Air Force Institute of Technology}
\date{\today}

\titlegraphic{
    \includegraphics[width=2cm]{afit_logo.png} \hfill
    \includegraphics[width=2cm]{en_logo.png}} 

\AtBeginSection[]{
  \begin{frame}
    \tableofcontents[currentsection, hideothersubsections]
  \end{frame}
}

\usetheme{Montpellier}
\usecolortheme{orchid}
\mode<presentation>

\begin{document}

\frame{\titlepage}
\begin{frame}
    \tableofcontents[subsectionstyle=hide, subsubsectionstyle=hide]
\end{frame}

\section{Introduction}

\begin{frame}{The Vision for Autonomy in Defense}
\begin{columns}
    \begin{column}{0.6\textwidth}
        \begin{itemize}
            \item \textbf{Department of Defense initiatives are actively advancing autonomous 
                system deployment.}
            \begin{itemize}
                \item \textbf{Replicator Initiative:} {Calls for the rapid integration of 
                    commercial-off-the-shelf (COTS) autonomous systems for scalable deployment.
                    \footnotemark[1] } % \footnote[frame]{\cite{robertson2023}} }
                \item \textbf{DARPA's OFFSET:} {Demonstrated feasibility of swarm-enabled ground 
                    and aerial assets in contested environments. 
                    \footnotemark[2] } % \footnote[frame]{\cite{zotero-2835}} }
            \end{itemize}
            \item \textbf{Limitation:} {Reliable autonomy in unpredictable environments 
                remains an unsolved challenge.}
        \end{itemize}
    \end{column}
    \begin{column}{0.4\textwidth}
        \begin{figure}[!h]
            \centering
            \includegraphics[width=0.95\textwidth]{replicator_collage.png}
            \caption{\textbf{Replicator Initiative} aimed at multiple platforms. 
                \footnotemark[1] } % \footnote[frame]{\cite{robertson2023}}}
            \label{fig:replicator_collage}
        \end{figure}
    \end{column}
\end{columns}
\footnotetext[1]{\cite{robertson2023}}
\footnotetext[2]{\cite{zotero-2835}}
\end{frame}


\begin{frame}{Rules-Based Control}
    \begin{itemize}
        \item Traditional autonomous systems operate on hand-coded rules.
        \item Effective in constrained environments with predictable dynamics.
        \item Limitations:
        \begin{itemize}
            \item Brittle under changing conditions.
            \item Poor generalization to novel scenarios.
        \end{itemize}
        \item Led to interest in adaptive learning systems.
    \end{itemize}
\end{frame}

\begin{frame}{Reinforcement Learning (RL)}
    \begin{itemize}
        \item RL enables agents to learn behaviors through trial and error in dynamic environments.
        \item Successes in single-agent settings with clear reward signals and isolated tasks.
        \item Limitations:
        \begin{itemize}
            \item Doesn't scale to multi-agent scenarios without coordination failures.
            \item Learned policies often lack robustness to new agent compositions or input changes.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Multi-Agent RL (MARL)}
    \begin{itemize}
        \item MARL extends RL to scenarios involving multiple learning agents.
        \item Used for both cooperative and competitive environments (e.g., games, simulations).
        \item Limitations:
        \begin{itemize}
            \item Assumes agents are often homogeneous or interchangeable.
            \item High coordination cost and non-stationarity challenges.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Heterogeneous-Agent RL (HARL)}
    \begin{itemize}
        \item HARL addresses coordination among agents with different capabilities, sensors, or roles.
        \item Promising for real-world applications with diverse platforms (e.g., COTS drones, mixed teams).
        \item Limitations:
        \begin{itemize}
            \item Limited scalability in training architectures.
            \item Fragile under observation or team composition changes.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Open Challenges in Scalable Multi-Agent Autonomy}
    \begin{itemize}
        \item Integration across heterogeneous platforms is constrained by inflexible model assumptions.
        \item Retraining cost grows rapidly with team size and complexity.
        \item Teams struggle to adapt dynamically to changing sensors or agent availability.
        \item Existing strategies for curriculum learning and network scaling are underdeveloped.
    \end{itemize}
\end{frame}



\begin{frame}{Why Heterogeneous Autonomy Matters}
\begin{columns}
    \begin{column}{0.6\textwidth}
    \begin{itemize}
        \item \textbf{Operational autonomy must account for platform diversity.}
        \begin{itemize}
            \item Teams may integrate UAVs and UGVs with different sensors, dynamics, and connectivity constraints.
            \item Heterogeneous-agent reinforcement learning (HARL) enables coordination across dissimilar platforms.
        \end{itemize}
        \item \textbf{Policy transfer and input-invariant networks} can support:
        \begin{itemize}
            \item Adaptability to new team compositions.
            \item Resilience to partial observation loss or sensor degradation.
        \end{itemize}
    \end{itemize}
    \end{column}
    \begin{column}{0.4\textwidth}
        \begin{figure}[!h]
            \centering
            \includegraphics[width=0.95\textwidth]{offset_collage.jpg}
            \caption{\textbf{OFFSET} test platforms.
                \footnote[frame]{\cite{zotero-2835}}}
            \label{fig:offset_collage}
        \end{figure}
    \end{column}
\end{columns}
\end{frame}


% Research Contributions
\begin{frame}{Research Contributions}
    \begin{itemize}
        \item {% Through-line:
            Advance the study of heterogeneous-agent reinforcement learning by exploring 
            scalable training architectures and policy designs that support adaptability 
            to agent diversity and team variation.}
        \item {% Contribution 1:
            Evaluate a policy upsampling strategy for training larger multi-agent teams 
            more efficiently using pretrained smaller-team policies.}
        \item {% {Contribution 2:
            Investigate input-invariant policy architectures for shared learning and 
            robustness in teams of heterogeneous agents.}
        \item {% Contribution 3: 
            Design and evaluate a curriculum that progressively expands network capacity 
            via tensor projection to improve training efficiency.}
    \end{itemize}
\end{frame}

\begin{frame}{Research Objectives}
    \textbf{Contribution 1}
    % \textbf{Contribution 1 - Direct Scaling of Teams}
    \begin{itemize}
        \item {Improve training efficiency by scaling teams from smaller pretrained groups 
            using policy duplication instead of retraining from scratch.}
        \item {Demonstrate that policy reuse across increasing agent counts can reduce 
            training cost while maintaining performance.}
    \end{itemize}
    \vspace{1em}
    \textbf{Contribution 2}
    % \textbf{Contribution 2 - Input-Invariant Policy Architectures}
    \begin{itemize}
        \item {Construct input-invariant policy architectures to enable shared training 
            updates across agents with heterogeneous observation structures.}
        \item {Evaluate the effectiveness of input-invariant models in improving learning 
            efficiency in teams with overlapping observations.}
        \item {Test whether these models maintain stable performance under dynamic 
            observation space changes during execution.}
    \end{itemize}
\end{frame}

\begin{frame}{Research Objectives}
    \textbf{Contribution 3}
    % \textbf{Contribution 3 - Progressive Network Growth via Tensor Projections}
    \begin{itemize}
        \item {Demonstrate the feasibility of expanding a policy network mid-training 
            using tensor projection without discarding prior knowledge.}
        \item {Identify when during training such growth provides learning advantages 
            over fixed-size architectures.}
        \item {Compare performance and efficiency of progressive-growth networks to 
            fixed-size baselines in terms of convergence and cost.}
    \end{itemize}
\end{frame}


\input{Contribution_1.tex}
\input{Contribution_2.tex}
\input{Contribution_3.tex}

% Prospectus, schedule

\section{Conclusion}

% Begin Lit review

% Evolution of RL in Games

% \begin{frame}{Evolution of Reinforcement Learning in Games}
%     \textbf{Key References:}
%     \begin{itemize}
%         \item Go \footcite{silver2016}$^,$\footcite{silver2017}
%         \item Chess\footcite{silver2017a}
%         \item StarCraft II\footcite{vinyals2019}
%         \item DOTA\footcite{berner2019}
%         \item \textbf{Challenge:} High resource demands highlight the need for more efficient 
%             training methods.
%     \end{itemize}
% \end{frame}

% % Foundations of MARL
% \begin{frame}{Foundations of MARL}
%     \textbf{Key References:}
%     \begin{itemize}
%         \item Shoham et al.\footcite{shoham2007a}, Busonui et al.\footcite{busoniu2008}, 
%             and Cao et al.\footcite{cao2012}.
%         \item Early concepts in multi-agent learning, distributed coordination.
%         \item \textbf{Challenges:} Non-stationarity, credit assignment, 
%             \textcolor{DarkBlue}{scalability issues}.
%     \end{itemize}
% \end{frame}

% % Scalability Challenges in MARL
% \begin{frame}{Scalability Challenges in MARL}
%     \textbf{Key Insights}
%     \begin{itemize}
%         \item \textbf{Agent Complexity:} Action-state space explosion.
%             \footcite{lillicrap2019}$^,$~\footcite{baker2019}$^,$~\footcite{leibo2021}
%         \item \textbf{Environment Complexity:} Increased observation-action space.
%             $^{11,}$~\footcite{ye2020}$^,$~\footcite{shukla2022}$^,$~\footcite{liang2024}
%         \item \textbf{Robustness:} Sensitivity to disturbances and adversarial influences.
%             \footcite{gleave2021}$^,$~\footcite{li2019}$^,$~\footcite{spooner2020}$^,$~\footcite{guo2022}
%     \end{itemize}
%     \vspace{0.5em}
% \end{frame}
% %(Liu et al., 2024) liu2024

% % MARL Scalability Strategies

% \begin{frame}{MARL Scalability Strategies}
%     \textbf{Notable Strategies:}
%     \begin{itemize}
%         \item Value decomposition to handle dynamic populations.
%             \footcite{zhang2021}$^,$~\footcite{nguyen2020}
%         \item Hierarchical learning and role assignment.
%             \footcite{cui2022}
%         \item \textcolor{DarkBlue}{Curriculum learning} and transfer learning for adaptability.
%             \footcite{shukla2022}$^,$~\footcite{shi2023}
%     \end{itemize}
% \end{frame}

% TODO: Add HARL Slide

% \begin{frame}{Heterogeneous-Agent Reinforcement Learning (HARL)}
%     \textbf{Harl Extends MARL Challenges:}
%     \begin{itemize}
%         \item \textbf{Coordination Complexity:} Heterogeneous roles further complicate 
%             cooperation and strategic alignment.
%             ~\footcite{wakilpoor2020}$^,$~\footcite{yang2020a}$^,$~\footcite{gronauer2022}
%         \item \textbf{Scalability Issues:} Increased joint state-action space makes 
%             learning computationally expensive.
%             ~\footcite{leibo2021}$^,$~\footcite{rizk2019}
%         \item \textbf{Adaptability and Robustness:} Policies must generalize across 
%             changing agent compositions and dynamic environments.
%             ~\footcite{yang2021a}$^,$~\footcite{koster2020}
%     \end{itemize}
% \end{frame}

% % Distinction Between MARL and HARL Variants

% \begin{frame}{Defining MARL and HARL Variants}
%     \textbf{Proposed Distinctions:}
%     \begin{itemize}
%         \item \textbf{MARL (Multi-Agent Reinforcement Learning):}
%         \begin{itemize}
%             \item Agents may share identical structures or learn different behaviors.
%             \item Focuses on collaborative or competitive dynamics among agents.
%         \end{itemize}
%         \item \textbf{Heterodox-Agent RL (Behavioral Heterogeneity):}
%         \begin{itemize}
%             \item \emph{Structural Homogeneity:} Agents have identical observation and action spaces.
%             \item \emph{Behavioral Diversity:} Independent policies allow agents to learn unique behaviors.
%             \item \textit{Example:} Identical drones taking on distinct roles in a cooperative task.
%         \end{itemize}
%         \item \textbf{Heterogeneous-Agent RL (Intrinsic Heterogeneity):}
%         \begin{itemize}
%             \item \emph{Structural Heterogeneity:} Agents differ in observation or action spaces.
%             \item \emph{Behavioral Specialization:} Policies adapt to agents' intrinsic differences.
%             \item \textit{Example:} Combined arms.
%         \end{itemize}
%     \end{itemize}
% \end{frame}

% % Contextual Background


\begin{frame}
    \centering
    \Huge
    Questions?
\end{frame}

\section{References}

\renewcommand*{\bibfont}{\tiny}
\frame[allowframebreaks]{\printbibliography}

\end{document}
