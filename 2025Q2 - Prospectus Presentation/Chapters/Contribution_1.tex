\section{Contribution 1}

% For each Contribution:
% Re-motiviate
%     Introduction
%         Lit review
%         Contribution
%     Methodology
%     Experimental Procedure
%     Results
%     Discussion

\subsection{Introduction}

\begin{frame}{Context for Contribution 1}
    \textbf{Motivation:}
    \begin{itemize}
        \item Inspired by Smit et al.'s~\footcite{smit2023} 
        work using MARL to scale training in 2v2 to playing 11v11 in a football-like game.
    \end{itemize}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \textbf{Method:}
            \begin{enumerate}
                \item Trained teams with 4 methods
                \begin{itemize}
                    \item Naive Heuristic
                    \item ``Team Work'' Heuristic
                    \item 2v2 Trained Agents
                    \item 11v11 Trained Agents
                \end{itemize}
                \item Evaluated through full games (11v11) against each other.
            \end{enumerate}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=0.45\linewidth]{smit2v2.png}
                \includegraphics[width=0.45\linewidth]{smit11v11.png}
                \caption{Smit's Game, 2v2 and 11v11 versions.}
                \label{table:smit_envs}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Context for Contribution 1}
    \begin{table}
        \centering
        \includegraphics[width=0.75\linewidth]{smit_results.png}
        \captionsetup{width=.75\linewidth}
        \caption{\footnotesize Win rate of row teams vs column teams;
            500 games of 11v11.~\footcite{smit2023}}
        \label{table:smit_winrates}
    \end{table}

    \textbf{Key Insights from Prior Work}
    \begin{itemize}
        \item Challenges: computational limit and generalization issues.
        \item Their findings suggest that agents trained in full-team settings demonstrated 
            \emph{better differentiation of individual responsibilities}.
    \end{itemize}
\end{frame}

\subsubsection{Literature Review}

% Begin Lit review

% Evolution of RL in Games

\begin{frame}{Evolution of Reinforcement Learning in Games}
    \textbf{Key References:}
    \begin{itemize}
        \item Go \footcite{silver2016}$^,$\footcite{silver2017}
        \item Chess\footcite{silver2017a}
        \item StarCraft II\footcite{vinyals2019}
        \item DOTA\footcite{berner2019}
        \item \textbf{Challenge:} High resource demands highlight the need for more efficient 
            training methods.
    \end{itemize}
\end{frame}

% Foundations of MARL
\begin{frame}{Foundations of MARL}
    \textbf{Key References:}
    \begin{itemize}
        \item Shoham et al.\footcite{shoham2007}, Busonui et al.\footcite{busoniu2008}, 
            and Cao et al.\footcite{cao2012}.
        \item Early concepts in multi-agent learning, distributed coordination.
        \item \textbf{Challenges:} Non-stationarity, credit assignment, 
            \textcolor{DarkBlue}{scalability issues}.
    \end{itemize}
\end{frame}

% Scalability Challenges in MARL
\begin{frame}{Scalability Challenges in MARL}
    \textbf{Key Insights}
    \begin{itemize}
        \item \textbf{Agent Complexity:} Action-state space explosion.
            \footcite{lillicrap2019}$^,$~\footcite{baker2019}$^,$~\footcite{leibo2021}
        \item \textbf{Environment Complexity:} Increased observation-action space.
            $^{11,}$~\footcite{ye2020}$^,$~\footcite{shukla2022}$^,$~\footcite{liang2024}
        \item \textbf{Robustness:} Sensitivity to disturbances and adversarial influences.
            \footcite{gleave2021}$^,$~\footcite{li2019}$^,$~\footcite{spooner2020}$^,$~\footcite{guo2022}
    \end{itemize}
    \vspace{0.5em}
\end{frame}
%(Liu et al., 2024) liu2024

% MARL Scalability Strategies

\begin{frame}{MARL Scalability Strategies}
    \textbf{Notable Strategies:}
    \begin{itemize}
        \item Value decomposition to handle dynamic populations.
            \footcite{zhang2021}$^,$~\footcite{nguyen2020}
        \item Hierarchical learning and role assignment.
            \footcite{cui2022}
        \item \textcolor{DarkBlue}{Curriculum learning} and transfer learning for adaptability.
            \footcite{shukla2022}$^,$~\footcite{shi2023}
    \end{itemize}
\end{frame}

\begin{frame}{Heterogeneous-Agent Reinforcement Learning (HARL)}
    \textbf{Harl Extends MARL Challenges:}
    \begin{itemize}
        \item \textbf{Coordination Complexity:} Heterogeneous roles further complicate 
            cooperation and strategic alignment.
            ~\footcite{wakilpoor2020}$^,$~\footcite{yang2020a}$^,$~\footcite{gronauer2022}
        \item \textbf{Scalability Issues:} Increased joint state-action space makes 
            learning computationally expensive.
            ~\footcite{leibo2021}$^,$~\footcite{rizk2019}
        \item \textbf{Adaptability and Robustness:} Policies must generalize across 
            changing agent compositions and dynamic environments.
            ~\footcite{yang2021a}$^,$~\footcite{koster2020}
    \end{itemize}
\end{frame}

% Distinction Between MARL and HARL Variants

\begin{frame}{Defining MARL and HARL Variants}
    \textbf{Proposed Distinctions:}
    \begin{itemize}
        \item \textbf{MARL (Multi-Agent Reinforcement Learning):}
        \begin{itemize}
            \item Agents may share identical structures or learn different behaviors.
            \item Focuses on collaborative or competitive dynamics among agents.
        \end{itemize}
        \item \textbf{Heterodox-Agent RL (Behavioral Heterogeneity):}
        \begin{itemize}
            \item \emph{Structural Homogeneity:} Agents have identical observation and action spaces.
            \item \emph{Behavioral Diversity:} Independent policies allow agents to learn unique behaviors.
            \item \textit{Example:} Identical drones taking on distinct roles in a cooperative task.
        \end{itemize}
        \item \textbf{Heterogeneous-Agent RL (Intrinsic Heterogeneity):}
        \begin{itemize}
            \item \emph{Structural Heterogeneity:} Agents differ in observation or action spaces.
            \item \emph{Behavioral Specialization:} Policies adapt to agents' intrinsic differences.
            \item \textit{Example:} Combined arms.
        \end{itemize}
    \end{itemize}
\end{frame}

\subsubsection{Research Questions}

\begin{frame}{Research Questions}
    \begin{enumerate}
        \item[RQ 1] {
            Can pretraining smaller teams of agents and then scaling to the target 
            team size via policy duplication and retraining improve training efficiency 
            without sacrificing final policy performance in MARL?}
        \item[RQ 2] {
            How does the effectiveness of this direct scaling strategy vary across 
            environments with different forms of agent heterogeneity 
            (e.g., behavioral vs. intrinsic)?}
    \end{enumerate}
\end{frame}

\begin{frame}{RQ 1 - Research Tasks}
    \begin{enumerate}
        \item[RQ 1] \textcolor{gray}{
            Can pretraining smaller teams of agents and then scaling to the target 
            team size via policy duplication and retraining improve training efficiency 
            without sacrificing final policy performance in HARL? } \vspace{1em}
    \begin{itemize}
        \item[RT 1.1] {
            Design an upsampling-based curriculum using policy duplication and retraining.}
        \item[RT 1.2] {
            Define a metric (agent-steps) accounting for agent count and training time.}
        \item[RT 1.3] {
            Train tabula rasa agents each target environment and team size as baselines.}
        \item[RT 1.4] {
            Evaluate training performance across various pretraining length and target team sizes.}
    \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}{RQ 2 - Research Tasks}
    \begin{enumerate}
        \item[RQ 2] \textcolor{gray}{
            How does the effectiveness of this direct scaling strategy vary across 
            environments with different forms of agent heterogeneity 
            (e.g., behavioral vs. intrinsic)? } \vspace{1em}
    \begin{itemize}
        \item[RT 2.1] {
            Select environments that represent distinct forms of agent heterogeneity. \\
            Behavioral, Intrinsic.}
        \item[RT 2.2] {
            Adapt observation structures to enable fixed policy architectures across team sizes.}
        \item[RT 2.3] {
            Evaluate the effect of heterogeneity type on the scalability and retraining benefit.}
    \end{itemize}
    \end{enumerate}
\end{frame}


\subsection{Methodology}

% Experimental Setup


\begin{frame}{Experimental Setup}
    \textbf{Environment(s):}
    \begin{itemize}
        \item Key attributes include:
        \begin{itemize}
            \item Agent count variability.
            \item Task complexity.
            \item Cooperation requirements.
            \item Potential for role differentiation.
        \end{itemize}
        \item The chosen environments are:
        \begin{itemize}
            \item From the SISL package\footcite{gupta2017} in Farama's PettingZoo.
            \item Continuous.
        \end{itemize}
    \end{itemize}
\end{frame}

% Environment Descriptions
\begin{frame}{Experimental Setup}
\begin{columns}
    \begin{column}{0.5\textwidth}
        \textbf{Environments:}
        \begin{itemize}
            \item Waterworld
            \begin{itemize}
                \item Collect food while avoiding negative reward obstacles (poison).
                \item Locating food provides a small reward, 
                    while collecting food provides a large reward but requires multiple agents.
            \end{itemize}
            \item Multiwalker
            \begin{itemize}
                \item A team of robots carry a box.
                \item A small reward is given for forward distance of the box.
                \item Dropping the box is a large negative reward.
            \end{itemize}
        \end{itemize}
        \vfil
    \end{column}
    \begin{column}{0.5\textwidth}
        \begin{figure}
            \includegraphics[width=0.35\textwidth]{waterworld.png}
            \caption{Waterworld environment}
        \end{figure}
        \vspace{-1em}
        \begin{figure}
            \includegraphics[width=0.35\textwidth]{multiwalker.png}
            \caption{Multiwalker environment}
        \end{figure}
    \end{column}
\end{columns}
\end{frame}

\begin{frame}{Experimental Setup}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \textbf{Environments:}
            \begin{itemize}
                \item Level-Based Foraging (LBF)
                \begin{itemize}
                    \item Grid-world environment with agents of varying skill levels.
                    \item Agents must cooperatively collect food, some of which require multiple agents.
                    \item Skill level determines which food an agent can collect.
                    \item Coordination is based on aligning agent skill with task requirements.
                \end{itemize}
            \end{itemize}
            \vfil
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \includegraphics[width=0.5\linewidth]{lbf.png}
                \caption{Level-Based Foraging environment.}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

% Training

\begin{frame}{Training}
    \begin{columns}
        \begin{column}{0.6\textwidth}
            
            \textbf{Algorithm:}
            \begin{itemize}
                \item PPO
                \item Independent policies
            \end{itemize}
        
            \textbf{Curriculum:}
            \begin{enumerate}
                \item Pre-training
                \begin{itemize}
                    \item $n$ iterations
                    \item With set of agents (fewer than target)
                    \item Exporting checkpoints periodically
                \end{itemize}
                \item Final Training
                \begin{itemize}
                    \item With target number of agents
                    \item Instantiated from pre-trained checkpoints
                    \item Upsampling performed without replacement; 
                        randomly when target number is not a multiple of pretrained agents.
                \end{itemize}
            \end{enumerate}
        \end{column}
        \begin{column}{0.4\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=0.95\linewidth]{training_phases_ww.png}
                \caption{Example progression of training phases.}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Observation Schema Adjustments}
    \textbf{Challenge:} Observation vector size varies with number of agents in LBF.\\[1em]
    \textbf{Solutions Implemented:}
    \begin{enumerate}
        \item \textbf{Ally-Ignorant Observations:}
            \begin{itemize}
                \item Agent does not receive any ally-specific features.
                \item Observation vector remains fixed regardless of team size.
            \end{itemize}
        \item \textbf{Truncated Observations:}
            \begin{itemize}
                \item Observation includes a fixed number of teammate feature slots.
                \item For larger teams, a random subset of teammates is used.
            \end{itemize}
    \end{enumerate}
    \textbf{Goal:} Maintain consistent policy architecture across configurations.
\end{frame}

% Evaluation Metrics

\begin{frame}{Evaluation Metrics}
    \textbf{Performance Metrics:}
    \begin{itemize}
        \item Reward progression (training curve) as the primary metric to measure performance and convergence.
        \item Comparisons based on per-agent average rewards to account for differing agent counts.
        \item We define training effort using \textbf{agent-steps}, the product of agent count and training iterations.
        \item Validated agent-steps as a proxy for training cost via timing analysis.
    \end{itemize}

    \textbf{Computational Efficiency:}
    \begin{itemize}
        \item Efficiency comparisons are normalized using agent-steps.
        \item Must address concerns about possible variance in cluster resource provisioning.
        \item CPU utilization metrics recorded but we considered of limited scientific value.
    \end{itemize}
\end{frame}

\subsection{Results}


\begin{frame}{Agent-Steps Validation}
    \textbf{Why Agent-Steps?}
    \begin{itemize}
        \item Training cost increases linearly with agent count.
        \item Empirical validation across environments shows Pearson $\rho > 0.999$.
        \item Enables direct comparison of training effort across team sizes.
    \end{itemize}
    \begin{center}
        \begin{figure}
            \includegraphics[width=0.5\linewidth]{iter_cost.png}
            \caption{Mean training iteration time vs. agent count.}
            \label{fig:agent_steps_costs}
        \end{figure}
    \end{center}
\end{frame}

% ---- Waterworld Results Slides ----
\begin{frame}{Waterworld Results (4 Agents)}
    \begin{columns}
        \begin{column}{0.35\linewidth}
            \textbf{Setup:}
            \begin{itemize}
                \item Target team size: 4 agents.
                \item Pretraining from 2-agent configuration.
            \end{itemize}
            \textbf{Findings:}
            \begin{itemize}
                \item Modest benefit from pretraining.
                \item Retrained agents achieved similar convergence, slightly faster.
            \end{itemize}
        \end{column}
        \begin{column}{0.65\linewidth}
            \begin{figure}
                \includegraphics[width=0.95\linewidth]{Waterworld-4-agent.png}
                \caption{4-agent Waterworld: retrained vs. tabula rasa.}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Waterworld Results (8 Agents)}
    \begin{columns}
        \begin{column}{0.35\linewidth}
            \textbf{Setup:}
            \begin{itemize}
                \item Target team size: 8 agents.
                \item Pretraining from 2-agent configuration.
            \end{itemize}
            \textbf{Findings:}
            \begin{itemize}
                \item Significant speedup from pretraining.
                \item Diminishing returns after 60 pretraining steps.
            \end{itemize}
        \end{column}
        \begin{column}{0.65\linewidth}
            \begin{figure}
                \includegraphics[width=0.95\linewidth]{Waterworld-8-agent.png}
                \caption{8-agent Waterworld: retrained vs. tabula rasa.}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}


\begin{frame}{Waterworld: Efficiency Summary}
    \begin{columns}
        \begin{column}{0.45\linewidth}
            \textbf{Metric:} Area Under Curve (AUC) compared to baseline. \\
            \textbf{Observation:}
            \begin{itemize}
                \item Greater benefits for larger target teams.
                \item Pretraining length shows diminishing returns.
                \item Optimal when pretraining team is much smaller than target team.
            \end{itemize}
        \end{column}
        \begin{column}{0.55\linewidth}
            \begin{figure}
                \includegraphics[width=0.95\linewidth]{Waterworld-AUCs.png}
                \caption{Relative AUC improvement over tabula rasa by team size and pretraining length.}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

% --- Multiwalker Results Slides ---
\begin{frame}{Multiwalker Results (5 Agents)}
    \begin{columns}
        \begin{column}{0.35\linewidth}
            \textbf{Setup:}
            \begin{itemize}
                \item Target team size: 5 agents.
                \item Pretraining from 3-agent configuration.
            \end{itemize}
            \textbf{Findings:}
            \begin{itemize}
                \item Modest benefit from pretraining.
                \item Retrained agents slightly accelerated learning.
            \end{itemize}
        \end{column}
        \begin{column}{0.65\linewidth}
            \begin{figure}
                \includegraphics[width=0.95\linewidth]{Multiwalker-5-agent.png}
                \caption{5-agent Multiwalker: retrained vs. tabula rasa.}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Multiwalker Results (6 Agents)}
    \begin{columns}
        \begin{column}{0.35\linewidth}
            \textbf{Setup:}
            \begin{itemize}
                \item Target team size: 6 agents.
                \item Pretraining from 3-agent configuration.
            \end{itemize}
            \textbf{Findings:}
            \begin{itemize}
                \item Tabula rasa often failed to converge.
                \item Pretraining stabilized learning and enabled progress.
            \end{itemize}
        \end{column}
        \begin{column}{0.65\linewidth}
            \begin{figure}
                \includegraphics[width=0.95\linewidth]{Multiwalker-6-agent.png}
                \caption{6-agent Multiwalker: retrained vs. tabula rasa.}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}


\begin{frame}{Multiwalker: Efficiency Summary}
    \begin{columns}
        \begin{column}{0.45\linewidth}
            \textbf{Metric:} Area Under Curve (AUC) compared to baseline. \\
            \textbf{Observation:}
            \begin{itemize}
                \item Clear benefit from pretraining at larger team sizes.
                \item Small-team scaling sometimes incurs cost without benefit.
                \item Stabilization effect notable in high-coordination regimes.
            \end{itemize}
        \end{column}
        \begin{column}{0.55\linewidth}
            \begin{figure}
                \includegraphics[width=0.95\linewidth]{Multiwalker-AUCs.png}
                \caption{Relative AUC improvement over tabula rasa by team size and pretraining length.}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

% --- LBF Results Slides ---
\begin{frame}{LBF: Observation Schema Impact}
            \textbf{Schemas Compared:}
            \begin{enumerate}
                \item Full: All teammate features included.
                \item Truncated: Fixed-size subset of teammate features.
                \item Ally-Ignorant: No teammate features included.
            \end{enumerate}

            \vspace{1em}
            \textbf{Methodology:}
            \begin{itemize}
                \item Pairwise $t$-tests conducted between schema variants at each agent count.
                \item Bonferroni correction applied across comparisons per team size to control for Type I error.
            \end{itemize}
\end{frame}

\begin{frame}{LBF: Schema Comparison Statistics}
    \begin{center}
        \scriptsize
        \begin{tabular}{cccccc}
            \toprule
            \textbf{Agents} & \textbf{Obs. 1} & \textbf{Obs. 2} & \textbf{t-stat} & \textbf{Bonf. p-value} \\
            \midrule
            3 & Ally-ignorant & Full      & 306.28 & \textbf{0.000} \\
            3 & Ally-ignorant & Truncated & 252.39 & \textbf{0.000} \\
            3 & Full          & Truncated &  -0.97 & 1.008 \\
            4 & Ally-ignorant & Full      &  53.09 & \textbf{0.000} \\
            4 & Ally-ignorant & Truncated &  88.01 & \textbf{0.000} \\
            4 & Full          & Truncated &   0.56 & 1.728 \\
            5 & Ally-ignorant & Full      &  73.35 & \textbf{0.000} \\
            5 & Ally-ignorant & Truncated &  23.91 & \textbf{0.000} \\
            5 & Full          & Truncated &  -1.49 & 0.423 \\
            6 & Ally-ignorant & Full      &  16.52 & \textbf{0.000} \\
            6 & Ally-ignorant & Truncated &  28.51 & \textbf{0.000} \\
            6 & Full          & Truncated &   1.47 & 0.440 \\
            7 & Ally-ignorant & Full      &  11.88 & \textbf{0.000} \\
            7 & Ally-ignorant & Truncated &  12.90 & \textbf{0.000} \\
            7 & Full          & Truncated &   0.45 & 1.955 \\
            \bottomrule
        \end{tabular}
    \end{center}
    \vspace{0.5em}
    \centering
    \footnotesize
    Pairwise $t$-tests comparing schema performance for each agent count. 
    Bonferroni correction applied within each group of comparisons by team size.
\end{frame}

\begin{frame}{LBF Results (7 Agents)}
    \begin{columns}
        \begin{column}{0.45\linewidth}
            \textbf{Setup:}
            \begin{itemize}
                \item Target team size: 7 agents.
                \item Pretraining from 2-agent configuration.
                \item Using ally-ignorant schema.
            \end{itemize}
            \textbf{Findings:}
            \begin{itemize}
                \item Retrained runs performed slightly better than baseline.
                \item High variance and limited consistency across durations.
            \end{itemize}
        \end{column}
        \begin{column}{0.55\linewidth}
            \begin{figure}
                \includegraphics[width=0.95\linewidth]{LBF-7-agent.png}
                \caption{7-agent LBF: retrained vs. tabula rasa.}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{LBF: Efficiency Summary}
    \begin{columns}
        \begin{column}{0.45\linewidth}
            \textbf{Metric:} Area Under Curve (AUC) compared to baseline. \\
            \textbf{Observation:}
            \begin{itemize}
                \item Benefit limited to smaller team sizes.
                \item Gains decrease sharply beyond 5 agents.
                \item Larger teams showed no consistent pretraining advantage.
            \end{itemize}
        \end{column}
        \begin{column}{0.55\linewidth}
            \begin{figure}
                \includegraphics[width=0.95\linewidth]{LBF-AUCs.png}
                \caption{Relative AUC improvement for LBF by team size and pretraining.}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}


% ------------------------------------------------------
% Discussion Section
% ------------------------------------------------------
\subsection{Discussion}

\begin{frame}{Discussion: Summary of Findings}
    \begin{itemize}
        \item Pretraining and direct scaling can improve training efficiency in MARL.
        \item Largest gains observed in symmetric environments with low coordination sensitivity.
        \item Effectiveness diminishes in environments requiring complex role specialization.
        \item Pretraining helps stabilize early learning in fragile configurations.
    \end{itemize}
\end{frame}

\begin{frame}{Discussion: Impact of Heterogeneity}
    \textbf{Behavioral vs. Intrinsic Heterogeneity}
    \begin{itemize}
        \item \textbf{Waterworld} (Behavioral): Strong performance gains from pretraining.
        \item \textbf{Multiwalker} (Static Intrinsic): Pretraining stabilizes convergence in larger teams.
        \item \textbf{LBF} (Dynamic Intrinsic): Minimal gains; coordination complexity overwhelms small-team priors.
    \end{itemize}
    \vspace{1em}
    \textbf{Insight:} Pretraining success is inversely related to heterogeneity complexity.
\end{frame}

\begin{frame}{Discussion: Observation Schema Design}
    \begin{itemize}
        \item LBF results showed schema choice significantly affects training outcomes.
        \item Full observability enables coordination in complex tasks.
        \item Truncated and ally-ignorant schemas showed no statistical difference.
        \item Suggests limited teammate data may be insufficient in high-heterogeneity settings.
    \end{itemize}
\end{frame}

\begin{frame}{Discussion: Implications and Limitations}
    \begin{itemize}
        \item Pretraining-based upsampling is a lightweight, effective method for symmetric or lightly heterogeneous tasks.
        \item Gains plateau quickly—short pretraining often sufficient.
        \item Not a substitute for full curriculum learning or adaptive specialization methods.
        \item Future strategies may combine this with role-aware curricula or invariant input architectures.
    \end{itemize}
\end{frame}
