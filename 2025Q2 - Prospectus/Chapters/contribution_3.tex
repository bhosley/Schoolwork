
\subsection{Motivation}
The third contribution investigates a progressive learning strategy in which policy 
networks grow in capacity during training. Rather than training a large network 
from the outset, which increases sample inefficiency and risk of overfitting, 
this approach proposes beginning with a smaller network and increasing its size over 
time through structured transformations. Inspired by ideas such as Net2Net~\cite{chen2016}, 
this work evaluates the use of tensor projection techniques to enable seamless 
expansion while preserving prior network behavior.

This contribution builds on lessons from the previous two. Contribution 1 
showed that smaller-team training can accelerate convergence, while Contribution 2 
intends to demonstrate the importance of input design for scalable policy reuse. 
Contribution 3 investigates whether network capacity itself can be staged similarly,
starting small to learn core dynamics, then growing to support more nuanced policies,
without discarding prior learning.

\subsection{Methodology}
The experiment will begin with small-capacity networks trained using standard 
PPO in PettingZoo-compatible environments. At predefined training milestones, 
the network will be expanded by projecting its weights into a higher-dimensional tensor space. 
Multiple projection strategies will be compared, including:
\begin{itemize}
    \item Identity-based expansion (e.g., block-diagonal initialization).
    \item Learned projection layers.
    \item Randomized low-rank initialization with partial freezing.
\end{itemize}
Training will then resume from the expanded model, and performance will 
be compared against fixed-size networks trained for the same duration.

\subsection{Resources}
The project will reuse the infrastructure from Contributions 1 and 2, 
including RLlib for training and Weights and Biases for tracking. 
Tensor projection layers will be implemented directly in PyTorch and 
integrated into the RLlib model registry.

We have not yet identified any specific features that we need in an environment
and thus anticipate being able to reuse the environments used in the 
earlier contributions.

\subsection{Anticipated Obstacles}
The primary challenge lies in selecting projection methods that preserve 
learned function approximations across architectural transitions. 
Naive expansion may disrupt policy performance or destabilize learning. 
Additional tuning may be needed to maintain gradient flow and convergence 
after projection. Furthermore, identifying optimal growth schedules will 
likely require additional experiments.