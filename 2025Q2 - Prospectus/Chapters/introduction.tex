% Introduction %
\section{Motivation}%
\label{sec:motivation}

In August 2023, at the National Defense Industrial Association's 
Emerging Technologies conference, Deputy Secretary of Defense Kathleen Hicks 
announced the \emph{Replicator Initiative}~\cite{robertson2023}. 
This initiative aims to field all-domain attritable autonomous (ADA2) systems,
leveraging autonomous technology to address production disadvantages the 
United States may face in great-power competition~\cite{zotero-2656}. 
Unlike previous efforts that sought bespoke military drones~\cite{bajak2023}, 
\emph{Replicator} aims to leverage more readily available technologies, 
potentially inspired by the demonstrated effectiveness of \gls{cots} \glspl{uas} 
employed by both belligerents during the Russian invasion of Ukraine~\cite{bajak2023a}.

Despite the strategic push of the \emph{Replicator Initiative}, 
a Rand Corporation study from February 2024~\cite{gerstein2024} predicted 
that effective, intelligent swarms are still several years from realization. 
These advancements will require the confluence of developments in several 
different areas; communications and signals, manufacturing, \gls{ai},
and usability. In the study they highlight a difficulty in the transition 
from what they call \emph{Surrogate Swarms} (many \gls{uas} controlled
by at least one human) to fully autonomous ones.
The area they termed \gls*{ai} likely refers to the systems used for autonomous 
decision making, which presents a host of constituent problems to be solved.

One of the primary difficulties in achieving effective swarm behavior 
lies in the training of these agents. Traditional methods often 
involve predefined team sizes and uniform capabilities among agents, 
which limits their adaptability and scalability. To overcome these limitations, 
it is essential to develop training methodologies that enable agents to 
generalize across variable team sizes and diverse hardware configurations. 
This adaptability is crucial for deploying flexible and resilient autonomous 
systems in dynamic environments.

\Gls{rl} has emerged as the most promising paradigm for addressing 
these challenges. \Gls{rl} has become the cornerstone for developing 
autonomous decision-making systems~\cite{sutton2018}. However, 
the application of RL to multi-agent systems, particularly in swarm scenarios, 
introduces additional layers of complexity. 
Agents must not only learn to achieve individual goals but also to 
collaborate effectively to maximize the collective reward~\cite{cao2012}.
\Gls{marl} extends beyond single-agent \gls{rl} to provide solutions 
to many such problems, even significantly exceeding the capabilities 
of the latter within certain domains that do not necessitate multiple 
agents~\cite{gronauer2022}. 

In the pursuit of a flexible training framework
it is essential to consider the potential of heterogeneous agents. 
Leveraging open-source and commercial off-the-shelf (COTS) resources 
logically extends to a framework that can utilize disparate hardware. 
This flexibility allows for the integration of various sensors, processors, 
and capabilities, maximizing the potential of each agent within the swarm. 

\Gls{harl}, is an extension of \gls{marl} characterized by agents having 
distinct roles, capabilities, policies, or objectives. It is an 
underexplored area, but early results suggest that \gls{harl} algorithms
have the potential to significantly improve problem-solving efficiency 
and adaptability~\cite{calvo2018}.

The practical applications of heterogeneous actors are vast, particularly in 
scenarios where coordination and cooperation are crucial. For example, 
drone swarms in search and rescue missions can leverage diverse capabilities, 
such as various sensory equipment or distinct maneuverability traits, 
enabling more thorough area coverage and expedited victim detection
~\cite{hoang2023,kouzeghar2023}.
Similarly, agricultural robots outfitted with different sensors and tools can 
concurrently execute multiple tasks—ranging from harvesting to soil analysis and
pest control—thereby substantially enhancing efficiency and increasing crop 
yields~\cite{carbone2018,amarasinghe2019}.

While some scenarios necessitate a \gls{harl} approach, the central aim of 
this dissertation is to investigate strategies that improve the training 
efficiency of multi-agent systems, including, heterogeneous settings. 
Across three planned contributions, we explore methods that reduce the 
computational cost of learning while maintaining or improving final policy 
performance. These methods span policy upsampling, invariant observation processing, 
and progressive network expansion. The remainder of this document 
presents a unified background and research plan supporting these contributions.

%To fully realize the potential benefits of \gls{harl}, it is crucial to 
%understand the trade-offs involved. This \printdoctype will survey current 
%approaches for training multiple agents, 
%identifying the strengths and limitations of each. 
%In \cref{sec:problem_statement,sec:research_question} we will 
%highlight several under-explored areas that warrant further investigation. 
%By addressing these gaps, we aim to advance the field and enhance the 
%effectiveness of multi-agent systems in real-world applications.

\section{Background}%
\label{sec:background}

    \subsection*{Reinforcement Learning: From Deep Blue to AlphaStar}%

\Gls{rl} has marked a number of significant milestones in outperforming humans 
in competitive domains. One of the most pivotal events occurred in 1997 
when IBM's Deep Blue defeated world chess champion Garry Kasparov. Though 
powered mainly by brute force computation and hand-tuned algorithms rather than 
learning-based approaches~\cite{campbell2002}, Deep Blue's victory set the 
stage for the broader application of \gls{ai} in complex strategic games.

The field progressed significantly with DeepMind's introduction of AlphaGo 
in 2015. AlphaGo employed a combination of deep neural networks and 
\gls{mcts}~\cite{silver2016}, initially trained on human expert games and 
further improved through self-play.
This method enabled AlphaGo to defeat Lee Sedol, one of the world's top Go 
players, illustrating \gls{rl}'s potential to tackle challenges in games with 
vast state spaces and decisions typically driven by human intuition.

This breakthrough was quickly followed by the development of 
AlphaZero~\cite{silver2017}, which revolutionized the field by mastering chess,
Go, and Shogi through self-play alone, without any human-derived 
data~\cite{silver2017a}. The method of self-play demonstrated not only 
versatility across different games but also the capacity of RL systems to 
develop domain-independent strategies.

A subsequent major advancement was achieved with DeepMind's 
AlphaStar~\cite{vinyals2019}, 
which demonstrated that advanced RL models could handle complex strategies, 
real-time decision-making, and intricate player interactions. 
AlphaStar's success in defeating professional StarCraft II players
was particularly notable due to the game's demand for long-term strategic 
planning and quick tactical responses in an open-ended scenario.

To achieve the level of proficiency demonstrated in AlphaStar, 
Vinyals et al.~\cite{vinyals2019} employed a multifaceted approach 
that integrated deep learning, imitation learning, 
reinforcement learning, and multi-agent learning. 
The specifics of these contributions are explored in detail 
in~\Cref{ch:literature_review}.

    \subsection*{Multi-agent Reinforcement Learning}%:Learning to Work Together}

Well before the the rise of \gls*{rl}, research in game theory 
provided foundational work that would be indispensable to multi-agent systems.
As early as 1951, Brown~\cite{brown1951iterative} proposed a method for 
calculating \gls{nash} in two-player games through a process he termed 
fictitious play, which involves iteratively updating strategies. 
Unlike simultaneous strategy updates, Brown's method applies updates 
sequentially—a condition that Berger~\cite{berger2005, berger2007} 
later proved to be sufficient for guaranteed convergence to 
\gls{nash} in nondegenerate ordinal games. 

The development in this area remained comparatively stunted until 
significant strides were made in single-agent methods. 
Traditional Bellman-Equation-style solutions, while effective in single-agent 
settings and certain types of multi-agent games like zero-sum and 
common-payoff games, faced greater difficulty in stochastic or 
degenerate games~\cite{shoham2007}.
These challenges highlighted the limitations of extending single-agent 
frameworks directly to multi-agent environments without modifications.

The introduction of multiple independent agents in an environment introduces 
additional complexity; the game becomes non-stationary from 
the perspective of any single agent~\cite{busoniu2008}. 
This non-stationarity poses unique challenges as each agent must adapt 
to the actions of others whose strategies are also evolving, 
significantly complicating the learning process.

In this realm, the extension into \gls{marl} allows for the consideration 
of a wide spectrum of interactions as described in game theory, 
ranging from purely competitive to purely cooperative. 
\gls{marl} addresses the multitude of challenges associated with these 
diverse styles of interaction, offering frameworks and strategies that 
are adaptable to varying degrees of cooperation and competition among 
agents~\cite{lowe2020}.

In some cases, the interactions of interest in \gls{marl} are asymmetrical, 
adding another layer of complexity to strategy formulation and 
execution~\cite*{sun2023}.
Among the most notable successes in handling mixed modes of cooperation 
and competition is OpenAI's achievement with OpenAI Five. In this project, 
a team of agents reached superhuman performance in the multiplayer game Dota 2,
utilizing a blend of techniques including a unique method of skill transfer 
known as ``surgery'' and extensive use of self-play~\cite{berner2019}.
This milestone not only demonstrated the capability of \gls{marl} systems to 
manage and excel in intricate, dynamically shifting competitive environments 
but also showcased the potential for these systems to develop and refine 
collaborative strategies among heterogeneous agents.

    \subsection*{The Game Theoretical Concerns}%:

In both papers describing AlphaStar~\cite{vinyals2019} and OpenAI 
Five~\cite{berner2019}, the authors mention in sparse detail the 
``game theoretic'' concerns their respective frameworks seek to address. 
These concerns are primarily attributed to the potential pitfalls of self-play. 
Two major problems are highlighted.

The first problem, often called strategic 
collapse~\cite{berner2019,vinyals2019}, describes a phenomenon 
where an agent overfits to a self-defeating strategy, 
resulting in a feedback loop and a counter-intuitive observation where 
cumulative rewards per episode may suddenly drop and become unrecoverable 
during continued training.

The second problem is cyclic strategy chasing, where multiple agents 
converge on a set of strategies that balance wins against one strategy 
with losses to another. An example of this is the game rock-paper-scissors. 
Balduzzi et al. (2019) discuss this phenomenon in~\cite{balduzzi2019}.

To mitigate these risks, AlphaStar implemented a structured league-play 
schema that continuously pitted different agent policies against each other. 
OpenAI Five, on the other hand, used a simpler approach by maintaining a pool
of previous milestone agents for ongoing comparison and refinement.


    % --- Bringing it together.
    \subsection*{Towards Flexible Training Methodologies}

The advancements made by AlphaStar~\cite{vinyals2019} and 
OpenAI Five~\cite{berner2019} have underscored the potential of \gls{marl}
to achieve superhuman performance in complex, dynamic environments,
and inspired a large amount of follow-on research.
However, their approaches still involve training agents to 
operate as a team with a predefined number of members;
AlphaStar~\cite{vinyals2019} effectively a team of one,
and OpenAI Five~\cite{berner2019} always a team of five.

Smit et al.~\cite{smit2023} was inspired by AlphaStar~\cite{vinyals2019},
and attempted to make significant efficiency improvements.
One of the methods that they tried (ultimately unsuccessfully) was to train a 
subset of the final team, effectively the same as the scalability problem.
We revisit Smit et al.~\cite{smit2023} in \cref{ch:literature_review}.

While these pioneering efforts have demonstrated remarkable achievements, 
they also highlight significant challenges that remain unresolved. 
Notably, the ability to train agents that can generalize across variable 
team sizes and configurations is crucial for advancing the field of 
multi-agent reinforcement learning. Addressing these challenges requires 
innovative methodologies that enhance the flexibility and efficiency of 
training processes. 

This leads us to our core research questions, which aim to evaluate
the potential of \gls{marl} or \gls{harl} to overcome these limitations 
and achieve scalable, robust, and adaptable autonomous systems.

%\section{Problem statement}%
%\label{sec:problem_statement}%
%
%This \printdoctype aims to examine several key aspects that 
%contributed to the success of those projects, 
%with the overarching goal of identifying how these methods can be 
%applied not only to address the game-theoretic challenges impacting 
%\gls{marl} but also to improve scalability and flexibility in other contexts. 
%Understanding how different components of these training frameworks 
%contribute to the effectiveness of the resulting agents and their relative 
%costs is crucial for enabling further research and broader applications of 
%\gls{marl} technologies. 
%Additionally, we hypothesize that \gls{harl} techniques will offer observable 
%benefits during the training process, not only in addressing game-theoretic 
%problems but also in enhancing the resulting agents' flexibility when 
%deployed in novel configurations.

\section{Research Questions}%
\label{sec:research_question}%
\label{sec:relevance_and_importance}

% --- REVISED RESEARCH QUESTIONS AND CONTRIBUTIONS ---
\begin{description}
    \item[] \textbf{\hyperref[ch:contribution_1]{Contribution 1}:} Policy Upsampling
    \begin{itemize}
        \item[RQ 1.1:] Can pretraining smaller teams of agents and then scaling to the target 
        team size via policy duplication and retraining improve training efficiency 
        without sacrificing final policy performance in MARL?
        \item[RQ 1.2:] How does the effectiveness of this direct scaling strategy vary across 
        environments with different forms of agent heterogeneity 
        (e.g., behavioral vs. intrinsic)?
    \end{itemize}
    \item[]\textbf{\hyperref[ch:contribution_2]{Contribution 2}:} Invariant Observation Processing
    \begin{itemize}
        \item[RQ 2.1:] How does incorporating input-invariant structures—such as policies 
        robust to observation permutations and variable team sizes—impact learning efficiency 
        and team robustness in settings with heterogeneous observations?
        \item[RQ 2.2:] Do input-invariant architectures support more stable policy behavior 
        under changes to team size and partial observation loss during execution?
    \end{itemize}
    \item[] \textbf{\hyperref[ch:contribution_3]{Contribution 3}:} Progressive Network Expansion
    \begin{itemize}
        \item[RQ 3.1:] Can tensor-based projections be used to grow a policy networks 
        capacity during training while preserving its prior functional behavior?
        \item[RQ 3.2:] Can we identify appropriate transition points during training 
        when projecting to a larger policy network yields the greatest benefit?
        \item[RQ 3.3:] Does this progressive architectural growth strategy reduce total training 
        cost or improve final policy performance compared to fixed-size architectures?
    \end{itemize}
\end{description}


\section{Outline}%

\begin{comment} %%%% Dissertation Version %%%%
The remainder of this document is designed to systematically explore 
the complex field of multi-agent reinforcement learning, 
particularly focusing on heterogeneous-agent systems.
Following this introductory chapter, 
\ref{ch:literature_review}: Literature Review provides a comprehensive analysis 
of the seminal and recent literature pertinent to our research focus. 
\Cref{ch:methodology} details the experimental and analytical techniques 
employed. \Cref{ch:results} presents the data and findings from our research, 
followed by \cref{ch:discussion}, where these results are interpreted 
in the context of existing knowledge and their implications for future 
research are explored.
The \emph{dissertation} will conclude with~\cref{ch:conclusion}, 
which summarizes the research and suggests avenues for further investigation. 
Each chapter builds upon the previous to provide a comprehensive understanding 
of the topic, aiming to contribute valuable insights to the field of \gls{harl}.
\end{comment}

%%%% Prospectus Version %%%%
The remainder of this document is organized as a dissertation prospectus 
supporting a k-paper dissertation format. It begins with a literature review in 
\cref{ch:literature_review}, establishing foundational context across reinforcement 
learning, multi-agent systems, and heterogeneous coordination. 
Following this, \cref{ch:contribution_1,ch:contribution_2,ch:contribution_3} 
contain each of the three planned contributions:
\begin{itemize}
    \item \textbf{\hyperref[ch:contribution_1]{Contribution 1}:} Investigates whether direct 
        upsampling of policies from smaller teams can reduce training cost while preserving 
        final performance.
    \item \textbf{\hyperref[ch:contribution_2]{Contribution 2}:} Evaluates input-invariant 
        policy architectures as a method to support shared learning across heterogeneous 
        observation structures.
    \item \textbf{\hyperref[ch:contribution_3]{Contribution 3}:} Explores progressive network 
        growth using tensor projection to improve training efficiency in high-capacity networks.
\end{itemize}
Each contribution is framed as a standalone study but collectively supports 
the overarching goal of building scalable, flexible HARL systems. 
The document concludes with a timeline and project plan for completing the proposed research.