
\section{Introduction}

\Gls{marl} has emerged as a powerful framework for modeling and 
training distributed systems of interacting agents. 
In cooperative \gls{marl} settings, agents often share a common goal and 
must coordinate their behaviors through learning. 
While many successful approaches rely on the assumption that agents are 
homogeneous and exchangeable, real-world applications increasingly 
require support for heterogeneous agents—each with distinct observation capabilities, 
action spaces, or roles. 
This shift introduces challenges for policy architecture design, 
particularly when aiming for efficient training and robust 
execution under dynamic team configurations.

A core insight from recent literature is that when agents are symmetric or 
play exchangeable roles, enforcing invariance to permutations in agent inputs 
and identities can significantly improve learning efficiency and generalization. 
Input-invariant architectures, such as those based on pooling, graph networks, 
or attention mechanisms, can leverage these symmetries directly in the 
policy or critic network. These methods reduce the complexity of the 
input space and support scalable learning across team sizes. 
However, extending these benefits to \gls{harl} remains an open challenge.

In this work, we propose and evaluate methods for incorporating 
input-invariance principles into \gls{harl} settings. 
We focus on a specific form of heterogeneity where agents 
have partially overlapping observation and action spaces, 
corresponding to distinct but sometimes shared sensor and effector modalities. 
We introduce a novel architectural approach that constructs a 
global observation-action spanning space and trains a 
shared policy using equivariant neural layers and dynamic masking. 
This approach supports flexible agent instantiation, input degradation, 
and team-size perturbation without retraining.

To evaluate these ideas, we design a custom multi-agent 
environment based on the \gls{mpe} framework, and we compare three architectures: 
a HAPPO baseline, a COMA model augmented with GNN and Transformer components, 
and our proposed equivariant spanning policy network. 
We assess both learning efficiency and runtime stability 
under controlled perturbations, including sensor dropout 
and dynamic changes in team composition.

Our results aim to contribute a deeper understanding of how 
input invariance can be adapted for heterogeneous agents, 
and what trade-offs arise in terms of performance, 
generalization, and computational cost.


\section{Related Work}

This section surveys prior work relevant to input-invariant architectures for \gls{harl}. 
The topics below are parallel threads that converge to inform our architectural design.
We begin with theoretical underpinnings of exchangeability and symmetry, 
followed by architectural strategies for permutation invariance and equivariance
via pooling, mean-field methods, graph networks, and attention. 
These are then examined through applications in \gls{marl} 
before concluding with \gls{harl} considerations.

We begin by grounding our discussion of input-invariant architectures 
in a key probabilistic result: de Finetti's theorem (\cref{eq:deFinnetti}). 
This theorem formalizes the idea that, under suitable conditions, 
the elements of an exchangeable sequence can be treated as if 
they were drawn independently from a shared latent distribution.

\begin{theorem}[de Finetti's Theorem]
    Let \(X = \{x_1, \ldots, x_M\}\) be a sequence of exchangeable random variables.
    Then there exists a latent variable \(\theta\) such that:
    \begin{equation}
        p(X \mid \alpha, M_0) = \int p(\theta \mid \alpha, M_0) 
        \prod_{m=1}^{M} p(x_m \mid \theta)\, d\theta
        \label{eq:deFinnetti}
    \end{equation}
    That is, the joint distribution over \(X\) can be represented as a mixture of 
    i.i.d. variables conditional on \(\theta\).
\end{theorem}

This result justifies the treatment of inputs that are unordered but statistically similar,
such as agent observations in a cooperative \gls{marl} task, 
as conditionally i.i.d. elements drawn from a shared distribution. 
In practice, this means that the ordering of inputs does not convey meaningful information, 
and architectures may (and perhaps should) be designed to respect this symmetry.

As an example, in environments like \gls{lbf}~\cite{papoudakis2021}, 
each agent's observation is constructed by concatenating their own state, 
the state of objectives, and the states of other agents in a fixed but arbitrary order. 
While the ordering is deterministic, it typically does not encode meaningful 
semantic distinctions. In such settings, treating the input as an exchangeable set,
rather than a structured sequence, aligns with the assumptions of de Finetti's theorem 
and motivates the permutation-invariant design choices in the rest of this section.

Permutation-invariant functions and networks (e.g., pooling operations or 
attention mechanisms without positional encoding) align with this insight, %%
embedding the inductive bias that order should not affect the outcome. 
This forms the basis for the architectural designs reviewed in the following sections.


\subsection{Permutation Invariance and Equivariance}

In their paper introducing Deep Sets, Zaheer et al.~\cite{zaheer2017} 
present a parameter-sharing scheme inspired by \cref{eq:deFinnetti}, 
which allows networks to handle unordered inputs by design.

They propose two architectures to achieve this.
For the invariant approach, they demonstrate that any 
permutation-invariant function over a set can 
be decomposed into a transformation (\(\rho\)) of a sum over 
transformed (\(\phi\)) elements,
\begin{equation*}
    \rho\left(\sum_i \phi(x_i)\right)
\end{equation*}
providing a universal approximator for such functions. 

Zaheer et al.~\cite{zaheer2017} also proposed an equivariant approach,
providing a neural network layer of the form:
\begin{equation*}
    \mathbf{f}(\mathbf{x}) \doteq \mathbf\sigma (\lambda\mathbf{Ix} 
    + \gamma \text{maxpool}(\mathbf{x})\mathbf{1}) 
\end{equation*}
where the weight matrix is constrained to the form 
\(\lambda\mathbf{I} + \gamma\mathbf{11}^\top\), ensuring all 
diagonal elements are equal and all off-diagonal elements are equal. 
This construction guarantees permutation equivariance.
The \(\sigma\) operation represents a non-linearity function (such as a sigmoid).
The authors argue that this equivariant form is functionally equivalent to the 
invariant form during inference; their distinction primarily emerges during backpropagation, 
where the structure of gradient updates differs between the two.

These structures dramatically reduce the effective symmetry 
group of the input space, enabling better generalization and 
sample efficiency in problems with intrinsic symmetries. 
In \gls{marl}, these ideas naturally extend to agent sets: 
policies or critics can process agents' states as elements 
of an unordered set, enforcing invariance to permutations in agent ordering. 
This supports scalable learning across varying team compositions 
and aligns with the theoretical underpinnings of exchangeability laid out earlier.


\subsection{Mean-Field and Exchangeable Approximations}

In scenarios where agents are approximately indistinguishable, 
a natural extension of Zaheer et al.'s~\cite{zaheer2017} 
permutation-invariant formulation is to normalize the summation of elements. 
Rather than summing over agent embeddings, one may average them,
yielding permutation invariance \emph{and} insensitivity to the number of agents. 
This refinement aligns closely with mean-field theory, which replaces complex 
many-body interactions with an average effect.

Yang et al.~\cite{yang2018} apply this idea in the context of \glsadd{marl}, 
introducing mean-field Q-learning and actor-critic algorithms. 
Their approach approximates the influence of neighboring agents with 
an average embedding, enabling each agent to learn a local policy that 
scales gracefully with team size. This is particularly 
useful in settings where each agent interacts with many others, 
but where the precise identity of those agents is irrelevant.

Building on this, Li et al.~\cite{li2021b} approach mean-field \gls{marl} 
problems formally as permutation-invariant Markov decision processes. 
They propose Mean-Field Proximal Policy Optimization (MF-PPO), 
a variant of PPO that incorporates mean-field assumptions into 
both actor and critic networks. The resulting architecture maintains a 
shared policy across agents, with inputs aggregated through mean pooling.

Theoretically, MF-PPO achieves convergence to a global optimum at 
a sublinear rate, and notably, its sample complexity is proven to be 
independent of the number of agents. This highlights the power of 
encoding exchangeability as an inductive bias: by exploiting the 
ergodic structure of agent interactions, MF-PPO updates a shared policy 
more efficiently than na\"ive multi-agent baselines.

Empirically, MF-PPO outperforms non-invariant baselines in the 
\gls{mpe}~\cite{li2021b}, demonstrating faster convergence, 
higher average returns, and significantly reduced parameter counts. 
This is largely attributed to weight sharing across agents and the 
use of input-aggregation strategies that preserve permutation symmetry. 

One significant downside to the examined mean-field approaches is that 
information about subset interactions is lost in the process of 
applying their respective commutative functions.
% #TODO: Appendix proof
% We argue that this is not a necessary limitation of mean-field methods,
This limitation motivated more sophisticated invariant architectures like 
graph-based networks and attention-based pooling, which we discuss next.


\subsection{Graph-Based Architectures}

\Glspl{gnn} naturally represent a set of entities (nodes) along with 
their relations (edges) in a way that is invariant to node ordering. 
\Glspl{gnn} are thus well-suited for \gls{marl}: each agent may be a node in a graph, 
with edges representing interactions (such as physical proximity, 
communication links, or joint team membership)~\cite{liu2020b}. 
By design, graph convolutions or message-passing layers treat permutations of 
node indices equivalently as they operate on the graph structure itself. 
Yang et al.~\cite{yang2021a} introduced Inductive Heterogeneous Graph Multi-agent 
Actor-critic (IHG-MA), where a multi-agent environment is modeled as a dynamic graph. 
Each agent shares a policy network that includes graph convolution layers, 
allowing it to adapt to changing neighbor relationships in highly dynamic environments. 
IHG-MA shows substantially improved cooperation in tasks where agents move and form time-varying 
interaction topologies, outperforming non-relational baselines~\cite{yang2021a}. 

\subsubsection{Centralized Critics with GNNs}
A prominent example of a graph-based architecture is the Permutation 
Invariant Critic (PIC) proposed by Liu et al.~\cite{liu2020b}. 
PIC uses a graph network as the critic in a \gls{ctde} framework. 
Instead of a monolithic \gls{mlp} that takes the concatenation of 
all agents' observations/actions, which would produce entirely different 
outputs if agents were relabeled, PIC's critic treats each agent as a node 
and applies graph convolutional layers to propagate information among agents. 
The final critic value is read out by pooling over node embeddings, 
yielding a single joint value that is invariant to agent permutations. 

Liu et al.~\cite{liu2020b} demonstrated that PIC scales effectively 
to environments with up to 200 agents, learning optimal policies where 
a standard \gls{mlp} critic failed. The graph-based critic flexibly adapts to 
different team sizes without redesign, as the \gls{gnn} simply expands to more nodes. 
In contrast, an \gls{mlp} critic struggles with larger input dimensions 
and inconsistent agent orderings. PIC also handles heterogeneity by 
assigning each node an attribute vector encoding its type or capabilities, 
allowing the shared \gls{gnn} to condition on agent-specific features while p
reserving symmetry among identical agents.

Noppakun and Akkarajitsakul~\cite{noppakun2022} explored a similar
approach by implementing a \gls{coma}~\cite{foerster2018}
replacing the standard \gls{mlp} critic with a \gls{gnn} one.
In this work they demonstrated the feasibility of adapting
existing actor-critic architectures to use \glspl{gnn}
for problems that might benefit from invariance.

Graph Attention Mean Field (GAT-MF)~\cite{hao2023} approach combines mean-field 
theory with a graph attention network to handle very large swarms of agents. 
By converting dense agent-agent interactions into an agent-“virtual neighbor” 
interaction via attention weights, GAT-MF remains invariant to agent permutations 
while focusing each agent's critic on the most influential neighbors, 
and has been applied to extremely large-scale scenarios (hundreds of agents)~\cite{hao2022}.


\subsection{Transformers and Attention for Sets}

The remarkable success of attention mechanisms in sequence modeling 
(notably Transformers~\cite{vaswani2017} applied to large language models) 
has carried over to set-based inputs by removing positional encodings. 
An attention layer, by default, is permutation-invariant to its inputs 
(when no order information is added); treating each query-key-value triplet 
agnostically to ordering and learning to weigh interactions based purely on content. 

Lee et al.~\cite{lee2019} 
formalized this in the Set Transformer, an architecture that uses 
self-attention to model interactions among elements of an input set. 
The Set Transformer can capture higher-order interactions
yielding far greater representational power than naive pooling. 

\subsubsection{Attention in Multi-Agent Critics}
Iqbal and Sha~\cite{iqbal2019} introduced a [Multi] Actor-Attention-Critic (MAAC), 
where the central critic uses an attention mechanism to dynamically 
favor agent interactions most impactful for a given agent's \Gls{q}-value. 
In MAAC, each agent's contribution to another's value is weighted by attention scores, 
which in effect, allows the critic to disregard some agents and emphasize others. 
This both handles varying numbers of agents and improves credit assignment by 
favoring joint partner agents that significantly affect outcomes. 

Hazra et al.~\cite{hazra2024} further demonstrated the effectiveness 
of attention mechanisms by applying a self-attention-based architecture 
to the StarCraft Multi-Agent Challenge (SMAC)~\cite{samvelyan2019}, 
achieving strong performance and scalability,
in a challenge more complex than those approached by the previous 
literature discussed in this section.

% Change this section as reflecting back to single agent applications
\subsubsection{Relational Reasoning in Single-Agent RL}
While much of the prior work focuses on multi-agent settings, 
the benefits of invariant relations extend to single-agent tasks as well. 
These architectures demonstrate that modeling inputs as a set of 
interacting entities can improve both performance and generalization.

Zambaldi et al.~\cite{zambaldi2018} showed that a single-agent \gls{rl} agent could 
benefit from interpreting its observations as a graph of entities and relations. 
Using a relation network with iterative message passing, their agent achieved 
superhuman performance on several StarCraft II~\cite{vinyals2019} mini-games, 
outperforming non-relational baselines by reasoning over unit interactions 
rather than raw input features.

In their single-agent formulation, Tang and Ha~\cite{tang2021} demonstrated 
that treating sensory channels as interchangeable input elements enabled 
the agent to maintain performance despite significant input corruption. 
By passing each sensory input through a shared encoder and aggregating 
with self-attention, the network learned to interpret observations based 
on contextual relationships rather than fixed positions. 
This design conferred resilience to occluded or missing inputs, 
allowing the agent to function even when sections of the sensory 
data were absent at test time. Their results suggest that invariance 
to input structure can provide strong robustness to
severe perceptual perturbations.

% # TODO: May consider ending on Tang and Ha for similarity purposes.
\begin{comment}
    They propose a very similar question:
        "Sensory substitution refers to the brain’s ability to use one sensory modality 
        (e.g., touch) to supply environmental information normally gathered by 
        another sense (e.g., vision)."
    - Extremely similar to my interest in sensory degradation.
    - However, in their experiments they instead demonstrate an invariance in a singular
    observational domain. (They take a 2d image, chunk it and shuffle.)
\end{comment}

\subsection{Heterogeneous Agents}
In many \gls{marl} scenarios, agents are not identical, a setting formally studied 
as \gls{harl}. Agents may differ in abilities, observation spaces, or roles. 
Many input-invariant architectures assume that all inputs are drawn from the 
same distribution and processed by a shared encoder, an assumption that breaks 
down when agents have structurally different observations. In such cases, 
invariant frameworks must be adapted to handle this heterogeneity, 
either by mapping observations into a shared embedding space or by 
augmenting the input with agent-specific identifiers.

One practical strategy for supporting heterogeneity in input-invariant architectures 
is to include an agent-specific feature encoding that captures type or 
role~\cite{liu2020b,hao2022,hao2023}. As noted earlier, PIC~\cite{liu2020b}
incorporates this by embedding attribute vectors into each node's representation, 
preserving symmetry among similar agents while allowing differentiation where needed.

Similarly, HPN~\cite{hao2023} uses a hypernetwork conditioned on 
identity to generate specialized agent modules, enabling tailored policies 
while maintaining permutation invariance at the architectural level. 
This structure supports diversity among agents while preserving the 
benefits of shared computation and symmetry where applicable.

Such flexibility is especially valuable in mixed-agent teams and 
competitive settings, where an agent may need to reason differently 
about teammates and opponents. The architecture should remain invariant 
when swapping two equivalent opponents but avoid conflating agents 
with distinct roles.

Despite the demonstrated advantages of invariant and equivariant architectures in handling 
agent symmetry, scalability, and heterogeneity, their practical cost remains underexplored. 
The reviewed methods, ranging from mean-field approximations and deep sets, 
to graph-based and attention-driven critics share a common goal: to encode 
structural priors that simplify learning in multi-agent settings. However, 
these methods often increase architectural complexity, requiring additional computation, 
memory, and tuning. Importantly, few studies offer direct comparisons of their 
training or inference costs relative to simpler baselines such as \glspl{mlp}. 
This gap complicates our understanding of the tradeoffs involved in deploying 
more expressive models in real-world systems.

In this work, we contribute to closing that gap by empirically evaluating the efficiency 
and robustness of input-invariant policy architectures in heterogeneous-agent settings, 
quantifying both their benefits and computational costs. In parallel, we 
propose a novel exchangeable approximation framework that extends the principles 
of input invariance to settings with observation and action heterogeneity. 
By constructing a shared observation-action spanning space and integrating dynamic 
masking within an equivariant network, we offer a lightweight and scalable 
approach to handling agent diversity. Together, these contributions aim to 
clarify not only when these architectures are helpful, 
but how they might be practically and broadly applied.


\section{Methodology}
\label{con2:sec:methodology}



\subsection{Environment Model}

We model the environment following the structure outlined by Powell~\cite{powell2022}, 
consisting of the state, decision/action, exogenous information, transition function, and 
objective function.

\paragraph{State.} 
Let \(T \subset \mathbb{N}\) be the set of decision epochs, 
and define entities \(E := (I, J, K, L)\), where 
\(i \in I\) are agents, 
\(j \in J\) are objectives, 
\(k \in K\) are hazards, and 
\(l \in L\) are obstacles. 
At each time \(t \in T\), the joint state \(s\in S\) is:
\[
    s_t := \{s_{te} \mid e \in E\}, \quad s_t \in S
\]
Where the marginal state for agents \(i \in I\), 
includes both location and orientation:
\[
    s_{ti} := (d_{ti}, \theta_{ti})
\]
and the non-agent entities \(e \in (J, K, L)\) are represented by location alone:
\[
    s_{te} := (d_{te})
\]
For spatial dimensions \(N \in \mathbb{N}\), 
let \(D^{(n)} \in \mathbb{F}\) be the domain of dimension \(n\).
Then, we define each entity \(e\)'s location as a tuple:
\[
    d_{te} := \left(d_{te}^{(1)},\ldots,d_{te}^{(N)}\right) \in \prod_{n=1}^N D_e^{(n)},
\]
and orientation is represented by the tuple:
\[
    \theta_{ti} := \left(\theta_{ti}^{(1,2)},\ldots,\theta_{ti}^{(N-1,N)}\right)
    \in \prod_{n=1}^{N-1} \Theta_i^{(n,n+1)}
\]

\paragraph{Observation Model.}
Let \({C} := \{c_1, \ldots, c_m\}\) denote a set of abstract sensor channels.
Each sensor channel \(c \in {C}\) corresponds to a linearly 
independent observation subspace.

For each entity visibility in each channel must be defined,
as a trait this may be referred to as \(e^{(c)} \in \{\text{True, False}\}\).
It may be encoded in the state as:
\[
s_{tec} = 
\begin{cases}
    1& \text{if \(e\) is visible in \(c\)} \\ 
    0& \text{if \(e\) is not visible in \(c\)}
\end{cases} 
\quad\forall t\in T, e\in E, c\in C.
\]
Thus the marginal states becomes
\[s_{ti} :(c_{ti},d_{ti},\theta_{ti})\]
for agents, and
\[s_{te} :(c_{te},d_{te})\]
for all other entities.

Each agent \(i \in I\) is assigned a subset of sensor channels 
\({C}_i \subseteq {C}\), 
with their observation defined as the concatenation of the outputs 
from the corresponding subspaces:
Note that an agent's ability to perceive an entity within a given 
subspace may be limited by spatial range, meaning each agent may 
only observe a portion of the environment within each active channel.
\[
    o_{ti} := \bigoplus_{c \in {C}_i} o_{tic}
\]
where \(o_{tic}\) is agent \(i\)'s observation of channel \(c\) at time \(t\), 
and \(\oplus\) denotes concatenation over the ordered tuple of subspaces.

The global observation space \(O\) is the Cartesian product of all channel subspaces:
\[
    O := \prod_{c \in {C}} O_c
\]
and each agent's observation is a masked subset of this space.
During policy training and inference, a binary mask vector \(m_i \in \{0,1\}^{|\mathcal{C}|}\) 
is applied to indicate which subspaces are active for agent \(i\), 
allowing a shared network to condition on heterogeneous observations without assuming symmetry.


\paragraph{Action Space.} 
The joint action \(a \in A\) consists of marginal agent actions:
\[
    a := (a_1, \ldots, a_{|I|}) \in A := \prod_{i \in I} A_i
\]
Each agent's action at time \(t\) is a tuple:
\[
    a_{ti} := (a_{ti}^\text{interact}, a_{ti}^\text{move}, \Delta\theta_{ti})
\]
where:
\begin{itemize}
    \item \(a_{ti}^\text{interact} \in \{0,1\}\) toggles interaction,
    \item \(a_{ti}^\text{move} \in M_i \subseteq \mathbb{F}\) denotes movement magnitude,
    \item \(\Delta\theta_{ti} \in \prod_{n=1}^{N-1} \Delta\Theta_i^{(n,n+1)}\) encodes orientation adjustments.
\end{itemize}

% \paragraph{Exogenous Information.} 
% Agents observe a partially observable state with complete reward knowledge. 
% Probabilistic channel observability is future work.

\paragraph{Transition Function.} 
Deterministic and synchronous:
\[
    s_{t+1} = \mathcal{T}(s_t, a_t) \quad \text{or} \quad \mathcal{T}(a_t \mid s_t) = s_{t+1}.
\]
Provided that the actions chosen by the agents are valid, they are executed.

\paragraph{Objective Function.} 
The cumulative reward is defined as:
\[
    \max \sum_{t \in T} \left[
        \sum_{i \in I, j \in J} r_j \cdot \mathbb{I}[\Xi]
        - \sum_{i \in I} r_i \cdot \mathbb{I}[a_{ti}^\text{interact} = 1]
        - \sum_{i \in I, k \in K} r_k \cdot \mathbb{I}[\|s_{ti} - s_{tk}\|_\infty \leq k^\text{range}]
    \right]
\]
where:
\begin{itemize}
    \item \(r_j \in \mathbb{R}_{>0}\) is the reward for collecting objective \(j \in J\),
    \item \(\mathbb{I}[\Xi]\) encodes conditions under which \(j\) is valid for collection,
    \item \(r_i \in \mathbb{R}_{<0}\) is the cost for agent \(i \in I\) to interact,
    \item \(r_k \in \mathbb{R}_{<0}\) is a penalty for agent \(i\) being near hazard \(k \in K\).
\end{itemize}






\subsection{Environment and Agent Design}

To evaluate agents with overlapping but non-identical observation capabilities, 
we design a custom environment derived from the \gls{mpe}. 
Each agent is equipped with a subset of sensor types, 
and each sensor type defines a linearly independent subspace of the 
full observation space. An agent's full observation vector is constructed by 
concatenating the outputs of its sensors, and overlapping sensor types across 
agents induce structured correlations.

The agents are heterogeneous in both their observation and action spaces. 
The union of all agent sensor types forms a global spanning observation space. 
Likewise, the union of all agent action spaces defines a spanning action set, 
from which agent-specific valid actions are masked during inference.

\subsection{Architectures Compared}
Three network architectures are evaluated:

\paragraph{0. HAPPO Baseline}
To provide a baseline comparison, we include a standard implementation 
of Heterogeneous-Agent Proximal Policy Optimization (HAPPO)~\cite{zhong2024}. 
With HAPPO each agent is trained with its own policy using PPO updates in a centralized 
training setting, but without shared parameters or input-invariant structure. 
HAPPO represents a strong non-invariant architecture designed specifically 
for heterogeneous teams, serving as a strong reference point for evaluating the 
efficiency and robustness benefits of the more structured approaches.

% \paragraph{1. COMA with GNN and Transformer}
% This model extends the counterfactual multi-agent policy gradient (COMA) 
% algorithm by incorporating a graph neural network (GNN) to encode agent relations, 
% and a transformer-based critic for contextual value estimation. 
% This structure follows prior work (e.g., PIC and MAAC) and 
% serves as a high-capacity input-invariant baseline. 
% The actor and critic operate under a centralized training with 
% decentralized execution (CTDE) paradigm, with the GNN providing relational inductive bias.

\paragraph{1. Equivariant Spanning Policy}
This architecture introduces a novel mechanism for enabling 
exchangeability in heterogeneous settings. A shared network 
is trained using inputs projected into the global observation 
spanning space, with a binary masking vector indicating which 
features are valid for a given agent. The network incorporates 
equivariant layers of the form $\lambda I + \gamma \mathbf{11}^\top$ 
to maintain structured permutation equivariance. The output layer 
is similarly masked to restrict the agent's available actions 
within the global action set. Policy updates are obtained using PPO.

% TODO: Continuous action masking strategies are not yet finalized. 
% Placeholder design assumes discrete masking via argmax over valid action subset.

\subsection{Training Regime}

To evaluate learning efficiency and generalization, 
we train each architecture under varying team configurations. 
Two primary variables are considered: the number of agents in the team, 
and the composition of agents based on their combinations of observation subspaces. 
Agent composition affects the overlap in observation space and thus 
the input complexity presented to the policy network. 
% (While the current focus is on observation-space heterogeneity, 
% additional forms such as action-space heterogeneity may be considered in future extensions, 
% pending environment support.)

For each architecture, training runs are conducted across multiple 
fixed team sizes (e.g., 2, 4, and 8 agents), and within each team size, 
several agent compositions are sampled to reflect varying degrees of sensor overlap. 
This allows us to evaluate the architecture's sensitivity to 
both agent count and observation diversity.

All configurations are evaluated across multiple independent 
training runs to account for stochastic variability.
Learning efficiency is defined in terms of convergence rate 
relative to total agent-training steps and computational cost.

\subsection{Perturbation Evaluation}

To assess runtime robustness, trained agents are tested under two types of perturbations:

\begin{itemize}
    \item \textbf{Sensor degradation}: Agents experience random dropout 
        of one or more observation channels during execution. 
        The masking vector is updated accordingly to simulate partial observability.
    \item \textbf{Team-size changes}: Agents are removed or added at test 
        time without retraining. Policies are evaluated for stability and 
        reward degradation in the new configuration.
\end{itemize}

Each condition is evaluated across multiple trials. 
Policy stability is measured by the variance and degradation in episode rewards, 
and recovery is assessed where applicable. 
Final comparisons are made between the two architectures across all perturbation conditions.


% --- End Dissertation Inclusion ---
% \clearpage
% \appendix
% \section*{Appendix}
% \addcontentsline{toc}{section}{Appendix}
