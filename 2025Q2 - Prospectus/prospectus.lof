\contentsline {figure}{\numberline {1}{\ignorespaces Fully reduced MDP representation of RL.}}{9}{figure.caption.4}%
\contentsline {figure}{\numberline {2}{\ignorespaces Generalized Policy Iteration.}}{11}{figure.caption.5}%
\contentsline {figure}{\numberline {3}{\ignorespaces Basic \gls {ctde}.}}{13}{figure.caption.6}%
\contentsline {figure}{\numberline {4}{\ignorespaces Actor-Critic \gls {ctde}.}}{13}{figure.caption.7}%
\contentsline {figure}{\numberline {5}{\ignorespaces League-play implemented by Vinyals et al.~\blx@tocontentsinit {0}\cite {vinyals2019}.}}{18}{figure.caption.8}%
\contentsline {figure}{\numberline {6}{\ignorespaces Environments used in this work.}}{23}{figure.caption.9}%
\contentsline {figure}{\numberline {7}{\ignorespaces Comparison of football environments.}}{25}{figure.caption.10}%
\contentsline {figure}{\numberline {8}{\ignorespaces Illustrative examples of the three environments used in this study: Waterworld (left), Multiwalker (center), and Level-Based Foraging (right).}}{28}{figure.caption.11}%
\contentsline {figure}{\numberline {9}{\ignorespaces Example of Training Progression for Waterworld and LBF }}{37}{figure.caption.13}%
\contentsline {figure}{\numberline {10}{\ignorespaces Example of Training Progression for Multiwalker}}{37}{figure.caption.14}%
\contentsline {figure}{\numberline {11}{\ignorespaces \textbf {Training Iteration Timing Across Agent Counts.} Mean wall-clock time per training iteration (in milliseconds) plotted against the number of agents, for each environment. Error bars represent the standard deviation.}}{40}{figure.caption.16}%
\contentsline {figure}{\numberline {12}{\ignorespaces \textbf {Waterworld (4 agents, 2-agent pretraining).} Comparison of 4 agent tabula rasa training, and retraining trajectories using average episodic return over normalized agent-steps steps.}}{41}{figure.caption.18}%
\contentsline {figure}{\numberline {13}{\ignorespaces \textbf {Waterworld (8 agents, 2-agent pretraining).} Comparison of 8 agent tabula rasa training, and retraining trajectories using average episodic return over normalized agent-steps steps.}}{42}{figure.caption.19}%
\contentsline {figure}{\numberline {14}{\ignorespaces \textbf {Waterworld Pretraining Impact.} Relative increase in cumulative training returns (AUC) compared to tabula rasa baselines. Rows represent final team sizes, and columns indicate the number of pretraining steps.}}{43}{figure.caption.20}%
\contentsline {figure}{\numberline {15}{\ignorespaces \textbf {Multiwalker (5 agents, 3-agent pretraining).} Comparison of 5 agent tabula rasa training, and retraining trajectories using average episodic return over normalized agent-steps steps.}}{44}{figure.caption.21}%
\contentsline {figure}{\numberline {16}{\ignorespaces \textbf {Multiwalker (6 agents, 3-agent pretraining).} Comparison of 6 agent tabula rasa training, and retraining trajectories using average episodic return over normalized agent-steps steps.}}{45}{figure.caption.22}%
\contentsline {figure}{\numberline {17}{\ignorespaces \textbf {Multiwalker Pretraining Impact.} Relative increase in cumulative training returns (AUC) compared to tabula rasa baselines. Rows represent final team sizes, and columns indicate the number of pretraining steps.}}{45}{figure.caption.23}%
\contentsline {figure}{\numberline {18}{\ignorespaces \textbf {Level-Based Foraging (7 agents, 2-agent pretraining).} Comparison of 7 agent tabula rasa training, and retraining trajectories using average episodic return over normalized agent-steps steps.}}{47}{figure.caption.25}%
\contentsline {figure}{\numberline {19}{\ignorespaces \textbf {Level-Based Foraging Pretraining Impact.} Relative increase in cumulative training returns (AUC) compared to tabula rasa baselines. Rows represent final team sizes, and columns indicate the number of pretraining steps.}}{48}{figure.caption.26}%
\contentsline {figure}{\numberline {20}{\ignorespaces Planned Timeline}}{49}{figure.caption.27}%
