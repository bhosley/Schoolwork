% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.3 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{none/global//global/global/global}
    \entry{robertson2023}{online}{}{}
      \name{author}{1}{}{%
        {{hash=2ded787643cdde3609f2946604ba703d}{%
           family={Robertson},
           familyi={R\bibinitperiod},
           given={Noah},
           giveni={N\bibinitperiod}}}%
      }
      \list{organization}{1}{%
        {Defense News}%
      }
      \strng{namehash}{2ded787643cdde3609f2946604ba703d}
      \strng{fullhash}{2ded787643cdde3609f2946604ba703d}
      \strng{fullhashraw}{2ded787643cdde3609f2946604ba703d}
      \strng{bibnamehash}{2ded787643cdde3609f2946604ba703d}
      \strng{authorbibnamehash}{2ded787643cdde3609f2946604ba703d}
      \strng{authornamehash}{2ded787643cdde3609f2946604ba703d}
      \strng{authorfullhash}{2ded787643cdde3609f2946604ba703d}
      \strng{authorfullhashraw}{2ded787643cdde3609f2946604ba703d}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The program will seek to scale unmanned, attritable systems to offset China's bulk capacity, Hicks said.}
      \field{day}{28}
      \field{hour}{15}
      \field{langid}{english}
      \field{minute}{50}
      \field{month}{8}
      \field{second}{14}
      \field{title}{Pentagon Unveils ‘{{Replicator}}’ Drone Program to Compete with {{China}}}
      \field{urlday}{25}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/BUYKV3CJ/pentagon-unveils-replicator-drone-program-to-compete-with-china.html
      \endverb
      \verb{urlraw}
      \verb https://www.defensenews.com/pentagon/2023/08/28/pentagon-unveils-replicator-drone-program-to-compete-with-china/
      \endverb
      \verb{url}
      \verb https://www.defensenews.com/pentagon/2023/08/28/pentagon-unveils-replicator-drone-program-to-compete-with-china/
      \endverb
    \endentry
    \entry{zotero-2656}{online}{}{}
      \list{organization}{1}{%
        {U.S. Department of Defense}%
      }
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labeltitlesource}{title}
      \field{abstract}{The U.S. military must capitalize on the country's greatest asset – the unparalleled innovation of its people, Deputy Defense Secretary Kathleen Hicks said.}
      \field{langid}{american}
      \field{title}{Hicks {{Discusses Replicator Initiative}}}
      \field{urlday}{25}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/PB7VWMBI/hicks-discusses-replicator-initiative.html
      \endverb
      \verb{urlraw}
      \verb https://www.defense.gov/News/News-Stories/Article/Article/3518827/hicks-discusses-replicator-initiative/https%3A%2F%2Fwww.defense.gov%2FNews%2FNews-Stories%2FArticle%2FArticle%2F3518827%2Fhicks-discusses-replicator-initiative%2F
      \endverb
      \verb{url}
      \verb https://www.defense.gov/News/News-Stories/Article/Article/3518827/hicks-discusses-replicator-initiative/https%3A%2F%2Fwww.defense.gov%2FNews%2FNews-Stories%2FArticle%2FArticle%2F3518827%2Fhicks-discusses-replicator-initiative%2F
      \endverb
    \endentry
    \entry{bajak2023}{online}{}{}
      \name{author}{1}{}{%
        {{hash=118a709838a2e15d83f6b3b429a5ee8a}{%
           family={Bajak},
           familyi={B\bibinitperiod},
           given={Frank},
           giveni={F\bibinitperiod}}}%
      }
      \list{organization}{1}{%
        {AP News}%
      }
      \strng{namehash}{118a709838a2e15d83f6b3b429a5ee8a}
      \strng{fullhash}{118a709838a2e15d83f6b3b429a5ee8a}
      \strng{fullhashraw}{118a709838a2e15d83f6b3b429a5ee8a}
      \strng{bibnamehash}{118a709838a2e15d83f6b3b429a5ee8a}
      \strng{authorbibnamehash}{118a709838a2e15d83f6b3b429a5ee8a}
      \strng{authornamehash}{118a709838a2e15d83f6b3b429a5ee8a}
      \strng{authorfullhash}{118a709838a2e15d83f6b3b429a5ee8a}
      \strng{authorfullhashraw}{118a709838a2e15d83f6b3b429a5ee8a}
      \field{extraname}{1}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Artificial intelligence employed by the U.S. military has piloted pint-sized surveillance drones in special operations forces' missions.}
      \field{day}{25}
      \field{hour}{15}
      \field{langid}{english}
      \field{minute}{49}
      \field{month}{11}
      \field{second}{50}
      \field{title}{Pentagon's {{AI}} Initiatives Accelerate Hard Decisions on Lethal Autonomous Weapons}
      \field{urlday}{25}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://apnews.com/article/us-military-ai-projects-0773b4937801e7a0573f44b57a9a5942
      \endverb
      \verb{url}
      \verb https://apnews.com/article/us-military-ai-projects-0773b4937801e7a0573f44b57a9a5942
      \endverb
    \endentry
    \entry{bajak2023a}{online}{}{}
      \name{author}{1}{}{%
        {{hash=118a709838a2e15d83f6b3b429a5ee8a}{%
           family={Bajak},
           familyi={B\bibinitperiod},
           given={Frank},
           giveni={F\bibinitperiod}}}%
      }
      \list{organization}{1}{%
        {AP News}%
      }
      \strng{namehash}{118a709838a2e15d83f6b3b429a5ee8a}
      \strng{fullhash}{118a709838a2e15d83f6b3b429a5ee8a}
      \strng{fullhashraw}{118a709838a2e15d83f6b3b429a5ee8a}
      \strng{bibnamehash}{118a709838a2e15d83f6b3b429a5ee8a}
      \strng{authorbibnamehash}{118a709838a2e15d83f6b3b429a5ee8a}
      \strng{authornamehash}{118a709838a2e15d83f6b3b429a5ee8a}
      \strng{authorfullhash}{118a709838a2e15d83f6b3b429a5ee8a}
      \strng{authorfullhashraw}{118a709838a2e15d83f6b3b429a5ee8a}
      \field{extraname}{2}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{KYIV, Ukraine (AP) — Drone advances in Ukraine have accelerated a long-anticipated technology trend that could soon bring the world's first fully autonomous fighting robots to the battlefield, inaugurating a new age of warfare.}
      \field{day}{3}
      \field{hour}{22}
      \field{langid}{english}
      \field{minute}{4}
      \field{month}{1}
      \field{second}{55}
      \field{title}{Drone Advances in {{Ukraine}} Could Bring Dawn of Killer Robots}
      \field{urlday}{25}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/V7BS8YJ5/technology-science-politics-military-drones-f4a42279515a067c6db2ce75128328c4.html
      \endverb
      \verb{urlraw}
      \verb https://apnews.com/article/technology-science-politics-military-drones-f4a42279515a067c6db2ce75128328c4
      \endverb
      \verb{url}
      \verb https://apnews.com/article/technology-science-politics-military-drones-f4a42279515a067c6db2ce75128328c4
      \endverb
    \endentry
    \entry{gerstein2024}{report}{}{}
      \name{author}{2}{}{%
        {{hash=ea7175379b2c36ccf0e79c92dcac4a0a}{%
           family={Gerstein},
           familyi={G\bibinitperiod},
           given={Daniel\bibnamedelima M.},
           giveni={D\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=c0db7d210b27c099373cebf8daba68c0}{%
           family={Leidy},
           familyi={L\bibinitperiod},
           given={Erin\bibnamedelima N.},
           giveni={E\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
      }
      \list{institution}{1}{%
        {RAND Corporation}%
      }
      \strng{namehash}{40d1200aa3c46406ced491c60fd45a60}
      \strng{fullhash}{40d1200aa3c46406ced491c60fd45a60}
      \strng{fullhashraw}{40d1200aa3c46406ced491c60fd45a60}
      \strng{bibnamehash}{40d1200aa3c46406ced491c60fd45a60}
      \strng{authorbibnamehash}{40d1200aa3c46406ced491c60fd45a60}
      \strng{authornamehash}{40d1200aa3c46406ced491c60fd45a60}
      \strng{authorfullhash}{40d1200aa3c46406ced491c60fd45a60}
      \strng{authorfullhashraw}{40d1200aa3c46406ced491c60fd45a60}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{{$<$}p{$>$}Researchers provide an assessment of the risk to the U.S. homeland from intelligent swarm technology using unmanned aerial systems or drones. They consider technology availability, threat, vulnerability, and consequences in the next three years, three to five years, and five to ten years.{$<$}/p{$>$}}
      \field{day}{15}
      \field{langid}{english}
      \field{month}{2}
      \field{shorttitle}{Emerging {{Technology}} and {{Risk Analysis}}}
      \field{title}{Emerging {{Technology}} and {{Risk Analysis}}: {{Unmanned Aerial Systems Intelligent Swarm Technology}}}
      \field{urlday}{25}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/9ZEMHAWQ/Gerstein_Leidy_2024_Emerging Technology and Risk Analysis.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.rand.org/pubs/research_reports/RRA2380-1.html
      \endverb
      \verb{url}
      \verb https://www.rand.org/pubs/research_reports/RRA2380-1.html
      \endverb
      \keyw{Civilian and Commercial Drones,Military Drones,Military Information Technology Systems,Threat Assessment}
    \endentry
    \entry{sutton2018}{book}{}{}
      \name{author}{2}{}{%
        {{hash=eb920e5277d3d5fd0903f3cd41e11871}{%
           family={Sutton},
           familyi={S\bibinitperiod},
           given={Richard\bibnamedelima S.},
           giveni={R\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=32a0a208f8bcf56a13b0a8e618aa806a}{%
           family={Barto},
           familyi={B\bibinitperiod},
           given={Andrew\bibnamedelima G.},
           giveni={A\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cambridge, Massachusetts}%
      }
      \list{publisher}{1}{%
        {The MIT Press}%
      }
      \strng{namehash}{c6212f1a1407d96a3d9f4fefbb07eade}
      \strng{fullhash}{c6212f1a1407d96a3d9f4fefbb07eade}
      \strng{fullhashraw}{c6212f1a1407d96a3d9f4fefbb07eade}
      \strng{bibnamehash}{c6212f1a1407d96a3d9f4fefbb07eade}
      \strng{authorbibnamehash}{c6212f1a1407d96a3d9f4fefbb07eade}
      \strng{authornamehash}{c6212f1a1407d96a3d9f4fefbb07eade}
      \strng{authorfullhash}{c6212f1a1407d96a3d9f4fefbb07eade}
      \strng{authorfullhashraw}{c6212f1a1407d96a3d9f4fefbb07eade}
      \field{sortinit}{7}
      \field{sortinithash}{108d0be1b1bee9773a1173443802c0a3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--}
      \field{edition}{Second edition}
      \field{isbn}{978-0-262-03924-6}
      \field{langid}{english}
      \field{pagetotal}{526}
      \field{series}{Adaptive Computation and Machine Learning Series}
      \field{shorttitle}{Reinforcement Learning}
      \field{title}{Reinforcement Learning: An Introduction}
      \field{year}{2018}
      \field{dateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/WBG2QEG6/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf
      \endverb
      \keyw{Reinforcement learning}
    \endentry
    \entry{cao2012}{online}{}{}
      \name{author}{4}{}{%
        {{hash=5657259856dc4521a4b0919759780670}{%
           family={Cao},
           familyi={C\bibinitperiod},
           given={Yongcan},
           giveni={Y\bibinitperiod}}}%
        {{hash=999899fd204d7ef5355e044f745c1067}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Wenwu},
           giveni={W\bibinitperiod}}}%
        {{hash=79312c25841a4f2341e462020d563806}{%
           family={Ren},
           familyi={R\bibinitperiod},
           given={Wei},
           giveni={W\bibinitperiod}}}%
        {{hash=c2465aadc98c418b040e810d4f8c1b79}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Guanrong},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{8b18bb86bd9a2a5b896c96c06643b341}
      \strng{fullhash}{07ea47f3ce2633e4f9f044c89817096a}
      \strng{fullhashraw}{07ea47f3ce2633e4f9f044c89817096a}
      \strng{bibnamehash}{07ea47f3ce2633e4f9f044c89817096a}
      \strng{authorbibnamehash}{07ea47f3ce2633e4f9f044c89817096a}
      \strng{authornamehash}{8b18bb86bd9a2a5b896c96c06643b341}
      \strng{authorfullhash}{07ea47f3ce2633e4f9f044c89817096a}
      \strng{authorfullhashraw}{07ea47f3ce2633e4f9f044c89817096a}
      \field{sortinit}{8}
      \field{sortinithash}{a231b008ebf0ecbe0b4d96dcc159445f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This article reviews some main results and progress in distributed multi-agent coordination, focusing on papers published in major control systems and robotics journals since 2006. Distributed coordination of multiple vehicles, including unmanned aerial vehicles, unmanned ground vehicles and unmanned underwater vehicles, has been a very active research subject studied extensively by the systems and control community. The recent results in this area are categorized into several directions, such as consensus, formation control, optimization, task assignment, and estimation. After the review, a short discussion section is included to summarize the existing research and to propose several promising research directions along with some open problems that are deemed important for further investigations.}
      \field{day}{4}
      \field{eprintclass}{math}
      \field{eprinttype}{arXiv}
      \field{month}{9}
      \field{pubstate}{prepublished}
      \field{title}{An {{Overview}} of {{Recent Progress}} in the {{Study}} of {{Distributed Multi-agent Coordination}}}
      \field{urlday}{12}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2012}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1207.3231
      \endverb
      \verb{eprint}
      \verb 1207.3231
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/WDMRVX7F/Cao et al_2012_An Overview of Recent Progress in the Study of Distributed Multi-agent.pdf;/Users/brandonhosley/Zotero/storage/IICE6PDC/1207.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1207.3231
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1207.3231
      \endverb
      \keyw{Mathematics - Optimization and Control}
    \endentry
    \entry{gronauer2022}{article}{}{}
      \name{author}{2}{}{%
        {{hash=ae0518f1e8f7a634269e7864240c575f}{%
           family={Gronauer},
           familyi={G\bibinitperiod},
           given={Sven},
           giveni={S\bibinitperiod}}}%
        {{hash=82d927455f03a14711dd2326b1fc9146}{%
           family={Diepold},
           familyi={D\bibinitperiod},
           given={Klaus},
           giveni={K\bibinitperiod}}}%
      }
      \strng{namehash}{bfac1a2a6d3fbc161611f8388c36fca5}
      \strng{fullhash}{bfac1a2a6d3fbc161611f8388c36fca5}
      \strng{fullhashraw}{bfac1a2a6d3fbc161611f8388c36fca5}
      \strng{bibnamehash}{bfac1a2a6d3fbc161611f8388c36fca5}
      \strng{authorbibnamehash}{bfac1a2a6d3fbc161611f8388c36fca5}
      \strng{authornamehash}{bfac1a2a6d3fbc161611f8388c36fca5}
      \strng{authorfullhash}{bfac1a2a6d3fbc161611f8388c36fca5}
      \strng{authorfullhashraw}{bfac1a2a6d3fbc161611f8388c36fca5}
      \field{sortinit}{9}
      \field{sortinithash}{0a5ebc79d83c96b6579069544c73c7d4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The advances in reinforcement learning have recorded sublime success in various domains. Although the multi-agent domain has been overshadowed by its single-agent counterpart during this progress, multi-agent reinforcement learning gains rapid traction, and the latest accomplishments address problems with real-world complexity. This article provides an overview of the current developments in the field of multi-agent deep reinforcement learning. We focus primarily on literature from recent years that combines deep reinforcement learning methods with a multi-agent scenario. To survey the works that constitute the contemporary landscape, the main~contents are divided into three parts. First, we analyze the structure of training schemes that are applied to train multiple agents. Second, we consider the emergent patterns of agent behavior in cooperative, competitive and mixed scenarios. Third, we systematically enumerate challenges that exclusively arise in the multi-agent domain and review methods that are leveraged to cope with these challenges. To conclude this survey, we discuss advances, identify trends, and outline possible directions for future work in this research area.}
      \field{day}{1}
      \field{issn}{1573-7462}
      \field{journaltitle}{Artificial Intelligence Review}
      \field{langid}{english}
      \field{month}{2}
      \field{number}{2}
      \field{shortjournal}{Artif Intell Rev}
      \field{shorttitle}{Multi-Agent Deep Reinforcement Learning}
      \field{title}{Multi-Agent Deep Reinforcement Learning: A Survey}
      \field{urlday}{26}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{volume}{55}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{895\bibrangedash 943}
      \range{pages}{49}
      \verb{doi}
      \verb 10.1007/s10462-021-09996-w
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/UJLZVR4C/Gronauer_Diepold_2022_Multi-agent deep reinforcement learning.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s10462-021-09996-w
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s10462-021-09996-w
      \endverb
      \keyw{Deep learning,Machine learning,Multi-agent learning,Multi-agent systems,Reinforcement learning,Survey}
    \endentry
    \entry{calvo2018}{article}{}{}
      \name{author}{2}{}{%
        {{hash=714301dd2a30055a734f8f8ddf9f18af}{%
           family={Calvo},
           familyi={C\bibinitperiod},
           given={Jeancarlo\bibnamedelima Arguello},
           giveni={J\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=d9cfb32feeafc29e489d22e64a37a8a9}{%
           family={Dusparic},
           familyi={D\bibinitperiod},
           given={Ivana},
           giveni={I\bibinitperiod}}}%
      }
      \strng{namehash}{7401ae94bd6c78bee9b0583a6cf64bd1}
      \strng{fullhash}{7401ae94bd6c78bee9b0583a6cf64bd1}
      \strng{fullhashraw}{7401ae94bd6c78bee9b0583a6cf64bd1}
      \strng{bibnamehash}{7401ae94bd6c78bee9b0583a6cf64bd1}
      \strng{authorbibnamehash}{7401ae94bd6c78bee9b0583a6cf64bd1}
      \strng{authornamehash}{7401ae94bd6c78bee9b0583a6cf64bd1}
      \strng{authorfullhash}{7401ae94bd6c78bee9b0583a6cf64bd1}
      \strng{authorfullhashraw}{7401ae94bd6c78bee9b0583a6cf64bd1}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Reinforcement Learning (RL) has been extensively used in Urban Traffic Control (UTC) optimization due its capability to learn the dynamics of complex problems from interactions with the environment. Recent advances in Deep Reinforcement Learning (DRL) have opened up the possibilities for extending this work to more complex situations due to it overcoming the curse of dimensionality resulting from the exponential growth of the state and action spaces when incorporating fine-grained information. DRL has been shown to work very well for UTC on a single intersection, however, due to large training times, multi-junction implementations have been limited to training a single agent and replicating behaviour to other junctions, assuming homogeneity of all agents.}
      \field{journaltitle}{AICS}
      \field{langid}{english}
      \field{month}{12}
      \field{title}{Heterogeneous {{Multi-Agent Deep Reinforcement Learning}} for {{Traﬃc Lights Control}}}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{pages}{2\bibrangedash 13}
      \range{pages}{12}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/KIDZZFHR/Calvo and Dusparic - Heterogeneous Multi-Agent Deep Reinforcement Learn.pdf
      \endverb
    \endentry
    \entry{hoang2023}{incollection}{}{}
      \name{author}{6}{}{%
        {{hash=519a2c85c408a53b30730d311071f6da}{%
           family={Hoang},
           familyi={H\bibinitperiod},
           given={Maria-Theresa\bibnamedelima Oanh},
           giveni={M\bibinithyphendelim T\bibinitperiod\bibinitdelim O\bibinitperiod}}}%
        {{hash=3833fbffd346f0912bc0ed96c6dc30b3}{%
           family={Grøntved},
           familyi={G\bibinitperiod},
           given={Kasper\bibnamedelimb Andreas\bibnamedelima Rømer},
           giveni={K\bibinitperiod\bibinitdelim A\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=685b9ea4bbe3cfd182b2f6de14392df7}{%
           family={Van\bibnamedelima Berkel},
           familyi={V\bibinitperiod\bibinitdelim B\bibinitperiod},
           given={Niels},
           giveni={N\bibinitperiod}}}%
        {{hash=9adb34bb3d94b6b03a5e7aeae989aca7}{%
           family={Skov},
           familyi={S\bibinitperiod},
           given={Mikael\bibnamedelima B},
           giveni={M\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=9bcfcdaddcb7ce07a993c8896270eecd}{%
           family={Christensen},
           familyi={C\bibinitperiod},
           given={Anders\bibnamedelima Lyhne},
           giveni={A\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=dc5a47d852a6263c511e4ba4f52ebc1b}{%
           family={Merritt},
           familyi={M\bibinitperiod},
           given={Timothy},
           giveni={T\bibinitperiod}}}%
      }
      \name{editor}{4}{}{%
        {{hash=a234b822a6d76c8a9cb5f078e801d411}{%
           family={Dunstan},
           familyi={D\bibinitperiod},
           given={Belinda\bibnamedelima J.},
           giveni={B\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=3e8eb7293272c2087e88443048b77198}{%
           family={Koh},
           familyi={K\bibinitperiod},
           given={Jeffrey\bibnamedelimb T.\bibnamedelimi K.\bibnamedelimi V.},
           giveni={J\bibinitperiod\bibinitdelim T\bibinitperiod\bibinitdelim K\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
        {{hash=0dbb722262c73e12749c4022266a4c7c}{%
           family={Turnbull\bibnamedelima Tillman},
           familyi={T\bibinitperiod\bibinitdelim T\bibinitperiod},
           given={Deborah},
           giveni={D\bibinitperiod}}}%
        {{hash=a8d06e1e090867d306cf81c9bdd48e85}{%
           family={Brown},
           familyi={B\bibinitperiod},
           given={Scott\bibnamedelima Andrew},
           giveni={S\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cham}%
      }
      \list{publisher}{1}{%
        {Springer International Publishing}%
      }
      \strng{namehash}{bba7e2d066a0f511a3ef8adea78073d2}
      \strng{fullhash}{9cb03b966aadaffa0ad4a6c8f281f01c}
      \strng{fullhashraw}{9cb03b966aadaffa0ad4a6c8f281f01c}
      \strng{bibnamehash}{9cb03b966aadaffa0ad4a6c8f281f01c}
      \strng{authorbibnamehash}{9cb03b966aadaffa0ad4a6c8f281f01c}
      \strng{authornamehash}{bba7e2d066a0f511a3ef8adea78073d2}
      \strng{authorfullhash}{9cb03b966aadaffa0ad4a6c8f281f01c}
      \strng{authorfullhashraw}{9cb03b966aadaffa0ad4a6c8f281f01c}
      \strng{editorbibnamehash}{a1cf2c6552df80185d8e46469a5a17b8}
      \strng{editornamehash}{b310fd650be253b5989b532de2bab857}
      \strng{editorfullhash}{a1cf2c6552df80185d8e46469a5a17b8}
      \strng{editorfullhashraw}{a1cf2c6552df80185d8e46469a5a17b8}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{booktitle}{Cultural {{Robotics}}: {{Social Robots}} and {{Their Emergent Cultural Ecologies}}}
      \field{isbn}{978-3-031-28137-2 978-3-031-28138-9}
      \field{langid}{english}
      \field{shorttitle}{Drone {{Swarms}} to {{Support Search}} and {{Rescue Operations}}}
      \field{title}{Drone {{Swarms}} to {{Support Search}} and {{Rescue Operations}}: {{Opportunities}} and {{Challenges}}}
      \field{urlday}{12}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{163\bibrangedash 176}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1007/978-3-031-28138-9_11
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/EWGATS8U/Hoang et al. - 2023 - Drone Swarms to Support Search and Rescue Operatio.pdf
      \endverb
      \verb{urlraw}
      \verb https://link.springer.com/10.1007/978-3-031-28138-9_11
      \endverb
      \verb{url}
      \verb https://link.springer.com/10.1007/978-3-031-28138-9_11
      \endverb
    \endentry
    \entry{kouzeghar2023}{online}{}{}
      \name{author}{4}{}{%
        {{hash=4f23240b0d55c7c46f5581cde138d239}{%
           family={Kouzeghar},
           familyi={K\bibinitperiod},
           given={Maryam},
           giveni={M\bibinitperiod}}}%
        {{hash=ea5a794151c4e50774de681b15695182}{%
           family={Song},
           familyi={S\bibinitperiod},
           given={Youngbin},
           giveni={Y\bibinitperiod}}}%
        {{hash=017c38c8d02eaf393174f99f1f2b7e09}{%
           family={Meghjani},
           familyi={M\bibinitperiod},
           given={Malika},
           giveni={M\bibinitperiod}}}%
        {{hash=0ef768a0f70c70e07476a88f403f2051}{%
           family={Bouffanais},
           familyi={B\bibinitperiod},
           given={Roland},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{4b6401a457970de28baeac6eb530d0f2}
      \strng{fullhash}{d049bfce8677e5f6f0770a1b929f6360}
      \strng{fullhashraw}{d049bfce8677e5f6f0770a1b929f6360}
      \strng{bibnamehash}{d049bfce8677e5f6f0770a1b929f6360}
      \strng{authorbibnamehash}{d049bfce8677e5f6f0770a1b929f6360}
      \strng{authornamehash}{4b6401a457970de28baeac6eb530d0f2}
      \strng{authorfullhash}{d049bfce8677e5f6f0770a1b929f6360}
      \strng{authorfullhashraw}{d049bfce8677e5f6f0770a1b929f6360}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Multi-agent pursuit-evasion tasks involving intelligent targets are notoriously challenging coordination problems. In this paper, we investigate new ways to learn such coordinated behaviors of unmanned aerial vehicles (UAVs) aimed at keeping track of multiple evasive targets. Within a Multi-Agent Reinforcement Learning (MARL) framework, we specifically propose a variant of the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) method. Our approach addresses multi-target pursuit-evasion scenarios within non-stationary and unknown environments with random obstacles. In addition, given the critical role played by collective exploration in terms of detecting possible targets, we implement heterogeneous roles for the pursuers for enhanced exploratory actions balanced by exploitation (i.e. tracking) of previously identified targets. Our proposed role-based MADDPG algorithm is not only able to track multiple targets, but also is able to explore for possible targets by means of the proposed Voronoi-based rewarding policy. We implemented, tested and validated our approach in a simulation environment prior to deploying a real-world multi-robot system comprising of Crazyflie drones. Our results demonstrate that a multi-agent pursuit team has the ability to learn highly efficient coordinated control policies in terms of target tracking and exploration even when confronted with multiple fast evasive targets in complex environments.}
      \field{day}{3}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{3}
      \field{pubstate}{prepublished}
      \field{title}{Multi-{{Target Pursuit}} by a {{Decentralized Heterogeneous UAV Swarm}} Using {{Deep Multi-Agent Reinforcement Learning}}}
      \field{urlday}{12}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2303.01799
      \endverb
      \verb{eprint}
      \verb 2303.01799
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/N93MAVU3/Kouzeghar et al_2023_Multi-Target Pursuit by a Decentralized Heterogeneous UAV Swarm using Deep.pdf;/Users/brandonhosley/Zotero/storage/LF8UKJHP/2303.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2303.01799
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2303.01799
      \endverb
      \keyw{Computer Science - Robotics}
    \endentry
    \entry{carbone2018}{article}{}{}
      \name{author}{3}{}{%
        {{hash=0eecef267ae50b609e6071fc3e961cdd}{%
           family={Carbone},
           familyi={C\bibinitperiod},
           given={Carlos},
           giveni={C\bibinitperiod}}}%
        {{hash=3a9793807eca6dd03c6208f76fb69c27}{%
           family={Garibaldi},
           familyi={G\bibinitperiod},
           given={Oscar},
           giveni={O\bibinitperiod}}}%
        {{hash=68103077f13cd42bb02901f28fa41d17}{%
           family={Kurt},
           familyi={K\bibinitperiod},
           given={Zohre},
           giveni={Z\bibinitperiod}}}%
      }
      \strng{namehash}{354394c5a2975ca3f4f7dcfa9f0934ae}
      \strng{fullhash}{bc8cf1a5c02848e3dfc4e09c46377305}
      \strng{fullhashraw}{bc8cf1a5c02848e3dfc4e09c46377305}
      \strng{bibnamehash}{bc8cf1a5c02848e3dfc4e09c46377305}
      \strng{authorbibnamehash}{bc8cf1a5c02848e3dfc4e09c46377305}
      \strng{authornamehash}{354394c5a2975ca3f4f7dcfa9f0934ae}
      \strng{authorfullhash}{bc8cf1a5c02848e3dfc4e09c46377305}
      \strng{authorfullhashraw}{bc8cf1a5c02848e3dfc4e09c46377305}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper summarizes the concept of swarm robotics and its applicability to crop inspections. To increase the agricultural yield it is essential to monitor the crop health. Hence, precision agriculture is becoming a common practice for farmers providing a system that can inspect the state of the plants (Khosla and others, 2010). One of the rising technologies used for agricultural inspections is the use of unmaned air vehicles (UAVs) which are used to take aerial pictures of the farms so that the images could be processed to extract data about the state of the crops (Das et al., 2015). For this process both fixed wings and quadrotors UAVs are used with a preference over the quadrotor since it’s easier to operate and has a milder learning curve compared to fixed wings (Kolodny, 2017). UAVs require battery replacement especially when the environmental conditions result in longer inspection times (“Agriculture - Maximize Yields with Aerial Imaging,” n.d., “Matrice 100 - DJI Wiki,” n.d.). As a result, inspection systems for crops using commercial quadrotors are limited by the quadrotor´s maximum flight speed, maximum flight height, quadrotor´s battery time, crops area, wind conditions, etc. (“Mission Estimates,” n.d.).Keywords: Swarm Robotics, Precision Agriculture, Unmanned Air Vehicle, Quadrotor, inspection.}
      \field{day}{11}
      \field{issn}{2518-6841}
      \field{journaltitle}{KnE Engineering}
      \field{langid}{english}
      \field{month}{2}
      \field{title}{Swarm {{Robotics}} as a {{Solution}} to {{Crops Inspection}} for {{Precision Agriculture}}}
      \field{urlday}{12}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{552\bibrangedash 562}
      \range{pages}{11}
      \verb{doi}
      \verb 10.18502/keg.v3i1.1459
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/ABHP2XRH/Carbone et al_2018_Swarm Robotics as a Solution to Crops Inspection for Precision Agriculture.pdf
      \endverb
      \verb{urlraw}
      \verb https://knepublishing.com/index.php/KnE-Engineering/article/view/1459
      \endverb
      \verb{url}
      \verb https://knepublishing.com/index.php/KnE-Engineering/article/view/1459
      \endverb
    \endentry
    \entry{amarasinghe2019}{inproceedings}{}{}
      \name{author}{4}{}{%
        {{hash=da23fe94c7f8f71eb7c73877e2232d27}{%
           family={Amarasinghe},
           familyi={A\bibinitperiod},
           given={Akarshani},
           giveni={A\bibinitperiod}}}%
        {{hash=a51e8d6f5618d7d2112cf554ac7d1d3b}{%
           family={Wijesuriya},
           familyi={W\bibinitperiod},
           given={Viraj\bibnamedelima B.},
           giveni={V\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=4288bf027f606bb8fedffc092c15a7e8}{%
           family={Ganepola},
           familyi={G\bibinitperiod},
           given={Dilshan},
           giveni={D\bibinitperiod}}}%
        {{hash=264e97c42f15b2b6a3dde3c133aadd2b}{%
           family={Jayaratne},
           familyi={J\bibinitperiod},
           given={Lakshman},
           giveni={L\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {New York, NY, USA}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery}%
      }
      \strng{namehash}{76f677664fb4d5f06aa24d3acf03aa50}
      \strng{fullhash}{5f63779e6afdef31866a0b9c7f43a2f9}
      \strng{fullhashraw}{5f63779e6afdef31866a0b9c7f43a2f9}
      \strng{bibnamehash}{5f63779e6afdef31866a0b9c7f43a2f9}
      \strng{authorbibnamehash}{5f63779e6afdef31866a0b9c7f43a2f9}
      \strng{authornamehash}{76f677664fb4d5f06aa24d3acf03aa50}
      \strng{authorfullhash}{5f63779e6afdef31866a0b9c7f43a2f9}
      \strng{authorfullhashraw}{5f63779e6afdef31866a0b9c7f43a2f9}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Pesticides are detrimental to the well-being of all living beings. Inappropriate pesticide usage has been identified as a major health hazard. Therefore, it is important to devise solutions that enable agricultural pest control without excessive use of pesticides. At present, drone technology is being increasingly applied in agriculture through soil and field analysis, planting, crop spraying, crop monitoring, irrigation and health assessment. The primary focus of this ongoing work is to leverage a swarm of drones solution that includes precision agriculture techniques, to efficiently spray pesticides in arable lands with optimal pesticides usage and minimum human intervention.}
      \field{booktitle}{Proceedings of the 17th {{Conference}} on {{Embedded Networked Sensor Systems}}}
      \field{day}{10}
      \field{isbn}{978-1-4503-6950-3}
      \field{month}{11}
      \field{series}{{{SenSys}} '19}
      \field{shorttitle}{A Swarm of Crop Spraying Drones Solution for Optimising Safe Pesticide Usage in Arable Lands}
      \field{title}{A Swarm of Crop Spraying Drones Solution for Optimising Safe Pesticide Usage in Arable Lands: Poster Abstract}
      \field{urlday}{12}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{410\bibrangedash 411}
      \range{pages}{2}
      \verb{doi}
      \verb 10.1145/3356250.3361948
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1145/3356250.3361948
      \endverb
      \verb{url}
      \verb https://doi.org/10.1145/3356250.3361948
      \endverb
      \keyw{drone systems,swarm of drones}
    \endentry
    \entry{campbell2002}{article}{}{}
      \name{author}{3}{}{%
        {{hash=0b29c33e62b9ff91d2cb5e73984c8017}{%
           family={Campbell},
           familyi={C\bibinitperiod},
           given={Murray},
           giveni={M\bibinitperiod}}}%
        {{hash=1ee11fcd414bf4c15bef7cbff8ebca99}{%
           family={Hoane},
           familyi={H\bibinitperiod},
           given={A.\bibnamedelimi Joseph},
           giveni={A\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=f4b058d44f9e5ad050dd6317be214f0b}{%
           family={Hsu},
           familyi={H\bibinitperiod},
           given={Feng-hsiung},
           giveni={F\bibinithyphendelim h\bibinitperiod}}}%
      }
      \strng{namehash}{0fcb95bb3df92a109853380fa2b08757}
      \strng{fullhash}{48895abef2eca3e38fdfdb88729f31a8}
      \strng{fullhashraw}{48895abef2eca3e38fdfdb88729f31a8}
      \strng{bibnamehash}{48895abef2eca3e38fdfdb88729f31a8}
      \strng{authorbibnamehash}{48895abef2eca3e38fdfdb88729f31a8}
      \strng{authornamehash}{0fcb95bb3df92a109853380fa2b08757}
      \strng{authorfullhash}{48895abef2eca3e38fdfdb88729f31a8}
      \strng{authorfullhashraw}{48895abef2eca3e38fdfdb88729f31a8}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: •a single-chip chess search engine,•a massively parallel system with multiple levels of parallelism,•a strong emphasis on search extensions,•a complex evaluation function, and•effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.}
      \field{day}{1}
      \field{issn}{0004-3702}
      \field{journaltitle}{Artificial Intelligence}
      \field{month}{1}
      \field{number}{1}
      \field{shortjournal}{Artificial Intelligence}
      \field{title}{Deep {{Blue}}}
      \field{urlday}{1}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{volume}{134}
      \field{year}{2002}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{57\bibrangedash 83}
      \range{pages}{27}
      \verb{doi}
      \verb 10.1016/S0004-3702(01)00129-1
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/4HB7XQYF/S0004370201001291.html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S0004370201001291
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S0004370201001291
      \endverb
      \keyw{Computer chess,Evaluation function,Game tree search,Parallel search,Search extensions,Selective search}
    \endentry
    \entry{silver2016}{article}{useprefix=true}{}
      \name{author}{20}{}{%
        {{hash=89dbd30410c2085cd059f32c57d4593e}{%
           family={Silver},
           familyi={S\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=ba4b200ce1412a2570cb113366cc9559}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Aja},
           giveni={A\bibinitperiod}}}%
        {{hash=7c3126321fcb4553dfda5ae96da928ce}{%
           family={Maddison},
           familyi={M\bibinitperiod},
           given={Chris\bibnamedelima J.},
           giveni={C\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=4131bd14e5ca890278ecd351e356dc34}{%
           family={Guez},
           familyi={G\bibinitperiod},
           given={Arthur},
           giveni={A\bibinitperiod}}}%
        {{hash=50d24de916599d306c5cb1a77156e4b9}{%
           family={Sifre},
           familyi={S\bibinitperiod},
           given={Laurent},
           giveni={L\bibinitperiod}}}%
        {{hash=54c5c572d4fdc5d822c240b59fcadad4}{%
           family={Driessche},
           familyi={D\bibinitperiod},
           given={George},
           giveni={G\bibinitperiod},
           prefix={van\bibnamedelima den},
           prefixi={v\bibinitperiod\bibinitdelim d\bibinitperiod}}}%
        {{hash=8fad8df927bc0014c0bd6a9feb7aa71d}{%
           family={Schrittwieser},
           familyi={S\bibinitperiod},
           given={Julian},
           giveni={J\bibinitperiod}}}%
        {{hash=af540e84ef1ecdaa70b1f7c90f59fd7d}{%
           family={Antonoglou},
           familyi={A\bibinitperiod},
           given={Ioannis},
           giveni={I\bibinitperiod}}}%
        {{hash=1f065703a4f7a60c69ecb59f499d3db3}{%
           family={Panneershelvam},
           familyi={P\bibinitperiod},
           given={Veda},
           giveni={V\bibinitperiod}}}%
        {{hash=3075d0e02c0833f6e5fe1addb880898f}{%
           family={Lanctot},
           familyi={L\bibinitperiod},
           given={Marc},
           giveni={M\bibinitperiod}}}%
        {{hash=a133b469b21b870786119a47b1b691eb}{%
           family={Dieleman},
           familyi={D\bibinitperiod},
           given={Sander},
           giveni={S\bibinitperiod}}}%
        {{hash=c19e654fe7f97a2862b0f06588d04c42}{%
           family={Grewe},
           familyi={G\bibinitperiod},
           given={Dominik},
           giveni={D\bibinitperiod}}}%
        {{hash=5560aef5a586c04b85130c3be44a3b6a}{%
           family={Nham},
           familyi={N\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod}}}%
        {{hash=d2d0778c1cdd451c75b874b58eec7564}{%
           family={Kalchbrenner},
           familyi={K\bibinitperiod},
           given={Nal},
           giveni={N\bibinitperiod}}}%
        {{hash=8d569d1d5b8b5a7836017a98b430f959}{%
           family={Sutskever},
           familyi={S\bibinitperiod},
           given={Ilya},
           giveni={I\bibinitperiod}}}%
        {{hash=3a6fdf4df9a25f1d2d506ad9e86e1f6c}{%
           family={Lillicrap},
           familyi={L\bibinitperiod},
           given={Timothy},
           giveni={T\bibinitperiod}}}%
        {{hash=958e13979b3920f7fb4793ed8dddcb33}{%
           family={Leach},
           familyi={L\bibinitperiod},
           given={Madeleine},
           giveni={M\bibinitperiod}}}%
        {{hash=481308b301166b521c74fde6566e97e6}{%
           family={Kavukcuoglu},
           familyi={K\bibinitperiod},
           given={Koray},
           giveni={K\bibinitperiod}}}%
        {{hash=368b9b2de627b852658c433b062d4e1e}{%
           family={Graepel},
           familyi={G\bibinitperiod},
           given={Thore},
           giveni={T\bibinitperiod}}}%
        {{hash=b160026950ebb1e2286dfb40c15482f5}{%
           family={Hassabis},
           familyi={H\bibinitperiod},
           given={Demis},
           giveni={D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Nature Publishing Group}%
      }
      \strng{namehash}{4ccacac34637df06f749428b6ec5052e}
      \strng{fullhash}{6ce4d589ddc1421afa003201a0a08ba0}
      \strng{fullhashraw}{6ce4d589ddc1421afa003201a0a08ba0}
      \strng{bibnamehash}{4ccacac34637df06f749428b6ec5052e}
      \strng{authorbibnamehash}{4ccacac34637df06f749428b6ec5052e}
      \strng{authornamehash}{4ccacac34637df06f749428b6ec5052e}
      \strng{authorfullhash}{6ce4d589ddc1421afa003201a0a08ba0}
      \strng{authorfullhashraw}{6ce4d589ddc1421afa003201a0a08ba0}
      \field{extraname}{1}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.}
      \field{issn}{1476-4687}
      \field{journaltitle}{Nature}
      \field{langid}{english}
      \field{month}{1}
      \field{number}{7587}
      \field{title}{Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search}
      \field{urlday}{15}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{529}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{484\bibrangedash 489}
      \range{pages}{6}
      \verb{doi}
      \verb 10.1038/nature16961
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/F29T7VGP/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/nature16961
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/nature16961
      \endverb
      \keyw{Computational science,Computer science,Reward}
    \endentry
    \entry{silver2017}{article}{useprefix=true}{}
      \name{author}{17}{}{%
        {{hash=89dbd30410c2085cd059f32c57d4593e}{%
           family={Silver},
           familyi={S\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=8fad8df927bc0014c0bd6a9feb7aa71d}{%
           family={Schrittwieser},
           familyi={S\bibinitperiod},
           given={Julian},
           giveni={J\bibinitperiod}}}%
        {{hash=9d16b7284df92c9adaee86c37ab992df}{%
           family={Simonyan},
           familyi={S\bibinitperiod},
           given={Karen},
           giveni={K\bibinitperiod}}}%
        {{hash=af540e84ef1ecdaa70b1f7c90f59fd7d}{%
           family={Antonoglou},
           familyi={A\bibinitperiod},
           given={Ioannis},
           giveni={I\bibinitperiod}}}%
        {{hash=ba4b200ce1412a2570cb113366cc9559}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Aja},
           giveni={A\bibinitperiod}}}%
        {{hash=4131bd14e5ca890278ecd351e356dc34}{%
           family={Guez},
           familyi={G\bibinitperiod},
           given={Arthur},
           giveni={A\bibinitperiod}}}%
        {{hash=80c63e95a9e243591a33a7e3156f1d78}{%
           family={Hubert},
           familyi={H\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
        {{hash=fe79638c2bf15e9cd4021e09ca0fa07b}{%
           family={Baker},
           familyi={B\bibinitperiod},
           given={Lucas},
           giveni={L\bibinitperiod}}}%
        {{hash=f311ac3fec4a3d3207fdec17a9887704}{%
           family={Lai},
           familyi={L\bibinitperiod},
           given={Matthew},
           giveni={M\bibinitperiod}}}%
        {{hash=fa98d81082da72fd49892566108f9cbe}{%
           family={Bolton},
           familyi={B\bibinitperiod},
           given={Adrian},
           giveni={A\bibinitperiod}}}%
        {{hash=5759ff4990bdd8c6f85835738834874a}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Yutian},
           giveni={Y\bibinitperiod}}}%
        {{hash=3a6fdf4df9a25f1d2d506ad9e86e1f6c}{%
           family={Lillicrap},
           familyi={L\bibinitperiod},
           given={Timothy},
           giveni={T\bibinitperiod}}}%
        {{hash=59f6c5b7be30cd01647770d56f297a69}{%
           family={Hui},
           familyi={H\bibinitperiod},
           given={Fan},
           giveni={F\bibinitperiod}}}%
        {{hash=50d24de916599d306c5cb1a77156e4b9}{%
           family={Sifre},
           familyi={S\bibinitperiod},
           given={Laurent},
           giveni={L\bibinitperiod}}}%
        {{hash=54c5c572d4fdc5d822c240b59fcadad4}{%
           family={Driessche},
           familyi={D\bibinitperiod},
           given={George},
           giveni={G\bibinitperiod},
           prefix={van\bibnamedelima den},
           prefixi={v\bibinitperiod\bibinitdelim d\bibinitperiod}}}%
        {{hash=368b9b2de627b852658c433b062d4e1e}{%
           family={Graepel},
           familyi={G\bibinitperiod},
           given={Thore},
           giveni={T\bibinitperiod}}}%
        {{hash=b160026950ebb1e2286dfb40c15482f5}{%
           family={Hassabis},
           familyi={H\bibinitperiod},
           given={Demis},
           giveni={D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Nature Publishing Group}%
      }
      \strng{namehash}{4ccacac34637df06f749428b6ec5052e}
      \strng{fullhash}{34faf92840e9c6b68a2c64d4df1b42c3}
      \strng{fullhashraw}{34faf92840e9c6b68a2c64d4df1b42c3}
      \strng{bibnamehash}{4ccacac34637df06f749428b6ec5052e}
      \strng{authorbibnamehash}{4ccacac34637df06f749428b6ec5052e}
      \strng{authornamehash}{4ccacac34637df06f749428b6ec5052e}
      \strng{authorfullhash}{34faf92840e9c6b68a2c64d4df1b42c3}
      \strng{authorfullhashraw}{34faf92840e9c6b68a2c64d4df1b42c3}
      \field{extraname}{2}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo. Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games. To beat world champions at the game of Go, the computer program AlphaGo has relied largely on supervised learning from millions of human expert moves. David Silver and colleagues have now produced a system called AlphaGo Zero, which is based purely on reinforcement learning and learns solely from self-play. Starting from random moves, it can reach superhuman level in just a couple of days of training and five million games of self-play, and can now beat all previous versions of AlphaGo. Because the machine independently discovers the same fundamental principles of the game that took humans millennia to conceptualize, the work suggests that such principles have some universal character, beyond human bias.}
      \field{issn}{1476-4687}
      \field{journaltitle}{Nature}
      \field{langid}{english}
      \field{month}{10}
      \field{number}{7676}
      \field{title}{Mastering the Game of {{Go}} without Human Knowledge}
      \field{urlday}{15}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{550}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{354\bibrangedash 359}
      \range{pages}{6}
      \verb{doi}
      \verb 10.1038/nature24270
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/RCPCCGV2/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/nature24270
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/nature24270
      \endverb
      \keyw{Computational science,Computer science,Reward}
    \endentry
    \entry{silver2017a}{online}{}{}
      \name{author}{13}{}{%
        {{hash=89dbd30410c2085cd059f32c57d4593e}{%
           family={Silver},
           familyi={S\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=80c63e95a9e243591a33a7e3156f1d78}{%
           family={Hubert},
           familyi={H\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
        {{hash=8fad8df927bc0014c0bd6a9feb7aa71d}{%
           family={Schrittwieser},
           familyi={S\bibinitperiod},
           given={Julian},
           giveni={J\bibinitperiod}}}%
        {{hash=af540e84ef1ecdaa70b1f7c90f59fd7d}{%
           family={Antonoglou},
           familyi={A\bibinitperiod},
           given={Ioannis},
           giveni={I\bibinitperiod}}}%
        {{hash=f311ac3fec4a3d3207fdec17a9887704}{%
           family={Lai},
           familyi={L\bibinitperiod},
           given={Matthew},
           giveni={M\bibinitperiod}}}%
        {{hash=4131bd14e5ca890278ecd351e356dc34}{%
           family={Guez},
           familyi={G\bibinitperiod},
           given={Arthur},
           giveni={A\bibinitperiod}}}%
        {{hash=3075d0e02c0833f6e5fe1addb880898f}{%
           family={Lanctot},
           familyi={L\bibinitperiod},
           given={Marc},
           giveni={M\bibinitperiod}}}%
        {{hash=50d24de916599d306c5cb1a77156e4b9}{%
           family={Sifre},
           familyi={S\bibinitperiod},
           given={Laurent},
           giveni={L\bibinitperiod}}}%
        {{hash=a425c81c03d315597e3f92690763e24d}{%
           family={Kumaran},
           familyi={K\bibinitperiod},
           given={Dharshan},
           giveni={D\bibinitperiod}}}%
        {{hash=368b9b2de627b852658c433b062d4e1e}{%
           family={Graepel},
           familyi={G\bibinitperiod},
           given={Thore},
           giveni={T\bibinitperiod}}}%
        {{hash=3a6fdf4df9a25f1d2d506ad9e86e1f6c}{%
           family={Lillicrap},
           familyi={L\bibinitperiod},
           given={Timothy},
           giveni={T\bibinitperiod}}}%
        {{hash=9d16b7284df92c9adaee86c37ab992df}{%
           family={Simonyan},
           familyi={S\bibinitperiod},
           given={Karen},
           giveni={K\bibinitperiod}}}%
        {{hash=b160026950ebb1e2286dfb40c15482f5}{%
           family={Hassabis},
           familyi={H\bibinitperiod},
           given={Demis},
           giveni={D\bibinitperiod}}}%
      }
      \strng{namehash}{4ccacac34637df06f749428b6ec5052e}
      \strng{fullhash}{016fd7164d5ebbbf1e175ef34971d5fa}
      \strng{fullhashraw}{016fd7164d5ebbbf1e175ef34971d5fa}
      \strng{bibnamehash}{4ccacac34637df06f749428b6ec5052e}
      \strng{authorbibnamehash}{4ccacac34637df06f749428b6ec5052e}
      \strng{authornamehash}{4ccacac34637df06f749428b6ec5052e}
      \strng{authorfullhash}{016fd7164d5ebbbf1e175ef34971d5fa}
      \strng{authorfullhashraw}{016fd7164d5ebbbf1e175ef34971d5fa}
      \field{extraname}{3}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.}
      \field{day}{5}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{12}
      \field{pubstate}{prepublished}
      \field{title}{Mastering {{Chess}} and {{Shogi}} by {{Self-Play}} with a {{General Reinforcement Learning Algorithm}}}
      \field{urlday}{15}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1712.01815
      \endverb
      \verb{eprint}
      \verb 1712.01815
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/PFGFAPGW/Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a Gene.pdf;/Users/brandonhosley/Zotero/storage/SR3IRUXR/1712.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1712.01815
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1712.01815
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
    \endentry
    \entry{vinyals2019}{article}{}{}
      \name{author}{42}{}{%
        {{hash=494b568c5dc85ba8f3f409635f9c5f25}{%
           family={Vinyals},
           familyi={V\bibinitperiod},
           given={Oriol},
           giveni={O\bibinitperiod}}}%
        {{hash=92efe2a8e13a9b7a3fb647951ee2391c}{%
           family={Babuschkin},
           familyi={B\bibinitperiod},
           given={Igor},
           giveni={I\bibinitperiod}}}%
        {{hash=5dc7de68ac7f0299a995822f38a3e705}{%
           family={Czarnecki},
           familyi={C\bibinitperiod},
           given={Wojciech\bibnamedelima M.},
           giveni={W\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=b3fbcff36ec4b37aed7c56f67c21038b}{%
           family={Mathieu},
           familyi={M\bibinitperiod},
           given={Michaël},
           giveni={M\bibinitperiod}}}%
        {{hash=76a7579d94d000dca5e0071fb3b69382}{%
           family={Dudzik},
           familyi={D\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
        {{hash=7a79e6bb4ca772c9b3b38f4e9f45b83c}{%
           family={Chung},
           familyi={C\bibinitperiod},
           given={Junyoung},
           giveni={J\bibinitperiod}}}%
        {{hash=17ad757032d822b4a43e828ae592c91e}{%
           family={Choi},
           familyi={C\bibinitperiod},
           given={David\bibnamedelima H.},
           giveni={D\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=2820daeb58d4d5a594fbdaf6db68f850}{%
           family={Powell},
           familyi={P\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
        {{hash=2687d58e59afed22249974d723704f56}{%
           family={Ewalds},
           familyi={E\bibinitperiod},
           given={Timo},
           giveni={T\bibinitperiod}}}%
        {{hash=1c5b83cefd7033a528994276b9c00e87}{%
           family={Georgiev},
           familyi={G\bibinitperiod},
           given={Petko},
           giveni={P\bibinitperiod}}}%
        {{hash=d6a1e759373b43eefda2aab3aec6b728}{%
           family={Oh},
           familyi={O\bibinitperiod},
           given={Junhyuk},
           giveni={J\bibinitperiod}}}%
        {{hash=c1a26cb7f963abdf071da408366e83a1}{%
           family={Horgan},
           familyi={H\bibinitperiod},
           given={Dan},
           giveni={D\bibinitperiod}}}%
        {{hash=be1b190734e6a891e14bf8c4adc4d1c3}{%
           family={Kroiss},
           familyi={K\bibinitperiod},
           given={Manuel},
           giveni={M\bibinitperiod}}}%
        {{hash=970650e9394fccd4144d4b829505d2b3}{%
           family={Danihelka},
           familyi={D\bibinitperiod},
           given={Ivo},
           giveni={I\bibinitperiod}}}%
        {{hash=ba4b200ce1412a2570cb113366cc9559}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Aja},
           giveni={A\bibinitperiod}}}%
        {{hash=50d24de916599d306c5cb1a77156e4b9}{%
           family={Sifre},
           familyi={S\bibinitperiod},
           given={Laurent},
           giveni={L\bibinitperiod}}}%
        {{hash=3d7a83ed6eb983ca17cec804631dc22e}{%
           family={Cai},
           familyi={C\bibinitperiod},
           given={Trevor},
           giveni={T\bibinitperiod}}}%
        {{hash=e4fa217e56ca1781ab11713ab27cf2b4}{%
           family={Agapiou},
           familyi={A\bibinitperiod},
           given={John\bibnamedelima P.},
           giveni={J\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=7dc1446dea7ff50b2b02fb83780cc9c6}{%
           family={Jaderberg},
           familyi={J\bibinitperiod},
           given={Max},
           giveni={M\bibinitperiod}}}%
        {{hash=06f6f348a3bb818c79944f71cf518f3f}{%
           family={Vezhnevets},
           familyi={V\bibinitperiod},
           given={Alexander\bibnamedelima S.},
           giveni={A\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=f64056fe9107fedd28b447d31bfc157b}{%
           family={Leblond},
           familyi={L\bibinitperiod},
           given={Rémi},
           giveni={R\bibinitperiod}}}%
        {{hash=b1bdbd9e1dcbed6591878a57d5058c54}{%
           family={Pohlen},
           familyi={P\bibinitperiod},
           given={Tobias},
           giveni={T\bibinitperiod}}}%
        {{hash=39809e64cffbcbfd42bd81da5153546a}{%
           family={Dalibard},
           familyi={D\bibinitperiod},
           given={Valentin},
           giveni={V\bibinitperiod}}}%
        {{hash=c23f8012677b40de82f339368c393522}{%
           family={Budden},
           familyi={B\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=22eaafdd6c3038933341b729e199e0ce}{%
           family={Sulsky},
           familyi={S\bibinitperiod},
           given={Yury},
           giveni={Y\bibinitperiod}}}%
        {{hash=88dca93fff01bbe55329c40c0891257d}{%
           family={Molloy},
           familyi={M\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod}}}%
        {{hash=f565c54b0935cdca35d3d90b831013cf}{%
           family={Paine},
           familyi={P\bibinitperiod},
           given={Tom\bibnamedelima L.},
           giveni={T\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=2adc0c92c308f233c731321d55efe58f}{%
           family={Gulcehre},
           familyi={G\bibinitperiod},
           given={Caglar},
           giveni={C\bibinitperiod}}}%
        {{hash=05bc8d503a2c310ef0976ace7f9d2734}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Ziyu},
           giveni={Z\bibinitperiod}}}%
        {{hash=a726408819b6f955652c3ffaaedef966}{%
           family={Pfaff},
           familyi={P\bibinitperiod},
           given={Tobias},
           giveni={T\bibinitperiod}}}%
        {{hash=954bef435a12336c9decd19360a640f5}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Yuhuai},
           giveni={Y\bibinitperiod}}}%
        {{hash=50349f5d4ef065e417356ee70e0f7069}{%
           family={Ring},
           familyi={R\bibinitperiod},
           given={Roman},
           giveni={R\bibinitperiod}}}%
        {{hash=14889c0769f78922934df82a671f0cf3}{%
           family={Yogatama},
           familyi={Y\bibinitperiod},
           given={Dani},
           giveni={D\bibinitperiod}}}%
        {{hash=616c76aced80eaed20688f6ab93db271}{%
           family={Wünsch},
           familyi={W\bibinitperiod},
           given={Dario},
           giveni={D\bibinitperiod}}}%
        {{hash=5f3f9f74e6c498a754b9a8a7b7a53311}{%
           family={McKinney},
           familyi={M\bibinitperiod},
           given={Katrina},
           giveni={K\bibinitperiod}}}%
        {{hash=e4c8c37e34209dd0f61d34723d9061fe}{%
           family={Smith},
           familyi={S\bibinitperiod},
           given={Oliver},
           giveni={O\bibinitperiod}}}%
        {{hash=a56e72b23778a835bdade7d0511e43a3}{%
           family={Schaul},
           familyi={S\bibinitperiod},
           given={Tom},
           giveni={T\bibinitperiod}}}%
        {{hash=3a6fdf4df9a25f1d2d506ad9e86e1f6c}{%
           family={Lillicrap},
           familyi={L\bibinitperiod},
           given={Timothy},
           giveni={T\bibinitperiod}}}%
        {{hash=481308b301166b521c74fde6566e97e6}{%
           family={Kavukcuoglu},
           familyi={K\bibinitperiod},
           given={Koray},
           giveni={K\bibinitperiod}}}%
        {{hash=b160026950ebb1e2286dfb40c15482f5}{%
           family={Hassabis},
           familyi={H\bibinitperiod},
           given={Demis},
           giveni={D\bibinitperiod}}}%
        {{hash=b7585cb31b9235c4a758d152b7f7e828}{%
           family={Apps},
           familyi={A\bibinitperiod},
           given={Chris},
           giveni={C\bibinitperiod}}}%
        {{hash=89dbd30410c2085cd059f32c57d4593e}{%
           family={Silver},
           familyi={S\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
      }
      \strng{namehash}{832d19b6ecbaa1d465342fe676ffb29a}
      \strng{fullhash}{e20baf229e73fd131dd91cbcd821c571}
      \strng{fullhashraw}{e20baf229e73fd131dd91cbcd821c571}
      \strng{bibnamehash}{832d19b6ecbaa1d465342fe676ffb29a}
      \strng{authorbibnamehash}{832d19b6ecbaa1d465342fe676ffb29a}
      \strng{authornamehash}{832d19b6ecbaa1d465342fe676ffb29a}
      \strng{authorfullhash}{e20baf229e73fd131dd91cbcd821c571}
      \strng{authorfullhashraw}{e20baf229e73fd131dd91cbcd821c571}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{annotation}{shorttitle: AlphaStar}
      \field{day}{14}
      \field{issn}{0028-0836, 1476-4687}
      \field{journaltitle}{Nature}
      \field{langid}{english}
      \field{month}{11}
      \field{number}{7782}
      \field{shortjournal}{Nature}
      \field{shorttitle}{{{AlphaStar}}}
      \field{title}{Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning}
      \field{urlday}{24}
      \field{urlmonth}{12}
      \field{urlyear}{2023}
      \field{volume}{575}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{350\bibrangedash 354}
      \range{pages}{5}
      \verb{doi}
      \verb 10.1038/s41586-019-1724-z
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/97RK6RVS/Vinyals et al. - 2019 - Grandmaster level in StarCraft II using multi-agen.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/s41586-019-1724-z
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/s41586-019-1724-z
      \endverb
    \endentry
    \entry{brown1951iterative}{article}{}{}
      \name{author}{1}{}{%
        {{hash=a84f0ecb7dc3c25662e69881f1b264ff}{%
           family={Brown},
           familyi={B\bibinitperiod},
           given={George\bibnamedelima W},
           giveni={G\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
      }
      \strng{namehash}{a84f0ecb7dc3c25662e69881f1b264ff}
      \strng{fullhash}{a84f0ecb7dc3c25662e69881f1b264ff}
      \strng{fullhashraw}{a84f0ecb7dc3c25662e69881f1b264ff}
      \strng{bibnamehash}{a84f0ecb7dc3c25662e69881f1b264ff}
      \strng{authorbibnamehash}{a84f0ecb7dc3c25662e69881f1b264ff}
      \strng{authornamehash}{a84f0ecb7dc3c25662e69881f1b264ff}
      \strng{authorfullhash}{a84f0ecb7dc3c25662e69881f1b264ff}
      \strng{authorfullhashraw}{a84f0ecb7dc3c25662e69881f1b264ff}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Act. Anal. Prod Allocation}
      \field{number}{1}
      \field{title}{Iterative Solution of Games by Fictitious Play}
      \field{volume}{13}
      \field{year}{1951}
      \field{dateera}{ce}
      \field{pages}{374}
      \range{pages}{1}
    \endentry
    \entry{berger2005}{article}{}{}
      \name{author}{1}{}{%
        {{hash=bc1dff07b3c80a25bc5ff1cc47bd2e6f}{%
           family={Berger},
           familyi={B\bibinitperiod},
           given={Ulrich},
           giveni={U\bibinitperiod}}}%
      }
      \strng{namehash}{bc1dff07b3c80a25bc5ff1cc47bd2e6f}
      \strng{fullhash}{bc1dff07b3c80a25bc5ff1cc47bd2e6f}
      \strng{fullhashraw}{bc1dff07b3c80a25bc5ff1cc47bd2e6f}
      \strng{bibnamehash}{bc1dff07b3c80a25bc5ff1cc47bd2e6f}
      \strng{authorbibnamehash}{bc1dff07b3c80a25bc5ff1cc47bd2e6f}
      \strng{authornamehash}{bc1dff07b3c80a25bc5ff1cc47bd2e6f}
      \strng{authorfullhash}{bc1dff07b3c80a25bc5ff1cc47bd2e6f}
      \strng{authorfullhashraw}{bc1dff07b3c80a25bc5ff1cc47bd2e6f}
      \field{extraname}{1}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{It is known that every discrete-time fictitious play process approaches equilibrium in nondegenerate 2 Â 2 games, and that every continuous-time fictitious play process approaches equilibrium in nondegenerate 2 Â 2 and 2 Â 3 games. It has also been conjectured that convergence to the set of equilibria holds generally for nondegenerate 2 Â n games. We give a simple geometric proof of this for the continuous-time process, and also extend the result to discrete-time fictitious play.}
      \field{issn}{00220531}
      \field{journaltitle}{Journal of Economic Theory}
      \field{langid}{english}
      \field{month}{2}
      \field{number}{2}
      \field{shortjournal}{Journal of Economic Theory}
      \field{title}{Fictitious Play in 2×n Games}
      \field{urlday}{13}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{volume}{120}
      \field{year}{2005}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{139\bibrangedash 154}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1016/j.jet.2004.02.003
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/2QRP9GVP/Berger - 2005 - Fictitious play in 2×n games.pdf
      \endverb
      \verb{urlraw}
      \verb https://linkinghub.elsevier.com/retrieve/pii/S0022053104000626
      \endverb
      \verb{url}
      \verb https://linkinghub.elsevier.com/retrieve/pii/S0022053104000626
      \endverb
    \endentry
    \entry{berger2007}{article}{}{}
      \name{author}{1}{}{%
        {{hash=bc1dff07b3c80a25bc5ff1cc47bd2e6f}{%
           family={Berger},
           familyi={B\bibinitperiod},
           given={Ulrich},
           giveni={U\bibinitperiod}}}%
      }
      \strng{namehash}{bc1dff07b3c80a25bc5ff1cc47bd2e6f}
      \strng{fullhash}{bc1dff07b3c80a25bc5ff1cc47bd2e6f}
      \strng{fullhashraw}{bc1dff07b3c80a25bc5ff1cc47bd2e6f}
      \strng{bibnamehash}{bc1dff07b3c80a25bc5ff1cc47bd2e6f}
      \strng{authorbibnamehash}{bc1dff07b3c80a25bc5ff1cc47bd2e6f}
      \strng{authornamehash}{bc1dff07b3c80a25bc5ff1cc47bd2e6f}
      \strng{authorfullhash}{bc1dff07b3c80a25bc5ff1cc47bd2e6f}
      \strng{authorfullhashraw}{bc1dff07b3c80a25bc5ff1cc47bd2e6f}
      \field{extraname}{2}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{What modern game theorists describe as fictitious play is not the learning process George W. Brown defined in his 1951 paper. Brown's original version differs in a subtle detail, namely the order of belief updating. In this note we revive Brown's original fictitious play process and demonstrate that this seemingly innocent detail allows for an extremely simple and intuitive proof of convergence in an interesting and large class of games: nondegenerate ordinal potential games.}
      \field{issn}{0022-0531}
      \field{journaltitle}{Journal of Economic Theory}
      \field{number}{1}
      \field{title}{Brown's {{Original Fictitious Play}}}
      \field{volume}{135}
      \field{year}{2007}
      \field{dateera}{ce}
      \field{pages}{572\bibrangedash 578}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1016/j.jet.2005.12.010
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/XLUDA35I/Berger_2007_Brown's Original Fictitious Play.pdf
      \endverb
    \endentry
    \entry{shoham2007}{article}{}{}
      \name{author}{3}{}{%
        {{hash=774e2e16526f33f383bed004b13bf326}{%
           family={Shoham},
           familyi={S\bibinitperiod},
           given={Yoav},
           giveni={Y\bibinitperiod}}}%
        {{hash=999a732f5e7d7aca84c5ff18e80f648d}{%
           family={Powers},
           familyi={P\bibinitperiod},
           given={Rob},
           giveni={R\bibinitperiod}}}%
        {{hash=978fa1abbba10679dc3d70d791eb1b43}{%
           family={Grenager},
           familyi={G\bibinitperiod},
           given={Trond},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{5b9f6f0e7739f6b355afc40ad1447f10}
      \strng{fullhash}{dd593b12b6198960022535ccb3413a01}
      \strng{fullhashraw}{dd593b12b6198960022535ccb3413a01}
      \strng{bibnamehash}{dd593b12b6198960022535ccb3413a01}
      \strng{authorbibnamehash}{dd593b12b6198960022535ccb3413a01}
      \strng{authornamehash}{5b9f6f0e7739f6b355afc40ad1447f10}
      \strng{authorfullhash}{dd593b12b6198960022535ccb3413a01}
      \strng{authorfullhashraw}{dd593b12b6198960022535ccb3413a01}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The area of learning in multi-agent systems is today one of the most fertile grounds for interaction between game theory and artificial intelligence. We focus on the foundational questions in this interdisciplinary area, and identify several distinct agendas that ought to, we argue, be separated. The goal of this article is to start a discussion in the research community that will result in firmer foundations for the area.1 © 2007 Published by Elsevier B.V.}
      \field{issn}{00043702}
      \field{journaltitle}{Artificial Intelligence}
      \field{langid}{english}
      \field{month}{5}
      \field{number}{7}
      \field{shortjournal}{Artificial Intelligence}
      \field{title}{If Multi-Agent Learning Is the Answer, What Is the Question?}
      \field{urlday}{12}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{volume}{171}
      \field{year}{2007}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{365\bibrangedash 377}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1016/j.artint.2006.02.006
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/722AZMMT/Shoham et al. - 2007 - If multi-agent learning is the answer, what is the.pdf
      \endverb
      \verb{urlraw}
      \verb https://linkinghub.elsevier.com/retrieve/pii/S0004370207000495
      \endverb
      \verb{url}
      \verb https://linkinghub.elsevier.com/retrieve/pii/S0004370207000495
      \endverb
    \endentry
    \entry{busoniu2008}{article}{}{}
      \name{author}{3}{}{%
        {{hash=7fb304698f0da43de984f81db7bf7a9a}{%
           family={Busoniu},
           familyi={B\bibinitperiod},
           given={Lucian},
           giveni={L\bibinitperiod}}}%
        {{hash=0156cb9902c793bb153564c4b86cf076}{%
           family={Babuska},
           familyi={B\bibinitperiod},
           given={Robert},
           giveni={R\bibinitperiod}}}%
        {{hash=3aa279196551f6d4f6429776156250b6}{%
           family={De\bibnamedelima Schutter},
           familyi={D\bibinitperiod\bibinitdelim S\bibinitperiod},
           given={Bart},
           giveni={B\bibinitperiod}}}%
      }
      \strng{namehash}{7480bfd69f596bbd28ce921862d8a55c}
      \strng{fullhash}{a78496e0aaf836592260a1445c99bad8}
      \strng{fullhashraw}{a78496e0aaf836592260a1445c99bad8}
      \strng{bibnamehash}{a78496e0aaf836592260a1445c99bad8}
      \strng{authorbibnamehash}{a78496e0aaf836592260a1445c99bad8}
      \strng{authornamehash}{7480bfd69f596bbd28ce921862d8a55c}
      \strng{authorfullhash}{a78496e0aaf836592260a1445c99bad8}
      \strng{authorfullhashraw}{a78496e0aaf836592260a1445c99bad8}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Multiagent systems are rapidly finding applications in a variety of domains, including robotics, distributed control, telecommunications, and economics. The complexity of many tasks arising in these domains makes them difficult to solve with preprogrammed agent behaviors. The agents must, instead, discover a solution on their own, using learning. A significant part of the research on multiagent learning concerns reinforcement learning techniques. This paper provides a comprehensive survey of multiagent reinforcement learning (MARL). A central issue in the field is the formal statement of the multiagent learning goal. Different viewpoints on this issue have led to the proposal of many different goals, among which two focal points can be distinguished: stability of the agents’ learning dynamics, and adaptation to the changing behavior of the other agents. The MARL algorithms described in the literature aim—either explicitly or implicitly—at one of these two goals or at a combination of both, in a fully cooperative, fully competitive, or more general setting. A representative selection of these algorithms is discussed in detail in this paper, together with the specific issues that arise in each category. Additionally, the benefits and challenges of MARL are described along with some of the problem domains where the MARL techniques have been applied. Finally, an outlook for the field is provided.}
      \field{issn}{1094-6977, 1558-2442}
      \field{journaltitle}{IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)}
      \field{langid}{english}
      \field{month}{3}
      \field{number}{2}
      \field{shortjournal}{IEEE Trans. Syst., Man, Cybern. C}
      \field{title}{A {{Comprehensive Survey}} of {{Multiagent Reinforcement Learning}}}
      \field{urlday}{12}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{volume}{38}
      \field{year}{2008}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{156\bibrangedash 172}
      \range{pages}{17}
      \verb{doi}
      \verb 10.1109/TSMCC.2007.913919
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/VC8GVJJT/Busoniu et al. - 2008 - A Comprehensive Survey of Multiagent Reinforcement.pdf
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/4445757/
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/4445757/
      \endverb
    \endentry
    \entry{lowe2020}{online}{}{}
      \name{author}{6}{}{%
        {{hash=7be01fa6277ef22b804ff6cc54c87b72}{%
           family={Lowe},
           familyi={L\bibinitperiod},
           given={Ryan},
           giveni={R\bibinitperiod}}}%
        {{hash=e2101a0f6a72a2fb022cd3e2d45461e1}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Yi},
           giveni={Y\bibinitperiod}}}%
        {{hash=41ea42ab7e09607e91466632ce419d8c}{%
           family={Tamar},
           familyi={T\bibinitperiod},
           given={Aviv},
           giveni={A\bibinitperiod}}}%
        {{hash=5de1025172ba2cd90c9f04ae0ba17006}{%
           family={Harb},
           familyi={H\bibinitperiod},
           given={Jean},
           giveni={J\bibinitperiod}}}%
        {{hash=e28d4ee199593959d8c29980a64f1974}{%
           family={Abbeel},
           familyi={A\bibinitperiod},
           given={Pieter},
           giveni={P\bibinitperiod}}}%
        {{hash=7570e7c3fc2c13d59af4d7cdb9962a4d}{%
           family={Mordatch},
           familyi={M\bibinitperiod},
           given={Igor},
           giveni={I\bibinitperiod}}}%
      }
      \strng{namehash}{dde2b308cb02077d5b0dcf690cd4ea7e}
      \strng{fullhash}{d649a4bfdfab1502cd7ae45ec8e10355}
      \strng{fullhashraw}{d649a4bfdfab1502cd7ae45ec8e10355}
      \strng{bibnamehash}{d649a4bfdfab1502cd7ae45ec8e10355}
      \strng{authorbibnamehash}{d649a4bfdfab1502cd7ae45ec8e10355}
      \strng{authornamehash}{dde2b308cb02077d5b0dcf690cd4ea7e}
      \strng{authorfullhash}{d649a4bfdfab1502cd7ae45ec8e10355}
      \strng{authorfullhashraw}{d649a4bfdfab1502cd7ae45ec8e10355}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.}
      \field{day}{14}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{3}
      \field{pubstate}{prepublished}
      \field{title}{Multi-{{Agent Actor-Critic}} for {{Mixed Cooperative-Competitive Environments}}}
      \field{urlday}{12}
      \field{urlmonth}{2}
      \field{urlyear}{2023}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1706.02275
      \endverb
      \verb{eprint}
      \verb 1706.02275
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/Q54TTJQQ/Lowe et al. - 2020 - Multi-Agent Actor-Critic for Mixed Cooperative-Com.pdf;/Users/brandonhosley/Zotero/storage/F3DU3EG3/1706.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1706.02275
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1706.02275
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
    \endentry
    \entry{sun2023}{online}{}{}
      \name{author}{7}{}{%
        {{hash=fe6879db1e1d69fefb0d33305d914373}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Chenglu},
           giveni={C\bibinitperiod}}}%
        {{hash=a382efbfffe467ef00312cf8a967abf3}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Yichi},
           giveni={Y\bibinitperiod}}}%
        {{hash=9a4f4a1ff661cd600eb26523a5ba8bb4}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Yu},
           giveni={Y\bibinitperiod}}}%
        {{hash=9651c0bcd02663a2b29973014eddd315}{%
           family={Lu},
           familyi={L\bibinitperiod},
           given={Ziling},
           giveni={Z\bibinitperiod}}}%
        {{hash=5d54bfa1b4a1165c0374ea2728f3a9f0}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Jingbin},
           giveni={J\bibinitperiod}}}%
        {{hash=07d9ed993304a19060b9450f1afaded4}{%
           family={Xu},
           familyi={X\bibinitperiod},
           given={Sijia},
           giveni={S\bibinitperiod}}}%
        {{hash=4fece3f163324172bba987dcd205bd8e}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Weidong},
           giveni={W\bibinitperiod}}}%
      }
      \strng{namehash}{b8f5f205073b77cc0e36b2fca069bcdd}
      \strng{fullhash}{e11873591560e39ac55d68062faead56}
      \strng{fullhashraw}{e11873591560e39ac55d68062faead56}
      \strng{bibnamehash}{b8f5f205073b77cc0e36b2fca069bcdd}
      \strng{authorbibnamehash}{b8f5f205073b77cc0e36b2fca069bcdd}
      \strng{authornamehash}{b8f5f205073b77cc0e36b2fca069bcdd}
      \strng{authorfullhash}{e11873591560e39ac55d68062faead56}
      \strng{authorfullhashraw}{e11873591560e39ac55d68062faead56}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Asymmetrical multiplayer (AMP) game is a popular game genre which involves multiple types of agents competing or collaborating with each other in the game. It is difficult to train powerful agents that can defeat top human players in AMP games by typical self-play training method because of unbalancing characteristics in their asymmetrical environments. We propose asymmetric-evolution training (AET), a novel multi-agent reinforcement learning framework that can train multiple kinds of agents simultaneously in AMP game. We designed adaptive data adjustment (ADA) and environment randomization (ER) to optimize the AET process. We tested our method in a complex AMP game named Tom \textbackslash\& Jerry, and our AIs trained without using any human data can achieve a win rate of 98.5\% against top human players over 65 matches. The ablation experiments indicated that the proposed modules are beneficial to the framework.}
      \field{day}{20}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{4}
      \field{pubstate}{prepublished}
      \field{title}{Mastering {{Asymmetrical Multiplayer Game}} with {{Multi-Agent Asymmetric-Evolution Reinforcement Learning}}}
      \field{urlday}{13}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2304.10124
      \endverb
      \verb{eprint}
      \verb 2304.10124
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/SCS6KMAT/Sun et al_2023_Mastering Asymmetrical Multiplayer Game with Multi-Agent Asymmetric-Evolution.pdf;/Users/brandonhosley/Zotero/storage/IQIGRGZG/2304.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2304.10124
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2304.10124
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems}
    \endentry
    \entry{berner2019}{online}{}{}
      \name{author}{25}{}{%
        {{hash=ca86811e7a0582a9e7cb8d33e7ab445d}{%
           family={Berner},
           familyi={B\bibinitperiod},
           given={Christopher},
           giveni={C\bibinitperiod}}}%
        {{hash=7ae2a0efeaf9c031f9b420bcb1c19e54}{%
           family={Brockman},
           familyi={B\bibinitperiod},
           given={Greg},
           giveni={G\bibinitperiod}}}%
        {{hash=bdcc84061540e15aac439cba59db6577}{%
           family={Chan},
           familyi={C\bibinitperiod},
           given={Brooke},
           giveni={B\bibinitperiod}}}%
        {{hash=ece7baf9b320c51ead8e24c1c6386dbf}{%
           family={Cheung},
           familyi={C\bibinitperiod},
           given={Vicki},
           giveni={V\bibinitperiod}}}%
        {{hash=5b9ef873a5cb802b4a76be3508f175b4}{%
           family={Dębiak},
           familyi={D\bibinitperiod},
           given={Przemysław},
           giveni={P\bibinitperiod}}}%
        {{hash=a509b9c62a0e81a58991a81caf48298d}{%
           family={Dennison},
           familyi={D\bibinitperiod},
           given={Christy},
           giveni={C\bibinitperiod}}}%
        {{hash=a562eb10ac2870de64b3956df2eb1896}{%
           family={Farhi},
           familyi={F\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=9077a5599d2a4f3aa8bb20cef3b6399a}{%
           family={Fischer},
           familyi={F\bibinitperiod},
           given={Quirin},
           giveni={Q\bibinitperiod}}}%
        {{hash=49e2023e4e856577f61450fc5149743d}{%
           family={Hashme},
           familyi={H\bibinitperiod},
           given={Shariq},
           giveni={S\bibinitperiod}}}%
        {{hash=07acc23f6ec051b64b82cd33255c0a69}{%
           family={Hesse},
           familyi={H\bibinitperiod},
           given={Chris},
           giveni={C\bibinitperiod}}}%
        {{hash=4eda39963a95c233a262d4191f198aa4}{%
           family={Józefowicz},
           familyi={J\bibinitperiod},
           given={Rafal},
           giveni={R\bibinitperiod}}}%
        {{hash=7006ca8c1ce969019b89de50fece60dd}{%
           family={Gray},
           familyi={G\bibinitperiod},
           given={Scott},
           giveni={S\bibinitperiod}}}%
        {{hash=bd24844b3abaa88e1f3c5c074ad37ab6}{%
           family={Olsson},
           familyi={O\bibinitperiod},
           given={Catherine},
           giveni={C\bibinitperiod}}}%
        {{hash=27d3d977b77156237cdeb5a7b7eaa560}{%
           family={Pachocki},
           familyi={P\bibinitperiod},
           given={Jakub},
           giveni={J\bibinitperiod}}}%
        {{hash=20bb006267a99bf6b2419ac00ec8decb}{%
           family={Petrov},
           familyi={P\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=f78abae164f9e9364dd3e0b31887fc08}{%
           family={Pinto},
           familyi={P\bibinitperiod},
           given={Henrique\bibnamedelimb P.\bibnamedelimi d\bibnamedelima O.},
           giveni={H\bibinitperiod\bibinitdelim P\bibinitperiod\bibinitdelim d\bibinitperiod\bibinitdelim O\bibinitperiod}}}%
        {{hash=9db43b2d2fa0f38a4051782eb9de8f87}{%
           family={Raiman},
           familyi={R\bibinitperiod},
           given={Jonathan},
           giveni={J\bibinitperiod}}}%
        {{hash=e6f76e1a4d058df028530916774ad3a7}{%
           family={Salimans},
           familyi={S\bibinitperiod},
           given={Tim},
           giveni={T\bibinitperiod}}}%
        {{hash=e3e581a0808055d754222c3b47b3a7ab}{%
           family={Schlatter},
           familyi={S\bibinitperiod},
           given={Jeremy},
           giveni={J\bibinitperiod}}}%
        {{hash=5167ef9d0b77bf68557730648baac9b7}{%
           family={Schneider},
           familyi={S\bibinitperiod},
           given={Jonas},
           giveni={J\bibinitperiod}}}%
        {{hash=6657859de18e844e4b1b815e03694c71}{%
           family={Sidor},
           familyi={S\bibinitperiod},
           given={Szymon},
           giveni={S\bibinitperiod}}}%
        {{hash=8d569d1d5b8b5a7836017a98b430f959}{%
           family={Sutskever},
           familyi={S\bibinitperiod},
           given={Ilya},
           giveni={I\bibinitperiod}}}%
        {{hash=d0b3008e85b1a8b38f46556abf1791c7}{%
           family={Tang},
           familyi={T\bibinitperiod},
           given={Jie},
           giveni={J\bibinitperiod}}}%
        {{hash=674ede0b9cd02a2bf5fc662972efb9f0}{%
           family={Wolski},
           familyi={W\bibinitperiod},
           given={Filip},
           giveni={F\bibinitperiod}}}%
        {{hash=24b5bdd4e149e4c9ed111f6051192cf8}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Susan},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{9d764cedbc33ef38116d07274416b052}
      \strng{fullhash}{841e942f61905cc71f1100c74388c62b}
      \strng{fullhashraw}{841e942f61905cc71f1100c74388c62b}
      \strng{bibnamehash}{9d764cedbc33ef38116d07274416b052}
      \strng{authorbibnamehash}{9d764cedbc33ef38116d07274416b052}
      \strng{authornamehash}{9d764cedbc33ef38116d07274416b052}
      \strng{authorfullhash}{841e942f61905cc71f1100c74388c62b}
      \strng{authorfullhashraw}{841e942f61905cc71f1100c74388c62b}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.}
      \field{day}{13}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arXiv}
      \field{month}{12}
      \field{pubstate}{prepublished}
      \field{title}{Dota 2 with {{Large Scale Deep Reinforcement Learning}}}
      \field{urlday}{15}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1912.06680
      \endverb
      \verb{eprint}
      \verb 1912.06680
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/GIAX6G7H/OpenAI et al. - 2019 - Dota 2 with Large Scale Deep Reinforcement Learnin.pdf;/Users/brandonhosley/Zotero/storage/HM7FXNK8/1912.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1912.06680
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1912.06680
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{balduzzi2019}{online}{}{}
      \name{author}{7}{}{%
        {{hash=fbb019d6a198795f750f89fcfc28125e}{%
           family={Balduzzi},
           familyi={B\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=0cdb8985c82d4beb394733ad29ad7813}{%
           family={Garnelo},
           familyi={G\bibinitperiod},
           given={Marta},
           giveni={M\bibinitperiod}}}%
        {{hash=2887e64ec6b9a86d7239578c822e78c3}{%
           family={Bachrach},
           familyi={B\bibinitperiod},
           given={Yoram},
           giveni={Y\bibinitperiod}}}%
        {{hash=5dc7de68ac7f0299a995822f38a3e705}{%
           family={Czarnecki},
           familyi={C\bibinitperiod},
           given={Wojciech\bibnamedelima M.},
           giveni={W\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=aaa00aabdab6adfa06bb972717d382b0}{%
           family={Perolat},
           familyi={P\bibinitperiod},
           given={Julien},
           giveni={J\bibinitperiod}}}%
        {{hash=7dc1446dea7ff50b2b02fb83780cc9c6}{%
           family={Jaderberg},
           familyi={J\bibinitperiod},
           given={Max},
           giveni={M\bibinitperiod}}}%
        {{hash=368b9b2de627b852658c433b062d4e1e}{%
           family={Graepel},
           familyi={G\bibinitperiod},
           given={Thore},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{f3bc6329e57abe5eb0f61cc59cef9a02}
      \strng{fullhash}{03ec269ef76a797ad60130d17c6586b8}
      \strng{fullhashraw}{03ec269ef76a797ad60130d17c6586b8}
      \strng{bibnamehash}{f3bc6329e57abe5eb0f61cc59cef9a02}
      \strng{authorbibnamehash}{f3bc6329e57abe5eb0f61cc59cef9a02}
      \strng{authornamehash}{f3bc6329e57abe5eb0f61cc59cef9a02}
      \strng{authorfullhash}{03ec269ef76a797ad60130d17c6586b8}
      \strng{authorfullhashraw}{03ec269ef76a797ad60130d17c6586b8}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Zero-sum games such as chess and poker are, abstractly, functions that evaluate pairs of agents, for example labeling them `winner' and `loser'. If the game is approximately transitive, then self-play generates sequences of agents of increasing strength. However, nontransitive games, such as rock-paper-scissors, can exhibit strategic cycles, and there is no longer a clear objective -- we want agents to increase in strength, but against whom is unclear. In this paper, we introduce a geometric framework for formulating agent objectives in zero-sum games, in order to construct adaptive sequences of objectives that yield open-ended learning. The framework allows us to reason about population performance in nontransitive games, and enables the development of a new algorithm (rectified Nash response, PSRO\_rN) that uses game-theoretic niching to construct diverse populations of effective agents, producing a stronger set of agents than existing algorithms. We apply PSRO\_rN to two highly nontransitive resource allocation games and find that PSRO\_rN consistently outperforms the existing alternatives.}
      \field{day}{13}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arXiv}
      \field{month}{5}
      \field{pubstate}{prepublished}
      \field{title}{Open-Ended {{Learning}} in {{Symmetric Zero-sum Games}}}
      \field{urlday}{15}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1901.08106
      \endverb
      \verb{eprint}
      \verb 1901.08106
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/JIHZMJL2/Balduzzi et al. - 2019 - Open-ended Learning in Symmetric Zero-sum Games.pdf;/Users/brandonhosley/Zotero/storage/NWLIW2GL/1901.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1901.08106
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1901.08106
      \endverb
      \keyw{Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning}
    \endentry
    \entry{smit2023}{article}{}{}
      \name{author}{4}{}{%
        {{hash=4b49a4ae46f23bdc584e1f6576d46f1d}{%
           family={Smit},
           familyi={S\bibinitperiod},
           given={Andries},
           giveni={A\bibinitperiod}}}%
        {{hash=1ad14539972beb801f05fa896097879e}{%
           family={Engelbrecht},
           familyi={E\bibinitperiod},
           given={Herman\bibnamedelima A.},
           giveni={H\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=03ceb98a1190ebc6471f95bf82e2c060}{%
           family={Brink},
           familyi={B\bibinitperiod},
           given={Willie},
           giveni={W\bibinitperiod}}}%
        {{hash=eb6e9b62595b1500b93f495d875bb6c2}{%
           family={Pretorius},
           familyi={P\bibinitperiod},
           given={Arnu},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{e57bd41a1b18a599096ba2b49b452d55}
      \strng{fullhash}{34a9e177b906a01ef2c329e82775bf13}
      \strng{fullhashraw}{34a9e177b906a01ef2c329e82775bf13}
      \strng{bibnamehash}{34a9e177b906a01ef2c329e82775bf13}
      \strng{authorbibnamehash}{34a9e177b906a01ef2c329e82775bf13}
      \strng{authornamehash}{e57bd41a1b18a599096ba2b49b452d55}
      \strng{authorfullhash}{34a9e177b906a01ef2c329e82775bf13}
      \strng{authorfullhashraw}{34a9e177b906a01ef2c329e82775bf13}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Robotic football has long been seen as a grand challenge in artificial intelligence. Despite recent success of learned policies over heuristics and handcrafted rules in general, current teams in the simulated RoboCup football leagues, where autonomous agents compete against each other, still rely on handcrafted strategies with only a few using reinforcement learning directly. This limits a learning agent’s ability to find stronger high-level strategies for the full game. In this paper, we show that it is possible for agents to learn competent football strategies on a full 22 player setting using limited computation resources (one GPU and one CPU), from tabula rasa through self-play. To do this, we build a 2D football simulator with faster simulation times than the RoboCup simulator. We propose various improvements to the standard single-agent PPO training algorithm which help it scale to our multi-agent setting. These improvements include (1) using a policy and critic network with an attention mechanism that scales linearly in the number of agents, (2) sharing networks between agents which allow for faster throughput using batching, and (3) using Polyak averaged opponents, league opponents and freezing the opponent team when necessary. We show through experimental results that stable training in the full 22 player setting is possible. Agents trained in the 22 player setting learn to defeat a variety of handcrafted strategies, and also achieve a higher win rate compared to agents trained in the 4 player setting and evaluated in the full game.}
      \field{annotation}{https://github.com/DriesSmit/MARL2DSoccer}
      \field{day}{24}
      \field{issn}{1573-7454}
      \field{journaltitle}{Autonomous Agents and Multi-Agent Systems}
      \field{langid}{english}
      \field{month}{3}
      \field{number}{1}
      \field{shortjournal}{Auton Agent Multi-Agent Syst}
      \field{title}{Scaling Multi-Agent Reinforcement Learning to Full 11 versus 11 Simulated Robotic Football}
      \field{urlday}{16}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{37}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{20}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1007/s10458-023-09603-y
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/5UPXMZ63/Smit et al. - 2023 - Scaling multi-agent reinforcement learning to full.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s10458-023-09603-y
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s10458-023-09603-y
      \endverb
      \keyw{11 versus 11 football,Multi-agent reinforcement learning,RoboCup,RoboCup 2D,Simulated robotic football,Soccer}
    \endentry
    \entry{puterman2005}{book}{}{}
      \name{author}{1}{}{%
        {{hash=b4258b9291d903d97b69d703eebf790a}{%
           family={Puterman},
           familyi={P\bibinitperiod},
           given={Martin\bibnamedelima L.},
           giveni={M\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Hoboken, NJ}%
      }
      \list{publisher}{1}{%
        {Wiley-Interscience}%
      }
      \strng{namehash}{b4258b9291d903d97b69d703eebf790a}
      \strng{fullhash}{b4258b9291d903d97b69d703eebf790a}
      \strng{fullhashraw}{b4258b9291d903d97b69d703eebf790a}
      \strng{bibnamehash}{b4258b9291d903d97b69d703eebf790a}
      \strng{authorbibnamehash}{b4258b9291d903d97b69d703eebf790a}
      \strng{authornamehash}{b4258b9291d903d97b69d703eebf790a}
      \strng{authorfullhash}{b4258b9291d903d97b69d703eebf790a}
      \strng{authorfullhashraw}{b4258b9291d903d97b69d703eebf790a}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{isbn}{978-0-471-72782-8}
      \field{langid}{english}
      \field{pagetotal}{649}
      \field{series}{Wiley Series in Probability and Statistics}
      \field{shorttitle}{Markov Decision Processes}
      \field{title}{Markov Decision Processes: Discrete Stochastic Dynamic Programming}
      \field{year}{2005}
      \field{dateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/ZEEACZCB/Puterman - 2005 - Markov decision processes discrete stochastic dyn.pdf
      \endverb
    \endentry
    \entry{watkins1992}{article}{}{}
      \name{author}{2}{}{%
        {{hash=6522b6087fe013d303faca6da91596ca}{%
           family={Watkins},
           familyi={W\bibinitperiod},
           given={Christopher\bibnamedelimb J.\bibnamedelimi C.\bibnamedelimi H.},
           giveni={C\bibinitperiod\bibinitdelim J\bibinitperiod\bibinitdelim C\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=ef0afbe0b4e15059c963e026fe213c34}{%
           family={Dayan},
           familyi={D\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{1f776b9a46cb729efd9599e3af8beeef}
      \strng{fullhash}{1f776b9a46cb729efd9599e3af8beeef}
      \strng{fullhashraw}{1f776b9a46cb729efd9599e3af8beeef}
      \strng{bibnamehash}{1f776b9a46cb729efd9599e3af8beeef}
      \strng{authorbibnamehash}{1f776b9a46cb729efd9599e3af8beeef}
      \strng{authornamehash}{1f776b9a46cb729efd9599e3af8beeef}
      \strng{authorfullhash}{1f776b9a46cb729efd9599e3af8beeef}
      \strng{authorfullhashraw}{1f776b9a46cb729efd9599e3af8beeef}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.}
      \field{day}{1}
      \field{issn}{1573-0565}
      \field{journaltitle}{Machine Learning}
      \field{langid}{english}
      \field{month}{5}
      \field{number}{3}
      \field{shortjournal}{Mach Learn}
      \field{title}{Q-Learning}
      \field{urlday}{24}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{volume}{8}
      \field{year}{1992}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{279\bibrangedash 292}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1007/BF00992698
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/AY54AD37/Watkins_Dayan_1992_Q-learning.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/BF00992698
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/BF00992698
      \endverb
      \keyw{asynchronous dynamic programming,Q-learning,reinforcement learning,temporal differences}
    \endentry
    \entry{pan2021}{inproceedings}{}{}
      \name{author}{5}{}{%
        {{hash=d7f499568ecb5ff763f12742dad2c404}{%
           family={Pan},
           familyi={P\bibinitperiod},
           given={Ling},
           giveni={L\bibinitperiod}}}%
        {{hash=8eb7df23daba9c7bf22402026effdae7}{%
           family={Rashid},
           familyi={R\bibinitperiod},
           given={Tabish},
           giveni={T\bibinitperiod}}}%
        {{hash=09355ad427f98680e1f04f5266d4d479}{%
           family={Peng},
           familyi={P\bibinitperiod},
           given={Bei},
           giveni={B\bibinitperiod}}}%
        {{hash=f188c97154d8c27e98dfec25407d43e2}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Longbo},
           giveni={L\bibinitperiod}}}%
        {{hash=0d05819cf2b4fe22ba972c9b2b5d8c9d}{%
           family={Whiteson},
           familyi={W\bibinitperiod},
           given={Shimon},
           giveni={S\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{881d864854342045edddbb4379bdfcbe}
      \strng{fullhash}{dabc96c90479f00be5e2084d872b0bbb}
      \strng{fullhashraw}{dabc96c90479f00be5e2084d872b0bbb}
      \strng{bibnamehash}{dabc96c90479f00be5e2084d872b0bbb}
      \strng{authorbibnamehash}{dabc96c90479f00be5e2084d872b0bbb}
      \strng{authornamehash}{881d864854342045edddbb4379bdfcbe}
      \strng{authorfullhash}{dabc96c90479f00be5e2084d872b0bbb}
      \strng{authorfullhashraw}{dabc96c90479f00be5e2084d872b0bbb}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{title}{Regularized {{Softmax Deep Multi-Agent Q-Learning}}}
      \field{urlday}{25}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{volume}{34}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1365\bibrangedash 1377}
      \range{pages}{13}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/UB6KUNBL/Pan et al_2021_Regularized Softmax Deep Multi-Agent Q-Learning.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2021/hash/0a113ef6b61820daa5611c870ed8d5ee-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2021/hash/0a113ef6b61820daa5611c870ed8d5ee-Abstract.html
      \endverb
    \endentry
    \entry{cesa-bianchi2017}{inproceedings}{}{}
      \name{author}{4}{}{%
        {{hash=4de530030244169e16641806ebdfde2b}{%
           family={Cesa-Bianchi},
           familyi={C\bibinithyphendelim B\bibinitperiod},
           given={Nicolò},
           giveni={N\bibinitperiod}}}%
        {{hash=004b95422ee5423a381510bcc5d1e41f}{%
           family={Gentile},
           familyi={G\bibinitperiod},
           given={Claudio},
           giveni={C\bibinitperiod}}}%
        {{hash=95d8be9c25ae0c29944d9771420a4664}{%
           family={Lugosi},
           familyi={L\bibinitperiod},
           given={Gabor},
           giveni={G\bibinitperiod}}}%
        {{hash=089be5c431fa95b113391f63810eff06}{%
           family={Neu},
           familyi={N\bibinitperiod},
           given={Gergely},
           giveni={G\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{0bb5a0d626ca0569ced6863f5842c159}
      \strng{fullhash}{17c8d65df1d4f283ff5c2857536b19c5}
      \strng{fullhashraw}{17c8d65df1d4f283ff5c2857536b19c5}
      \strng{bibnamehash}{17c8d65df1d4f283ff5c2857536b19c5}
      \strng{authorbibnamehash}{17c8d65df1d4f283ff5c2857536b19c5}
      \strng{authornamehash}{0bb5a0d626ca0569ced6863f5842c159}
      \strng{authorfullhash}{17c8d65df1d4f283ff5c2857536b19c5}
      \strng{authorfullhashraw}{17c8d65df1d4f283ff5c2857536b19c5}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{title}{Boltzmann {{Exploration Done Right}}}
      \field{urlday}{25}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{volume}{30}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/C356RIX8/Cesa-Bianchi et al_2017_Boltzmann Exploration Done Right.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2017/hash/b299ad862b6f12cb57679f0538eca514-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2017/hash/b299ad862b6f12cb57679f0538eca514-Abstract.html
      \endverb
    \endentry
    \entry{kaelbling1996}{article}{}{}
      \name{author}{3}{}{%
        {{hash=0bf25a24f49af5c83d08966e17941417}{%
           family={Kaelbling},
           familyi={K\bibinitperiod},
           given={L.\bibnamedelimi P.},
           giveni={L\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=352da7ce0de499b5d902333a3af53e06}{%
           family={Littman},
           familyi={L\bibinitperiod},
           given={M.\bibnamedelimi L.},
           giveni={M\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=3a0e422c2af3516bd3a2f5a6f04fbb10}{%
           family={Moore},
           familyi={M\bibinitperiod},
           given={A.\bibnamedelimi W.},
           giveni={A\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
      }
      \strng{namehash}{ed593953d07ec841820bdd2385919e72}
      \strng{fullhash}{8bb1da5e8fe51882f3a1a7517caf35f4}
      \strng{fullhashraw}{8bb1da5e8fe51882f3a1a7517caf35f4}
      \strng{bibnamehash}{8bb1da5e8fe51882f3a1a7517caf35f4}
      \strng{authorbibnamehash}{8bb1da5e8fe51882f3a1a7517caf35f4}
      \strng{authornamehash}{ed593953d07ec841820bdd2385919e72}
      \strng{authorfullhash}{8bb1da5e8fe51882f3a1a7517caf35f4}
      \strng{authorfullhashraw}{8bb1da5e8fe51882f3a1a7517caf35f4}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.}
      \field{day}{1}
      \field{issn}{1076-9757}
      \field{journaltitle}{Journal of Artificial Intelligence Research}
      \field{month}{5}
      \field{shortjournal}{jair}
      \field{shorttitle}{Reinforcement {{Learning}}}
      \field{title}{Reinforcement {{Learning}}: {{A Survey}}}
      \field{urlday}{25}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{volume}{4}
      \field{year}{1996}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{237\bibrangedash 285}
      \range{pages}{49}
      \verb{doi}
      \verb 10.1613/jair.301
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/CAYL6YGI/Kaelbling et al_1996_Reinforcement Learning.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.jair.org/index.php/jair/article/view/10166
      \endverb
      \verb{url}
      \verb https://www.jair.org/index.php/jair/article/view/10166
      \endverb
    \endentry
    \entry{vermorel2005}{inproceedings}{}{}
      \name{author}{2}{}{%
        {{hash=522c96218415155b64682d3f35d525ad}{%
           family={Vermorel},
           familyi={V\bibinitperiod},
           given={Joannès},
           giveni={J\bibinitperiod}}}%
        {{hash=8298c87c77cd46a5b2b7e799ba087945}{%
           family={Mohri},
           familyi={M\bibinitperiod},
           given={Mehryar},
           giveni={M\bibinitperiod}}}%
      }
      \name{editor}{5}{}{%
        {{hash=844d99c5b8c67291826b6a62fea44989}{%
           family={Gama},
           familyi={G\bibinitperiod},
           given={João},
           giveni={J\bibinitperiod}}}%
        {{hash=9a1e7968165529040cd1943b6844249a}{%
           family={Camacho},
           familyi={C\bibinitperiod},
           given={Rui},
           giveni={R\bibinitperiod}}}%
        {{hash=d74a16b5e68d673c2e82bf8e1a4b5966}{%
           family={Brazdil},
           familyi={B\bibinitperiod},
           given={Pavel\bibnamedelima B.},
           giveni={P\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=dad8472df6d5abccdd47a93963b0e58b}{%
           family={Jorge},
           familyi={J\bibinitperiod},
           given={Alípio\bibnamedelima Mário},
           giveni={A\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=80494bb62925fbf9e68e4635b0b7f4a6}{%
           family={Torgo},
           familyi={T\bibinitperiod},
           given={Luís},
           giveni={L\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Berlin, Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{172761fd0695e52c3408c31b7346fb44}
      \strng{fullhash}{172761fd0695e52c3408c31b7346fb44}
      \strng{fullhashraw}{172761fd0695e52c3408c31b7346fb44}
      \strng{bibnamehash}{172761fd0695e52c3408c31b7346fb44}
      \strng{authorbibnamehash}{172761fd0695e52c3408c31b7346fb44}
      \strng{authornamehash}{172761fd0695e52c3408c31b7346fb44}
      \strng{authorfullhash}{172761fd0695e52c3408c31b7346fb44}
      \strng{authorfullhashraw}{172761fd0695e52c3408c31b7346fb44}
      \strng{editorbibnamehash}{6ac322808ac7a2092a9ecc1c825dc796}
      \strng{editornamehash}{56431deb8901de6306944b9ccd741193}
      \strng{editorfullhash}{6ac322808ac7a2092a9ecc1c825dc796}
      \strng{editorfullhashraw}{6ac322808ac7a2092a9ecc1c825dc796}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The multi-armed bandit problem for a gambler is to decide which arm of a K-slot machine to pull to maximize his total reward in a series of trials. Many real-world learning and optimization problems can be modeled in this way. Several strategies or algorithms have been proposed as a solution to this problem in the last two decades, but, to our knowledge, there has been no common evaluation of these algorithms.}
      \field{booktitle}{Machine {{Learning}}: {{ECML}} 2005}
      \field{isbn}{978-3-540-31692-3}
      \field{langid}{english}
      \field{title}{Multi-Armed {{Bandit Algorithms}} and {{Empirical Evaluation}}}
      \field{year}{2005}
      \field{dateera}{ce}
      \field{pages}{437\bibrangedash 448}
      \range{pages}{12}
      \verb{doi}
      \verb 10.1007/11564096_42
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/M3DJXLRA/Vermorel_Mohri_2005_Multi-armed Bandit Algorithms and Empirical Evaluation.pdf
      \endverb
      \keyw{Bandit Problem,Content Distribution Network,Empirical Evaluation,Greedy Strategy,Reward Distribution}
    \endentry
    \entry{littman1994}{inproceedings}{}{}
      \name{author}{1}{}{%
        {{hash=211e529e9f10ad1bfacff2284da10ea7}{%
           family={Littman},
           familyi={L\bibinitperiod},
           given={Michael\bibnamedelima L.},
           giveni={M\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Elsevier}%
      }
      \strng{namehash}{211e529e9f10ad1bfacff2284da10ea7}
      \strng{fullhash}{211e529e9f10ad1bfacff2284da10ea7}
      \strng{fullhashraw}{211e529e9f10ad1bfacff2284da10ea7}
      \strng{bibnamehash}{211e529e9f10ad1bfacff2284da10ea7}
      \strng{authorbibnamehash}{211e529e9f10ad1bfacff2284da10ea7}
      \strng{authornamehash}{211e529e9f10ad1bfacff2284da10ea7}
      \strng{authorfullhash}{211e529e9f10ad1bfacff2284da10ea7}
      \strng{authorfullhashraw}{211e529e9f10ad1bfacff2284da10ea7}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Semantic Scholar extracted view of "Markov Games as a Framework for Multi-Agent Reinforcement Learning" by M. Littman}
      \field{booktitle}{Machine {{Learning Proceedings}} 1994}
      \field{isbn}{978-1-55860-335-6}
      \field{langid}{english}
      \field{title}{Markov Games as a Framework for Multi-Agent Reinforcement Learning}
      \field{urlday}{28}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{1994}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{157\bibrangedash 163}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1016/B978-1-55860-335-6.50027-1
      \endverb
      \verb{urlraw}
      \verb https://linkinghub.elsevier.com/retrieve/pii/B9781558603356500271
      \endverb
      \verb{url}
      \verb https://linkinghub.elsevier.com/retrieve/pii/B9781558603356500271
      \endverb
    \endentry
    \entry{albrecht2024}{book}{}{}
      \name{author}{3}{}{%
        {{hash=9da4f3f413d514d731efe34067976909}{%
           family={Albrecht},
           familyi={A\bibinitperiod},
           given={Stefano\bibnamedelima V.},
           giveni={S\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
        {{hash=f355421623a9ce0d54965817f05f6ad0}{%
           family={Christianos},
           familyi={C\bibinitperiod},
           given={Filippos},
           giveni={F\bibinitperiod}}}%
        {{hash=c32fcecebc45af27f83fbcfea1c93b31}{%
           family={Schäfer},
           familyi={S\bibinitperiod},
           given={Lukas},
           giveni={L\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cambridge, Massachusetts}%
      }
      \list{publisher}{1}{%
        {The MIT Press}%
      }
      \strng{namehash}{2399b7bc1ddbf337ba5d505312bf70c2}
      \strng{fullhash}{d9adbde549d4078b80f9846f86320d2e}
      \strng{fullhashraw}{d9adbde549d4078b80f9846f86320d2e}
      \strng{bibnamehash}{d9adbde549d4078b80f9846f86320d2e}
      \strng{authorbibnamehash}{d9adbde549d4078b80f9846f86320d2e}
      \strng{authornamehash}{2399b7bc1ddbf337ba5d505312bf70c2}
      \strng{authorfullhash}{d9adbde549d4078b80f9846f86320d2e}
      \strng{authorfullhashraw}{d9adbde549d4078b80f9846f86320d2e}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{"This book provides an accessible technical introduction to the field of Multi-Agent Reinforcement Learning (MARL)"--}
      \field{isbn}{978-0-262-04937-5}
      \field{langid}{english}
      \field{shorttitle}{Multi-Agent Reinforcement Learning}
      \field{title}{Multi-Agent Reinforcement Learning: Foundations and Modern Approaches}
      \field{year}{2024}
      \field{dateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/BNWH6I6A/Albrecht et al. - 2024 - Multi-agent reinforcement learning foundations an.pdf
      \endverb
      \keyw{Intelligent agents (Computer software),Reinforcement learning}
    \endentry
    \entry{li2023d}{article}{}{}
      \name{author}{5}{}{%
        {{hash=f61d280d0ccef8b64bf5fae966ddbe55}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Wenhao},
           giveni={W\bibinitperiod}}}%
        {{hash=d53a3a9a480ceecf96118b556f76b990}{%
           family={Jin},
           familyi={J\bibinitperiod},
           given={Bo},
           giveni={B\bibinitperiod}}}%
        {{hash=51c990b7d096ce2090300c4dca7ebe46}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Xiangfeng},
           giveni={X\bibinitperiod}}}%
        {{hash=9a7a7c8d0b927c0b0f6c62cef2f118d5}{%
           family={Yan},
           familyi={Y\bibinitperiod},
           given={Junchi},
           giveni={J\bibinitperiod}}}%
        {{hash=55bb21a31fded4aad976b7902dc8206a}{%
           family={Zha},
           familyi={Z\bibinitperiod},
           given={Hongyuan},
           giveni={H\bibinitperiod}}}%
      }
      \strng{namehash}{a4ca27c65a61dd0d61ef7be2f6cff445}
      \strng{fullhash}{607c94ed4f1a553468733e3f8197d9f2}
      \strng{fullhashraw}{607c94ed4f1a553468733e3f8197d9f2}
      \strng{bibnamehash}{607c94ed4f1a553468733e3f8197d9f2}
      \strng{authorbibnamehash}{607c94ed4f1a553468733e3f8197d9f2}
      \strng{authornamehash}{a4ca27c65a61dd0d61ef7be2f6cff445}
      \strng{authorfullhash}{607c94ed4f1a553468733e3f8197d9f2}
      \strng{authorfullhashraw}{607c94ed4f1a553468733e3f8197d9f2}
      \field{extraname}{1}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Traditional centralized multi-agent reinforcement learning (MARL) algorithms are sometimes unpractical in complicated applications due to non-interactivity between agents, the curse of dimensionality, and computation complexity. Hence, several decentralized MARL algorithms are motivated. However, existing decentralized methods only handle the fully cooperative setting where massive information needs to be transmitted in training. The block coordinate gradient descent scheme they used for successive independent actor and critic steps can simplify the calculation, but it causes serious bias. This paper proposes a flexible fully decentralized actor-critic MARL framework, which can combine most of the actor-critic methods and handle large-scale general cooperative multi-agent settings. A primal-dual hybrid gradient descent type algorithm framework is designed to learn individual agents separately for decentralization. From the perspective of each agent, policy improvement and value evaluation are jointly optimized, which can stabilize multi-agent policy learning. Furthermore, the proposed framework can achieve scalability and stability for the large-scale environment. This framework also reduces information transmission by the parameter sharing mechanism and novel modeling-other-agents methods based on theory-of-mind and online supervised learning. Sufficient experiments in cooperative Multi-agent Particle Environment and StarCraft II show that the proposed decentralized MARL instantiation algorithms perform competitively against conventional centralized and decentralized methods.}
      \field{issn}{1533-7928}
      \field{journaltitle}{Journal of Machine Learning Research}
      \field{number}{178}
      \field{shorttitle}{{{F2A2}}}
      \field{title}{{{F2A2}}: {{Flexible Fully-decentralized Approximate Actor-critic}} for {{Cooperative Multi-agent Reinforcement Learning}}}
      \field{urlday}{30}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{volume}{24}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 75}
      \range{pages}{75}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/DJAXR86Q/Li et al_2023_F2A2.pdf
      \endverb
      \verb{urlraw}
      \verb http://jmlr.org/papers/v24/20-700.html
      \endverb
      \verb{url}
      \verb http://jmlr.org/papers/v24/20-700.html
      \endverb
    \endentry
    \entry{zheng2017}{online}{}{}
      \name{author}{6}{}{%
        {{hash=0beb8f75088bfac1e81ca3840d0e89f0}{%
           family={Zheng},
           familyi={Z\bibinitperiod},
           given={Lianmin},
           giveni={L\bibinitperiod}}}%
        {{hash=8b073ea281f7b7362369e25115e52b8f}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Jiacheng},
           giveni={J\bibinitperiod}}}%
        {{hash=2b7464d9067c81c55766d572c1785250}{%
           family={Cai},
           familyi={C\bibinitperiod},
           given={Han},
           giveni={H\bibinitperiod}}}%
        {{hash=fbc51a6317f158547173b93086d9b1a2}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Weinan},
           giveni={W\bibinitperiod}}}%
        {{hash=2f3ea981fa5a715a69118b48e576a9f5}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Jun},
           giveni={J\bibinitperiod}}}%
        {{hash=c447a713c11f58a40c2a899040774a98}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Yong},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{293dcbfb5b9446f7f90cf8d346a542fe}
      \strng{fullhash}{2b6ef308e477b824f568f49af12aca7b}
      \strng{fullhashraw}{2b6ef308e477b824f568f49af12aca7b}
      \strng{bibnamehash}{2b6ef308e477b824f568f49af12aca7b}
      \strng{authorbibnamehash}{2b6ef308e477b824f568f49af12aca7b}
      \strng{authornamehash}{293dcbfb5b9446f7f90cf8d346a542fe}
      \strng{authorfullhash}{2b6ef308e477b824f568f49af12aca7b}
      \strng{authorfullhashraw}{2b6ef308e477b824f568f49af12aca7b}
      \field{extraname}{1}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We introduce MAgent, a platform to support research and development of many-agent reinforcement learning. Unlike previous research platforms on single or multi-agent reinforcement learning, MAgent focuses on supporting the tasks and the applications that require hundreds to millions of agents. Within the interactions among a population of agents, it enables not only the study of learning algorithms for agents' optimal polices, but more importantly, the observation and understanding of individual agent's behaviors and social phenomena emerging from the AI society, including communication languages, leaderships, altruism. MAgent is highly scalable and can host up to one million agents on a single GPU server. MAgent also provides flexible configurations for AI researchers to design their customized environments and agents. In this demo, we present three environments designed on MAgent and show emerged collective intelligence by learning from scratch.}
      \field{day}{2}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{12}
      \field{pubstate}{prepublished}
      \field{shorttitle}{{{MAgent}}}
      \field{title}{{{MAgent}}: {{A Many-Agent Reinforcement Learning Platform}} for {{Artificial Collective Intelligence}}}
      \field{urlday}{25}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1712.00600
      \endverb
      \verb{eprint}
      \verb 1712.00600
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/6KKHUIZ4/Zheng et al_2017_MAgent.pdf;/Users/brandonhosley/Zotero/storage/UU2EXIJC/1712.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1712.00600
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1712.00600
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems}
    \endentry
    \entry{foerster2017}{online}{}{}
      \name{author}{5}{}{%
        {{hash=d62c75690ab7c754f6e7217a242a4318}{%
           family={Foerster},
           familyi={F\bibinitperiod},
           given={Jakob},
           giveni={J\bibinitperiod}}}%
        {{hash=de7fc27bb6e6105440cd3039a5c9b684}{%
           family={Farquhar},
           familyi={F\bibinitperiod},
           given={Gregory},
           giveni={G\bibinitperiod}}}%
        {{hash=c15e1ba2ced206763fad02b814aa569e}{%
           family={Afouras},
           familyi={A\bibinitperiod},
           given={Triantafyllos},
           giveni={T\bibinitperiod}}}%
        {{hash=a07891d2d6d3da5f1827c01269cc6da6}{%
           family={Nardelli},
           familyi={N\bibinitperiod},
           given={Nantas},
           giveni={N\bibinitperiod}}}%
        {{hash=0d05819cf2b4fe22ba972c9b2b5d8c9d}{%
           family={Whiteson},
           familyi={W\bibinitperiod},
           given={Shimon},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{bacd311c6a08d138cde55b969b44b0f3}
      \strng{fullhash}{9b69be1387a30ea5191e83c832c30ed5}
      \strng{fullhashraw}{9b69be1387a30ea5191e83c832c30ed5}
      \strng{bibnamehash}{9b69be1387a30ea5191e83c832c30ed5}
      \strng{authorbibnamehash}{9b69be1387a30ea5191e83c832c30ed5}
      \strng{authornamehash}{bacd311c6a08d138cde55b969b44b0f3}
      \strng{authorfullhash}{9b69be1387a30ea5191e83c832c30ed5}
      \strng{authorfullhashraw}{9b69be1387a30ea5191e83c832c30ed5}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Cooperative multi-agent systems can be naturally used to model many real world problems, such as network packet routing and the coordination of autonomous vehicles. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.}
      \field{day}{14}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{12}
      \field{pubstate}{prepublished}
      \field{title}{Counterfactual {{Multi-Agent Policy Gradients}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1705.08926
      \endverb
      \verb{eprint}
      \verb 1705.08926
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/2XKJVLZB/Foerster et al_2017_Counterfactual Multi-Agent Policy Gradients.pdf;/Users/brandonhosley/Zotero/storage/CUA5ZIGJ/1705.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1705.08926
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1705.08926
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems}
    \endentry
    \entry{rashid2018}{online}{useprefix=true}{}
      \name{author}{6}{}{%
        {{hash=8eb7df23daba9c7bf22402026effdae7}{%
           family={Rashid},
           familyi={R\bibinitperiod},
           given={Tabish},
           giveni={T\bibinitperiod}}}%
        {{hash=c76172ee0c2ebda81796624b4a81ae54}{%
           family={Samvelyan},
           familyi={S\bibinitperiod},
           given={Mikayel},
           giveni={M\bibinitperiod}}}%
        {{hash=d056cdb8ed6bebb55e13c2246059af79}{%
           family={Witt},
           familyi={W\bibinitperiod},
           given={Christian\bibnamedelima Schroeder},
           giveni={C\bibinitperiod\bibinitdelim S\bibinitperiod},
           prefix={de},
           prefixi={d\bibinitperiod}}}%
        {{hash=de7fc27bb6e6105440cd3039a5c9b684}{%
           family={Farquhar},
           familyi={F\bibinitperiod},
           given={Gregory},
           giveni={G\bibinitperiod}}}%
        {{hash=d62c75690ab7c754f6e7217a242a4318}{%
           family={Foerster},
           familyi={F\bibinitperiod},
           given={Jakob},
           giveni={J\bibinitperiod}}}%
        {{hash=0d05819cf2b4fe22ba972c9b2b5d8c9d}{%
           family={Whiteson},
           familyi={W\bibinitperiod},
           given={Shimon},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{50d98948d170d7e9f0685c5c2364a676}
      \strng{fullhash}{d69266a177b46120cc1dbedeef7d4a43}
      \strng{fullhashraw}{d69266a177b46120cc1dbedeef7d4a43}
      \strng{bibnamehash}{d69266a177b46120cc1dbedeef7d4a43}
      \strng{authorbibnamehash}{d69266a177b46120cc1dbedeef7d4a43}
      \strng{authornamehash}{50d98948d170d7e9f0685c5c2364a676}
      \strng{authorfullhash}{d69266a177b46120cc1dbedeef7d4a43}
      \strng{authorfullhashraw}{d69266a177b46120cc1dbedeef7d4a43}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods.}
      \field{day}{6}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arXiv}
      \field{month}{6}
      \field{pubstate}{prepublished}
      \field{shorttitle}{{{QMIX}}}
      \field{title}{{{QMIX}}: {{Monotonic Value Function Factorisation}} for {{Deep Multi-Agent Reinforcement Learning}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1803.11485
      \endverb
      \verb{eprint}
      \verb 1803.11485
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/9U68HQRD/Rashid et al_2018_QMIX.pdf;/Users/brandonhosley/Zotero/storage/4FPV8YXI/1803.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1803.11485
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1803.11485
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning}
    \endentry
    \entry{fotouhi2019}{inproceedings}{}{}
      \name{author}{6}{}{%
        {{hash=a6a186dacd9f83299e5f08ad5381b4d3}{%
           family={Fotouhi},
           familyi={F\bibinitperiod},
           given={Azade},
           giveni={A\bibinitperiod}}}%
        {{hash=46a3f0b5c048254bc7f6910cd7e8cd63}{%
           family={Ding},
           familyi={D\bibinitperiod},
           given={Ming},
           giveni={M\bibinitperiod}}}%
        {{hash=f3d48f522634372697e29c861c940349}{%
           family={Galati\bibnamedelima Giordano},
           familyi={G\bibinitperiod\bibinitdelim G\bibinitperiod},
           given={Lorenzo},
           giveni={L\bibinitperiod}}}%
        {{hash=83871a45ec521dd742e3ae9c5f290bf3}{%
           family={Hassan},
           familyi={H\bibinitperiod},
           given={Mahbub},
           giveni={M\bibinitperiod}}}%
        {{hash=bc20e54573d2db853852f86802ab83de}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Jun},
           giveni={J\bibinitperiod}}}%
        {{hash=eacb5c6f48e0a3d0df2bd50831fcf94d}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Zihuai},
           giveni={Z\bibinitperiod}}}%
      }
      \strng{namehash}{7812982fa1c1d16bcb4028d8061a2324}
      \strng{fullhash}{89d3cf6116c65a0f39d1750b3e9e2d84}
      \strng{fullhashraw}{89d3cf6116c65a0f39d1750b3e9e2d84}
      \strng{bibnamehash}{89d3cf6116c65a0f39d1750b3e9e2d84}
      \strng{authorbibnamehash}{89d3cf6116c65a0f39d1750b3e9e2d84}
      \strng{authornamehash}{7812982fa1c1d16bcb4028d8061a2324}
      \strng{authorfullhash}{89d3cf6116c65a0f39d1750b3e9e2d84}
      \strng{authorfullhashraw}{89d3cf6116c65a0f39d1750b3e9e2d84}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper, we study the application of unmanned aerial vehicle (UAV) base stations (BSs) in order to improve the cellular network capacity. We consider flying BSs where BS equipments are mounted on UAVs, making it possible to move BSs freely in space. We study the optimization of UAVs' trajectory in a network with mobile users to improve the system throughput. We consider practical two-hop communications, i.e., the access link between a user and the UAV BS, and the backhaul link between the UAV BS and a macrocell BS plugged into the core network. We propose a reinforcement learning based algorithm to control the UAVs' mobility. Additionally, the proposed algorithm is subject to physical constraints of UAV mobility. Simulation results show that considering both the backhaul and access links in the UAV mobility optimization is highly effective in improving the system performance than only focusing on the access link.}
      \field{booktitle}{2019 {{IEEE Globecom Workshops}} ({{GC Wkshps}})}
      \field{eventtitle}{2019 {{IEEE Globecom Workshops}} ({{GC Wkshps}})}
      \field{month}{12}
      \field{title}{Joint {{Optimization}} of {{Access}} and {{Backhaul Links}} for {{UAVs Based}} on {{Reinforcement Learning}}}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{pages}{1\bibrangedash 6}
      \range{pages}{6}
      \verb{doi}
      \verb 10.1109/GCWkshps45667.2019.9024685
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Library/Mobile Documents/com~apple~CloudDocs/ZotFile/Fotouhi et al_2019_Joint Optimization of Access and Backhaul Links for UAVs Based on Reinforcement.pdf;/Users/brandonhosley/Zotero/storage/4WI354EX/stamp.html
      \endverb
      \keyw{Base stations,Cellular networks,Drones,Learning (artificial intelligence),Optimization,Satellite broadcasting}
    \endentry
    \entry{papoudakis2021}{online}{}{}
      \name{author}{4}{}{%
        {{hash=d0631db9b5ca2e6150a04082663ca08d}{%
           family={Papoudakis},
           familyi={P\bibinitperiod},
           given={Georgios},
           giveni={G\bibinitperiod}}}%
        {{hash=f355421623a9ce0d54965817f05f6ad0}{%
           family={Christianos},
           familyi={C\bibinitperiod},
           given={Filippos},
           giveni={F\bibinitperiod}}}%
        {{hash=c32fcecebc45af27f83fbcfea1c93b31}{%
           family={Schäfer},
           familyi={S\bibinitperiod},
           given={Lukas},
           giveni={L\bibinitperiod}}}%
        {{hash=9da4f3f413d514d731efe34067976909}{%
           family={Albrecht},
           familyi={A\bibinitperiod},
           given={Stefano\bibnamedelima V.},
           giveni={S\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
      }
      \strng{namehash}{f2473647f2bd5df0777ba1def078b0af}
      \strng{fullhash}{fb48708063b34a78b84029004ce47bdb}
      \strng{fullhashraw}{fb48708063b34a78b84029004ce47bdb}
      \strng{bibnamehash}{fb48708063b34a78b84029004ce47bdb}
      \strng{authorbibnamehash}{fb48708063b34a78b84029004ce47bdb}
      \strng{authornamehash}{f2473647f2bd5df0777ba1def078b0af}
      \strng{authorfullhash}{fb48708063b34a78b84029004ce47bdb}
      \strng{authorfullhashraw}{fb48708063b34a78b84029004ce47bdb}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Multi-agent deep reinforcement learning (MARL) suffers from a lack of commonly-used evaluation tasks and criteria, making comparisons between approaches difficult. In this work, we provide a systematic evaluation and comparison of three different classes of MARL algorithms (independent learning, centralised multi-agent policy gradient, value decomposition) in a diverse range of cooperative multi-agent learning tasks. Our experiments serve as a reference for the expected performance of algorithms across different learning tasks, and we provide insights regarding the effectiveness of different learning approaches. We open-source EPyMARL, which extends the PyMARL codebase to include additional algorithms and allow for flexible configuration of algorithm implementation details such as parameter sharing. Finally, we open-source two environments for multi-agent research which focus on coordination under sparse rewards.}
      \field{day}{9}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arXiv}
      \field{month}{11}
      \field{pubstate}{prepublished}
      \field{title}{Benchmarking {{Multi-Agent Deep Reinforcement Learning Algorithms}} in {{Cooperative Tasks}}}
      \field{urlday}{28}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2006.07869
      \endverb
      \verb{eprint}
      \verb 2006.07869
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/X55NBJGK/Papoudakis et al_2021_Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative.pdf;/Users/brandonhosley/Zotero/storage/9S95S7UZ/2006.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2006.07869
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2006.07869
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning}
    \endentry
    \entry{zhou2023}{online}{}{}
      \name{author}{8}{}{%
        {{hash=23a8d038a13e60e9376d006d3f9ff961}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Yihe},
           giveni={Y\bibinitperiod}}}%
        {{hash=9b0b8f748e5e6218072feb6d0738e877}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Shunyu},
           giveni={S\bibinitperiod}}}%
        {{hash=a21190ef9f56daf7961575f3c231915d}{%
           family={Qing},
           familyi={Q\bibinitperiod},
           given={Yunpeng},
           giveni={Y\bibinitperiod}}}%
        {{hash=3355e5c28ccf93453b5b4ad11b11f634}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Kaixuan},
           giveni={K\bibinitperiod}}}%
        {{hash=1149e9b799ef6e92e469da707fc20264}{%
           family={Zheng},
           familyi={Z\bibinitperiod},
           given={Tongya},
           giveni={T\bibinitperiod}}}%
        {{hash=0ba704ec82b8ef276a8876277de31250}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Yanhao},
           giveni={Y\bibinitperiod}}}%
        {{hash=f47cc1ff1ed8ea8025326fb247ba534f}{%
           family={Song},
           familyi={S\bibinitperiod},
           given={Jie},
           giveni={J\bibinitperiod}}}%
        {{hash=62f2f4ff0e18b5e38eaf0a72e3e88a86}{%
           family={Song},
           familyi={S\bibinitperiod},
           given={Mingli},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{f08d4fe69f49047305ab3f5ebd5b6599}
      \strng{fullhash}{e7a93bd74d81fe20d51425237dcc6806}
      \strng{fullhashraw}{e7a93bd74d81fe20d51425237dcc6806}
      \strng{bibnamehash}{f08d4fe69f49047305ab3f5ebd5b6599}
      \strng{authorbibnamehash}{f08d4fe69f49047305ab3f5ebd5b6599}
      \strng{authornamehash}{f08d4fe69f49047305ab3f5ebd5b6599}
      \strng{authorfullhash}{e7a93bd74d81fe20d51425237dcc6806}
      \strng{authorfullhashraw}{e7a93bd74d81fe20d51425237dcc6806}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Centralized Training with Decentralized Execution (CTDE) has recently emerged as a popular framework for cooperative Multi-Agent Reinforcement Learning (MARL), where agents can use additional global state information to guide training in a centralized way and make their own decisions only based on decentralized local policies. Despite the encouraging results achieved, CTDE makes an independence assumption on agent policies, which limits agents to adopt global cooperative information from each other during centralized training. Therefore, we argue that existing CTDE methods cannot fully utilize global information for training, leading to an inefficient joint-policy exploration and even suboptimal results. In this paper, we introduce a novel Centralized Advising and Decentralized Pruning (CADP) framework for multi-agent reinforcement learning, that not only enables an efficacious message exchange among agents during training but also guarantees the independent policies for execution. Firstly, CADP endows agents the explicit communication channel to seek and take advices from different agents for more centralized training. To further ensure the decentralized execution, we propose a smooth model pruning mechanism to progressively constraint the agent communication into a closed one without degradation in agent cooperation capability. Empirical evaluations on StarCraft II micromanagement and Google Research Football benchmarks demonstrate that the proposed framework achieves superior performance compared with the state-of-the-art counterparts. Our code will be made publicly available.}
      \field{day}{26}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{5}
      \field{pubstate}{prepublished}
      \field{title}{Is {{Centralized Training}} with {{Decentralized Execution Framework Centralized Enough}} for {{MARL}}?}
      \field{urlday}{25}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2305.17352
      \endverb
      \verb{eprint}
      \verb 2305.17352
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/EMWBIU4Q/Zhou et al_2023_Is Centralized Training with Decentralized Execution Framework Centralized.pdf;/Users/brandonhosley/Zotero/storage/MBC3AKQ9/2305.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2305.17352
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2305.17352
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems}
    \endentry
    \entry{li2023c}{online}{}{}
      \name{author}{2}{}{%
        {{hash=f1a4bf543e31856011d61ed1139a76f0}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Hepeng},
           giveni={H\bibinitperiod}}}%
        {{hash=bb5eb80bb4dc903dbeef6eb15c0155e5}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Haibo},
           giveni={H\bibinitperiod}}}%
      }
      \strng{namehash}{6891da59fca3ff1cb2040463026ed8bb}
      \strng{fullhash}{6891da59fca3ff1cb2040463026ed8bb}
      \strng{fullhashraw}{6891da59fca3ff1cb2040463026ed8bb}
      \strng{bibnamehash}{6891da59fca3ff1cb2040463026ed8bb}
      \strng{authorbibnamehash}{6891da59fca3ff1cb2040463026ed8bb}
      \strng{authornamehash}{6891da59fca3ff1cb2040463026ed8bb}
      \strng{authorfullhash}{6891da59fca3ff1cb2040463026ed8bb}
      \strng{authorfullhashraw}{6891da59fca3ff1cb2040463026ed8bb}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We extend trust region policy optimization (TRPO) to multi-agent reinforcement learning (MARL) problems. We show that the policy update of TRPO can be transformed into a distributed consensus optimization problem for multi-agent cases. By making a series of approximations to the consensus optimization model, we propose a decentralized MARL algorithm, which we call multi-agent TRPO (MATRPO). This algorithm can optimize distributed policies based on local observations and private rewards. The agents do not need to know observations, rewards, policies or value/action-value functions of other agents. The agents only share a likelihood ratio with their neighbors during the training process. The algorithm is fully decentralized and privacy-preserving. Our experiments on two cooperative games demonstrate its robust performance on complicated MARL tasks.}
      \field{day}{4}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{8}
      \field{pubstate}{prepublished}
      \field{title}{Multi-{{Agent Trust Region Policy Optimization}}}
      \field{urlday}{30}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2010.07916
      \endverb
      \verb{eprint}
      \verb 2010.07916
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/QRM8IW7W/Li_He_2023_Multi-Agent Trust Region Policy Optimization.pdf;/Users/brandonhosley/Zotero/storage/Y6EAEDL4/2010.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2010.07916
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2010.07916
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems}
    \endentry
    \entry{ackermann2019}{online}{}{}
      \name{author}{4}{}{%
        {{hash=d6773c2b5996cd51cdb0ab7775fb8fe8}{%
           family={Ackermann},
           familyi={A\bibinitperiod},
           given={Johannes},
           giveni={J\bibinitperiod}}}%
        {{hash=ac38577e4a1419b425cfacd667ad0716}{%
           family={Gabler},
           familyi={G\bibinitperiod},
           given={Volker},
           giveni={V\bibinitperiod}}}%
        {{hash=463fb8e057cd6542341e251ef86c6176}{%
           family={Osa},
           familyi={O\bibinitperiod},
           given={Takayuki},
           giveni={T\bibinitperiod}}}%
        {{hash=39334f77cbd67c8922133952b8095d89}{%
           family={Sugiyama},
           familyi={S\bibinitperiod},
           given={Masashi},
           giveni={M\bibinitperiod}}}%
      }
      \list{organization}{1}{%
        {arXiv.org}%
      }
      \strng{namehash}{97e03abbbb1513e13b9fc491ccfe70a8}
      \strng{fullhash}{5107ada9512eb732ac00b348d7e58d81}
      \strng{fullhashraw}{5107ada9512eb732ac00b348d7e58d81}
      \strng{bibnamehash}{5107ada9512eb732ac00b348d7e58d81}
      \strng{authorbibnamehash}{5107ada9512eb732ac00b348d7e58d81}
      \strng{authornamehash}{97e03abbbb1513e13b9fc491ccfe70a8}
      \strng{authorfullhash}{5107ada9512eb732ac00b348d7e58d81}
      \strng{authorfullhashraw}{5107ada9512eb732ac00b348d7e58d81}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Many real world tasks require multiple agents to work together. Multi-agent reinforcement learning (RL) methods have been proposed in recent years to solve these tasks, but current methods often fail to efficiently learn policies. We thus investigate the presence of a common weakness in single-agent RL, namely value function overestimation bias, in the multi-agent setting. Based on our findings, we propose an approach that reduces this bias by using double centralized critics. We evaluate it on six mixed cooperative-competitive tasks, showing a significant advantage over current methods. Finally, we investigate the application of multi-agent methods to high-dimensional robotic tasks and show that our approach can be used to learn decentralized policies in this domain.}
      \field{day}{3}
      \field{langid}{english}
      \field{month}{10}
      \field{title}{Reducing {{Overestimation Bias}} in {{Multi-Agent Domains Using Double Centralized Critics}}}
      \field{urlday}{26}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/3DUYWQRZ/Ackermann et al_2019_Reducing Overestimation Bias in Multi-Agent Domains Using Double Centralized.pdf
      \endverb
      \verb{urlraw}
      \verb https://arxiv.org/abs/1910.01465v2
      \endverb
      \verb{url}
      \verb https://arxiv.org/abs/1910.01465v2
      \endverb
    \endentry
    \entry{leonardos2021}{inproceedings}{}{}
      \name{author}{3}{}{%
        {{hash=dbea480d13ab1adbde2376c96f012a2f}{%
           family={Leonardos},
           familyi={L\bibinitperiod},
           given={Stefanos},
           giveni={S\bibinitperiod}}}%
        {{hash=3ce2ec5c4bae794e313102e2e8808336}{%
           family={Piliouras},
           familyi={P\bibinitperiod},
           given={Georgios},
           giveni={G\bibinitperiod}}}%
        {{hash=6d7efd767109532e9cc84aa2ce9ebc27}{%
           family={Spendlove},
           familyi={S\bibinitperiod},
           given={Kelly},
           giveni={K\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{510e1655c0e1b6fe0c2509e9dc3c5147}
      \strng{fullhash}{231d1dea6c61cb7c8143657f2e41bb90}
      \strng{fullhashraw}{231d1dea6c61cb7c8143657f2e41bb90}
      \strng{bibnamehash}{231d1dea6c61cb7c8143657f2e41bb90}
      \strng{authorbibnamehash}{231d1dea6c61cb7c8143657f2e41bb90}
      \strng{authornamehash}{510e1655c0e1b6fe0c2509e9dc3c5147}
      \strng{authorfullhash}{231d1dea6c61cb7c8143657f2e41bb90}
      \strng{authorfullhashraw}{231d1dea6c61cb7c8143657f2e41bb90}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The interplay between exploration and exploitation in competitive multi-agent learning is still far from being well understood. Motivated by this, we study smooth Q-learning, a prototypical learning model that explicitly captures the balance between game rewards and exploration costs. We show that Q-learning always converges to the unique quantal-response equilibrium (QRE), the standard solution concept for games under bounded rationality, in weighted zero-sum polymatrix games with heterogeneous learning agents using positive exploration rates. Complementing recent results about convergence in weighted potential games [16,34], we show that fast convergence of Q-learning in competitive settings obtains regardless of the number of agents and without any need for parameter fine-tuning. As showcased by our experiments in network zero-sum games, these theoretical results provide the necessary guarantees for an algorithmic approach to the currently open problem of equilibrium selection in competitive multi-agent settings.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{shorttitle}{Exploration-{{Exploitation}} in {{Multi-Agent Competition}}}
      \field{title}{Exploration-{{Exploitation}} in {{Multi-Agent Competition}}: {{Convergence}} with {{Bounded Rationality}}}
      \field{urlday}{25}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{volume}{34}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{26318\bibrangedash 26331}
      \range{pages}{14}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/NHZ3NPT4/Leonardos et al_2021_Exploration-Exploitation in Multi-Agent Competition.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2021/hash/dd1970fb03877a235d530476eb727dab-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2021/hash/dd1970fb03877a235d530476eb727dab-Abstract.html
      \endverb
    \endentry
    \entry{sukhbaatar2016}{online}{}{}
      \name{author}{3}{}{%
        {{hash=72c54cfe76db45d84a3413df9e903ff4}{%
           family={Sukhbaatar},
           familyi={S\bibinitperiod},
           given={Sainbayar},
           giveni={S\bibinitperiod}}}%
        {{hash=b31b3f03b1e1ee0eec6ff58f9a9df960}{%
           family={Szlam},
           familyi={S\bibinitperiod},
           given={Arthur},
           giveni={A\bibinitperiod}}}%
        {{hash=a6784304d1cc890b2cb6c6c7f2f3fd76}{%
           family={Fergus},
           familyi={F\bibinitperiod},
           given={Rob},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{140eda2c6b05153d13195f7f9b9bcfed}
      \strng{fullhash}{a9cb358542c5136a8832a15c37b6233d}
      \strng{fullhashraw}{a9cb358542c5136a8832a15c37b6233d}
      \strng{bibnamehash}{a9cb358542c5136a8832a15c37b6233d}
      \strng{authorbibnamehash}{a9cb358542c5136a8832a15c37b6233d}
      \strng{authornamehash}{140eda2c6b05153d13195f7f9b9bcfed}
      \strng{authorfullhash}{a9cb358542c5136a8832a15c37b6233d}
      \strng{authorfullhashraw}{a9cb358542c5136a8832a15c37b6233d}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Many tasks in AI require the collaboration of multiple agents. Typically, the communication protocol between agents is manually specified and not altered during training. In this paper we explore a simple neural model, called CommNet, that uses continuous communication for fully cooperative tasks. The model consists of multiple agents and the communication between them is learned alongside their policy. We apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines. In some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand.}
      \field{day}{31}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{10}
      \field{pubstate}{prepublished}
      \field{title}{Learning {{Multiagent Communication}} with {{Backpropagation}}}
      \field{urlday}{28}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1605.07736
      \endverb
      \verb{eprint}
      \verb 1605.07736
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/LFWPCTDS/Sukhbaatar et al_2016_Learning Multiagent Communication with Backpropagation.pdf;/Users/brandonhosley/Zotero/storage/R7FJJZ2M/1605.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1605.07736
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1605.07736
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
    \endentry
    \entry{zhong2024}{article}{}{}
      \name{author}{6}{}{%
        {{hash=b95d1a2055e09b96f430caf3d4d35616}{%
           family={Zhong},
           familyi={Z\bibinitperiod},
           given={Yifan},
           giveni={Y\bibinitperiod}}}%
        {{hash=0fb5c75e0d48bc9eacbbc94daa482547}{%
           family={Kuba},
           familyi={K\bibinitperiod},
           given={Jakub\bibnamedelima Grudzien},
           giveni={J\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=0e1c640014bac9108baa5a3600006e86}{%
           family={Feng},
           familyi={F\bibinitperiod},
           given={Xidong},
           giveni={X\bibinitperiod}}}%
        {{hash=ee936fa0fa279c00e808275482941636}{%
           family={Hu},
           familyi={H\bibinitperiod},
           given={Siyi},
           giveni={S\bibinitperiod}}}%
        {{hash=f08383385e5e86d608bf053e10579e54}{%
           family={Ji},
           familyi={J\bibinitperiod},
           given={Jiaming},
           giveni={J\bibinitperiod}}}%
        {{hash=616d3b0b19e3536f5c65dfab2a48a659}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Yaodong},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{2d1f4bb5b8a73da311fb4b059b2b8b41}
      \strng{fullhash}{811d441c2dcb8ddce046201441d67105}
      \strng{fullhashraw}{811d441c2dcb8ddce046201441d67105}
      \strng{bibnamehash}{811d441c2dcb8ddce046201441d67105}
      \strng{authorbibnamehash}{811d441c2dcb8ddce046201441d67105}
      \strng{authornamehash}{2d1f4bb5b8a73da311fb4b059b2b8b41}
      \strng{authorfullhash}{811d441c2dcb8ddce046201441d67105}
      \strng{authorfullhashraw}{811d441c2dcb8ddce046201441d67105}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Journal of Machine Learning Research}
      \field{langid}{english}
      \field{number}{32}
      \field{shortjournal}{JMLR}
      \field{title}{Heterogeneous-{{Agent Reinforcement Learning}}}
      \field{volume}{25}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{pages}{1\bibrangedash 67}
      \range{pages}{67}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/RP4HSFZU/Zhong et al. - Heterogeneous-Agent Reinforcement Learning.pdf
      \endverb
      \verb{urlraw}
      \verb http://jmlr.org/papers/v25/23-0488.html
      \endverb
      \verb{url}
      \verb http://jmlr.org/papers/v25/23-0488.html
      \endverb
    \endentry
    \entry{zotero-2643}{online}{}{}
      \field{sortinit}{7}
      \field{sortinithash}{108d0be1b1bee9773a1173443802c0a3}
      \field{labeltitlesource}{title}
      \field{abstract}{The list of all Heroes in Dota 2}
      \field{title}{Dota 2 {{Heroes}}}
      \field{urlday}{25}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://www.dota2.com/heroes/><meta property=
      \endverb
      \verb{url}
      \verb https://www.dota2.com/heroes/%3E%3Cmeta%20property=
      \endverb
    \endentry
    \entry{gall1975}{book}{}{}
      \name{author}{1}{}{%
        {{hash=d639a65956971feb1529df9003847b5d}{%
           family={Gall},
           familyi={G\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {General Systemantics Press}%
      }
      \strng{namehash}{d639a65956971feb1529df9003847b5d}
      \strng{fullhash}{d639a65956971feb1529df9003847b5d}
      \strng{fullhashraw}{d639a65956971feb1529df9003847b5d}
      \strng{bibnamehash}{d639a65956971feb1529df9003847b5d}
      \strng{authorbibnamehash}{d639a65956971feb1529df9003847b5d}
      \strng{authornamehash}{d639a65956971feb1529df9003847b5d}
      \strng{authorfullhash}{d639a65956971feb1529df9003847b5d}
      \strng{authorfullhashraw}{d639a65956971feb1529df9003847b5d}
      \field{sortinit}{7}
      \field{sortinithash}{108d0be1b1bee9773a1173443802c0a3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{eprinttype}{googlebooks}
      \field{langid}{english}
      \field{pagetotal}{152}
      \field{shorttitle}{General {{Systemantics}}}
      \field{title}{General {{Systemantics}}: {{An Essay}} on How {{Systems Work}}, and {{Especially}} How {{They Fail}}, {{Together}} with the {{Very First Annotated Compendium}} of {{Basic Systems Axioms}} : A {{Handbook}} and {{Ready Reference}} for {{Scientists}}, {{Engineers}}, {{Laboratory Workers}}, {{Administrators}}, {{Public Officials}}, {{Systems Analysts}}, {{Etc}}., {{Etc}}., {{Etc}}., and the {{General Public}}}
      \field{year}{1975}
      \field{dateera}{ce}
      \verb{eprint}
      \verb 6FgeAAAAMAAJ
      \endverb
    \endentry
    \entry{chen2016}{online}{}{}
      \name{author}{3}{}{%
        {{hash=48461ccdf50ba5a54e29a354aa734c0b}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Tianqi},
           giveni={T\bibinitperiod}}}%
        {{hash=5d2585c11210cf1d4512e6e0a03ec315}{%
           family={Goodfellow},
           familyi={G\bibinitperiod},
           given={Ian},
           giveni={I\bibinitperiod}}}%
        {{hash=8f128e70084608a2c29c497ebd794f87}{%
           family={Shlens},
           familyi={S\bibinitperiod},
           given={Jonathon},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{4a1b902110b26e322484f62946cb7718}
      \strng{fullhash}{f9b1631e01fd72e564b9b731590c56b7}
      \strng{fullhashraw}{f9b1631e01fd72e564b9b731590c56b7}
      \strng{bibnamehash}{f9b1631e01fd72e564b9b731590c56b7}
      \strng{authorbibnamehash}{f9b1631e01fd72e564b9b731590c56b7}
      \strng{authornamehash}{4a1b902110b26e322484f62946cb7718}
      \strng{authorfullhash}{f9b1631e01fd72e564b9b731590c56b7}
      \strng{authorfullhashraw}{f9b1631e01fd72e564b9b731590c56b7}
      \field{sortinit}{7}
      \field{sortinithash}{108d0be1b1bee9773a1173443802c0a3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.}
      \field{day}{23}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{4}
      \field{pubstate}{prepublished}
      \field{shorttitle}{{{Net2Net}}}
      \field{title}{{{Net2Net}}: {{Accelerating Learning}} via {{Knowledge Transfer}}}
      \field{urlday}{25}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1511.05641
      \endverb
      \verb{eprint}
      \verb 1511.05641
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/JRZPUISW/Chen et al_2016_Net2Net.pdf;/Users/brandonhosley/Zotero/storage/2RZBC4W3/1511.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1511.05641
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1511.05641
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
    \entry{mnih2016}{online}{}{}
      \name{author}{8}{}{%
        {{hash=f7d23cfe4ca0e6bf7a8c251bfa78aca6}{%
           family={Mnih},
           familyi={M\bibinitperiod},
           given={Volodymyr},
           giveni={V\bibinitperiod}}}%
        {{hash=db6afdf70ea61bf8ed973a75b90ff818}{%
           family={Badia},
           familyi={B\bibinitperiod},
           given={Adrià\bibnamedelima Puigdomènech},
           giveni={A\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=9e80f4779b032f68a6106e1424345450}{%
           family={Mirza},
           familyi={M\bibinitperiod},
           given={Mehdi},
           giveni={M\bibinitperiod}}}%
        {{hash=dfca94b0427da7f9088af596e23b46c0}{%
           family={Graves},
           familyi={G\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
        {{hash=2a321a868e44d49baf52b5e2d816fb71}{%
           family={Lillicrap},
           familyi={L\bibinitperiod},
           given={Timothy\bibnamedelima P.},
           giveni={T\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=b63b9b2d91f6ba1b1739563edc0432ab}{%
           family={Harley},
           familyi={H\bibinitperiod},
           given={Tim},
           giveni={T\bibinitperiod}}}%
        {{hash=89dbd30410c2085cd059f32c57d4593e}{%
           family={Silver},
           familyi={S\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=481308b301166b521c74fde6566e97e6}{%
           family={Kavukcuoglu},
           familyi={K\bibinitperiod},
           given={Koray},
           giveni={K\bibinitperiod}}}%
      }
      \strng{namehash}{aff24b211b34cdd515b2fd148ce0c920}
      \strng{fullhash}{e067addbfefbaf06a859610fa608b21e}
      \strng{fullhashraw}{e067addbfefbaf06a859610fa608b21e}
      \strng{bibnamehash}{aff24b211b34cdd515b2fd148ce0c920}
      \strng{authorbibnamehash}{aff24b211b34cdd515b2fd148ce0c920}
      \strng{authornamehash}{aff24b211b34cdd515b2fd148ce0c920}
      \strng{authorfullhash}{e067addbfefbaf06a859610fa608b21e}
      \strng{authorfullhashraw}{e067addbfefbaf06a859610fa608b21e}
      \field{extraname}{1}
      \field{sortinit}{7}
      \field{sortinithash}{108d0be1b1bee9773a1173443802c0a3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.}
      \field{day}{16}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{6}
      \field{pubstate}{prepublished}
      \field{title}{Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}}
      \field{urlday}{5}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1602.01783
      \endverb
      \verb{eprint}
      \verb 1602.01783
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/ZNCS528Y/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf;/Users/brandonhosley/Zotero/storage/MJV4AMZK/1602.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1602.01783
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1602.01783
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
    \entry{fujimoto2018}{online}{useprefix=true}{}
      \name{author}{3}{}{%
        {{hash=064fb8795fb3fe6537e4d29fc8578ec4}{%
           family={Fujimoto},
           familyi={F\bibinitperiod},
           given={Scott},
           giveni={S\bibinitperiod}}}%
        {{hash=f2db100b30806f7c8e894892db68dadb}{%
           family={Hoof},
           familyi={H\bibinitperiod},
           given={Herke},
           giveni={H\bibinitperiod},
           prefix={van},
           prefixi={v\bibinitperiod}}}%
        {{hash=a5137015bf373eff6cd1b6a96b145c3c}{%
           family={Meger},
           familyi={M\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
      }
      \strng{namehash}{cd118bb9028b160d277524f7453fb213}
      \strng{fullhash}{fa31fcf8611ccec7d2a280d8c08326e7}
      \strng{fullhashraw}{fa31fcf8611ccec7d2a280d8c08326e7}
      \strng{bibnamehash}{fa31fcf8611ccec7d2a280d8c08326e7}
      \strng{authorbibnamehash}{fa31fcf8611ccec7d2a280d8c08326e7}
      \strng{authornamehash}{cd118bb9028b160d277524f7453fb213}
      \strng{authorfullhash}{fa31fcf8611ccec7d2a280d8c08326e7}
      \strng{authorfullhashraw}{fa31fcf8611ccec7d2a280d8c08326e7}
      \field{sortinit}{8}
      \field{sortinithash}{a231b008ebf0ecbe0b4d96dcc159445f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.}
      \field{day}{22}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arXiv}
      \field{month}{10}
      \field{pubstate}{prepublished}
      \field{title}{Addressing {{Function Approximation Error}} in {{Actor-Critic Methods}}}
      \field{urlday}{25}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1802.09477
      \endverb
      \verb{eprint}
      \verb 1802.09477
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/53LNWQEE/Fujimoto et al_2018_Addressing Function Approximation Error in Actor-Critic Methods.pdf;/Users/brandonhosley/Zotero/storage/HKDN2XRX/1802.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1802.09477
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1802.09477
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{schulman2017}{online}{}{}
      \name{author}{5}{}{%
        {{hash=3e09bd2a25d237ebdaed560a07c0451e}{%
           family={Schulman},
           familyi={S\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod}}}%
        {{hash=c9545da54b33744da25943cdf66eadac}{%
           family={Levine},
           familyi={L\bibinitperiod},
           given={Sergey},
           giveni={S\bibinitperiod}}}%
        {{hash=a901fd78fe1108cfa7d11129644967c7}{%
           family={Moritz},
           familyi={M\bibinitperiod},
           given={Philipp},
           giveni={P\bibinitperiod}}}%
        {{hash=8a36116840c7ee55901618c95fd08a58}{%
           family={Jordan},
           familyi={J\bibinitperiod},
           given={Michael\bibnamedelima I.},
           giveni={M\bibinitperiod\bibinitdelim I\bibinitperiod}}}%
        {{hash=e28d4ee199593959d8c29980a64f1974}{%
           family={Abbeel},
           familyi={A\bibinitperiod},
           given={Pieter},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{8d6613ae59595910252a86a61babf6c8}
      \strng{fullhash}{945251de1d41bc2767a541a8148f51da}
      \strng{fullhashraw}{945251de1d41bc2767a541a8148f51da}
      \strng{bibnamehash}{945251de1d41bc2767a541a8148f51da}
      \strng{authorbibnamehash}{945251de1d41bc2767a541a8148f51da}
      \strng{authornamehash}{8d6613ae59595910252a86a61babf6c8}
      \strng{authorfullhash}{945251de1d41bc2767a541a8148f51da}
      \strng{authorfullhashraw}{945251de1d41bc2767a541a8148f51da}
      \field{extraname}{1}
      \field{sortinit}{8}
      \field{sortinithash}{a231b008ebf0ecbe0b4d96dcc159445f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.}
      \field{day}{20}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{4}
      \field{pubstate}{prepublished}
      \field{title}{Trust {{Region Policy Optimization}}}
      \field{urlday}{25}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1502.05477
      \endverb
      \verb{eprint}
      \verb 1502.05477
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/WB9X428C/Schulman et al_2017_Trust Region Policy Optimization.pdf;/Users/brandonhosley/Zotero/storage/N4BJ2EUW/1502.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1502.05477
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1502.05477
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
    \entry{kakade2002}{online}{}{}
      \name{author}{2}{}{%
        {{hash=d45f41902c25eaa9ac992a04bff99dc6}{%
           family={Kakade},
           familyi={K\bibinitperiod},
           given={Sham},
           giveni={S\bibinitperiod}}}%
        {{hash=3b32f20ce03981cb315259a6fd0a3068}{%
           family={Langford},
           familyi={L\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Proceedings of the Nineteenth International Conference on Machine Learning}%
      }
      \strng{namehash}{f86aad93aad0a893a9c3c4ec2b3070c2}
      \strng{fullhash}{f86aad93aad0a893a9c3c4ec2b3070c2}
      \strng{fullhashraw}{f86aad93aad0a893a9c3c4ec2b3070c2}
      \strng{bibnamehash}{f86aad93aad0a893a9c3c4ec2b3070c2}
      \strng{authorbibnamehash}{f86aad93aad0a893a9c3c4ec2b3070c2}
      \strng{authornamehash}{f86aad93aad0a893a9c3c4ec2b3070c2}
      \strng{authorfullhash}{f86aad93aad0a893a9c3c4ec2b3070c2}
      \strng{authorfullhashraw}{f86aad93aad0a893a9c3c4ec2b3070c2}
      \field{sortinit}{8}
      \field{sortinithash}{a231b008ebf0ecbe0b4d96dcc159445f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Approximately {{Optimal Approximate Reinforcement Learning}}}
      \field{year}{2002}
      \field{dateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/CV4MXZ2Q/approximately-optimal-approximate-reinforcement-learning.html
      \endverb
    \endentry
    \entry{schulman2017a}{online}{}{}
      \name{author}{5}{}{%
        {{hash=3e09bd2a25d237ebdaed560a07c0451e}{%
           family={Schulman},
           familyi={S\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod}}}%
        {{hash=674ede0b9cd02a2bf5fc662972efb9f0}{%
           family={Wolski},
           familyi={W\bibinitperiod},
           given={Filip},
           giveni={F\bibinitperiod}}}%
        {{hash=4164e43d8cf919f5e3f8d80f5ea23f36}{%
           family={Dhariwal},
           familyi={D\bibinitperiod},
           given={Prafulla},
           giveni={P\bibinitperiod}}}%
        {{hash=a812c46caad94fc8701be37871f303ba}{%
           family={Radford},
           familyi={R\bibinitperiod},
           given={Alec},
           giveni={A\bibinitperiod}}}%
        {{hash=be38507da6ab98e7ac01ac2c6b7e13eb}{%
           family={Klimov},
           familyi={K\bibinitperiod},
           given={Oleg},
           giveni={O\bibinitperiod}}}%
      }
      \strng{namehash}{8d6613ae59595910252a86a61babf6c8}
      \strng{fullhash}{a6c6ed5a5aeb74e536f16291276ae392}
      \strng{fullhashraw}{a6c6ed5a5aeb74e536f16291276ae392}
      \strng{bibnamehash}{a6c6ed5a5aeb74e536f16291276ae392}
      \strng{authorbibnamehash}{a6c6ed5a5aeb74e536f16291276ae392}
      \strng{authornamehash}{8d6613ae59595910252a86a61babf6c8}
      \strng{authorfullhash}{a6c6ed5a5aeb74e536f16291276ae392}
      \strng{authorfullhashraw}{a6c6ed5a5aeb74e536f16291276ae392}
      \field{extraname}{2}
      \field{sortinit}{8}
      \field{sortinithash}{a231b008ebf0ecbe0b4d96dcc159445f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.}
      \field{day}{28}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{8}
      \field{pubstate}{prepublished}
      \field{title}{Proximal {{Policy Optimization Algorithms}}}
      \field{urlday}{25}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1707.06347
      \endverb
      \verb{eprint}
      \verb 1707.06347
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/XDWVYD3Y/Schulman et al_2017_Proximal Policy Optimization Algorithms.pdf;/Users/brandonhosley/Zotero/storage/5UT3G33Z/1707.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1707.06347
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1707.06347
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
    \entry{yu2022}{online}{}{}
      \name{author}{7}{}{%
        {{hash=a10237d1d57fbf19311761320dafceb5}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Chao},
           giveni={C\bibinitperiod}}}%
        {{hash=113762107382f1db1aebbc7a01ac3084}{%
           family={Velu},
           familyi={V\bibinitperiod},
           given={Akash},
           giveni={A\bibinitperiod}}}%
        {{hash=c8a6a2cc029452189ac51ad8cb0d6d0b}{%
           family={Vinitsky},
           familyi={V\bibinitperiod},
           given={Eugene},
           giveni={E\bibinitperiod}}}%
        {{hash=48e044f1d97f0e7b51746a59b87dbf49}{%
           family={Gao},
           familyi={G\bibinitperiod},
           given={Jiaxuan},
           giveni={J\bibinitperiod}}}%
        {{hash=b2eccb185095fccfeb40fdffd8b2105f}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Yu},
           giveni={Y\bibinitperiod}}}%
        {{hash=b294f0d3a2c6bab74ae96ed8c1a11b01}{%
           family={Bayen},
           familyi={B\bibinitperiod},
           given={Alexandre},
           giveni={A\bibinitperiod}}}%
        {{hash=e2101a0f6a72a2fb022cd3e2d45461e1}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Yi},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{e8e7ddbab774078659f78cda4aa60fe4}
      \strng{fullhash}{606d06502f6751690748eb76e0b730b7}
      \strng{fullhashraw}{606d06502f6751690748eb76e0b730b7}
      \strng{bibnamehash}{e8e7ddbab774078659f78cda4aa60fe4}
      \strng{authorbibnamehash}{e8e7ddbab774078659f78cda4aa60fe4}
      \strng{authornamehash}{e8e7ddbab774078659f78cda4aa60fe4}
      \strng{authorfullhash}{606d06502f6751690748eb76e0b730b7}
      \strng{authorfullhashraw}{606d06502f6751690748eb76e0b730b7}
      \field{sortinit}{8}
      \field{sortinithash}{a231b008ebf0ecbe0b4d96dcc159445f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Proximal Policy Optimization (PPO) is a ubiquitous on-policy reinforcement learning algorithm but is significantly less utilized than off-policy learning algorithms in multi-agent settings. This is often due to the belief that PPO is significantly less sample efficient than off-policy methods in multi-agent systems. In this work, we carefully study the performance of PPO in cooperative multi-agent settings. We show that PPO-based multi-agent algorithms achieve surprisingly strong performance in four popular multi-agent testbeds: the particle-world environments, the StarCraft multi-agent challenge, Google Research Football, and the Hanabi challenge, with minimal hyperparameter tuning and without any domain-specific algorithmic modifications or architectures. Importantly, compared to competitive off-policy methods, PPO often achieves competitive or superior results in both final returns and sample efficiency. Finally, through ablation studies, we analyze implementation and hyperparameter factors that are critical to PPO's empirical performance, and give concrete practical suggestions regarding these factors. Our results show that when using these practices, simple PPO-based methods can be a strong baseline in cooperative multi-agent reinforcement learning. Source code is released at \textbackslash url\{https://github.com/marlbenchmark/on-policy\}.}
      \field{day}{4}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{11}
      \field{pubstate}{prepublished}
      \field{title}{The {{Surprising Effectiveness}} of {{PPO}} in {{Cooperative}}, {{Multi-Agent Games}}}
      \field{urlday}{25}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2103.01955
      \endverb
      \verb{eprint}
      \verb 2103.01955
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/SDH5KMVS/Yu et al_2022_The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games.pdf;/Users/brandonhosley/Zotero/storage/3KM2DXK5/2103.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2103.01955
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2103.01955
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems}
    \endentry
    \entry{espeholt2018}{online}{}{}
      \name{author}{12}{}{%
        {{hash=31f15faeab280fcda1466b9710381955}{%
           family={Espeholt},
           familyi={E\bibinitperiod},
           given={Lasse},
           giveni={L\bibinitperiod}}}%
        {{hash=d7617682498ee9735efdd8bf09b26c0f}{%
           family={Soyer},
           familyi={S\bibinitperiod},
           given={Hubert},
           giveni={H\bibinitperiod}}}%
        {{hash=ce2c8d8bf1e96e9845a930fb6264204f}{%
           family={Munos},
           familyi={M\bibinitperiod},
           given={Remi},
           giveni={R\bibinitperiod}}}%
        {{hash=9d16b7284df92c9adaee86c37ab992df}{%
           family={Simonyan},
           familyi={S\bibinitperiod},
           given={Karen},
           giveni={K\bibinitperiod}}}%
        {{hash=1a7c6a8be0f4fe186ecfa6cf2525699b}{%
           family={Mnih},
           familyi={M\bibinitperiod},
           given={Volodymir},
           giveni={V\bibinitperiod}}}%
        {{hash=acbc538c39a6208410bc3c1f6ed25386}{%
           family={Ward},
           familyi={W\bibinitperiod},
           given={Tom},
           giveni={T\bibinitperiod}}}%
        {{hash=237b733d6914e7eb0f991903046d13a3}{%
           family={Doron},
           familyi={D\bibinitperiod},
           given={Yotam},
           giveni={Y\bibinitperiod}}}%
        {{hash=1015dfb797dbe89a93236b472ff6fa95}{%
           family={Firoiu},
           familyi={F\bibinitperiod},
           given={Vlad},
           giveni={V\bibinitperiod}}}%
        {{hash=b63b9b2d91f6ba1b1739563edc0432ab}{%
           family={Harley},
           familyi={H\bibinitperiod},
           given={Tim},
           giveni={T\bibinitperiod}}}%
        {{hash=6a09fd9d4b66412cec2802755aa5942e}{%
           family={Dunning},
           familyi={D\bibinitperiod},
           given={Iain},
           giveni={I\bibinitperiod}}}%
        {{hash=afdf5ed50a24cdca0f42433e4f4848d5}{%
           family={Legg},
           familyi={L\bibinitperiod},
           given={Shane},
           giveni={S\bibinitperiod}}}%
        {{hash=481308b301166b521c74fde6566e97e6}{%
           family={Kavukcuoglu},
           familyi={K\bibinitperiod},
           given={Koray},
           giveni={K\bibinitperiod}}}%
      }
      \strng{namehash}{99bb96646b2245368bf0300a789bc2a9}
      \strng{fullhash}{263d1c4be977a3712ed6a6c202639fa9}
      \strng{fullhashraw}{263d1c4be977a3712ed6a6c202639fa9}
      \strng{bibnamehash}{99bb96646b2245368bf0300a789bc2a9}
      \strng{authorbibnamehash}{99bb96646b2245368bf0300a789bc2a9}
      \strng{authornamehash}{99bb96646b2245368bf0300a789bc2a9}
      \strng{authorfullhash}{263d1c4be977a3712ed6a6c202639fa9}
      \strng{authorfullhashraw}{263d1c4be977a3712ed6a6c202639fa9}
      \field{sortinit}{9}
      \field{sortinithash}{0a5ebc79d83c96b6579069544c73c7d4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.}
      \field{day}{28}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{6}
      \field{pubstate}{prepublished}
      \field{shorttitle}{{{IMPALA}}}
      \field{title}{{{IMPALA}}: {{Scalable Distributed Deep-RL}} with {{Importance Weighted Actor-Learner Architectures}}}
      \field{urlday}{26}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1802.01561
      \endverb
      \verb{eprint}
      \verb 1802.01561
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/HLB5SX6X/Espeholt et al_2018_IMPALA.pdf;/Users/brandonhosley/Zotero/storage/L8QQH9KD/1802.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1802.01561
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1802.01561
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
    \endentry
    \entry{samvelyan2019}{online}{useprefix=true}{}
      \name{author}{10}{}{%
        {{hash=c76172ee0c2ebda81796624b4a81ae54}{%
           family={Samvelyan},
           familyi={S\bibinitperiod},
           given={Mikayel},
           giveni={M\bibinitperiod}}}%
        {{hash=8eb7df23daba9c7bf22402026effdae7}{%
           family={Rashid},
           familyi={R\bibinitperiod},
           given={Tabish},
           giveni={T\bibinitperiod}}}%
        {{hash=d056cdb8ed6bebb55e13c2246059af79}{%
           family={Witt},
           familyi={W\bibinitperiod},
           given={Christian\bibnamedelima Schroeder},
           giveni={C\bibinitperiod\bibinitdelim S\bibinitperiod},
           prefix={de},
           prefixi={d\bibinitperiod}}}%
        {{hash=de7fc27bb6e6105440cd3039a5c9b684}{%
           family={Farquhar},
           familyi={F\bibinitperiod},
           given={Gregory},
           giveni={G\bibinitperiod}}}%
        {{hash=a07891d2d6d3da5f1827c01269cc6da6}{%
           family={Nardelli},
           familyi={N\bibinitperiod},
           given={Nantas},
           giveni={N\bibinitperiod}}}%
        {{hash=2b8a7b8bb154710457a1fe68adc9cb07}{%
           family={Rudner},
           familyi={R\bibinitperiod},
           given={Tim\bibnamedelimb G.\bibnamedelimi J.},
           giveni={T\bibinitperiod\bibinitdelim G\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=ca083f59b63a6ee2837027775301af6b}{%
           family={Hung},
           familyi={H\bibinitperiod},
           given={Chia-Man},
           giveni={C\bibinithyphendelim M\bibinitperiod}}}%
        {{hash=3269ee96fb4b61a52f4c01b02fc0751c}{%
           family={Torr},
           familyi={T\bibinitperiod},
           given={Philip\bibnamedelimb H.\bibnamedelimi S.},
           giveni={P\bibinitperiod\bibinitdelim H\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=d62c75690ab7c754f6e7217a242a4318}{%
           family={Foerster},
           familyi={F\bibinitperiod},
           given={Jakob},
           giveni={J\bibinitperiod}}}%
        {{hash=0d05819cf2b4fe22ba972c9b2b5d8c9d}{%
           family={Whiteson},
           familyi={W\bibinitperiod},
           given={Shimon},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{db017d19c7fbc59e66933da4bbd4deeb}
      \strng{fullhash}{439111c8508bc2979f9a0c48a59c96da}
      \strng{fullhashraw}{439111c8508bc2979f9a0c48a59c96da}
      \strng{bibnamehash}{db017d19c7fbc59e66933da4bbd4deeb}
      \strng{authorbibnamehash}{db017d19c7fbc59e66933da4bbd4deeb}
      \strng{authornamehash}{db017d19c7fbc59e66933da4bbd4deeb}
      \strng{authorfullhash}{439111c8508bc2979f9a0c48a59c96da}
      \strng{authorfullhashraw}{439111c8508bc2979f9a0c48a59c96da}
      \field{sortinit}{9}
      \field{sortinithash}{0a5ebc79d83c96b6579069544c73c7d4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In the last few years, deep multi-agent reinforcement learning (RL) has become a highly active area of research. A particularly challenging class of problems in this area is partially observable, cooperative, multi-agent learning, in which teams of agents must learn to coordinate their behaviour while conditioning only on their private observations. This is an attractive research area since such problems are relevant to a large number of real-world systems and are also more amenable to evaluation than general-sum problems. Standardised environments such as the ALE and MuJoCo have allowed single-agent RL to move beyond toy domains, such as grid worlds. However, there is no comparable benchmark for cooperative multi-agent RL. As a result, most papers in this field use one-off toy problems, making it difficult to measure real progress. In this paper, we propose the StarCraft Multi-Agent Challenge (SMAC) as a benchmark problem to fill this gap. SMAC is based on the popular real-time strategy game StarCraft II and focuses on micromanagement challenges where each unit is controlled by an independent agent that must act based on local observations. We offer a diverse set of challenge maps and recommendations for best practices in benchmarking and evaluations. We also open-source a deep multi-agent RL learning framework including state-of-the-art algorithms. We believe that SMAC can provide a standard benchmark environment for years to come. Videos of our best agents for several SMAC scenarios are available at: https://youtu.be/VZ7zmQ\_obZ0.}
      \field{day}{9}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arXiv}
      \field{month}{12}
      \field{pubstate}{prepublished}
      \field{title}{The {{StarCraft Multi-Agent Challenge}}}
      \field{urlday}{26}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1902.04043
      \endverb
      \verb{eprint}
      \verb 1902.04043
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/TJCL63RQ/Samvelyan et al_2019_The StarCraft Multi-Agent Challenge.pdf;/Users/brandonhosley/Zotero/storage/N4RKHKPB/1902.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1902.04043
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1902.04043
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning}
    \endentry
    \entry{ellis2023}{inproceedings}{}{}
      \name{author}{8}{}{%
        {{hash=8013a1eb9ec691ea154fb9523884e3b2}{%
           family={Ellis},
           familyi={E\bibinitperiod},
           given={Benjamin},
           giveni={B\bibinitperiod}}}%
        {{hash=a071fde4f0800a3a8483eed7d8a4381f}{%
           family={Cook},
           familyi={C\bibinitperiod},
           given={Jonathan},
           giveni={J\bibinitperiod}}}%
        {{hash=f701e0d41900a34dfdf2a9e51698dde7}{%
           family={Moalla},
           familyi={M\bibinitperiod},
           given={Skander},
           giveni={S\bibinitperiod}}}%
        {{hash=c76172ee0c2ebda81796624b4a81ae54}{%
           family={Samvelyan},
           familyi={S\bibinitperiod},
           given={Mikayel},
           giveni={M\bibinitperiod}}}%
        {{hash=709c3aa92386ace0c21c1eec7c2ab736}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Mingfei},
           giveni={M\bibinitperiod}}}%
        {{hash=ac63a3aecba18da9492226922d51b6ae}{%
           family={Mahajan},
           familyi={M\bibinitperiod},
           given={Anuj},
           giveni={A\bibinitperiod}}}%
        {{hash=265add78d7143f911d5b2a0b954ff3c2}{%
           family={Foerster},
           familyi={F\bibinitperiod},
           given={Jakob\bibnamedelima Nicolaus},
           giveni={J\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
        {{hash=0d05819cf2b4fe22ba972c9b2b5d8c9d}{%
           family={Whiteson},
           familyi={W\bibinitperiod},
           given={Shimon},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{473e2569a38c6a23206dba318d2462f6}
      \strng{fullhash}{cf3af7798e5cdeca6f6c23b21529b92e}
      \strng{fullhashraw}{cf3af7798e5cdeca6f6c23b21529b92e}
      \strng{bibnamehash}{473e2569a38c6a23206dba318d2462f6}
      \strng{authorbibnamehash}{473e2569a38c6a23206dba318d2462f6}
      \strng{authornamehash}{473e2569a38c6a23206dba318d2462f6}
      \strng{authorfullhash}{cf3af7798e5cdeca6f6c23b21529b92e}
      \strng{authorfullhashraw}{cf3af7798e5cdeca6f6c23b21529b92e}
      \field{sortinit}{9}
      \field{sortinithash}{0a5ebc79d83c96b6579069544c73c7d4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The availability of challenging benchmarks has played a key role in the recent progress of machine learning. In cooperative multi-agent reinforcement learning, the StarCraft Multi-Agent Challenge (SMAC) has become a popular testbed for centralised training with decentralised execution. However, after years of sustained improvement on SMAC, algorithms now achieve near-perfect performance. In this work, we conduct new analysis demonstrating that SMAC lacks the stochasticity and partial observability to require complex *closed-loop* policies. In particular, we show that an *open-loop* policy conditioned only on the timestep can achieve non-trivial win rates for many SMAC scenarios. To address this limitation, we introduce SMACv2, a new version of the benchmark where scenarios are procedurally generated and require agents to generalise to previously unseen settings (from the same distribution) during evaluation. We also introduce the extended partial observability challenge (EPO), which augments SMACv2 to ensure meaningful partial observability. We show that these changes ensure the benchmark requires the use of *closed-loop* policies. We evaluate state-of-the-art algorithms on SMACv2 and show that it presents significant challenges not present in the original benchmark. Our analysis illustrates that SMACv2 addresses the discovered deficiencies of SMAC and can help benchmark the next generation of MARL methods. Videos of training are available on our [website](https://sites.google.com/view/smacv2).}
      \field{day}{2}
      \field{eventtitle}{Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}}}
      \field{langid}{english}
      \field{month}{11}
      \field{shorttitle}{{{SMACv2}}}
      \field{title}{{{SMACv2}}: {{An Improved Benchmark}} for {{Cooperative Multi-Agent Reinforcement Learning}}}
      \field{urlday}{24}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/JQSY6GHU/Ellis et al_2023_SMACv2.pdf
      \endverb
      \verb{urlraw}
      \verb https://openreview.net/forum?id=5OjLGiJW3u
      \endverb
      \verb{url}
      \verb https://openreview.net/forum?id=5OjLGiJW3u
      \endverb
    \endentry
    \entry{gupta2017}{inproceedings}{}{}
      \name{author}{3}{}{%
        {{hash=fe52f7f3365727a86a07b7c519c958b1}{%
           family={Gupta},
           familyi={G\bibinitperiod},
           given={Jayesh\bibnamedelima K},
           giveni={J\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=f54d2cf50019ddff2faf8a3ff0b086fd}{%
           family={Egorov},
           familyi={E\bibinitperiod},
           given={Maxim},
           giveni={M\bibinitperiod}}}%
        {{hash=e36ad022497396d3ca41ea1594ac09ec}{%
           family={Kochenderfer},
           familyi={K\bibinitperiod},
           given={Mykel},
           giveni={M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{d9d07e34ca39a7a3342b5a69a6e64a11}
      \strng{fullhash}{e96796c55326b509d6061753f49c4b82}
      \strng{fullhashraw}{e96796c55326b509d6061753f49c4b82}
      \strng{bibnamehash}{e96796c55326b509d6061753f49c4b82}
      \strng{authorbibnamehash}{e96796c55326b509d6061753f49c4b82}
      \strng{authornamehash}{d9d07e34ca39a7a3342b5a69a6e64a11}
      \strng{authorfullhash}{e96796c55326b509d6061753f49c4b82}
      \strng{authorfullhashraw}{e96796c55326b509d6061753f49c4b82}
      \field{extraname}{1}
      \field{sortinit}{9}
      \field{sortinithash}{0a5ebc79d83c96b6579069544c73c7d4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{International Conference on Autonomous Agents and Multiagent Systems}
      \field{title}{Cooperative Multi-Agent Control Using Deep Reinforcement Learning}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{pages}{66\bibrangedash 83}
      \range{pages}{18}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/2FJVAMV9/Gupta et al_2017_Cooperative multi-agent control using deep reinforcement learning.pdf
      \endverb
    \endentry
    \entry{hernandez-leal2019}{article}{}{}
      \name{author}{3}{}{%
        {{hash=994e8fe8d289061af2ca9d999708550f}{%
           family={Hernandez-Leal},
           familyi={H\bibinithyphendelim L\bibinitperiod},
           given={Pablo},
           giveni={P\bibinitperiod}}}%
        {{hash=8403ec4af976c188e59539f83a7c2efe}{%
           family={Kartal},
           familyi={K\bibinitperiod},
           given={Bilal},
           giveni={B\bibinitperiod}}}%
        {{hash=986ede3e90e206372b7eeb16dddc9ac6}{%
           family={Taylor},
           familyi={T\bibinitperiod},
           given={Matthew\bibnamedelima E.},
           giveni={M\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \strng{namehash}{4a8f2946dd773bb282e2bdbc66de37e6}
      \strng{fullhash}{bab3c5e694c4decb4811c5e79333ccb9}
      \strng{fullhashraw}{bab3c5e694c4decb4811c5e79333ccb9}
      \strng{bibnamehash}{bab3c5e694c4decb4811c5e79333ccb9}
      \strng{authorbibnamehash}{bab3c5e694c4decb4811c5e79333ccb9}
      \strng{authornamehash}{4a8f2946dd773bb282e2bdbc66de37e6}
      \strng{authorfullhash}{bab3c5e694c4decb4811c5e79333ccb9}
      \strng{authorfullhashraw}{bab3c5e694c4decb4811c5e79333ccb9}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep reinforcement learning (RL) has achieved outstanding results in recent years. This has led to a dramatic increase in the number of applications and methods. Recent works have explored learning beyond single-agent scenarios and have considered multiagent learning (MAL) scenarios. Initial results report successes in complex multiagent domains, although there are several challenges to be addressed. The primary goal of this article is to provide a clear overview of current multiagent deep reinforcement learning (MDRL) literature. Additionally, we complement the overview with a broader analysis: (i) we revisit previous key components, originally presented in MAL and RL, and highlight how they have been adapted to multiagent deep reinforcement learning settings. (ii) We provide general guidelines to new practitioners in the area: describing lessons learned from MDRL works, pointing to recent benchmarks, and outlining open avenues of research. (iii) We take a more critical tone raising practical challenges of MDRL (e.g., implementation and computational demands). We expect this article will help unify and motivate future research to take advantage of the abundant literature that exists (e.g., RL and MAL) in a joint effort to promote fruitful research in the multiagent community.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{issn}{1387-2532, 1573-7454}
      \field{journaltitle}{Autonomous Agents and Multi-Agent Systems}
      \field{month}{11}
      \field{number}{6}
      \field{shortjournal}{Auton Agent Multi-Agent Syst}
      \field{title}{A {{Survey}} and {{Critique}} of {{Multiagent Deep Reinforcement Learning}}}
      \field{urlday}{26}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{volume}{33}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{750\bibrangedash 797}
      \range{pages}{48}
      \verb{doi}
      \verb 10.1007/s10458-019-09421-1
      \endverb
      \verb{eprint}
      \verb 1810.05587
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/8DGHEDHI/Hernandez-Leal et al_2019_A Survey and Critique of Multiagent Deep Reinforcement Learning.pdf;/Users/brandonhosley/Zotero/storage/BY8BGNZ9/1810.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1810.05587
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1810.05587
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems}
    \endentry
    \entry{mohddaud2022}{article}{}{}
      \name{author}{7}{}{%
        {{hash=27b0c51140f71a45e7b1d28684b48920}{%
           family={Mohd\bibnamedelima Daud},
           familyi={M\bibinitperiod\bibinitdelim D\bibinitperiod},
           given={Sharifah\bibnamedelimb Mastura\bibnamedelima Syed},
           giveni={S\bibinitperiod\bibinitdelim M\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=5a23b028ddd68b282fe8d8bc2d0edb75}{%
           family={Mohd\bibnamedelima Yusof},
           familyi={M\bibinitperiod\bibinitdelim Y\bibinitperiod},
           given={Mohd\bibnamedelimb Yusmiaidil\bibnamedelima Putera},
           giveni={M\bibinitperiod\bibinitdelim Y\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=529324ed7ab598b3437467a720152e3b}{%
           family={Heo},
           familyi={H\bibinitperiod},
           given={Chong\bibnamedelima Chin},
           giveni={C\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
        {{hash=0927254108c11c377c69d7290f3513c2}{%
           family={Khoo},
           familyi={K\bibinitperiod},
           given={Lay\bibnamedelima See},
           giveni={L\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=bd159b60cc70242f4f80bc1f5d6bcdc9}{%
           family={Chainchel\bibnamedelima Singh},
           familyi={C\bibinitperiod\bibinitdelim S\bibinitperiod},
           given={Mansharan\bibnamedelima Kaur},
           giveni={M\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=4091835b73c7dc03a8d09418e070b5af}{%
           family={Mahmood},
           familyi={M\bibinitperiod},
           given={Mohd\bibnamedelima Shah},
           giveni={M\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=b0ef36e956f29b948a45125d0eb198ba}{%
           family={Nawawi},
           familyi={N\bibinitperiod},
           given={Hapizah},
           giveni={H\bibinitperiod}}}%
      }
      \strng{namehash}{50c55015234cd8812c36d70920db1c8c}
      \strng{fullhash}{fd2c5cc94f6fc748ca162ef88b7d5da6}
      \strng{fullhashraw}{fd2c5cc94f6fc748ca162ef88b7d5da6}
      \strng{bibnamehash}{50c55015234cd8812c36d70920db1c8c}
      \strng{authorbibnamehash}{50c55015234cd8812c36d70920db1c8c}
      \strng{authornamehash}{50c55015234cd8812c36d70920db1c8c}
      \strng{authorfullhash}{fd2c5cc94f6fc748ca162ef88b7d5da6}
      \strng{authorfullhashraw}{fd2c5cc94f6fc748ca162ef88b7d5da6}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The use of drones has rapidly evolved over the past decade involving a variety of fields ranging from agriculture, commercial and becoming increasingly used in disaster management or humanitarian aid. Unfortunately, the evidence of its use in mass disasters is still unclear and scarce. This article aims to evaluate the current drone feasibility projects and to discuss a number of challenges related to the deployment of drones in mass disasters in the hopes of empowering and inspiring possible future work. This research follows Arksey and O'Malley framework and updated by Joanna Briggs Institute Framework for Scoping Reviews methodology to summarise the results of 52 research papers over the past ten years, from 2009 to 2020, outlining the research trend of drone application in disaster. A literature search was performed in Medline, CINAHL, Scopus, individual journals, grey literature and google search with assessment based on their content and significance. Potential application of drones in disaster are broad. Based on articles identified, drone application in disasters are classified into four categories; (1) mapping or disaster management which has shown the highest contribution, (2) search and rescue, (3) transportation and (4) training. Although there is a significant increase in the number of publications on use of drone in disaster within the last five years, there is however limited discussion to address post-disaster healthcare situation especially with regards to disaster victim identification. It is evident that drone applications need to be further explored; to focus more on drone assistance to humans especially in victim identification. It is envisaged that with sufficient development, the application of drones appears to be promising and will improve their effectiveness especially in disaster management.}
      \field{day}{1}
      \field{issn}{1355-0306}
      \field{journaltitle}{Science \& Justice}
      \field{month}{1}
      \field{number}{1}
      \field{shortjournal}{Science \& Justice}
      \field{shorttitle}{Applications of Drone in Disaster Management}
      \field{title}{Applications of Drone in Disaster Management: {{A}} Scoping Review}
      \field{urlday}{16}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{62}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{30\bibrangedash 42}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1016/j.scijus.2021.11.002
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S1355030621001477
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S1355030621001477
      \endverb
      \keyw{Disaster,Drones,Humanitarian aid,UAV}
    \endentry
    \entry{hambling2021}{online}{}{}
      \name{author}{1}{}{%
        {{hash=48366d2d6dd4dcea134d9ea9b79a0268}{%
           family={Hambling},
           familyi={H\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
      }
      \list{organization}{1}{%
        {Forbes}%
      }
      \strng{namehash}{48366d2d6dd4dcea134d9ea9b79a0268}
      \strng{fullhash}{48366d2d6dd4dcea134d9ea9b79a0268}
      \strng{fullhashraw}{48366d2d6dd4dcea134d9ea9b79a0268}
      \strng{bibnamehash}{48366d2d6dd4dcea134d9ea9b79a0268}
      \strng{authorbibnamehash}{48366d2d6dd4dcea134d9ea9b79a0268}
      \strng{authornamehash}{48366d2d6dd4dcea134d9ea9b79a0268}
      \strng{authorfullhash}{48366d2d6dd4dcea134d9ea9b79a0268}
      \strng{authorfullhashraw}{48366d2d6dd4dcea134d9ea9b79a0268}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A slew of countries have announced military drone swarm projects in the last few weeks. Here's a primer on what swarms are, how they work and the advantages they bring.}
      \field{day}{1}
      \field{langid}{english}
      \field{month}{3}
      \field{title}{What {{Are Drone Swarms And Why Does Every Military Suddenly Want One}}?}
      \field{urlday}{22}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/N3AQWPT3/what-are-drone-swarms-and-why-does-everyone-suddenly-want-one.html
      \endverb
      \verb{urlraw}
      \verb https://www.forbes.com/sites/davidhambling/2021/03/01/what-are-drone-swarms-and-why-does-everyone-suddenly-want-one/
      \endverb
      \verb{url}
      \verb https://www.forbes.com/sites/davidhambling/2021/03/01/what-are-drone-swarms-and-why-does-everyone-suddenly-want-one/
      \endverb
    \endentry
    \entry{rogers2022}{online}{}{}
      \name{author}{1}{}{%
        {{hash=f5206ca06f64da7e9c2ba00f43b17659}{%
           family={Rogers},
           familyi={R\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod}}}%
      }
      \list{organization}{1}{%
        {Centre for International Governance Innovation}%
      }
      \strng{namehash}{f5206ca06f64da7e9c2ba00f43b17659}
      \strng{fullhash}{f5206ca06f64da7e9c2ba00f43b17659}
      \strng{fullhashraw}{f5206ca06f64da7e9c2ba00f43b17659}
      \strng{bibnamehash}{f5206ca06f64da7e9c2ba00f43b17659}
      \strng{authorbibnamehash}{f5206ca06f64da7e9c2ba00f43b17659}
      \strng{authornamehash}{f5206ca06f64da7e9c2ba00f43b17659}
      \strng{authorfullhash}{f5206ca06f64da7e9c2ba00f43b17659}
      \strng{authorfullhashraw}{f5206ca06f64da7e9c2ba00f43b17659}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The Third Drone Age is characterized by non-state actors using the latest advancements in drone technologies to pursue their political objectives.}
      \field{day}{28}
      \field{langid}{english}
      \field{month}{11}
      \field{shorttitle}{The {{Third Drone Age}}}
      \field{title}{The {{Third Drone Age}}: {{Visions Out}} to 2040}
      \field{urlday}{22}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/JYUJBM8F/the-third-drone-age-visions-out-to-2040.html
      \endverb
      \verb{urlraw}
      \verb https://www.cigionline.org/articles/the-third-drone-age-visions-out-to-2040/
      \endverb
      \verb{url}
      \verb https://www.cigionline.org/articles/the-third-drone-age-visions-out-to-2040/
      \endverb
    \endentry
    \entry{kallenborn2024}{online}{}{}
      \name{author}{1}{}{%
        {{hash=139eeea638a49da3badcc8cdab2c4217}{%
           family={Kallenborn},
           familyi={K\bibinitperiod},
           given={Zachary},
           giveni={Z\bibinitperiod}}}%
      }
      \strng{namehash}{139eeea638a49da3badcc8cdab2c4217}
      \strng{fullhash}{139eeea638a49da3badcc8cdab2c4217}
      \strng{fullhashraw}{139eeea638a49da3badcc8cdab2c4217}
      \strng{bibnamehash}{139eeea638a49da3badcc8cdab2c4217}
      \strng{authorbibnamehash}{139eeea638a49da3badcc8cdab2c4217}
      \strng{authornamehash}{139eeea638a49da3badcc8cdab2c4217}
      \strng{authorfullhash}{139eeea638a49da3badcc8cdab2c4217}
      \strng{authorfullhashraw}{139eeea638a49da3badcc8cdab2c4217}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{A 2020 New America report cataloged thirty-eight states with armed drone programs, twenty-eight with programs in development, and eleven that have used drones in combat. In less than four years since that report was published, drones’ rapidly growing influence on battlefields from Nagorno-Karabakh to Ukraine to Gaza has almost certainly increased states’ interest in developing}
      \field{day}{20}
      \field{hour}{10}
      \field{langid}{american}
      \field{minute}{29}
      \field{month}{3}
      \field{second}{33}
      \field{shorttitle}{Swarm {{Clouds}} on the {{Horizon}}?}
      \field{timezone}{Z}
      \field{title}{Swarm {{Clouds}} on the {{Horizon}}? {{Exploring}} the {{Future}} of {{Drone Swarm Proliferation}} - {{Modern War Institute}}}
      \field{urlday}{22}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/HA8V2ZYG/swarm-clouds-on-the-horizon-exploring-the-future-of-drone-swarm-proliferation.html
      \endverb
      \verb{urlraw}
      \verb https://mwi.westpoint.edu/swarm-clouds-on-the-horizon-exploring-the-future-of-drone-swarm-proliferation/, https://mwi.westpoint.edu/swarm-clouds-on-the-horizon-exploring-the-future-of-drone-swarm-proliferation/
      \endverb
      \verb{url}
      \verb https://mwi.westpoint.edu/swarm-clouds-on-the-horizon-exploring-the-future-of-drone-swarm-proliferation/,%20https://mwi.westpoint.edu/swarm-clouds-on-the-horizon-exploring-the-future-of-drone-swarm-proliferation/
      \endverb
    \endentry
    \entry{zotero-2835}{online}{}{}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labeltitlesource}{title}
      \field{title}{{{OFFSET Swarm Systems Integrators Demonstrate Tactics}} to {{Conduct Urban Raid}}}
      \field{urlday}{16}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/8ENUG5EW/offset-swarm-urban-raid.html
      \endverb
      \verb{urlraw}
      \verb https://www.darpa.mil/news/2020/offset-swarm-urban-raid
      \endverb
      \verb{url}
      \verb https://www.darpa.mil/news/2020/offset-swarm-urban-raid
      \endverb
    \endentry
    \entry{jin2025}{online}{}{}
      \name{author}{6}{}{%
        {{hash=e50468811cd22d27b53d5663a4310021}{%
           family={Jin},
           familyi={J\bibinitperiod},
           given={Weiqiang},
           giveni={W\bibinitperiod}}}%
        {{hash=0a33edcd9cf079959a9301994561b007}{%
           family={Du},
           familyi={D\bibinitperiod},
           given={Hongyang},
           giveni={H\bibinitperiod}}}%
        {{hash=e0a8441ea5e9cb783ba0bcb3fc549126}{%
           family={Zhao},
           familyi={Z\bibinitperiod},
           given={Biao},
           giveni={B\bibinitperiod}}}%
        {{hash=4ce2153d6b98c2c5f0c681ad4c0dbeeb}{%
           family={Tian},
           familyi={T\bibinitperiod},
           given={Xingwu},
           giveni={X\bibinitperiod}}}%
        {{hash=1e3d82466f2347b8ede6c970ceab13b5}{%
           family={Shi},
           familyi={S\bibinitperiod},
           given={Bohang},
           giveni={B\bibinitperiod}}}%
        {{hash=11a33b1aa9e89479e304318dff521b9c}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Guang},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{2f0d809d993de22304f454b21f857387}
      \strng{fullhash}{7dd07e5f7cc5a6b58a95c3de84648976}
      \strng{fullhashraw}{7dd07e5f7cc5a6b58a95c3de84648976}
      \strng{bibnamehash}{7dd07e5f7cc5a6b58a95c3de84648976}
      \strng{authorbibnamehash}{7dd07e5f7cc5a6b58a95c3de84648976}
      \strng{authornamehash}{2f0d809d993de22304f454b21f857387}
      \strng{authorfullhash}{7dd07e5f7cc5a6b58a95c3de84648976}
      \strng{authorfullhashraw}{7dd07e5f7cc5a6b58a95c3de84648976}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{With the rapid development of artificial intelligence, intelligent decision-making techniques have gradually surpassed human levels in various human-machine competitions, especially in complex multi-agent cooperative task scenarios. Multi-agent cooperative decision-making involves multiple agents working together to complete established tasks and achieve specific objectives. These techniques are widely applicable in real-world scenarios such as autonomous driving, drone navigation, disaster rescue, and simulated military confrontations. This paper begins with a comprehensive survey of the leading simulation environments and platforms used for multi-agent cooperative decision-making. Specifically, we provide an in-depth analysis for these simulation environments from various perspectives, including task formats, reward allocation, and the underlying technologies employed. Subsequently, we provide a comprehensive overview of the mainstream intelligent decision-making approaches, algorithms and models for multi-agent systems (MAS). Theseapproaches can be broadly categorized into five types: rule-based (primarily fuzzy logic), game theory-based, evolutionary algorithms-based, deep multi-agent reinforcement learning (MARL)-based, and large language models(LLMs)reasoning-based. Given the significant advantages of MARL andLLMs-baseddecision-making methods over the traditional rule, game theory, and evolutionary algorithms, this paper focuses on these multi-agent methods utilizing MARL and LLMs-based techniques. We provide an in-depth discussion of these approaches, highlighting their methodology taxonomies, advantages, and drawbacks. Further, several prominent research directions in the future and potential challenges of multi-agent cooperative decision-making are also detailed.}
      \field{day}{17}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{3}
      \field{pubstate}{prepublished}
      \field{shorttitle}{A {{Comprehensive Survey}} on {{Multi-Agent Cooperative Decision-Making}}}
      \field{title}{A {{Comprehensive Survey}} on {{Multi-Agent Cooperative Decision-Making}}: {{Scenarios}}, {{Approaches}}, {{Challenges}} and {{Perspectives}}}
      \field{urlday}{16}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{version}{1}
      \field{year}{2025}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2503.13415
      \endverb
      \verb{eprint}
      \verb 2503.13415
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/4VBMU7EK/Jin et al_2025_A Comprehensive Survey on Multi-Agent Cooperative Decision-Making.pdf;/Users/brandonhosley/Zotero/storage/NGAVTUPQ/2503.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2503.13415
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2503.13415
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems}
    \endentry
    \entry{rizk2019}{book}{}{}
      \name{author}{3}{}{%
        {{hash=3bf0bdfcd1ad3a4d6ed3e1a1a5d94555}{%
           family={Rizk},
           familyi={R\bibinitperiod},
           given={Yara},
           giveni={Y\bibinitperiod}}}%
        {{hash=ab9ad0fd1072886327bd8fcf16882d22}{%
           family={Awad},
           familyi={A\bibinitperiod},
           given={Mariette},
           giveni={M\bibinitperiod}}}%
        {{hash=a696139e89c3f6d7152594d068cab433}{%
           family={Tunstel},
           familyi={T\bibinitperiod},
           given={E.},
           giveni={E\bibinitperiod}}}%
      }
      \strng{namehash}{63d6456a06899559626b710300422783}
      \strng{fullhash}{92270213c4e822e5c8cb023f26b5a06b}
      \strng{fullhashraw}{92270213c4e822e5c8cb023f26b5a06b}
      \strng{bibnamehash}{92270213c4e822e5c8cb023f26b5a06b}
      \strng{authorbibnamehash}{92270213c4e822e5c8cb023f26b5a06b}
      \strng{authornamehash}{63d6456a06899559626b710300422783}
      \strng{authorfullhash}{92270213c4e822e5c8cb023f26b5a06b}
      \strng{authorfullhashraw}{92270213c4e822e5c8cb023f26b5a06b}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The emergence of the Internet of things and the wide spread deployment of diverse computing systems have led to the formation of heterogeneous multi-agent systems (MAS) to complete a variety of tasks. Motivated to highlight the state of the art on existing MAS while identifying their limitations, remaining challenges and possible future directions, we survey recent contributions to the field. We focus on robot agents and emphasize the challenges of MAS sub-fields including task decomposition, coalition formation, task allocation, perception, and multi-agent planning and control. While some components have seen more advancements than others, more research is required before effective autonomous MAS can be deployed in real smart city settings that are less restrictive than the assumed validation environments of MAS. Specifically, more autonomous end-to-end solutions need to be experimentally tested and developed while incorporating natural language ontology and dictionaries to automate complex task decomposition and leveraging big data advancements to improve perception algorithms for robotics. Y. Rizk et al. 1 INTRODUCTION The dynamic and unpredictable nature of the world we live in makes it difficult to design one autonomous robot that can efficiently adapt to all circumstances. Therefore, robots of various shapes, sizes and capabilities such as unmanned aerial vehicles (UAVs), unmanned ground vehicles (UGVs), humanoids and others have been designed to cooperate with each other and humans, to successfully accomplish complex tasks. Allowing these diverse connected devices, expected to surpass \$20 billion by 2020 with the emergence of the Internet of things (IoT) [39], to cooperate will significantly increase the spectrum of automated tasks. Integrating these devices in areas such as health care, transportation systems, emergency response systems, household chores, and elderly care, among others will make smart cities even smarter. In this work, we discuss the literature on automating complex tasks using heterogeneous multi-robot systems (MRS), after a brief overview of the more general multi-agent system (MAS) field. We present the main components of a workflow to automate MRS: task decomposition, coalition formation, task allocation, perception, and MAS planning and control, survey existing work in each area and identify some remaining challenges and possible future research directions. However, many aspects of heterogeneous MAS are not covered in this survey. They include, but are not limited to, credit assignment which studies reward distribution among agents [226], consensus which investigates protocols that ensure agent agreement under different circumstances [102, 237], containment control which is a type of consensus in leader-follower models [236] and robot hardware design. Although communication protocols for robot-robot communication which enable agent cooperation through information exchange [30, 56] and the information flow problem which seeks to design efficient information exchange strategies [30] are an important component of MAS, we do not include them in this survey. Instead readers are invited to check the literature of [30, 56]. Few end-to-end frameworks have been presented for MRS, the most notable is swarmanoids [55] which accomplished search and retrieval tasks using a swarm of three types of robots. Many solutions still required significant human intervention to achieve complex tasks. Furthermore, individual aspects of MRS have been tackled, such as giving robots access to information on the cloud [175] and simultaneous coalition formation and task allocation [233]. However, more end-to-end testing on IoT-aided robotics [77] and MRS applications should be conducted to achieve more progress. Natural language ontology and dictionaries could help automate complex task decomposition and big data advancements could be leveraged to improve perception and consequently decision making. Two of the closest surveys to our work were published almost a decade ago; one covered MRS coordination including task allocation, decomposition and resource distribution [54]. However, it focused on market-based approaches and did not include work on coalition formation and decision making models. A more recent survey discussed existing MRS architectures, communication schemes, swarm robotics, task allocation, and learning [157] in applications like foraging, formation control, cooperative object manipulation and displacement, path planning and soccer [157]. Other surveys had a narrower scope than our work and focused on a specific research area such as cooperative MAS planning and control models and algorithms [155] and distributed consensus in MAS [235]. Some surveys focused on MRS and their assigned tasks; Arai et al. identified seven main research areas in MRS, including robot architectures, mapping and exploration, and motion coordination, and discussed state of the art research and challenges in each area [13]. Ota surveyed tasks assigned to MRS, classifying them into point reaching, region sweeping and compound tasks, in addition to one-time and many-time tasks, i.e. tasks that require multiple iterations for completion [152]. Murray surveyed cooperative control of multi-vehicle systems and their applications in the}
      \field{day}{7}
      \field{month}{1}
      \field{shorttitle}{Cooperative {{Heterogeneous Multi-Robot Systems}}}
      \field{title}{Cooperative {{Heterogeneous Multi-Robot Systems}}: {{A Survey}}}
      \field{year}{2019}
      \field{dateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/HNKQIAI6/Rizk et al_2019_Cooperative Heterogeneous Multi-Robot Systems.pdf
      \endverb
    \endentry
    \entry{liang2024}{inproceedings}{}{}
      \name{author}{6}{}{%
        {{hash=ea7a01cea17c40d48aee4d4188ec1f55}{%
           family={Liang},
           familyi={L\bibinitperiod},
           given={Zhiuxan},
           giveni={Z\bibinitperiod}}}%
        {{hash=cd59d436917c02c252d25762a26d0955}{%
           family={Cao},
           familyi={C\bibinitperiod},
           given={Jiannong},
           giveni={J\bibinitperiod}}}%
        {{hash=25ce97132dd55778e1dbf3c9bf2d3a37}{%
           family={Jiang},
           familyi={J\bibinitperiod},
           given={Shan},
           giveni={S\bibinitperiod}}}%
        {{hash=b5eaa8ef61a1b6fe783d35509459e28a}{%
           family={Saxena},
           familyi={S\bibinitperiod},
           given={Divya},
           giveni={D\bibinitperiod}}}%
        {{hash=e08dfef32541890832ffa41226eaac1b}{%
           family={Cao},
           familyi={C\bibinitperiod},
           given={Rui},
           giveni={R\bibinitperiod}}}%
        {{hash=afc4c59ddb7b37930593f3ae117e1d15}{%
           family={Xu},
           familyi={X\bibinitperiod},
           given={Huafeng},
           giveni={H\bibinitperiod}}}%
      }
      \strng{namehash}{4f5b8884406e78871e218a75968b0b16}
      \strng{fullhash}{7e548b150baf7a0ca32b7c250a67c5bb}
      \strng{fullhashraw}{7e548b150baf7a0ca32b7c250a67c5bb}
      \strng{bibnamehash}{7e548b150baf7a0ca32b7c250a67c5bb}
      \strng{authorbibnamehash}{7e548b150baf7a0ca32b7c250a67c5bb}
      \strng{authornamehash}{4f5b8884406e78871e218a75968b0b16}
      \strng{authorfullhash}{7e548b150baf7a0ca32b7c250a67c5bb}
      \strng{authorfullhashraw}{7e548b150baf7a0ca32b7c250a67c5bb}
      \field{extraname}{1}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Multi-robot reinforcement learning (MRRL) is a promising approach to solving cooperation problems and has been widely adopted in many applications. In the past decades, researchers have proposed various approaches to improve the efficiency of MRRL. However, most of them are trained and evaluated only in simulated environments with simple interaction scenarios. The problem of how these methods perform in the real-world environment with complex interaction scenarios remains unsolved. To meet this emergent need, we introduce a scalable multi-robot reinforcement learning platform (SMART) for training and evaluation. Specifically, SMART consists of two components: 1) a simulation environment with an uncertainty-aware social agent model that provides a variety of complex interaction scenarios for training and 2) a real-world multi-robot system for realistic performance evaluation. To evaluate the generalizability of MRRL baselines, we introduce a novel generalization metric that takes into account their performance across changes in the environment as well as the policies of other agents. Furthermore, we conduct a case study on the multi-vehicle cooperative lane change and summarize the unique challenges of MRRL, which are rarely considered previously. Finally, we open-source the simulation environments, associated benchmark tasks, and state-of-the-art baselines to encourage and empower MRRL research. Our code is available at https://github.com/Blackmamba-xuan/MRST.}
      \field{booktitle}{2024 {{IEEE}} 30th {{International Conference}} on {{Parallel}} and {{Distributed Systems}} ({{ICPADS}})}
      \field{eventtitle}{2024 {{IEEE}} 30th {{International Conference}} on {{Parallel}} and {{Distributed Systems}} ({{ICPADS}})}
      \field{issn}{2690-5965}
      \field{month}{10}
      \field{shorttitle}{From {{Agents}} to {{Robots}}}
      \field{title}{From {{Agents}} to {{Robots}}: {{A Training}} and {{Evaluation Platform}} for {{Multi-robot Reinforcement Learning}}}
      \field{urlday}{21}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{593\bibrangedash 600}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1109/ICPADS63350.2024.00083
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/VTJ38CWP/10763906.html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/10763906
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/10763906
      \endverb
      \keyw{Autonomous vehicles,Benchmark testing,Codes,Faces,Multi-robot Reinforcement Learning,Multi-robot Simulator,Multi-Robot System,Multi-robot systems,Performance evaluation,Reinforcement learning,Robots,Training}
    \endentry
    \entry{abeywickrama2022}{article}{}{}
      \name{author}{4}{}{%
        {{hash=4f884fa609977e1fa16831b5675b64c0}{%
           family={Abeywickrama},
           familyi={A\bibinitperiod},
           given={Dhaminda\bibnamedelima B.},
           giveni={D\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=17625f2a58799d3a11011e0fd098c2b6}{%
           family={Griffiths},
           familyi={G\bibinitperiod},
           given={Nathan},
           giveni={N\bibinitperiod}}}%
        {{hash=897528556e80e2d0207255647d606299}{%
           family={Xu},
           familyi={X\bibinitperiod},
           given={Zhou},
           giveni={Z\bibinitperiod}}}%
        {{hash=a94f9d5da349879782befc62a9f8ff16}{%
           family={Mouzakitis},
           familyi={M\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{55a8ee9ad9d90ed7e2d453b167c63cdd}
      \strng{fullhash}{2c5dc646b976e4fb07a3453fd95f80bb}
      \strng{fullhashraw}{2c5dc646b976e4fb07a3453fd95f80bb}
      \strng{bibnamehash}{2c5dc646b976e4fb07a3453fd95f80bb}
      \strng{authorbibnamehash}{2c5dc646b976e4fb07a3453fd95f80bb}
      \strng{authornamehash}{55a8ee9ad9d90ed7e2d453b167c63cdd}
      \strng{authorfullhash}{2c5dc646b976e4fb07a3453fd95f80bb}
      \strng{authorfullhashraw}{2c5dc646b976e4fb07a3453fd95f80bb}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Autonomous agents are becoming increasingly ubiquitous and are playing an increasing role in wide range of safety-critical systems, such as driverless cars, exploration robots and unmanned aerial vehicles. These agents operate in highly dynamic and heterogeneous environments, resulting in complex behaviour and interactions. Therefore, the need arises to model and understand more complex and nuanced agent interactions than have previously been studied. In this paper, we propose a novel agent-based modelling approach to investigating norm emergence, in which such interactions can be investigated. To this end, while there may be an ideal set of optimally compatible actions there are also combinations that have positive rewards and are also compatible. Our approach provides a step towards identifying the conditions under which globally compatible norms are likely to emerge in the context of complex rewards. Our model is illustrated using the motivating example of self-driving cars, and we present the scenario of an autonomous vehicle performing a left-turn at a T-intersection.}
      \field{day}{26}
      \field{issn}{1573-7454}
      \field{journaltitle}{Autonomous Agents and Multi-Agent Systems}
      \field{langid}{english}
      \field{month}{10}
      \field{number}{1}
      \field{shortjournal}{Auton Agent Multi-Agent Syst}
      \field{title}{Emergence of Norms in Interactions with Complex Rewards}
      \field{urlday}{18}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{volume}{37}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{2}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1007/s10458-022-09585-3
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/YYXAD6N3/Abeywickrama et al_2022_Emergence of norms in interactions with complex rewards.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s10458-022-09585-3
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s10458-022-09585-3
      \endverb
      \keyw{Agent interactions,Agent-based modelling,Norm emergence,Reinforcement learning}
    \endentry
    \entry{yang2021a}{article}{}{}
      \name{author}{4}{}{%
        {{hash=7efa6952cff713d0987625215131d93a}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Shantian},
           giveni={S\bibinitperiod}}}%
        {{hash=d6f2b26e2b50e389f737db615a2d7427}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Bo},
           giveni={B\bibinitperiod}}}%
        {{hash=be20b75fce95276667b40fe95adc4dd6}{%
           family={Kang},
           familyi={K\bibinitperiod},
           given={Zhongfeng},
           giveni={Z\bibinitperiod}}}%
        {{hash=c9d4f94a15d098764eed5fe05f8dd346}{%
           family={Deng},
           familyi={D\bibinitperiod},
           given={Lihui},
           giveni={L\bibinitperiod}}}%
      }
      \strng{namehash}{aae906ef6d96c7f8b76f5f2b09d48742}
      \strng{fullhash}{a290b86273d352110a228c6f5f663f87}
      \strng{fullhashraw}{a290b86273d352110a228c6f5f663f87}
      \strng{bibnamehash}{a290b86273d352110a228c6f5f663f87}
      \strng{authorbibnamehash}{a290b86273d352110a228c6f5f663f87}
      \strng{authornamehash}{aae906ef6d96c7f8b76f5f2b09d48742}
      \strng{authorfullhash}{a290b86273d352110a228c6f5f663f87}
      \strng{authorfullhashraw}{a290b86273d352110a228c6f5f663f87}
      \field{extraname}{1}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Multi-agent deep reinforcement learning (MDRL) has been widely applied in multi-intersection traffic signal control. The MDRL algorithms produce the decentralized cooperative traffic-signal policies via specialized multi-agent settings in certain traffic networks. However, the state-of-the-art MDRL algorithms seem to have some drawbacks. (1) It is desirable that the traffic-signal policies can be smoothly transferred to diverse traffic networks, however, the adopted specialized multi-agent settings hinder the traffic-signal policies to transfer and generalize to new traffic networks. (2) Existing MDRL algorithms which are based on deep neural networks cannot flexibly tackle a time-varying number of vehicles traversing the traffic networks. (3) Existing MDRL algorithms which are based on homogeneous graph neural networks fail to capture the heterogeneous features of objects in traffic networks. Motivated by the above observations, in this paper, we propose an algorithm, referred to as Inductive Heterogeneous Graph Multi-agent Actor–critic (IHG-MA) algorithm, for multi-intersection traffic signal control. The proposed IHG-MA algorithm has two features: (1) It conducts representation learning using a proposed inductive heterogeneous graph neural network (IHG), which is an inductive algorithm. The proposed IHG algorithm can generate embeddings for previously unseen nodes (e.g., new entry vehicles) and new graphs (e.g., new traffic networks). But unlike the algorithms based on the homogeneous graph neural network, IHG algorithm not only encodes heterogeneous features of each node, but also encodes heterogeneous structural (graph) information. (2) It also conducts policy learning using a proposed multi-agent actor–critic(MA), which is a decentralized cooperative framework. The proposed MA framework employs the final embeddings to compute the Q-value and policy, and then optimizes the whole algorithm via the Q-value and policy loss. Experimental results on different traffic datasets illustrate that IHG-MA algorithm outperforms the state-of-the-art algorithms in terms of multiple traffic metrics, which seems to be a new promising algorithm for multi-intersection traffic signal control.}
      \field{day}{1}
      \field{issn}{0893-6080}
      \field{journaltitle}{Neural Networks}
      \field{month}{7}
      \field{shortjournal}{Neural Networks}
      \field{shorttitle}{{{IHG-MA}}}
      \field{title}{{{IHG-MA}}: {{Inductive}} Heterogeneous Graph Multi-Agent Reinforcement Learning for Multi-Intersection Traffic Signal Control}
      \field{urlday}{14}
      \field{urlmonth}{2}
      \field{urlyear}{2025}
      \field{volume}{139}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{265\bibrangedash 277}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1016/j.neunet.2021.03.015
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/BDL2X5JB/S0893608021000952.html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S0893608021000952
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S0893608021000952
      \endverb
      \keyw{Cooperative traffic signal control,Heterogeneous graph neural network,Inductive heterogeneous graph representation learning,Multi-agent reinforcement learning,Transfer learning}
    \endentry
    \entry{iqbal2021}{online}{}{}
      \name{author}{6}{}{%
        {{hash=7fb7c7ca1c2dd528601fdff6327cb704}{%
           family={Iqbal},
           familyi={I\bibinitperiod},
           given={Shariq},
           giveni={S\bibinitperiod}}}%
        {{hash=33ca447eeab5880cac6e91d2f93b47c1}{%
           family={Witt},
           familyi={W\bibinitperiod},
           given={Christian\bibnamedelimb A.\bibnamedelimi Schroeder},
           giveni={C\bibinitperiod\bibinitdelim A\bibinitperiod\bibinitdelim S\bibinitperiod},
           prefix={de},
           prefixi={d\bibinitperiod}}}%
        {{hash=09355ad427f98680e1f04f5266d4d479}{%
           family={Peng},
           familyi={P\bibinitperiod},
           given={Bei},
           giveni={B\bibinitperiod}}}%
        {{hash=c0cf2fea367eee2f127eef13920b442b}{%
           family={Böhmer},
           familyi={B\bibinitperiod},
           given={Wendelin},
           giveni={W\bibinitperiod}}}%
        {{hash=0d05819cf2b4fe22ba972c9b2b5d8c9d}{%
           family={Whiteson},
           familyi={W\bibinitperiod},
           given={Shimon},
           giveni={S\bibinitperiod}}}%
        {{hash=86e3e396d826309a838ac02c93e21550}{%
           family={Sha},
           familyi={S\bibinitperiod},
           given={Fei},
           giveni={F\bibinitperiod}}}%
      }
      \strng{namehash}{f19915e7ec1063c149806dd45d1f01df}
      \strng{fullhash}{4a46a7ad96a79c0a2f091b13cd89351c}
      \strng{fullhashraw}{4a46a7ad96a79c0a2f091b13cd89351c}
      \strng{bibnamehash}{4a46a7ad96a79c0a2f091b13cd89351c}
      \strng{authorbibnamehash}{4a46a7ad96a79c0a2f091b13cd89351c}
      \strng{authornamehash}{f19915e7ec1063c149806dd45d1f01df}
      \strng{authorfullhash}{4a46a7ad96a79c0a2f091b13cd89351c}
      \strng{authorfullhashraw}{4a46a7ad96a79c0a2f091b13cd89351c}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Multi-agent settings in the real world often involve tasks with varying types and quantities of agents and non-agent entities; however, common patterns of behavior often emerge among these agents/entities. Our method aims to leverage these commonalities by asking the question: ``What is the expected utility of each agent when only considering a randomly selected sub-group of its observed entities?'' By posing this counterfactual question, we can recognize state-action trajectories within sub-groups of entities that we may have encountered in another task and use what we learned in that task to inform our prediction in the current one. We then reconstruct a prediction of the full returns as a combination of factors considering these disjoint groups of entities and train this ``randomly factorized" value function as an auxiliary objective for value-based multi-agent reinforcement learning. By doing so, our model can recognize and leverage similarities across tasks to improve learning efficiency in a multi-task setting. Our approach, Randomized Entity-wise Factorization for Imagined Learning (REFIL), outperforms all strong baselines by a significant margin in challenging multi-task StarCraft micromanagement settings.}
      \field{day}{11}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{6}
      \field{pubstate}{prepublished}
      \field{title}{Randomized {{Entity-wise Factorization}} for {{Multi-Agent Reinforcement Learning}}}
      \field{urlday}{14}
      \field{urlmonth}{2}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2006.04222
      \endverb
      \verb{eprint}
      \verb 2006.04222
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/K3BBXYGS/Iqbal et al_2021_Randomized Entity-wise Factorization for Multi-Agent Reinforcement Learning.pdf;/Users/brandonhosley/Zotero/storage/H46GEC6S/2006.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2006.04222
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2006.04222
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning}
    \endentry
    \entry{terry2021}{online}{}{}
      \name{author}{13}{}{%
        {{hash=6227c82f39a2c06c345e7fafd8e8e6cc}{%
           family={Terry},
           familyi={T\bibinitperiod},
           given={J.\bibnamedelimi K.},
           giveni={J\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=bfd91affe76615f3a790a36f0715a769}{%
           family={Black},
           familyi={B\bibinitperiod},
           given={Benjamin},
           giveni={B\bibinitperiod}}}%
        {{hash=562720b3b3dc0da7beff80f5a9658a96}{%
           family={Grammel},
           familyi={G\bibinitperiod},
           given={Nathaniel},
           giveni={N\bibinitperiod}}}%
        {{hash=c6b7ed7facca2921b67761e33e954c30}{%
           family={Jayakumar},
           familyi={J\bibinitperiod},
           given={Mario},
           giveni={M\bibinitperiod}}}%
        {{hash=ea644330b231fbdbe9a3f5ad956fb601}{%
           family={Hari},
           familyi={H\bibinitperiod},
           given={Ananth},
           giveni={A\bibinitperiod}}}%
        {{hash=da9fc58d71c244b06f2afe82f94430a7}{%
           family={Sullivan},
           familyi={S\bibinitperiod},
           given={Ryan},
           giveni={R\bibinitperiod}}}%
        {{hash=7d8a57fad01c681de6134f0135f6336c}{%
           family={Santos},
           familyi={S\bibinitperiod},
           given={Luis},
           giveni={L\bibinitperiod}}}%
        {{hash=356a6213ea7de064fa570072df596cd1}{%
           family={Perez},
           familyi={P\bibinitperiod},
           given={Rodrigo},
           giveni={R\bibinitperiod}}}%
        {{hash=cbb7c6298f0b856d468231591ac8bc87}{%
           family={Horsch},
           familyi={H\bibinitperiod},
           given={Caroline},
           giveni={C\bibinitperiod}}}%
        {{hash=66cae0bed81aff7faa72e799a98883e8}{%
           family={Dieffendahl},
           familyi={D\bibinitperiod},
           given={Clemens},
           giveni={C\bibinitperiod}}}%
        {{hash=15a75a6608ae12ea4a04455ea8348c5d}{%
           family={Williams},
           familyi={W\bibinitperiod},
           given={Niall\bibnamedelima L.},
           giveni={N\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=97e09e24277306103d594812cbf50ea2}{%
           family={Lokesh},
           familyi={L\bibinitperiod},
           given={Yashas},
           giveni={Y\bibinitperiod}}}%
        {{hash=5be170706e483e6984f1f174c31fbc59}{%
           family={Ravi},
           familyi={R\bibinitperiod},
           given={Praveen},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{68cf4dcac41c5e149332a5e77e1ffb3a}
      \strng{fullhash}{54a94de3c57b2fbf7c47f6bf069b96a4}
      \strng{fullhashraw}{54a94de3c57b2fbf7c47f6bf069b96a4}
      \strng{bibnamehash}{68cf4dcac41c5e149332a5e77e1ffb3a}
      \strng{authorbibnamehash}{68cf4dcac41c5e149332a5e77e1ffb3a}
      \strng{authornamehash}{68cf4dcac41c5e149332a5e77e1ffb3a}
      \strng{authorfullhash}{54a94de3c57b2fbf7c47f6bf069b96a4}
      \strng{authorfullhashraw}{54a94de3c57b2fbf7c47f6bf069b96a4}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{This paper introduces the PettingZoo library and the accompanying Agent Environment Cycle ("AEC") games model. PettingZoo is a library of diverse sets of multi-agent environments with a universal, elegant Python API. PettingZoo was developed with the goal of accelerating research in Multi-Agent Reinforcement Learning ("MARL"), by making work more interchangeable, accessible and reproducible akin to what OpenAI's Gym library did for single-agent reinforcement learning. PettingZoo's API, while inheriting many features of Gym, is unique amongst MARL APIs in that it's based around the novel AEC games model. We argue, in part through case studies on major problems in popular MARL environments, that the popular game models are poor conceptual models of games commonly used in MARL and accordingly can promote confusing bugs that are hard to detect, and that the AEC games model addresses these problems.}
      \field{day}{26}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{10}
      \field{pubstate}{prepublished}
      \field{shorttitle}{{{PettingZoo}}}
      \field{title}{{{PettingZoo}}: {{Gym}} for {{Multi-Agent Reinforcement Learning}}}
      \field{urlday}{25}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2009.14471
      \endverb
      \verb{eprint}
      \verb 2009.14471
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/BUBMKBB8/Terry et al. - 2021 - PettingZoo Gym for Multi-Agent Reinforcement Lear.pdf;/Users/brandonhosley/Zotero/storage/YVZNGRAH/2009.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2009.14471
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2009.14471
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning}
    \endentry
    \entry{kurach2020}{online}{}{}
      \name{author}{11}{}{%
        {{hash=5de3bfed29969b7034c3e7108031b1c7}{%
           family={Kurach},
           familyi={K\bibinitperiod},
           given={Karol},
           giveni={K\bibinitperiod}}}%
        {{hash=b456173e93f42e0bb093f27668a968f7}{%
           family={Raichuk},
           familyi={R\bibinitperiod},
           given={Anton},
           giveni={A\bibinitperiod}}}%
        {{hash=b20f72a4478d4b8236071f03c27d1867}{%
           family={Stańczyk},
           familyi={S\bibinitperiod},
           given={Piotr},
           giveni={P\bibinitperiod}}}%
        {{hash=7cf4267d81dee550aa8b02092fb5afa3}{%
           family={Zając},
           familyi={Z\bibinitperiod},
           given={Michał},
           giveni={M\bibinitperiod}}}%
        {{hash=b8d6ee5b98a48b04ccf119b64508d63c}{%
           family={Bachem},
           familyi={B\bibinitperiod},
           given={Olivier},
           giveni={O\bibinitperiod}}}%
        {{hash=31f15faeab280fcda1466b9710381955}{%
           family={Espeholt},
           familyi={E\bibinitperiod},
           given={Lasse},
           giveni={L\bibinitperiod}}}%
        {{hash=cee59af5363f5cb5e5e95d437dd91534}{%
           family={Riquelme},
           familyi={R\bibinitperiod},
           given={Carlos},
           giveni={C\bibinitperiod}}}%
        {{hash=917cc9d59f6ad3c51ab0377febd20143}{%
           family={Vincent},
           familyi={V\bibinitperiod},
           given={Damien},
           giveni={D\bibinitperiod}}}%
        {{hash=508b11a7934e1a9be0357f29469ea2f4}{%
           family={Michalski},
           familyi={M\bibinitperiod},
           given={Marcin},
           giveni={M\bibinitperiod}}}%
        {{hash=cf4fea95321213e7dc3532fd33b85112}{%
           family={Bousquet},
           familyi={B\bibinitperiod},
           given={Olivier},
           giveni={O\bibinitperiod}}}%
        {{hash=450963f966620925cb8fecd32f7a6ee1}{%
           family={Gelly},
           familyi={G\bibinitperiod},
           given={Sylvain},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{e36a6e812ea86429ee245c524b240c2d}
      \strng{fullhash}{8a887d04fcd8fe1bed1cd5ea3773dfc8}
      \strng{fullhashraw}{8a887d04fcd8fe1bed1cd5ea3773dfc8}
      \strng{bibnamehash}{e36a6e812ea86429ee245c524b240c2d}
      \strng{authorbibnamehash}{e36a6e812ea86429ee245c524b240c2d}
      \strng{authornamehash}{e36a6e812ea86429ee245c524b240c2d}
      \strng{authorfullhash}{8a887d04fcd8fe1bed1cd5ea3773dfc8}
      \strng{authorfullhashraw}{8a887d04fcd8fe1bed1cd5ea3773dfc8}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Recent progress in the field of reinforcement learning has been accelerated by virtual learning environments such as video games, where novel algorithms and ideas can be quickly tested in a safe and reproducible manner. We introduce the Google Research Football Environment, a new reinforcement learning environment where agents are trained to play football in an advanced, physics-based 3D simulator. The resulting environment is challenging, easy to use and customize, and it is available under a permissive open-source license. In addition, it provides support for multiplayer and multi-agent experiments. We propose three full-game scenarios of varying difficulty with the Football Benchmarks and report baseline results for three commonly used reinforcement algorithms (IMPALA, PPO, and Ape-X DQN). We also provide a diverse set of simpler scenarios with the Football Academy and showcase several promising research directions.}
      \field{day}{14}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arXiv}
      \field{month}{4}
      \field{pubstate}{prepublished}
      \field{shorttitle}{Google {{Research Football}}}
      \field{title}{Google {{Research Football}}: {{A Novel Reinforcement Learning Environment}}}
      \field{urlday}{18}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1907.11180
      \endverb
      \verb{eprint}
      \verb 1907.11180
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/DNBMDCA2/Kurach et al_2020_Google Research Football.pdf;/Users/brandonhosley/Zotero/storage/3X73G4DX/1907.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1907.11180
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1907.11180
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{canese2021}{article}{}{}
      \name{author}{7}{}{%
        {{hash=1e9cd968439d1ce1a4811ac247d39bf0}{%
           family={Canese},
           familyi={C\bibinitperiod},
           given={Lorenzo},
           giveni={L\bibinitperiod}}}%
        {{hash=38d35bd3858486c4ed1e58f9ab8d07c6}{%
           family={Cardarilli},
           familyi={C\bibinitperiod},
           given={Gian\bibnamedelima Carlo},
           giveni={G\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
        {{hash=8a94f6be09a7243131fb661f3b9ca83f}{%
           family={Di\bibnamedelima Nunzio},
           familyi={D\bibinitperiod\bibinitdelim N\bibinitperiod},
           given={Luca},
           giveni={L\bibinitperiod}}}%
        {{hash=95606a870779f6783427c94bf660365a}{%
           family={Fazzolari},
           familyi={F\bibinitperiod},
           given={Rocco},
           giveni={R\bibinitperiod}}}%
        {{hash=46d05bf67ac60105ff28759467e84253}{%
           family={Giardino},
           familyi={G\bibinitperiod},
           given={Daniele},
           giveni={D\bibinitperiod}}}%
        {{hash=51a48be033404de5c2027a90f13ada68}{%
           family={Re},
           familyi={R\bibinitperiod},
           given={Marco},
           giveni={M\bibinitperiod}}}%
        {{hash=fcc3435f64a45f95f6b120c69b02ae26}{%
           family={Spanò},
           familyi={S\bibinitperiod},
           given={Sergio},
           giveni={S\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Multidisciplinary Digital Publishing Institute}%
      }
      \strng{namehash}{ed78b4c350aab6f69347e265227fcfb8}
      \strng{fullhash}{a012182fbc452022ce8b21bd0659ccd6}
      \strng{fullhashraw}{a012182fbc452022ce8b21bd0659ccd6}
      \strng{bibnamehash}{ed78b4c350aab6f69347e265227fcfb8}
      \strng{authorbibnamehash}{ed78b4c350aab6f69347e265227fcfb8}
      \strng{authornamehash}{ed78b4c350aab6f69347e265227fcfb8}
      \strng{authorfullhash}{a012182fbc452022ce8b21bd0659ccd6}
      \strng{authorfullhashraw}{a012182fbc452022ce8b21bd0659ccd6}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{In this review, we present an analysis of the most used multi-agent reinforcement learning algorithms. Starting with the single-agent reinforcement learning algorithms, we focus on the most critical issues that must be taken into account in their extension to multi-agent scenarios. The analyzed algorithms were grouped according to their features. We present a detailed taxonomy of the main multi-agent approaches proposed in the literature, focusing on their related mathematical models. For each algorithm, we describe the possible application fields, while pointing out its pros and cons. The described multi-agent algorithms are compared in terms of the most important characteristics for multi-agent reinforcement learning applications—namely, nonstationarity, scalability, and observability. We also describe the most common benchmark environments used to evaluate the performances of the considered methods.}
      \field{issn}{2076-3417}
      \field{issue}{11}
      \field{journaltitle}{Applied Sciences}
      \field{langid}{english}
      \field{month}{1}
      \field{number}{11}
      \field{shorttitle}{Multi-{{Agent Reinforcement Learning}}}
      \field{title}{Multi-{{Agent Reinforcement Learning}}: {{A Review}} of {{Challenges}} and {{Applications}}}
      \field{urlday}{16}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{11}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{4948}
      \range{pages}{1}
      \verb{doi}
      \verb 10.3390/app11114948
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/565YJNJJ/Canese et al_2021_Multi-Agent Reinforcement Learning.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.mdpi.com/2076-3417/11/11/4948
      \endverb
      \verb{url}
      \verb https://www.mdpi.com/2076-3417/11/11/4948
      \endverb
      \keyw{machine learning,multi-agent,reinforcement learning,swarm}
    \endentry
    \entry{krouka2022}{article}{}{}
      \name{author}{4}{}{%
        {{hash=616e6d8961032bfb45ba2d908c5bd579}{%
           family={Krouka},
           familyi={K\bibinitperiod},
           given={Mounssif},
           giveni={M\bibinitperiod}}}%
        {{hash=3a3569c9399ccbb0e5a839392163d978}{%
           family={Elgabli},
           familyi={E\bibinitperiod},
           given={Anis},
           giveni={A\bibinitperiod}}}%
        {{hash=dc57afe6da11419e9f5baa24fc305874}{%
           family={Issaid},
           familyi={I\bibinitperiod},
           given={Chaouki\bibnamedelima Ben},
           giveni={C\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=eaf666d93aee903b2bbd8a40853a4243}{%
           family={Bennis},
           familyi={B\bibinitperiod},
           given={Mehdi},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{7824fd2537fe178ac791ce5b3523e4d1}
      \strng{fullhash}{1a92a0bbb9600f65ab789efffe6fce9f}
      \strng{fullhashraw}{1a92a0bbb9600f65ab789efffe6fce9f}
      \strng{bibnamehash}{1a92a0bbb9600f65ab789efffe6fce9f}
      \strng{authorbibnamehash}{1a92a0bbb9600f65ab789efffe6fce9f}
      \strng{authornamehash}{7824fd2537fe178ac791ce5b3523e4d1}
      \strng{authorfullhash}{1a92a0bbb9600f65ab789efffe6fce9f}
      \strng{authorfullhashraw}{1a92a0bbb9600f65ab789efffe6fce9f}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper, we consider a distributed reinforcement learning setting where agents are communicating with a central entity in a shared environment to maximize a global reward. A main challenge in this setting is that the randomness of the wireless channel perturbs each agent’s model update while multiple agents’ updates may cause interference when communicating under limited bandwidth. To address this issue, we propose a novel distributed reinforcement learning algorithm based on the alternating direction method of multipliers (ADMM) and “over air aggregation” using analog transmission scheme, referred to as A-RLADMM. Our algorithm incorporates the wireless channel into the formulation of the ADMM method, which enables agents to transmit each element of their updated models over the same channel using analog communication. Numerical experiments on a multi-agent collaborative navigation task show that our proposed algorithm significantly outperforms the digital communication baseline of A-RLADMM (D-RLADMM), the lazily aggregated policy gradient (RL-LAPG), as well as the analog and the digital communication versions of the vanilla FL, (A-FRL) and (D-FRL) respectively.}
      \field{issn}{2332-7731}
      \field{journaltitle}{IEEE Transactions on Cognitive Communications and Networking}
      \field{month}{3}
      \field{number}{1}
      \field{title}{Communication-{{Efficient}} and {{Federated Multi-Agent Reinforcement Learning}}}
      \field{urlday}{16}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{8}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{311\bibrangedash 320}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1109/TCCN.2021.3130993
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/YXIAE97H/Krouka et al_2022_Communication-Efficient and Federated Multi-Agent Reinforcement Learning.pdf
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/abstract/document/9627728
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/abstract/document/9627728
      \endverb
      \keyw{ADMM,analog communications,Collaboration,Computational modeling,Convergence,Digital communication,distributed optimization,Numerical models,policy gradient,Privacy,Reinforcement learning}
    \endentry
    \entry{wei2022}{article}{}{}
      \name{author}{8}{}{%
        {{hash=33004e7f04179d2de636a44fe0130abc}{%
           family={Wei},
           familyi={W\bibinitperiod},
           given={Kang},
           giveni={K\bibinitperiod}}}%
        {{hash=bc20e54573d2db853852f86802ab83de}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Jun},
           giveni={J\bibinitperiod}}}%
        {{hash=28c2f8feae8557397170c998fd2e3b47}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Chuan},
           giveni={C\bibinitperiod}}}%
        {{hash=46a3f0b5c048254bc7f6910cd7e8cd63}{%
           family={Ding},
           familyi={D\bibinitperiod},
           given={Ming},
           giveni={M\bibinitperiod}}}%
        {{hash=185fc3c4c509ddb27c3370e69064bd91}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Cailian},
           giveni={C\bibinitperiod}}}%
        {{hash=c59d8dd54aa89c2a33d932d96e6f4bbd}{%
           family={Jin},
           familyi={J\bibinitperiod},
           given={Shi},
           giveni={S\bibinitperiod}}}%
        {{hash=21502e7d52af09e49d31a5bc40073591}{%
           family={Han},
           familyi={H\bibinitperiod},
           given={Zhu},
           giveni={Z\bibinitperiod}}}%
        {{hash=de7b0e512fcdd087f5750837f7c7cb9f}{%
           family={Poor},
           familyi={P\bibinitperiod},
           given={H.\bibnamedelimi Vincent},
           giveni={H\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
      }
      \strng{namehash}{6a74074103e01caaba0403c70ffc229c}
      \strng{fullhash}{071d3123f29f727b2d5f0b555bea5b89}
      \strng{fullhashraw}{071d3123f29f727b2d5f0b555bea5b89}
      \strng{bibnamehash}{6a74074103e01caaba0403c70ffc229c}
      \strng{authorbibnamehash}{6a74074103e01caaba0403c70ffc229c}
      \strng{authornamehash}{6a74074103e01caaba0403c70ffc229c}
      \strng{authorfullhash}{071d3123f29f727b2d5f0b555bea5b89}
      \strng{authorfullhashraw}{071d3123f29f727b2d5f0b555bea5b89}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In federated learning (FL), model training is distributed over clients and local models are aggregated by a central server. The performance of uploaded models in such situations can vary widely due to imbalanced data distributions, potential demands on privacy protections, and quality of transmissions. In this paper, we aim to minimize FL training delay over wireless channels, constrained by overall training performance as well as each client’s differential privacy (DP) requirement. We solve this problem in a multi-agent multi-armed bandit (MAMAB) framework to deal with the situation where there are multiple clients confronting different unknown transmission environments, e.g., channel fading and interference. Specifically, we first transform long-term constraints on both training performance and each client’s DP into a virtual queue based on the Lyapunov drift technique. Then, we convert the MAMAB to a max-min bipartite matching problem at each communication round, by estimating rewards with the upper confidence bound (UCB) approach. More importantly, we propose two efficient solutions to this matching problem, i.e., a modified Hungarian algorithm and greedy matching with a better alternative (GMBA), of which the former can achieve the optimal solution with high complexity while the latter approaches a better trade-off by enabling verified low-complexity with little performance loss. In addition, we develop an upper bound on the expected regret of this MAMAB based FL framework, which shows a linear growth over the logarithm of communication rounds, justifying its theoretical feasibility. Extensive experimental results are conducted to validate the effectiveness of our proposed algorithms, and the impacts of various parameters on the FL performance over wireless edge networks are also discussed.}
      \field{eventtitle}{{{IEEE Journal}} on {{Selected Areas}} in {{Communications}}}
      \field{issn}{1558-0008}
      \field{journaltitle}{IEEE Journal on Selected Areas in Communications}
      \field{month}{1}
      \field{number}{1}
      \field{title}{Low-{{Latency Federated Learning Over Wireless Channels With Differential Privacy}}}
      \field{volume}{40}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{pages}{290\bibrangedash 307}
      \range{pages}{18}
      \verb{doi}
      \verb 10.1109/JSAC.2021.3126052
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Library/Mobile Documents/com~apple~CloudDocs/ZotFile/Wei et al_2022_Low-Latency Federated Learning Over Wireless Channels With Differential Privacy.pdf
      \endverb
      \keyw{Computational modeling,Data models,differential privacy,Federated learning,Interference,max-min bipartite matching,multi-agent multi-armed bandit,Servers,Training,Wireless communication,Wireless sensor networks}
    \endentry
    \entry{shukla2022}{inproceedings}{}{}
      \name{author}{5}{}{%
        {{hash=7b1f0c483cb560ca440334f258326043}{%
           family={Shukla},
           familyi={S\bibinitperiod},
           given={Yash},
           giveni={Y\bibinitperiod}}}%
        {{hash=adf45752398fa062c4a8793be1085a2b}{%
           family={Thierauf},
           familyi={T\bibinitperiod},
           given={Christopher},
           giveni={C\bibinitperiod}}}%
        {{hash=bc6d51ee57ea806361549ea3cbae3bfe}{%
           family={Hosseini},
           familyi={H\bibinitperiod},
           given={Ramtin},
           giveni={R\bibinitperiod}}}%
        {{hash=5a338d60fbeeaf0b3883fd08d660e474}{%
           family={Tatiya},
           familyi={T\bibinitperiod},
           given={Gyan},
           giveni={G\bibinitperiod}}}%
        {{hash=3e3700e7eb6c746805a17e22eda92536}{%
           family={Sinapov},
           familyi={S\bibinitperiod},
           given={Jivko},
           giveni={J\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Richland, SC}%
      }
      \list{publisher}{1}{%
        {International Foundation for Autonomous Agents and Multiagent Systems}%
      }
      \strng{namehash}{048b71bb0b1a5f0160027e981395f8ba}
      \strng{fullhash}{78857cfb4a2382092d8571369af5df90}
      \strng{fullhashraw}{78857cfb4a2382092d8571369af5df90}
      \strng{bibnamehash}{78857cfb4a2382092d8571369af5df90}
      \strng{authorbibnamehash}{78857cfb4a2382092d8571369af5df90}
      \strng{authornamehash}{048b71bb0b1a5f0160027e981395f8ba}
      \strng{authorfullhash}{78857cfb4a2382092d8571369af5df90}
      \strng{authorfullhashraw}{78857cfb4a2382092d8571369af5df90}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Despite recent advances in Reinforcement Learning (RL), many problems, especially real-world tasks, remain prohibitively expensive to learn. To address this issue, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum to learn a problem that may otherwise be too difficult to learn from scratch. However, generating and optimizing a curriculum in a realistic scenario still requires extensive interactions with the environment. To address this challenge, we formulate the curriculum transfer problem, in which the schema of a curriculum optimized in a simpler, easy-to-solve environment (e.g., a grid world) is transferred to a complex, realistic scenario (e.g., a physics-based robotics simulation or the real world). We present "ACuTE", Automatic Curriculum Transfer from Simple to Complex Environments, a novel framework to solve this problem, and evaluate our proposed method by comparing it to other baseline approaches (e.g., domain adaptation) designed to speed up learning. We observe that our approach produces improved jumpstart and time-to-threshold performance even when adding task elements that further increase the difficulty of the realistic scenario. Finally, we demonstrate that our approach is independent of the learning algorithm used for curriculum generation, and is Sim2Real transferable to a real world scenario using a physical robot.}
      \field{booktitle}{Proceedings of the 21st {{International Conference}} on {{Autonomous Agents}} and {{Multiagent Systems}}}
      \field{day}{9}
      \field{isbn}{978-1-4503-9213-6}
      \field{month}{5}
      \field{series}{{{AAMAS}} '22}
      \field{shorttitle}{{{ACuTE}}}
      \field{title}{{{ACuTE}}: {{Automatic Curriculum Transfer}} from {{Simple}} to {{Complex Environments}}}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{pages}{1192\bibrangedash 1200}
      \range{pages}{9}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/WNDIQVTP/Shukla et al_2022_ACuTE.pdf
      \endverb
    \endentry
    \entry{shi2023}{article}{}{}
      \name{author}{4}{}{%
        {{hash=367ad0815e91f7dab9b88f6a423e0b0a}{%
           family={Shi},
           familyi={S\bibinitperiod},
           given={Haobin},
           giveni={H\bibinitperiod}}}%
        {{hash=9b9b6cc94f3eacfcc32c837d182deb36}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Jingchen},
           giveni={J\bibinitperiod}}}%
        {{hash=7367ca91f0da6760e96432392109c59d}{%
           family={Mao},
           familyi={M\bibinitperiod},
           given={Jiahui},
           giveni={J\bibinitperiod}}}%
        {{hash=a973ebce3a15a8b4d5484b771dc26bdc}{%
           family={Hwang},
           familyi={H\bibinitperiod},
           given={Kao-Shing},
           giveni={K\bibinithyphendelim S\bibinitperiod}}}%
      }
      \strng{namehash}{e64bd603ce97f095c01d75b758264727}
      \strng{fullhash}{c4873a4e11fd0ce6ac97e2e0a87cdaf8}
      \strng{fullhashraw}{c4873a4e11fd0ce6ac97e2e0a87cdaf8}
      \strng{bibnamehash}{c4873a4e11fd0ce6ac97e2e0a87cdaf8}
      \strng{authorbibnamehash}{c4873a4e11fd0ce6ac97e2e0a87cdaf8}
      \strng{authornamehash}{e64bd603ce97f095c01d75b758264727}
      \strng{authorfullhash}{c4873a4e11fd0ce6ac97e2e0a87cdaf8}
      \strng{authorfullhashraw}{c4873a4e11fd0ce6ac97e2e0a87cdaf8}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Some researchers have introduced transfer learning mechanisms to multiagent reinforcement learning (MARL). However, the existing works devoted to cross-task transfer for multiagent systems were designed just for homogeneous agents or similar domains. This work proposes an all-purpose cross-transfer method, called multiagent lateral transfer (MALT), assisting MARL with alleviating the training burden. We discuss several challenges in developing an all-purpose multiagent cross-task transfer learning method and provide a feasible way of reusing knowledge for MARL. In the developed method, we take features as the transfer object rather than policies or experiences, inspired by the progressive network. To achieve more efficient transfer, we assign pretrained policy networks for agents based on clustering, while an attention module is introduced to enhance the transfer framework. The proposed method has no strict requirements for the source task and target task. Compared with the existing works, our method can transfer knowledge among heterogeneous agents and also avoid negative transfer in the case of fully different tasks. As far as we know, this article is the first work denoted to all-purpose cross-task transfer for MARL. Several experiments in various scenarios have been conducted to compare the performance of the proposed method with baselines. The results demonstrate that the method is sufficiently flexible for most settings, including cooperative, competitive, homogeneous, and heterogeneous configurations.}
      \field{eventtitle}{{{IEEE Transactions}} on {{Cybernetics}}}
      \field{issn}{2168-2275}
      \field{journaltitle}{IEEE Transactions on Cybernetics}
      \field{month}{3}
      \field{number}{3}
      \field{title}{Lateral {{Transfer Learning}} for {{Multiagent Reinforcement Learning}}}
      \field{urlday}{21}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{volume}{53}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1699\bibrangedash 1711}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1109/TCYB.2021.3108237
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/6GRL3NSF/9535269.html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/9535269
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/9535269
      \endverb
      \keyw{Attention mechanism,Costs,Multi-agent systems,multiagent reinforcement learning (MARL),Neural networks,Reinforcement learning,Task analysis,Training,transfer learning,Transfer learning}
    \endentry
    \entry{cui2022}{online}{}{}
      \name{author}{7}{}{%
        {{hash=ae9f731739b88b9963c4f9ea7295d961}{%
           family={Cui},
           familyi={C\bibinitperiod},
           given={Kai},
           giveni={K\bibinitperiod}}}%
        {{hash=e8582bf631f0f40a64f82023f2b83a74}{%
           family={Tahir},
           familyi={T\bibinitperiod},
           given={Anam},
           giveni={A\bibinitperiod}}}%
        {{hash=68c1cafc9ec95b3f3f88f6cca23bf157}{%
           family={Ekinci},
           familyi={E\bibinitperiod},
           given={Gizem},
           giveni={G\bibinitperiod}}}%
        {{hash=9dbcf8624ee66dea23c946a8d82eee59}{%
           family={Elshamanhory},
           familyi={E\bibinitperiod},
           given={Ahmed},
           giveni={A\bibinitperiod}}}%
        {{hash=3e34c81db85d4a070fdf0035898f480d}{%
           family={Eich},
           familyi={E\bibinitperiod},
           given={Yannick},
           giveni={Y\bibinitperiod}}}%
        {{hash=58c0f51a031f07e3e346957de1340fda}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Mengguang},
           giveni={M\bibinitperiod}}}%
        {{hash=836aadfafb99a418e0308c121a0ac626}{%
           family={Koeppl},
           familyi={K\bibinitperiod},
           given={Heinz},
           giveni={H\bibinitperiod}}}%
      }
      \strng{namehash}{6404d2b3ac97f6dca0707439520ff9c8}
      \strng{fullhash}{6997df192fa15d8f62a6176bf4b4d112}
      \strng{fullhashraw}{6997df192fa15d8f62a6176bf4b4d112}
      \strng{bibnamehash}{6404d2b3ac97f6dca0707439520ff9c8}
      \strng{authorbibnamehash}{6404d2b3ac97f6dca0707439520ff9c8}
      \strng{authornamehash}{6404d2b3ac97f6dca0707439520ff9c8}
      \strng{authorfullhash}{6997df192fa15d8f62a6176bf4b4d112}
      \strng{authorfullhashraw}{6997df192fa15d8f62a6176bf4b4d112}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The analysis and control of large-population systems is of great interest to diverse areas of research and engineering, ranging from epidemiology over robotic swarms to economics and finance. An increasingly popular and effective approach to realizing sequential decision-making in multi-agent systems is through multi-agent reinforcement learning, as it allows for an automatic and model-free analysis of highly complex systems. However, the key issue of scalability complicates the design of control and reinforcement learning algorithms particularly in systems with large populations of agents. While reinforcement learning has found resounding empirical success in many scenarios with few agents, problems with many agents quickly become intractable and necessitate special consideration. In this survey, we will shed light on current approaches to tractably understanding and analyzing large-population systems, both through multi-agent reinforcement learning and through adjacent areas of research such as mean-field games, collective intelligence, or complex network theory. These classically independent subject areas offer a variety of approaches to understanding or modeling large-population systems, which may be of great use for the formulation of tractable MARL algorithms in the future. Finally, we survey potential areas of application for large-scale control and identify fruitful future applications of learning algorithms in practical systems. We hope that our survey could provide insight and future directions to junior and senior researchers in theoretical and applied sciences alike.}
      \field{day}{8}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{9}
      \field{pubstate}{prepublished}
      \field{title}{A {{Survey}} on {{Large-Population Systems}} and {{Scalable Multi-Agent Reinforcement Learning}}}
      \field{urlday}{21}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2209.03859
      \endverb
      \verb{eprint}
      \verb 2209.03859
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/AM9XH9WA/Cui et al_2022_A Survey on Large-Population Systems and Scalable Multi-Agent Reinforcement.pdf;/Users/brandonhosley/Zotero/storage/UQDSHI2V/2209.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2209.03859
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2209.03859
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems}
    \endentry
    \entry{gupta2017a}{online}{}{}
      \name{author}{5}{}{%
        {{hash=d0792eb04b685020eafedbc871aeb84f}{%
           family={Gupta},
           familyi={G\bibinitperiod},
           given={Abhishek},
           giveni={A\bibinitperiod}}}%
        {{hash=a8f9ab847e5d07be6c35edcb4da8f0c6}{%
           family={Devin},
           familyi={D\bibinitperiod},
           given={Coline},
           giveni={C\bibinitperiod}}}%
        {{hash=8dbb6d17d807c3708f0ea6bf371527f6}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={YuXuan},
           giveni={Y\bibinitperiod}}}%
        {{hash=e28d4ee199593959d8c29980a64f1974}{%
           family={Abbeel},
           familyi={A\bibinitperiod},
           given={Pieter},
           giveni={P\bibinitperiod}}}%
        {{hash=c9545da54b33744da25943cdf66eadac}{%
           family={Levine},
           familyi={L\bibinitperiod},
           given={Sergey},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{d2f56625487873e41edcfcb7ea5206af}
      \strng{fullhash}{0e726b9fe38266db9b6585e1c7308b86}
      \strng{fullhashraw}{0e726b9fe38266db9b6585e1c7308b86}
      \strng{bibnamehash}{0e726b9fe38266db9b6585e1c7308b86}
      \strng{authorbibnamehash}{0e726b9fe38266db9b6585e1c7308b86}
      \strng{authornamehash}{d2f56625487873e41edcfcb7ea5206af}
      \strng{authorfullhash}{0e726b9fe38266db9b6585e1c7308b86}
      \strng{authorfullhashraw}{0e726b9fe38266db9b6585e1c7308b86}
      \field{extraname}{2}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{People can learn a wide range of tasks from their own experience, but can also learn from observing other creatures. This can accelerate acquisition of new skills even when the observed agent differs substantially from the learning agent in terms of morphology. In this paper, we examine how reinforcement learning algorithms can transfer knowledge between morphologically different agents (e.g., different robots). We introduce a problem formulation where two agents are tasked with learning multiple skills by sharing information. Our method uses the skills that were learned by both agents to train invariant feature spaces that can then be used to transfer other skills from one agent to another. The process of learning these invariant feature spaces can be viewed as a kind of "analogy making", or implicit learning of partial correspondences between two distinct domains. We evaluate our transfer learning algorithm in two simulated robotic manipulation skills, and illustrate that we can transfer knowledge between simulated robotic arms with different numbers of links, as well as simulated arms with different actuation mechanisms, where one robot is torque-driven while the other is tendon-driven.}
      \field{day}{8}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{3}
      \field{pubstate}{prepublished}
      \field{title}{Learning {{Invariant Feature Spaces}} to {{Transfer Skills}} with {{Reinforcement Learning}}}
      \field{urlday}{25}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1703.02949
      \endverb
      \verb{eprint}
      \verb 1703.02949
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/SC4QV825/Gupta et al_2017_Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning.pdf;/Users/brandonhosley/Zotero/storage/FPAI6T9H/1703.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1703.02949
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1703.02949
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Robotics}
    \endentry
    \entry{huttenrauch2019}{online}{}{}
      \name{author}{3}{}{%
        {{hash=57c5488cca6508b0a6dc29ab3c9afe54}{%
           family={Hüttenrauch},
           familyi={H\bibinitperiod},
           given={Maximilian},
           giveni={M\bibinitperiod}}}%
        {{hash=a41b551ba01941f0498ae8a8ab62e712}{%
           family={Šošić},
           familyi={Š\bibinitperiod},
           given={Adrian},
           giveni={A\bibinitperiod}}}%
        {{hash=1896ab7f5b0b179c9ff54fbf39b31106}{%
           family={Neumann},
           familyi={N\bibinitperiod},
           given={Gerhard},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{612856197b008899a144ad44593a6e5a}
      \strng{fullhash}{5409ec807d10bb1eb53ea4cf718d77d0}
      \strng{fullhashraw}{5409ec807d10bb1eb53ea4cf718d77d0}
      \strng{bibnamehash}{5409ec807d10bb1eb53ea4cf718d77d0}
      \strng{authorbibnamehash}{5409ec807d10bb1eb53ea4cf718d77d0}
      \strng{authornamehash}{612856197b008899a144ad44593a6e5a}
      \strng{authorfullhash}{5409ec807d10bb1eb53ea4cf718d77d0}
      \strng{authorfullhashraw}{5409ec807d10bb1eb53ea4cf718d77d0}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recently, deep reinforcement learning (RL) methods have been applied successfully to multi-agent scenarios. Typically, these methods rely on a concatenation of agent states to represent the information content required for decentralized decision making. However, concatenation scales poorly to swarm systems with a large number of homogeneous agents as it does not exploit the fundamental properties inherent to these systems: (i) the agents in the swarm are interchangeable and (ii) the exact number of agents in the swarm is irrelevant. Therefore, we propose a new state representation for deep multi-agent RL based on mean embeddings of distributions. We treat the agents as samples of a distribution and use the empirical mean embedding as input for a decentralized policy. We define different feature spaces of the mean embedding using histograms, radial basis functions and a neural network learned end-to-end. We evaluate the representation on two well known problems from the swarm literature (rendezvous and pursuit evasion), in a globally and locally observable setup. For the local setup we furthermore introduce simple communication protocols. Of all approaches, the mean embedding representation using neural network features enables the richest information exchange between neighboring agents facilitating the development of more complex collective strategies.}
      \field{day}{6}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{6}
      \field{pubstate}{prepublished}
      \field{title}{Deep {{Reinforcement Learning}} for {{Swarm Systems}}}
      \field{urlday}{25}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1807.06613
      \endverb
      \verb{eprint}
      \verb 1807.06613
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/NNR2UM6G/Hüttenrauch et al. - 2019 - Deep Reinforcement Learning for Swarm Systems.pdf;/Users/brandonhosley/Zotero/storage/5WTS988E/1807.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1807.06613
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1807.06613
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Computer Science - Systems and Control,Statistics - Machine Learning}
    \endentry
    \entry{bitzer2010}{inproceedings}{}{}
      \name{author}{3}{}{%
        {{hash=9d87f64c55110b4e8516dc9cc4fa35c9}{%
           family={Bitzer},
           familyi={B\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
        {{hash=dc744818d7b192b3af78600e7a85d196}{%
           family={Howard},
           familyi={H\bibinitperiod},
           given={Matthew},
           giveni={M\bibinitperiod}}}%
        {{hash=7b2c17552ac9061afc4606fc8fc554b9}{%
           family={Vijayakumar},
           familyi={V\bibinitperiod},
           given={Sethu},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{a12c7d4101ce7a1d2e2bc6ab62b56d6e}
      \strng{fullhash}{8c28210a51b6434ee1abc2548da2962b}
      \strng{fullhashraw}{8c28210a51b6434ee1abc2548da2962b}
      \strng{bibnamehash}{8c28210a51b6434ee1abc2548da2962b}
      \strng{authorbibnamehash}{8c28210a51b6434ee1abc2548da2962b}
      \strng{authornamehash}{a12c7d4101ce7a1d2e2bc6ab62b56d6e}
      \strng{authorfullhash}{8c28210a51b6434ee1abc2548da2962b}
      \strng{authorfullhashraw}{8c28210a51b6434ee1abc2548da2962b}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Reinforcement learning in the high-dimensional, continuous spaces typical in robotics, remains a challenging problem. To overcome this challenge, a popular approach has been to use demonstrations to find an appropriate initialisation of the policy in an attempt to reduce the number of iterations needed to find a solution. Here, we present an alternative way to incorporate prior knowledge from demonstrations of individual postures into learning, by extracting the inherent problem structure to find an efficient state representation. In particular, we use probabilistic, nonlinear dimensionality reduction to capture latent constraints present in the data. By learning policies in the learnt latent space, we are able to solve the planning problem in a reduced space that automatically satisfies task constraints. As shown in our experiments, this reduces the exploration needed and greatly accelerates the learning. We demonstrate our approach for learning a bimanual reaching task on the 19-DOF KHR-1HV humanoid.}
      \field{booktitle}{2010 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}}
      \field{eventtitle}{2010 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}}
      \field{issn}{2153-0866}
      \field{month}{10}
      \field{title}{Using Dimensionality Reduction to Exploit Constraints in Reinforcement Learning}
      \field{urlday}{25}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{2010}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{3219\bibrangedash 3225}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1109/IROS.2010.5650243
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/X35W5R32/Bitzer et al_2010_Using dimensionality reduction to exploit constraints in reinforcement learning.pdf;/Users/brandonhosley/Zotero/storage/8MZPUUAV/5650243.html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/5650243
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/5650243
      \endverb
      \keyw{Aerospace electronics,Joints,Learning,Manifolds,Principal component analysis,Robots,Trajectory}
    \endentry
    \entry{tangkaratt2016}{article}{}{}
      \name{author}{3}{}{%
        {{hash=add71ed9a02b60c00333454974b85f9d}{%
           family={Tangkaratt},
           familyi={T\bibinitperiod},
           given={Voot},
           giveni={V\bibinitperiod}}}%
        {{hash=27c6cae8e17b3c38ffcd03efd6861ffd}{%
           family={Morimoto},
           familyi={M\bibinitperiod},
           given={Jun},
           giveni={J\bibinitperiod}}}%
        {{hash=39334f77cbd67c8922133952b8095d89}{%
           family={Sugiyama},
           familyi={S\bibinitperiod},
           given={Masashi},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{537028dd3632877043f4b739baa0e6fc}
      \strng{fullhash}{8bceb1c909d1101cadce18bf0ffda77e}
      \strng{fullhashraw}{8bceb1c909d1101cadce18bf0ffda77e}
      \strng{bibnamehash}{8bceb1c909d1101cadce18bf0ffda77e}
      \strng{authorbibnamehash}{8bceb1c909d1101cadce18bf0ffda77e}
      \strng{authornamehash}{537028dd3632877043f4b739baa0e6fc}
      \strng{authorfullhash}{8bceb1c909d1101cadce18bf0ffda77e}
      \strng{authorfullhashraw}{8bceb1c909d1101cadce18bf0ffda77e}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The goal of reinforcement learning is to learn an optimal policy which controls an agent to acquire the maximum cumulative reward. The model-based reinforcement learning approach learns a transition model of the environment from data, and then derives the optimal policy using the transition model. However, learning an accurate transition model in high-dimensional environments requires a large amount of data which is difficult to obtain. To overcome this difficulty, in this paper, we propose to combine model-based reinforcement learning with the recently developed least-squares conditional entropy (LSCE) method, which simultaneously performs transition model estimation and dimension reduction. We also further extend the proposed method to imitation learning scenarios. The experimental results show that policy search combined with LSCE performs well for high-dimensional control tasks including real humanoid robot control.}
      \field{day}{1}
      \field{issn}{0893-6080}
      \field{journaltitle}{Neural Networks}
      \field{month}{12}
      \field{shortjournal}{Neural Networks}
      \field{title}{Model-Based Reinforcement Learning with Dimension Reduction}
      \field{urlday}{25}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{volume}{84}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 16}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1016/j.neunet.2016.08.005
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/LDFDBMSV/S0893608016301095.html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S0893608016301095
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S0893608016301095
      \endverb
      \keyw{Model-based reinforcement learning,Sufficient dimension reduction,Transition model estimation}
    \endentry
    \entry{wakilpoor2020}{online}{}{}
      \name{author}{4}{}{%
        {{hash=5b27318667ca33a8d2e71093a7748ccc}{%
           family={Wakilpoor},
           familyi={W\bibinitperiod},
           given={Ceyer},
           giveni={C\bibinitperiod}}}%
        {{hash=f904e82926fc93b2cbe045456925de2b}{%
           family={Martin},
           familyi={M\bibinitperiod},
           given={Patrick\bibnamedelima J.},
           giveni={P\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=cb641d3c18344237680a5d49f6103964}{%
           family={Rebhuhn},
           familyi={R\bibinitperiod},
           given={Carrie},
           giveni={C\bibinitperiod}}}%
        {{hash=96962575233d78cc7ac45b135169ec24}{%
           family={Vu},
           familyi={V\bibinitperiod},
           given={Amanda},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{9a39042ad597fc22648c12c305fe1cd1}
      \strng{fullhash}{e954e02accfc9b5cfcc9f60513bd9022}
      \strng{fullhashraw}{e954e02accfc9b5cfcc9f60513bd9022}
      \strng{bibnamehash}{e954e02accfc9b5cfcc9f60513bd9022}
      \strng{authorbibnamehash}{e954e02accfc9b5cfcc9f60513bd9022}
      \strng{authornamehash}{9a39042ad597fc22648c12c305fe1cd1}
      \strng{authorfullhash}{e954e02accfc9b5cfcc9f60513bd9022}
      \strng{authorfullhashraw}{e954e02accfc9b5cfcc9f60513bd9022}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Reinforcement learning in heterogeneous multi-agent scenarios is important for real-world applications but presents challenges beyond those seen in homogeneous settings and simple benchmarks. In this work, we present an actor-critic algorithm that allows a team of heterogeneous agents to learn decentralized control policies for covering an unknown environment. This task is of interest to national security and emergency response organizations that would like to enhance situational awareness in hazardous areas by deploying teams of unmanned aerial vehicles. To solve this multi-agent coverage path planning problem in unknown environments, we augment a multi-agent actor-critic architecture with a new state encoding structure and triplet learning loss to support heterogeneous agent learning. We developed a simulation environment that includes real-world environmental factors such as turbulence, delayed communication, and agent loss, to train teams of agents as well as probe their robustness and flexibility to such disturbances.}
      \field{day}{6}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{10}
      \field{pubstate}{prepublished}
      \field{title}{Heterogeneous {{Multi-Agent Reinforcement Learning}} for {{Unknown Environment Mapping}}}
      \field{urlday}{14}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2010.02663
      \endverb
      \verb{eprint}
      \verb 2010.02663
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/JRDBJMX5/Wakilpoor et al_2020_Heterogeneous Multi-Agent Reinforcement Learning for Unknown Environment Mapping.pdf;/Users/brandonhosley/Zotero/storage/G975SLIL/2010.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2010.02663
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2010.02663
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems}
    \endentry
    \entry{koster2020}{online}{}{}
      \name{author}{4}{}{%
        {{hash=1c6b022b3dc230f7672b476207a85bd2}{%
           family={Köster},
           familyi={K\bibinitperiod},
           given={Raphael},
           giveni={R\bibinitperiod}}}%
        {{hash=d39f872255b10943403c9e34b5f85174}{%
           family={Hadfield-Menell},
           familyi={H\bibinithyphendelim M\bibinitperiod},
           given={Dylan},
           giveni={D\bibinitperiod}}}%
        {{hash=84bd71cc4f1432d2228fcdb7544b5409}{%
           family={Hadfield},
           familyi={H\bibinitperiod},
           given={Gillian\bibnamedelima K.},
           giveni={G\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=698896c85bec4f93adca09e2a21cdf78}{%
           family={Leibo},
           familyi={L\bibinitperiod},
           given={Joel\bibnamedelima Z.},
           giveni={J\bibinitperiod\bibinitdelim Z\bibinitperiod}}}%
      }
      \strng{namehash}{2b752cb042477c8d0f0286a4aedd5392}
      \strng{fullhash}{5761ea1dfb7340a5704172a44b89133d}
      \strng{fullhashraw}{5761ea1dfb7340a5704172a44b89133d}
      \strng{bibnamehash}{5761ea1dfb7340a5704172a44b89133d}
      \strng{authorbibnamehash}{5761ea1dfb7340a5704172a44b89133d}
      \strng{authornamehash}{2b752cb042477c8d0f0286a4aedd5392}
      \strng{authorfullhash}{5761ea1dfb7340a5704172a44b89133d}
      \strng{authorfullhashraw}{5761ea1dfb7340a5704172a44b89133d}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{How can societies learn to enforce and comply with social norms? Here we investigate the learning dynamics and emergence of compliance and enforcement of social norms in a foraging game, implemented in a multi-agent reinforcement learning setting. In this spatiotemporally extended game, individuals are incentivized to implement complex berry-foraging policies and punish transgressions against social taboos covering specific berry types. We show that agents benefit when eating poisonous berries is taboo, meaning the behavior is punished by other agents, as this helps overcome a credit-assignment problem in discovering delayed health effects. Critically, however, we also show that introducing an additional taboo, which results in punishment for eating a harmless berry, improves the rate and stability with which agents learn to punish taboo violations and comply with taboos. Counterintuitively, our results show that an arbitrary taboo (a "silly rule") can enhance social learning dynamics and achieve better outcomes in the middle stages of learning. We discuss the results in the context of studying normativity as a group-level emergent phenomenon.}
      \field{day}{25}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{1}
      \field{pubstate}{prepublished}
      \field{title}{Silly Rules Improve the Capacity of Agents to Learn Stable Enforcement and Compliance Behaviors}
      \field{urlday}{17}
      \field{urlmonth}{8}
      \field{urlyear}{2023}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2001.09318
      \endverb
      \verb{eprint}
      \verb 2001.09318
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/E86J9BM9/Köster et al_2020_Silly rules improve the capacity of agents to learn stable enforcement and.pdf;/Users/brandonhosley/Zotero/storage/5RUU4NII/2001.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2001.09318
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2001.09318
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems}
    \endentry
    \entry{leibo2021}{inproceedings}{}{}
      \name{author}{10}{}{%
        {{hash=698896c85bec4f93adca09e2a21cdf78}{%
           family={Leibo},
           familyi={L\bibinitperiod},
           given={Joel\bibnamedelima Z.},
           giveni={J\bibinitperiod\bibinitdelim Z\bibinitperiod}}}%
        {{hash=9cdb350d500215943fb341d35562135f}{%
           family={Dueñez-Guzman},
           familyi={D\bibinithyphendelim G\bibinitperiod},
           given={Edgar\bibnamedelima A.},
           giveni={E\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=c1e61cb8b50d4e339d37abb4716fed2f}{%
           family={Vezhnevets},
           familyi={V\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
        {{hash=e4fa217e56ca1781ab11713ab27cf2b4}{%
           family={Agapiou},
           familyi={A\bibinitperiod},
           given={John\bibnamedelima P.},
           giveni={J\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=daf6cbed703aa44e31d950c3c2720056}{%
           family={Sunehag},
           familyi={S\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
        {{hash=b7e696bb21d8792432e4ddbb1668839e}{%
           family={Koster},
           familyi={K\bibinitperiod},
           given={Raphael},
           giveni={R\bibinitperiod}}}%
        {{hash=c83c8bcac45e46ecbae55ea990780c0a}{%
           family={Matyas},
           familyi={M\bibinitperiod},
           given={Jayd},
           giveni={J\bibinitperiod}}}%
        {{hash=c6ccf15676d3236596232dfadad38ac6}{%
           family={Beattie},
           familyi={B\bibinitperiod},
           given={Charlie},
           giveni={C\bibinitperiod}}}%
        {{hash=7570e7c3fc2c13d59af4d7cdb9962a4d}{%
           family={Mordatch},
           familyi={M\bibinitperiod},
           given={Igor},
           giveni={I\bibinitperiod}}}%
        {{hash=368b9b2de627b852658c433b062d4e1e}{%
           family={Graepel},
           familyi={G\bibinitperiod},
           given={Thore},
           giveni={T\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{01a06d76ef2b39d070e5b4e6e26e7f0c}
      \strng{fullhash}{f1a737a9866a7d3cd9c695e38313725a}
      \strng{fullhashraw}{f1a737a9866a7d3cd9c695e38313725a}
      \strng{bibnamehash}{01a06d76ef2b39d070e5b4e6e26e7f0c}
      \strng{authorbibnamehash}{01a06d76ef2b39d070e5b4e6e26e7f0c}
      \strng{authornamehash}{01a06d76ef2b39d070e5b4e6e26e7f0c}
      \strng{authorfullhash}{f1a737a9866a7d3cd9c695e38313725a}
      \strng{authorfullhashraw}{f1a737a9866a7d3cd9c695e38313725a}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Existing evaluation suites for multi-agent reinforcement learning (MARL) do not assess generalization to novel situations as their primary objective (unlike supervised learning benchmarks). Our contribution, Melting Pot, is a MARL evaluation suite that fills this gap and uses reinforcement learning to reduce the human labor required to create novel test scenarios. This works because one agent’s behavior constitutes (part of) another agent’s environment. To demonstrate scalability, we have created over 80 unique test scenarios covering a broad range of research topics such as social dilemmas, reciprocity, resource sharing, and task partitioning. We apply these test scenarios to standard MARL training algorithms, and demonstrate how Melting Pot reveals weaknesses not apparent from training performance alone.}
      \field{booktitle}{Proceedings of the 38th {{International Conference}} on {{Machine Learning}}}
      \field{day}{1}
      \field{eventtitle}{International {{Conference}} on {{Machine Learning}}}
      \field{issn}{2640-3498}
      \field{langid}{english}
      \field{month}{7}
      \field{title}{Scalable {{Evaluation}} of {{Multi-Agent Reinforcement Learning}} with {{Melting Pot}}}
      \field{urlday}{21}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{6187\bibrangedash 6199}
      \range{pages}{13}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/3JIXPLFK/Leibo et al. - 2021 - Scalable Evaluation of Multi-Agent Reinforcement L.pdf;/Users/brandonhosley/Zotero/storage/RTMNFLZN/Leibo et al_2021_Scalable Evaluation of Multi-Agent Reinforcement Learning with Melting Pot.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v139/leibo21a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v139/leibo21a.html
      \endverb
    \endentry
    \entry{guo2024}{online}{}{}
      \name{author}{4}{}{%
        {{hash=1c2bf81b5c6942ce905c399b610f0139}{%
           family={Guo},
           familyi={G\bibinitperiod},
           given={Xudong},
           giveni={X\bibinitperiod}}}%
        {{hash=17e62ce42a4dcfea4157418890de2a16}{%
           family={Shi},
           familyi={S\bibinitperiod},
           given={Daming},
           giveni={D\bibinitperiod}}}%
        {{hash=516d3f191d8c8a0f30577af4d59ac160}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Junjie},
           giveni={J\bibinitperiod}}}%
        {{hash=58e9a114f77a9bbce3d73aac6ab241fa}{%
           family={Fan},
           familyi={F\bibinitperiod},
           given={Wenhui},
           giveni={W\bibinitperiod}}}%
      }
      \strng{namehash}{c4d6c31c6af9a6a12169a8745f840974}
      \strng{fullhash}{4d61092ffb2b98a47397f05e62b77284}
      \strng{fullhashraw}{4d61092ffb2b98a47397f05e62b77284}
      \strng{bibnamehash}{4d61092ffb2b98a47397f05e62b77284}
      \strng{authorbibnamehash}{4d61092ffb2b98a47397f05e62b77284}
      \strng{authornamehash}{c4d6c31c6af9a6a12169a8745f840974}
      \strng{authorfullhash}{4d61092ffb2b98a47397f05e62b77284}
      \strng{authorfullhashraw}{4d61092ffb2b98a47397f05e62b77284}
      \field{extraname}{1}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The emergence of multi-agent reinforcement learning (MARL) is significantly transforming various fields like autonomous vehicle networks. However, real-world multi-agent systems typically contain multiple roles, and the scale of these systems dynamically fluctuates. Consequently, in order to achieve zero-shot scalable collaboration, it is essential that strategies for different roles can be updated flexibly according to the scales, which is still a challenge for current MARL frameworks. To address this, we propose a novel MARL framework named Scalable and Heterogeneous Proximal Policy Optimization (SHPPO), integrating heterogeneity into parameter-shared PPO-based MARL networks. We first leverage a latent network to learn strategy patterns for each agent adaptively. Second, we introduce a heterogeneous layer to be inserted into decision-making networks, whose parameters are specifically generated by the learned latent variables. Our approach is scalable as all the parameters are shared except for the heterogeneous layer, and gains both inter-individual and temporal heterogeneity, allowing SHPPO to adapt effectively to varying scales. SHPPO exhibits superior performance in classic MARL environments like Starcraft Multi-Agent Challenge (SMAC) and Google Research Football (GRF), showcasing enhanced zero-shot scalability, and offering insights into the learned latent variables' impact on team performance by visualization.}
      \field{day}{2}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{10}
      \field{pubstate}{prepublished}
      \field{title}{Heterogeneous {{Multi-Agent Reinforcement Learning}} for {{Zero-Shot Scalable Collaboration}}}
      \field{urlday}{20}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2404.03869
      \endverb
      \verb{eprint}
      \verb 2404.03869
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/PV5L5KGI/Guo et al. - 2024 - Heterogeneous Multi-Agent Reinforcement Learning f.pdf;/Users/brandonhosley/Zotero/storage/ITWRM7CS/2404.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2404.03869
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2404.03869
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Computer Science - Robotics,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control}
    \endentry
    \entry{zotero-2605}{online}{}{}
      \list{organization}{1}{%
        {SpringerLink}%
      }
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labeltitlesource}{title}
      \field{abstract}{The journal provides a leading forum for disseminating significant original research results in the foundations, theory, development, analysis, and ...}
      \field{langid}{english}
      \field{title}{Autonomous {{Agents}} and {{Multi-Agent Systems}}}
      \field{urlday}{18}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/XCU7R6JA/aims-and-scope.html
      \endverb
      \verb{urlraw}
      \verb https://link.springer.com/journal/10458/aims-and-scope
      \endverb
      \verb{url}
      \verb https://link.springer.com/journal/10458/aims-and-scope
      \endverb
    \endentry
    \entry{baker2019}{inproceedings}{}{}
      \name{author}{7}{}{%
        {{hash=779704e9a9d50c2ced00816ca81c4bda}{%
           family={Baker},
           familyi={B\bibinitperiod},
           given={Bowen},
           giveni={B\bibinitperiod}}}%
        {{hash=4f48144fbadd76d831d2f2bb8f1b3197}{%
           family={Kanitscheider},
           familyi={K\bibinitperiod},
           given={Ingmar},
           giveni={I\bibinitperiod}}}%
        {{hash=7d22c5307fa9df5a494775d00c45f6a0}{%
           family={Markov},
           familyi={M\bibinitperiod},
           given={Todor},
           giveni={T\bibinitperiod}}}%
        {{hash=e2101a0f6a72a2fb022cd3e2d45461e1}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Yi},
           giveni={Y\bibinitperiod}}}%
        {{hash=34d0d7625e65761eaef54be0603bbf12}{%
           family={Powell},
           familyi={P\bibinitperiod},
           given={Glenn},
           giveni={G\bibinitperiod}}}%
        {{hash=6da1a977c02cecc78ec16c61217dcc42}{%
           family={McGrew},
           familyi={M\bibinitperiod},
           given={Bob},
           giveni={B\bibinitperiod}}}%
        {{hash=7570e7c3fc2c13d59af4d7cdb9962a4d}{%
           family={Mordatch},
           familyi={M\bibinitperiod},
           given={Igor},
           giveni={I\bibinitperiod}}}%
      }
      \strng{namehash}{6b5e46212c5d5ce1523125c5a8366eb4}
      \strng{fullhash}{9d001ffb5b4c91b86de6632cd84af95e}
      \strng{fullhashraw}{9d001ffb5b4c91b86de6632cd84af95e}
      \strng{bibnamehash}{6b5e46212c5d5ce1523125c5a8366eb4}
      \strng{authorbibnamehash}{6b5e46212c5d5ce1523125c5a8366eb4}
      \strng{authornamehash}{6b5e46212c5d5ce1523125c5a8366eb4}
      \strng{authorfullhash}{9d001ffb5b4c91b86de6632cd84af95e}
      \strng{authorfullhashraw}{9d001ffb5b4c91b86de6632cd84af95e}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.}
      \field{day}{25}
      \field{eventtitle}{International {{Conference}} on {{Learning Representations}}}
      \field{langid}{english}
      \field{month}{9}
      \field{title}{Emergent {{Tool Use From Multi-Agent Autocurricula}}}
      \field{urlday}{21}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/J65PVYA3/Baker et al. - 2019 - Emergent Tool Use From Multi-Agent Autocurricula.pdf
      \endverb
      \verb{urlraw}
      \verb https://openreview.net/forum?id=SkxpxJBKwS
      \endverb
      \verb{url}
      \verb https://openreview.net/forum?id=SkxpxJBKwS
      \endverb
    \endentry
    \entry{brambilla2013}{article}{}{}
      \name{author}{4}{}{%
        {{hash=a8079e3675150c189ea840c78fa2474c}{%
           family={Brambilla},
           familyi={B\bibinitperiod},
           given={Manuele},
           giveni={M\bibinitperiod}}}%
        {{hash=95db106c3b066f5b0191f7d8ff170ae9}{%
           family={Ferrante},
           familyi={F\bibinitperiod},
           given={Eliseo},
           giveni={E\bibinitperiod}}}%
        {{hash=97c2ed3dc5dc65fd915a8be83f7011dc}{%
           family={Birattari},
           familyi={B\bibinitperiod},
           given={Mauro},
           giveni={M\bibinitperiod}}}%
        {{hash=26c8dbd8b76182fc666371044ae8dfa4}{%
           family={Dorigo},
           familyi={D\bibinitperiod},
           given={Marco},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{bb1fc2b89f3ed9592a880438434f9af3}
      \strng{fullhash}{32c0c09e006243df068398e883cff27b}
      \strng{fullhashraw}{32c0c09e006243df068398e883cff27b}
      \strng{bibnamehash}{32c0c09e006243df068398e883cff27b}
      \strng{authorbibnamehash}{32c0c09e006243df068398e883cff27b}
      \strng{authornamehash}{bb1fc2b89f3ed9592a880438434f9af3}
      \strng{authorfullhash}{32c0c09e006243df068398e883cff27b}
      \strng{authorfullhashraw}{32c0c09e006243df068398e883cff27b}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Swarm robotics is an approach to collective robotics that takes inspiration from the self-organized behaviors of social animals. Through simple rules and local interactions, swarm robotics aims at designing robust, scalable, and flexible collective behaviors for the coordination of large numbers of robots. In this paper, we analyze the literature from the point of view of swarm engineering: we focus mainly on ideas and concepts that contribute to the advancement of swarm robotics as an engineering field and that could be relevant to tackle real-world applications. Swarm engineering is an emerging discipline that aims at defining systematic and well founded procedures for modeling, designing, realizing, verifying, validating, operating, and maintaining a swarm robotics system. We propose two taxonomies: in the first taxonomy, we classify works that deal with design and analysis methods; in the second taxonomy, we classify works according to the collective behavior studied. We conclude with a discussion of the current limits of swarm robotics as an engineering discipline and with suggestions for future research directions.}
      \field{day}{1}
      \field{journaltitle}{Swarm Intelligence}
      \field{month}{3}
      \field{shortjournal}{Swarm Intelligence}
      \field{shorttitle}{Swarm {{Robotics}}}
      \field{title}{Swarm {{Robotics}}: {{A Review}} from the {{Swarm Engineering Perspective}}}
      \field{volume}{7}
      \field{year}{2013}
      \field{dateera}{ce}
      \true{nocite}
      \field{pages}{1\bibrangedash 41}
      \range{pages}{41}
      \verb{doi}
      \verb 10.1007/s11721-012-0075-2
      \endverb
    \endentry
    \entry{buchanan1984}{book}{}{}
      \name{author}{2}{}{%
        {{hash=eff2611de42fbc5bb2509d4e48d4b50c}{%
           family={Buchanan},
           familyi={B\bibinitperiod},
           given={Bruce},
           giveni={B\bibinitperiod}}}%
        {{hash=527264dca1fb07de7edf94bb720d6c84}{%
           family={Shortliffe},
           familyi={S\bibinitperiod},
           given={Edward},
           giveni={E\bibinitperiod}}}%
      }
      \strng{namehash}{56da5aae58ca20732bd5eef4659d24f5}
      \strng{fullhash}{56da5aae58ca20732bd5eef4659d24f5}
      \strng{fullhashraw}{56da5aae58ca20732bd5eef4659d24f5}
      \strng{bibnamehash}{56da5aae58ca20732bd5eef4659d24f5}
      \strng{authorbibnamehash}{56da5aae58ca20732bd5eef4659d24f5}
      \strng{authornamehash}{56da5aae58ca20732bd5eef4659d24f5}
      \strng{authorfullhash}{56da5aae58ca20732bd5eef4659d24f5}
      \strng{authorfullhashraw}{56da5aae58ca20732bd5eef4659d24f5}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Full-text available online at http://www.shortliffe.net/Buchanan-Shortliffe-1984/MYCIN Book.htm or at http://aitopics.org/publication/rule-based-expert-systems-mycin-experiments-stanford-heuristic-programming-project Artificial intelligence, or AI, is largely an experimental science—at least as much progress has been made by building and analyzing programs as by examining theoretical questions. MYCIN is one of several well-known programs that embody some intelligence and provide data on the extent to which intelligent behavior can be programmed. As with other AI programs, its development was slow and not always in a forward direction. But we feel we learned some useful lessons in the course of nearly a decade of work on MYCIN and related programs. In this book we share the results of many experiments performed in that time, and we try to paint a coherent picture of the work. The book is intended to be a critical analysis of several pieces of related research, performed by a large number of scientists. We believe that the whole field of AI will benefit from such attempts to take a detailed retrospective look at experiments, for in this way the scientific foundations of the field will gradually be defined. It is for all these reasons that we have prepared this analysis of the MYCIN experiments.}
      \field{day}{1}
      \field{journaltitle}{SERBIULA (sistema Librum 2.0)}
      \field{month}{1}
      \field{title}{Rule-Based {{Expert System}} – {{The MYCIN Experiments}} of the {{Stanford Heuristic Programming Project}}}
      \field{year}{1984}
      \field{dateera}{ce}
      \true{nocite}
    \endentry
    \entry{gleave2021}{online}{}{}
      \name{author}{6}{}{%
        {{hash=f443af3aea10cb9b968ba9d4c0dcbd0d}{%
           family={Gleave},
           familyi={G\bibinitperiod},
           given={Adam},
           giveni={A\bibinitperiod}}}%
        {{hash=0a1a38a21af1799da5600f9fa75da3a6}{%
           family={Dennis},
           familyi={D\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=74ac80f06fe0a680add67e03140dfce5}{%
           family={Wild},
           familyi={W\bibinitperiod},
           given={Cody},
           giveni={C\bibinitperiod}}}%
        {{hash=8ee18864b81378cf58c41e04df4156b8}{%
           family={Kant},
           familyi={K\bibinitperiod},
           given={Neel},
           giveni={N\bibinitperiod}}}%
        {{hash=c9545da54b33744da25943cdf66eadac}{%
           family={Levine},
           familyi={L\bibinitperiod},
           given={Sergey},
           giveni={S\bibinitperiod}}}%
        {{hash=143fa183327d9fcd9de18eec99d6ca97}{%
           family={Russell},
           familyi={R\bibinitperiod},
           given={Stuart},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{23de2d798ba9a5a61371e3275f7b780c}
      \strng{fullhash}{4b6b806bab0b83b2a92ccfaee80e1aa4}
      \strng{fullhashraw}{4b6b806bab0b83b2a92ccfaee80e1aa4}
      \strng{bibnamehash}{4b6b806bab0b83b2a92ccfaee80e1aa4}
      \strng{authorbibnamehash}{4b6b806bab0b83b2a92ccfaee80e1aa4}
      \strng{authornamehash}{23de2d798ba9a5a61371e3275f7b780c}
      \strng{authorfullhash}{4b6b806bab0b83b2a92ccfaee80e1aa4}
      \strng{authorfullhashraw}{4b6b806bab0b83b2a92ccfaee80e1aa4}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at https://adversarialpolicies.github.io/.}
      \field{day}{17}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{1}
      \field{pubstate}{prepublished}
      \field{shorttitle}{Adversarial {{Policies}}}
      \field{title}{Adversarial {{Policies}}: {{Attacking Deep Reinforcement Learning}}}
      \field{urlday}{21}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \verb{doi}
      \verb 10.48550/arXiv.1905.10615
      \endverb
      \verb{eprint}
      \verb 1905.10615
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/MJUBPQM3/Gleave et al_2021_Adversarial Policies.pdf;/Users/brandonhosley/Zotero/storage/N3F82HTT/1905.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1905.10615
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1905.10615
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{guo2022}{inproceedings}{}{}
      \name{author}{6}{}{%
        {{hash=febec431d07cf7e676489756ac13c50a}{%
           family={Guo},
           familyi={G\bibinitperiod},
           given={Jun},
           giveni={J\bibinitperiod}}}%
        {{hash=7b82d2b03abb75b096e83080823c3ae3}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Yonghong},
           giveni={Y\bibinitperiod}}}%
        {{hash=1416e525fd9f38d8f1cbfd438b8e866c}{%
           family={Hao},
           familyi={H\bibinitperiod},
           given={Yihang},
           giveni={Y\bibinitperiod}}}%
        {{hash=3c0ab0d11aab16d58c42e81255873743}{%
           family={Yin},
           familyi={Y\bibinitperiod},
           given={Zixin},
           giveni={Z\bibinitperiod}}}%
        {{hash=d02196f42625cb1981c00eda6ca7ffbc}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Yin},
           giveni={Y\bibinitperiod}}}%
        {{hash=0a50f55a790bd639105dc4bb1e7edb2b}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Simin},
           giveni={S\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {New Orleans, LA, USA}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{27170fe86a602c3e9608275e2a66b0c0}
      \strng{fullhash}{ff64b0cb937f18b192ccc7b32cca0e59}
      \strng{fullhashraw}{ff64b0cb937f18b192ccc7b32cca0e59}
      \strng{bibnamehash}{ff64b0cb937f18b192ccc7b32cca0e59}
      \strng{authorbibnamehash}{ff64b0cb937f18b192ccc7b32cca0e59}
      \strng{authornamehash}{27170fe86a602c3e9608275e2a66b0c0}
      \strng{authorfullhash}{ff64b0cb937f18b192ccc7b32cca0e59}
      \strng{authorfullhashraw}{ff64b0cb937f18b192ccc7b32cca0e59}
      \field{extraname}{2}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{While deep neural networks (DNNs) have strengthened the performance of cooperative multi-agent reinforcement learning (c-MARL), the agent policy can be easily perturbed by adversarial examples. Considering the safety critical applications of c-MARL, such as traffic management, power management and unmanned aerial vehicle control, it is crucial to test the robustness of c-MARL algorithm before it was deployed in reality. Existing adversarial attacks for MARL could be used for testing, but is limited to one robustness aspects (e.g., reward, state, action), while c-MARL model could be attacked from any aspect. To overcome the challenge, we propose MARLSafe, the first robustness testing framework for c-MARL algorithms. First, motivated by Markov Decision Process (MDP), MARLSafe consider the robustness of c-MARL algorithms comprehensively from three aspects, namely state robustness, action robustness and reward robustness. Any c-MARL algorithm must simultaneously satisfy these robustness aspects to be considered secure. Second, due to the scarceness of cMARL attack, we propose c-MARL attacks as robustness testing algorithms from multiple aspects. Experiments on SMAC environment reveals that many state-of-the-art cMARL algorithms are of low robustness in all aspect, pointing out the urgent need to test and enhance robustness of c-MARL algorithms.}
      \field{booktitle}{2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})}
      \field{eventtitle}{2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})}
      \field{isbn}{978-1-66548-739-9}
      \field{langid}{english}
      \field{month}{6}
      \field{title}{Towards {{Comprehensive Testing}} on the {{Robustness}} of {{Cooperative Multi-agent Reinforcement Learning}}}
      \field{urlday}{21}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \field{pages}{114\bibrangedash 121}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1109/CVPRW56347.2022.00022
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/LFXQ2EB5/Guo et al. - 2022 - Towards Comprehensive Testing on the Robustness of.pdf
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/9857346/
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/9857346/
      \endverb
    \endentry
    \entry{hao2022}{inproceedings}{}{}
      \name{author}{8}{}{%
        {{hash=8f534f1e9ccc15c7efd4e787cda9a148}{%
           family={Hao},
           familyi={H\bibinitperiod},
           given={Jianye},
           giveni={J\bibinitperiod}}}%
        {{hash=efe34afec9afc150088fa4a9106ad5ab}{%
           family={Hao},
           familyi={H\bibinitperiod},
           given={Xiaotian},
           giveni={X\bibinitperiod}}}%
        {{hash=429256dbc53f2fb070555d63ad43e27a}{%
           family={Mao},
           familyi={M\bibinitperiod},
           given={Hangyu},
           giveni={H\bibinitperiod}}}%
        {{hash=29d73b73b328530ce54e9af1c6311a76}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Weixun},
           giveni={W\bibinitperiod}}}%
        {{hash=616d3b0b19e3536f5c65dfab2a48a659}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Yaodong},
           giveni={Y\bibinitperiod}}}%
        {{hash=ecfd1d34e9c9c1aec231ae3da61ff906}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Dong},
           giveni={D\bibinitperiod}}}%
        {{hash=6daf989b29340de2216f8031aeb90474}{%
           family={Zheng},
           familyi={Z\bibinitperiod},
           given={Yan},
           giveni={Y\bibinitperiod}}}%
        {{hash=d23f8780b31a9738eb2ec5f9c954bcb4}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Zhen},
           giveni={Z\bibinitperiod}}}%
      }
      \strng{namehash}{bb48611f66f1b081905520e0adf03bd4}
      \strng{fullhash}{eb6028fbc0db431c1ebd1b911e4af4a6}
      \strng{fullhashraw}{eb6028fbc0db431c1ebd1b911e4af4a6}
      \strng{bibnamehash}{bb48611f66f1b081905520e0adf03bd4}
      \strng{authorbibnamehash}{bb48611f66f1b081905520e0adf03bd4}
      \strng{authornamehash}{bb48611f66f1b081905520e0adf03bd4}
      \strng{authorfullhash}{eb6028fbc0db431c1ebd1b911e4af4a6}
      \strng{authorfullhashraw}{eb6028fbc0db431c1ebd1b911e4af4a6}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The state space in Multiagent Reinforcement Learning (MARL) grows exponentially with the agent number. Such a curse of dimensionality results in poor scalability and low sample efficiency, inhibiting MARL for decades. To break this curse, we propose a unified agent permutation framework that exploits the permutation invariance (PI) and permutation equivariance (PE) inductive biases to reduce the multiagent state space. Our insight is that permuting the order of entities in the factored multiagent state space does not change the information. Specifically, we propose two novel implementations: a Dynamic Permutation Network (DPN) and a Hyper Policy Network (HPN). The core idea is to build separate entity-wise PI input and PE output network modules to connect the entity-factored state space and action space in an end-to-end way. DPN achieves such connections by two separate module selection networks, which consistently assign the same input module to the same input entity (guarantee PI) and assign the same output module to the same entity-related output (guarantee PE). To enhance the representation capability, HPN replaces the module selection networks of DPN with hypernetworks to directly generate the corresponding module weights. Extensive experiments in SMAC, Google Research Football and MPE validate that the proposed methods significantly boost the performance and the learning efficiency of existing MARL algorithms. Remarkably, in SMAC, we achieve 100\% win rates in almost all hard and super-hard scenarios (never achieved before).}
      \field{day}{29}
      \field{eventtitle}{The {{Eleventh International Conference}} on {{Learning Representations}}}
      \field{langid}{english}
      \field{month}{9}
      \field{title}{Boosting {{Multiagent Reinforcement Learning}} via {{Permutation Invariant}} and {{Permutation Equivariant Networks}}}
      \field{urlday}{14}
      \field{urlmonth}{5}
      \field{urlyear}{2025}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/Z8RJJHRJ/Hao et al_2022_Boosting Multiagent Reinforcement Learning via Permutation Invariant and.pdf
      \endverb
      \verb{urlraw}
      \verb https://openreview.net/forum?id=OxNQXyZK-K8
      \endverb
      \verb{url}
      \verb https://openreview.net/forum?id=OxNQXyZK-K8
      \endverb
    \endentry
    \entry{hazra2024}{inproceedings}{}{}
      \name{author}{3}{}{%
        {{hash=289a2a5285e0ed2c6b7b92c1f7c575ce}{%
           family={Hazra},
           familyi={H\bibinitperiod},
           given={Somnath},
           giveni={S\bibinitperiod}}}%
        {{hash=0ca334cf4c5e0c0439e040099f8011e9}{%
           family={Dasgupta},
           familyi={D\bibinitperiod},
           given={Pallab},
           giveni={P\bibinitperiod}}}%
        {{hash=243d2061b84ce3299e33cacc5509b1c6}{%
           family={Dey},
           familyi={D\bibinitperiod},
           given={Soumyajit},
           giveni={S\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Richland, SC}%
      }
      \list{publisher}{1}{%
        {International Foundation for Autonomous Agents and Multiagent Systems}%
      }
      \strng{namehash}{cf63341eab814d945b939d45cdd93c4a}
      \strng{fullhash}{9ea219014e2c97805a0328d289e48852}
      \strng{fullhashraw}{9ea219014e2c97805a0328d289e48852}
      \strng{bibnamehash}{9ea219014e2c97805a0328d289e48852}
      \strng{authorbibnamehash}{9ea219014e2c97805a0328d289e48852}
      \strng{authornamehash}{cf63341eab814d945b939d45cdd93c4a}
      \strng{authorfullhash}{9ea219014e2c97805a0328d289e48852}
      \strng{authorfullhashraw}{9ea219014e2c97805a0328d289e48852}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In Reinforcement Learning, deep neural networks play a crucial role, especially in Multi-Agent Systems. Owing to information from multiple sources, the challenge lies in handling input permutations efficiently, causing sample inefficiency and delayed convergence. Traditional approaches treat each permutation source as individual nodes for inference. Our novel approach integrates an attention mechanism, allowing us to capture temporal dependencies and contextually align inputs. The attention mechanism enhances the alignment process, allowing for improved information processing. Empirical evaluations on SMAC environments demonstrate superior performance compared to baselines, achieving a higher win rate on 68\% of test evaluations.}
      \field{booktitle}{Proceedings of the 23rd {{International Conference}} on {{Autonomous Agents}} and {{Multiagent Systems}}}
      \field{day}{6}
      \field{isbn}{9798400704864}
      \field{month}{5}
      \field{series}{{{AAMAS}} '24}
      \field{title}{Addressing {{Permutation Challenges}} in {{Multi-Agent Reinforcement Learning}}}
      \field{year}{2024}
      \field{dateera}{ce}
      \true{nocite}
      \field{pages}{2303\bibrangedash 2305}
      \range{pages}{3}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/ET6ZSJ76/Hazra et al_2024_Addressing Permutation Challenges in Multi-Agent Reinforcement Learning.pdf
      \endverb
    \endentry
    \entry{howard2020deep}{book}{}{}
      \name{author}{4}{}{%
        {{hash=4d67eba52a6b56932ec7d8d119f801c0}{%
           family={Howard},
           familyi={H\bibinitperiod},
           given={J.},
           giveni={J\bibinitperiod}}}%
        {{hash=3820bb3ae2eea4e0df9490a52484d667}{%
           family={Gugger},
           familyi={G\bibinitperiod},
           given={S.},
           giveni={S\bibinitperiod}}}%
        {{hash=86fc03d0029c254991429f823ddef3e8}{%
           family={Chintala},
           familyi={C\bibinitperiod},
           given={S.},
           giveni={S\bibinitperiod}}}%
        {{hash=3df9cfb5d4acf71ba1f27cd87020448a}{%
           family={Safari},
           familyi={S\bibinitperiod},
           given={an\bibnamedelima O'Reilly\bibnamedelimb Media\bibnamedelima Company},
           giveni={a\bibinitperiod\bibinitdelim O\bibinitperiod\bibinitdelim M\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {O'Reilly Media, Incorporated}%
      }
      \strng{namehash}{0fbd713939cf2fe77454a58676543402}
      \strng{fullhash}{48ce54c06a635b7b3beabd1a09ef448f}
      \strng{fullhashraw}{48ce54c06a635b7b3beabd1a09ef448f}
      \strng{bibnamehash}{48ce54c06a635b7b3beabd1a09ef448f}
      \strng{authorbibnamehash}{48ce54c06a635b7b3beabd1a09ef448f}
      \strng{authornamehash}{0fbd713939cf2fe77454a58676543402}
      \strng{authorfullhash}{48ce54c06a635b7b3beabd1a09ef448f}
      \strng{authorfullhashraw}{48ce54c06a635b7b3beabd1a09ef448f}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{978-1-4920-4552-6}
      \field{title}{Deep Learning for Coders with Fastai and {{PyTorch}}: {{AI}} Applications without a {{PhD}}}
      \field{year}{2020}
      \field{dateera}{ce}
      \true{nocite}
      \verb{urlraw}
      \verb https://books.google.com/books?id=xd6LxgEACAAJ
      \endverb
      \verb{url}
      \verb https://books.google.com/books?id=xd6LxgEACAAJ
      \endverb
    \endentry
    \entry{iqbal2019}{inproceedings}{}{}
      \name{author}{2}{}{%
        {{hash=7fb7c7ca1c2dd528601fdff6327cb704}{%
           family={Iqbal},
           familyi={I\bibinitperiod},
           given={Shariq},
           giveni={S\bibinitperiod}}}%
        {{hash=86e3e396d826309a838ac02c93e21550}{%
           family={Sha},
           familyi={S\bibinitperiod},
           given={Fei},
           giveni={F\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{4a9b2351aaffefc484c7173ffcae4505}
      \strng{fullhash}{4a9b2351aaffefc484c7173ffcae4505}
      \strng{fullhashraw}{4a9b2351aaffefc484c7173ffcae4505}
      \strng{bibnamehash}{4a9b2351aaffefc484c7173ffcae4505}
      \strng{authorbibnamehash}{4a9b2351aaffefc484c7173ffcae4505}
      \strng{authornamehash}{4a9b2351aaffefc484c7173ffcae4505}
      \strng{authorfullhash}{4a9b2351aaffefc484c7173ffcae4505}
      \strng{authorfullhashraw}{4a9b2351aaffefc484c7173ffcae4505}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Reinforcement learning in multi-agent scenarios is important for real-world applications but presents challenges beyond those seen in single-agent settings. We present an actor-critic algorithm that trains decentralized policies in multi-agent settings, using centrally computed critics that share an attention mechanism which selects relevant information for each agent at every timestep. This attention mechanism enables more effective and scalable learning in complex multi-agent environments, when compared to recent approaches. Our approach is applicable not only to cooperative settings with shared rewards, but also individualized reward settings, including adversarial settings, as well as settings that do not provide global states, and it makes no assumptions about the action spaces of the agents. As such, it is flexible enough to be applied to most multi-agent learning problems.}
      \field{booktitle}{Proceedings of the 36th {{International Conference}} on {{Machine Learning}}}
      \field{day}{24}
      \field{eventtitle}{International {{Conference}} on {{Machine Learning}}}
      \field{issn}{2640-3498}
      \field{langid}{english}
      \field{month}{5}
      \field{title}{Actor-{{Attention-Critic}} for {{Multi-Agent Reinforcement Learning}}}
      \field{urlday}{14}
      \field{urlmonth}{5}
      \field{urlyear}{2025}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \field{pages}{2961\bibrangedash 2970}
      \range{pages}{10}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/F347RG3Q/Iqbal_Sha_2019_Actor-Attention-Critic for Multi-Agent Reinforcement Learning.pdf;/Users/brandonhosley/Zotero/storage/FYTX3WJ8/Iqbal and Sha - 2019 - Actor-Attention-Critic for Multi-Agent Reinforceme.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v97/iqbal19a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v97/iqbal19a.html
      \endverb
    \endentry
    \entry{kapetanakis2005}{inproceedings}{}{}
      \name{author}{2}{}{%
        {{hash=a306a8149c0c95303cfd660c47f290bc}{%
           family={Kapetanakis},
           familyi={K\bibinitperiod},
           given={Spiros},
           giveni={S\bibinitperiod}}}%
        {{hash=752e7b3c05f8277152ea44286065a69f}{%
           family={Kudenko},
           familyi={K\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
      }
      \name{editor}{3}{}{%
        {{hash=752e7b3c05f8277152ea44286065a69f}{%
           family={Kudenko},
           familyi={K\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
        {{hash=f11f4601e1749bc2699f258472cf27b0}{%
           family={Kazakov},
           familyi={K\bibinitperiod},
           given={Dimitar},
           giveni={D\bibinitperiod}}}%
        {{hash=323111175d843c8c67f0a5e46230e164}{%
           family={Alonso},
           familyi={A\bibinitperiod},
           given={Eduardo},
           giveni={E\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Berlin, Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{88f592ebb6c0bcd888aa17c468e9be9f}
      \strng{fullhash}{88f592ebb6c0bcd888aa17c468e9be9f}
      \strng{fullhashraw}{88f592ebb6c0bcd888aa17c468e9be9f}
      \strng{bibnamehash}{88f592ebb6c0bcd888aa17c468e9be9f}
      \strng{authorbibnamehash}{88f592ebb6c0bcd888aa17c468e9be9f}
      \strng{authornamehash}{88f592ebb6c0bcd888aa17c468e9be9f}
      \strng{authorfullhash}{88f592ebb6c0bcd888aa17c468e9be9f}
      \strng{authorfullhashraw}{88f592ebb6c0bcd888aa17c468e9be9f}
      \strng{editorbibnamehash}{4ba440940b0c7ce30e6b09a4859282ab}
      \strng{editornamehash}{a1a7789cfda61ecf939f6b671583a371}
      \strng{editorfullhash}{4ba440940b0c7ce30e6b09a4859282ab}
      \strng{editorfullhashraw}{4ba440940b0c7ce30e6b09a4859282ab}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Most approaches to the learning of coordination in multi-agent systems (MAS) to date require all agents to use the same learning algorithm with similar (or even the same) parameter settings. In today’s open networks and high inter-connectivity such an assumption becomes increasingly unrealistic. Developers are starting to have less control over the agents that join the system and the learning algorithms they employ. This makes effective coordination and good learning performance extremely difficult to achieve, especially in the absence of learning agent standards. In this paper we investigate the problem of learning to coordinate with heterogeneous agents. We show that an agent employing the FMQ algorithm, a recently developed multi-agent learning method, has the ability to converge towards the optimal joint action when teamed-up with one or more simple Q-learners. Specifically, we show such convergence in scenarios where simple Q-learners alone are unable to converge towards an optimum. Our results show that system designers may improve learning and coordination performance by adding a “smart” agent to the MAS.}
      \field{booktitle}{Adaptive {{Agents}} and {{Multi-Agent Systems II}}}
      \field{isbn}{978-3-540-32274-0}
      \field{langid}{english}
      \field{title}{Reinforcement {{Learning}} of {{Coordination}} in {{Heterogeneous Cooperative Multi-agent Systems}}}
      \field{year}{2005}
      \field{dateera}{ce}
      \true{nocite}
      \field{pages}{119\bibrangedash 131}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1007/978-3-540-32274-0_8
      \endverb
    \endentry
    \entry{kimura2024}{online}{}{}
      \name{author}{5}{}{%
        {{hash=2b82f312253370733b91f73ace97c064}{%
           family={Kimura},
           familyi={K\bibinitperiod},
           given={Masanari},
           giveni={M\bibinitperiod}}}%
        {{hash=24df700e08e4abe43547f22ec66a7ac1}{%
           family={Shimizu},
           familyi={S\bibinitperiod},
           given={Ryotaro},
           giveni={R\bibinitperiod}}}%
        {{hash=c4156bffe0336ed27f908131db42c946}{%
           family={Hirakawa},
           familyi={H\bibinitperiod},
           given={Yuki},
           giveni={Y\bibinitperiod}}}%
        {{hash=af2c1ac11f45526cada884ee2317fdee}{%
           family={Goto},
           familyi={G\bibinitperiod},
           given={Ryosuke},
           giveni={R\bibinitperiod}}}%
        {{hash=7474f4f6eb65253b9b8f78ba4491c6af}{%
           family={Saito},
           familyi={S\bibinitperiod},
           given={Yuki},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{a4faad139a82c38c7695daac9d536096}
      \strng{fullhash}{32a3d37af45dda939fe85ba4e0953a2d}
      \strng{fullhashraw}{32a3d37af45dda939fe85ba4e0953a2d}
      \strng{bibnamehash}{32a3d37af45dda939fe85ba4e0953a2d}
      \strng{authorbibnamehash}{32a3d37af45dda939fe85ba4e0953a2d}
      \strng{authornamehash}{a4faad139a82c38c7695daac9d536096}
      \strng{authorfullhash}{32a3d37af45dda939fe85ba4e0953a2d}
      \strng{authorfullhashraw}{32a3d37af45dda939fe85ba4e0953a2d}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Conventional machine learning algorithms have traditionally been designed under the assumption that input data follows a vector-based format, with an emphasis on vector-centric paradigms. However, as the demand for tasks involving set-based inputs has grown, there has been a paradigm shift in the research community towards addressing these challenges. In recent years, the emergence of neural network architectures such as Deep Sets and Transformers has presented a significant advancement in the treatment of set-based data. These architectures are specifically engineered to naturally accommodate sets as input, enabling more effective representation and processing of set structures. Consequently, there has been a surge of research endeavors dedicated to exploring and harnessing the capabilities of these architectures for various tasks involving the approximation of set functions. This comprehensive survey aims to provide an overview of the diverse problem settings and ongoing research efforts pertaining to neural networks that approximate set functions. By delving into the intricacies of these approaches and elucidating the associated challenges, the survey aims to equip readers with a comprehensive understanding of the field. Through this comprehensive perspective, we hope that researchers can gain valuable insights into the potential applications, inherent limitations, and future directions of set-based neural networks. Indeed, from this survey we gain two insights: i) Deep Sets and its variants can be generalized by differences in the aggregation function, and ii) the behavior of Deep Sets is sensitive to the choice of the aggregation function. From these observations, we show that Deep Sets, one of the well-known permutation-invariant neural networks, can be generalized in the sense of a quasi-arithmetic mean.}
      \field{day}{28}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{3}
      \field{pubstate}{prepublished}
      \field{title}{On Permutation-Invariant Neural Networks}
      \field{urlday}{25}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \verb{doi}
      \verb 10.48550/arXiv.2403.17410
      \endverb
      \verb{eprint}
      \verb 2403.17410
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/X2735TTS/Kimura et al_2024_On permutation-invariant neural networks.pdf;/Users/brandonhosley/Zotero/storage/DYBQGT6G/2403.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2403.17410
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2403.17410
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{lee2019}{inproceedings}{}{}
      \name{author}{6}{}{%
        {{hash=40d23cafcc5280932c05fcca24885574}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Juho},
           giveni={J\bibinitperiod}}}%
        {{hash=b8513bf69f4e67a914a8fa28e3888b47}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Yoonho},
           giveni={Y\bibinitperiod}}}%
        {{hash=f3a5b588b00bf3d8184c654d5ba97d6b}{%
           family={Kim},
           familyi={K\bibinitperiod},
           given={Jungtaek},
           giveni={J\bibinitperiod}}}%
        {{hash=72724b5e2a0b5a804db4065728b25a7c}{%
           family={Kosiorek},
           familyi={K\bibinitperiod},
           given={Adam},
           giveni={A\bibinitperiod}}}%
        {{hash=45c988f59df2de470839e889bf0af951}{%
           family={Choi},
           familyi={C\bibinitperiod},
           given={Seungjin},
           giveni={S\bibinitperiod}}}%
        {{hash=a71fae003da84f44e31d26e868859945}{%
           family={Teh},
           familyi={T\bibinitperiod},
           given={Yee\bibnamedelima Whye},
           giveni={Y\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{21816a60d79583f7d8e7a4a853043e50}
      \strng{fullhash}{52f79fe5978c936f6d96c5122475ad98}
      \strng{fullhashraw}{52f79fe5978c936f6d96c5122475ad98}
      \strng{bibnamehash}{52f79fe5978c936f6d96c5122475ad98}
      \strng{authorbibnamehash}{52f79fe5978c936f6d96c5122475ad98}
      \strng{authornamehash}{21816a60d79583f7d8e7a4a853043e50}
      \strng{authorfullhash}{52f79fe5978c936f6d96c5122475ad98}
      \strng{authorfullhashraw}{52f79fe5978c936f6d96c5122475ad98}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.}
      \field{booktitle}{Proceedings of the 36th {{International Conference}} on {{Machine Learning}}}
      \field{day}{24}
      \field{eventtitle}{International {{Conference}} on {{Machine Learning}}}
      \field{issn}{2640-3498}
      \field{langid}{english}
      \field{month}{5}
      \field{shorttitle}{Set {{Transformer}}}
      \field{title}{Set {{Transformer}}: {{A Framework}} for {{Attention-based Permutation-Invariant Neural Networks}}}
      \field{urlday}{14}
      \field{urlmonth}{5}
      \field{urlyear}{2025}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \field{pages}{3744\bibrangedash 3753}
      \range{pages}{10}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/FGT9HZIT/Lee et al. - 2019 - Set Transformer A Framework for Attention-based P.pdf;/Users/brandonhosley/Zotero/storage/PVWBQTUV/Lee et al_2019_Set Transformer.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v97/lee19d.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v97/lee19d.html
      \endverb
    \endentry
    \entry{li2019}{inproceedings}{}{}
      \name{author}{6}{}{%
        {{hash=7266c97c38d539ddac52c6bae5a664c1}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Shihui},
           giveni={S\bibinitperiod}}}%
        {{hash=e2101a0f6a72a2fb022cd3e2d45461e1}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Yi},
           giveni={Y\bibinitperiod}}}%
        {{hash=6ab99857f26211186698796ef27312d9}{%
           family={Cui},
           familyi={C\bibinitperiod},
           given={Xinyue},
           giveni={X\bibinitperiod}}}%
        {{hash=ec5d818996d19f4fa65fb21f1fb075d6}{%
           family={Dong},
           familyi={D\bibinitperiod},
           given={Honghua},
           giveni={H\bibinitperiod}}}%
        {{hash=50229c3fdb5e7f30b2bd918528bdc0df}{%
           family={Fang},
           familyi={F\bibinitperiod},
           given={Fei},
           giveni={F\bibinitperiod}}}%
        {{hash=143fa183327d9fcd9de18eec99d6ca97}{%
           family={Russell},
           familyi={R\bibinitperiod},
           given={Stuart},
           giveni={S\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Honolulu, Hawaii, USA}%
      }
      \list{publisher}{1}{%
        {AAAI Press}%
      }
      \strng{namehash}{a65aa8c66282a339027184311fa2bf20}
      \strng{fullhash}{55e800746dee7221a79263fdec89d92c}
      \strng{fullhashraw}{55e800746dee7221a79263fdec89d92c}
      \strng{bibnamehash}{55e800746dee7221a79263fdec89d92c}
      \strng{authorbibnamehash}{55e800746dee7221a79263fdec89d92c}
      \strng{authornamehash}{a65aa8c66282a339027184311fa2bf20}
      \strng{authorfullhash}{55e800746dee7221a79263fdec89d92c}
      \strng{authorfullhashraw}{55e800746dee7221a79263fdec89d92c}
      \field{extraname}{2}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Despite the recent advances of deep reinforcement learning (DRL), agents trained by DRL tend to be brittle and sensitive to the training environment, especially in the multi-agent scenarios. In the multi-agent setting, a DRL agent's policy can easily get stuck in a poor local optima w.r.t. its training partners - the learned policy may be only locally optimal to other agents' current policies. In this paper, we focus on the problem of training robust DRL agents with continuous actions in the multi-agent learning setting so that the trained agents can still generalize when its opponents' policies alter. To tackle this problem, we proposed a new algorithm, MiniMax Multi-agent Deep Deterministic Policy Gradient (M3DDPG) with the following contributions: (1) we introduce a minimax extension of the popular multi-agent deep deterministic policy gradient algorithm (MADDPG), for robust policy learning; (2) since the continuous action space leads to computational intractability in our minimax learning objective, we propose Multi-Agent Adversarial Learning (MAAL) to efficiently solve our proposed formulation. We empirically evaluate our M3DDPG algorithm in four mixed cooperative and competitive multi-agent environments and the agents trained by our method significantly outperforms existing baselines.}
      \field{booktitle}{Proceedings of the {{Thirty-Third AAAI Conference}} on {{Artificial Intelligence}} and {{Thirty-First Innovative Applications}} of {{Artificial Intelligence Conference}} and {{Ninth AAAI Symposium}} on {{Educational Advances}} in {{Artificial Intelligence}}}
      \field{day}{27}
      \field{isbn}{978-1-57735-809-1}
      \field{month}{1}
      \field{series}{{{AAAI}}'19/{{IAAI}}'19/{{EAAI}}'19}
      \field{title}{Robust Multi-Agent Reinforcement Learning via Minimax Deep Deterministic Policy Gradient}
      \field{urlday}{20}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{volume}{33}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \field{pages}{4213\bibrangedash 4220}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1609/aaai.v33i01.33014213
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/B2DADNNL/Li et al_2019_Robust multi-agent reinforcement learning via minimax deep deterministic policy.pdf
      \endverb
      \verb{urlraw}
      \verb https://dl.acm.org/doi/10.1609/aaai.v33i01.33014213
      \endverb
      \verb{url}
      \verb https://dl.acm.org/doi/10.1609/aaai.v33i01.33014213
      \endverb
    \endentry
    \entry{li2021b}{online}{}{}
      \name{author}{7}{}{%
        {{hash=92e14a8c19eb5a0c7c199c85db74f106}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Yan},
           giveni={Y\bibinitperiod}}}%
        {{hash=d4f96a51137bfc328512a67d7a9065e5}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Lingxiao},
           giveni={L\bibinitperiod}}}%
        {{hash=1f21ad90edcfdb3420a806dea0de128b}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Jiachen},
           giveni={J\bibinitperiod}}}%
        {{hash=af27ba6c036313d1e3b27de6ff616fa8}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Ethan},
           giveni={E\bibinitperiod}}}%
        {{hash=d63cdb851aafe1a8147d77747d26e1d3}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Zhaoran},
           giveni={Z\bibinitperiod}}}%
        {{hash=5aabe643c77f92846adf1b83e9c78387}{%
           family={Zhao},
           familyi={Z\bibinitperiod},
           given={Tuo},
           giveni={T\bibinitperiod}}}%
        {{hash=55bb21a31fded4aad976b7902dc8206a}{%
           family={Zha},
           familyi={Z\bibinitperiod},
           given={Hongyuan},
           giveni={H\bibinitperiod}}}%
      }
      \strng{namehash}{a71fd46be5707627f7ffea030721fc93}
      \strng{fullhash}{db059bb88cbbcf9c255e8dac93712b28}
      \strng{fullhashraw}{db059bb88cbbcf9c255e8dac93712b28}
      \strng{bibnamehash}{a71fd46be5707627f7ffea030721fc93}
      \strng{authorbibnamehash}{a71fd46be5707627f7ffea030721fc93}
      \strng{authornamehash}{a71fd46be5707627f7ffea030721fc93}
      \strng{authorfullhash}{db059bb88cbbcf9c255e8dac93712b28}
      \strng{authorfullhashraw}{db059bb88cbbcf9c255e8dac93712b28}
      \field{extraname}{3}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Multi-agent reinforcement learning (MARL) becomes more challenging in the presence of more agents, as the capacity of the joint state and action spaces grows exponentially in the number of agents. To address such a challenge of scale, we identify a class of cooperative MARL problems with permutation invariance, and formulate it as a mean-field Markov decision processes (MDP). To exploit the permutation invariance therein, we propose the mean-field proximal policy optimization (MF-PPO) algorithm, at the core of which is a permutation-invariant actor-critic neural architecture. We prove that MF-PPO attains the globally optimal policy at a sublinear rate of convergence. Moreover, its sample complexity is independent of the number of agents. We validate the theoretical advantages of MF-PPO with numerical experiments in the multi-agent particle environment (MPE). In particular, we show that the inductive bias introduced by the permutation-invariant neural architecture enables MF-PPO to outperform existing competitors with a smaller number of model parameters, which is the key to its generalization performance.}
      \field{day}{18}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{5}
      \field{pubstate}{prepublished}
      \field{shorttitle}{Permutation {{Invariant Policy Optimization}} for {{Mean-Field Multi-Agent Reinforcement Learning}}}
      \field{title}{Permutation {{Invariant Policy Optimization}} for {{Mean-Field Multi-Agent Reinforcement Learning}}: {{A Principled Approach}}}
      \field{urlday}{14}
      \field{urlmonth}{5}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \verb{doi}
      \verb 10.48550/arXiv.2105.08268
      \endverb
      \verb{eprint}
      \verb 2105.08268
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/SG2RIJN5/Li et al_2021_Permutation Invariant Policy Optimization for Mean-Field Multi-Agent.pdf;/Users/brandonhosley/Zotero/storage/C3SCQED4/2105.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2105.08268
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2105.08268
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Multiagent Systems}
    \endentry
    \entry{liang2018}{online}{}{}
      \name{author}{9}{}{%
        {{hash=882d47329b54117fb0b9bba0fecb0e2b}{%
           family={Liang},
           familyi={L\bibinitperiod},
           given={Eric},
           giveni={E\bibinitperiod}}}%
        {{hash=3eb896f5341af6688c143d8081eac1fe}{%
           family={Liaw},
           familyi={L\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
        {{hash=a901fd78fe1108cfa7d11129644967c7}{%
           family={Moritz},
           familyi={M\bibinitperiod},
           given={Philipp},
           giveni={P\bibinitperiod}}}%
        {{hash=60380b3a2f3cf35fc1d43074792d7567}{%
           family={Nishihara},
           familyi={N\bibinitperiod},
           given={Robert},
           giveni={R\bibinitperiod}}}%
        {{hash=2e2931827e1fdb0741f367fdd045a598}{%
           family={Fox},
           familyi={F\bibinitperiod},
           given={Roy},
           giveni={R\bibinitperiod}}}%
        {{hash=1776deb0a86da5a75d0800ae967f4bd6}{%
           family={Goldberg},
           familyi={G\bibinitperiod},
           given={Ken},
           giveni={K\bibinitperiod}}}%
        {{hash=80175f489665023b28dbb4cf1775c0ef}{%
           family={Gonzalez},
           familyi={G\bibinitperiod},
           given={Joseph\bibnamedelima E.},
           giveni={J\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=8a36116840c7ee55901618c95fd08a58}{%
           family={Jordan},
           familyi={J\bibinitperiod},
           given={Michael\bibnamedelima I.},
           giveni={M\bibinitperiod\bibinitdelim I\bibinitperiod}}}%
        {{hash=51d9ef26181e2e8a9dc546b401e0bb26}{%
           family={Stoica},
           familyi={S\bibinitperiod},
           given={Ion},
           giveni={I\bibinitperiod}}}%
      }
      \strng{namehash}{616477137366f9c946c3545e9e0a351a}
      \strng{fullhash}{3e4e9c14299de68f2f7534d593679d33}
      \strng{fullhashraw}{3e4e9c14299de68f2f7534d593679d33}
      \strng{bibnamehash}{616477137366f9c946c3545e9e0a351a}
      \strng{authorbibnamehash}{616477137366f9c946c3545e9e0a351a}
      \strng{authornamehash}{616477137366f9c946c3545e9e0a351a}
      \strng{authorfullhash}{3e4e9c14299de68f2f7534d593679d33}
      \strng{authorfullhashraw}{3e4e9c14299de68f2f7534d593679d33}
      \field{extraname}{2}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Reinforcement learning (RL) algorithms involve the deep nesting of highly irregular computation patterns, each of which typically exhibits opportunities for distributed computation. We argue for distributing RL components in a composable way by adapting algorithms for top-down hierarchical control, thereby encapsulating parallelism and resource requirements within short-running compute tasks. We demonstrate the benefits of this principle through RLlib: a library that provides scalable software primitives for RL. These primitives enable a broad range of algorithms to be implemented with high performance, scalability, and substantial code reuse. RLlib is available at https://rllib.io/.}
      \field{day}{28}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{6}
      \field{pubstate}{prepublished}
      \field{shorttitle}{{{RLlib}}}
      \field{title}{{{RLlib}}: {{Abstractions}} for {{Distributed Reinforcement Learning}}}
      \field{urlday}{17}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \verb{doi}
      \verb 10.48550/arXiv.1712.09381
      \endverb
      \verb{eprint}
      \verb 1712.09381
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/YGAES56T/Liang et al_2018_RLlib.pdf;/Users/brandonhosley/Zotero/storage/M8KXJK8I/1712.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1712.09381
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1712.09381
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning}
    \endentry
    \entry{liaw2018tune}{unpublished}{}{}
      \name{author}{6}{}{%
        {{hash=3eb896f5341af6688c143d8081eac1fe}{%
           family={Liaw},
           familyi={L\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
        {{hash=882d47329b54117fb0b9bba0fecb0e2b}{%
           family={Liang},
           familyi={L\bibinitperiod},
           given={Eric},
           giveni={E\bibinitperiod}}}%
        {{hash=60380b3a2f3cf35fc1d43074792d7567}{%
           family={Nishihara},
           familyi={N\bibinitperiod},
           given={Robert},
           giveni={R\bibinitperiod}}}%
        {{hash=a901fd78fe1108cfa7d11129644967c7}{%
           family={Moritz},
           familyi={M\bibinitperiod},
           given={Philipp},
           giveni={P\bibinitperiod}}}%
        {{hash=80175f489665023b28dbb4cf1775c0ef}{%
           family={Gonzalez},
           familyi={G\bibinitperiod},
           given={Joseph\bibnamedelima E},
           giveni={J\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=51d9ef26181e2e8a9dc546b401e0bb26}{%
           family={Stoica},
           familyi={S\bibinitperiod},
           given={Ion},
           giveni={I\bibinitperiod}}}%
      }
      \strng{namehash}{bc862ede41e39cd8560817b54f4bdf53}
      \strng{fullhash}{6987a606e6370c1400c8e5f29eb66fcf}
      \strng{fullhashraw}{6987a606e6370c1400c8e5f29eb66fcf}
      \strng{bibnamehash}{6987a606e6370c1400c8e5f29eb66fcf}
      \strng{authorbibnamehash}{6987a606e6370c1400c8e5f29eb66fcf}
      \strng{authornamehash}{bc862ede41e39cd8560817b54f4bdf53}
      \strng{authorfullhash}{6987a606e6370c1400c8e5f29eb66fcf}
      \strng{authorfullhashraw}{6987a606e6370c1400c8e5f29eb66fcf}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{eprinttype}{arXiv}
      \field{title}{Tune: A Research Platform for Distributed Model Selection and Training}
      \field{year}{2018}
      \field{dateera}{ce}
      \true{nocite}
      \verb{eprint}
      \verb 1807.05118
      \endverb
    \endentry
    \entry{lillicrap2019}{online}{}{}
      \name{author}{8}{}{%
        {{hash=2a321a868e44d49baf52b5e2d816fb71}{%
           family={Lillicrap},
           familyi={L\bibinitperiod},
           given={Timothy\bibnamedelima P.},
           giveni={T\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=e0c5cec59a932511b2af6220473fad61}{%
           family={Hunt},
           familyi={H\bibinitperiod},
           given={Jonathan\bibnamedelima J.},
           giveni={J\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=4295e4094c426f01903ac60155866130}{%
           family={Pritzel},
           familyi={P\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
        {{hash=231312991eab5915498d6c19c2a8cd4e}{%
           family={Heess},
           familyi={H\bibinitperiod},
           given={Nicolas},
           giveni={N\bibinitperiod}}}%
        {{hash=3b79ca47f08451987877fd8682e971e5}{%
           family={Erez},
           familyi={E\bibinitperiod},
           given={Tom},
           giveni={T\bibinitperiod}}}%
        {{hash=58112cb88a80c1bc045b6eaab26a8695}{%
           family={Tassa},
           familyi={T\bibinitperiod},
           given={Yuval},
           giveni={Y\bibinitperiod}}}%
        {{hash=89dbd30410c2085cd059f32c57d4593e}{%
           family={Silver},
           familyi={S\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=d7805381550fb5f8360345f7f72c0b49}{%
           family={Wierstra},
           familyi={W\bibinitperiod},
           given={Daan},
           giveni={D\bibinitperiod}}}%
      }
      \strng{namehash}{e9929f8db9dd2c08064f17b4d9a7c551}
      \strng{fullhash}{9f2f0357b439f9ae4a3953d81392aaa0}
      \strng{fullhashraw}{9f2f0357b439f9ae4a3953d81392aaa0}
      \strng{bibnamehash}{e9929f8db9dd2c08064f17b4d9a7c551}
      \strng{authorbibnamehash}{e9929f8db9dd2c08064f17b4d9a7c551}
      \strng{authornamehash}{e9929f8db9dd2c08064f17b4d9a7c551}
      \strng{authorfullhash}{9f2f0357b439f9ae4a3953d81392aaa0}
      \strng{authorfullhashraw}{9f2f0357b439f9ae4a3953d81392aaa0}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.}
      \field{day}{5}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arXiv}
      \field{month}{7}
      \field{pubstate}{prepublished}
      \field{title}{Continuous Control with Deep Reinforcement Learning}
      \field{urlday}{25}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \verb{doi}
      \verb 10.48550/arXiv.1509.02971
      \endverb
      \verb{eprint}
      \verb 1509.02971
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/4SDUFMAZ/Lillicrap et al_2019_Continuous control with deep reinforcement learning.pdf;/Users/brandonhosley/Zotero/storage/YUCJ4FU9/1509.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1509.02971
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1509.02971
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{liu2019b}{online}{}{}
      \name{author}{3}{}{%
        {{hash=1658eb9cc7f03d088ee134bd5c51d441}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Iou-Jen},
           giveni={I\bibinithyphendelim J\bibinitperiod}}}%
        {{hash=53b85b47b32486923f598b78fc90443f}{%
           family={Yeh},
           familyi={Y\bibinitperiod},
           given={Raymond\bibnamedelima A.},
           giveni={R\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=0c14478283bed70b2c487f3fba643a7b}{%
           family={Schwing},
           familyi={S\bibinitperiod},
           given={Alexander\bibnamedelima G.},
           giveni={A\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
      }
      \strng{namehash}{79f4ccfabaa76925cf406dfb3b772aa1}
      \strng{fullhash}{bc7473e651fd56a500db06682bc9392a}
      \strng{fullhashraw}{bc7473e651fd56a500db06682bc9392a}
      \strng{bibnamehash}{bc7473e651fd56a500db06682bc9392a}
      \strng{authorbibnamehash}{bc7473e651fd56a500db06682bc9392a}
      \strng{authornamehash}{79f4ccfabaa76925cf406dfb3b772aa1}
      \strng{authorfullhash}{bc7473e651fd56a500db06682bc9392a}
      \strng{authorfullhashraw}{bc7473e651fd56a500db06682bc9392a}
      \field{extraname}{1}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Sample efficiency and scalability to a large number of agents are two important goals for multi-agent reinforcement learning systems. Recent works got us closer to those goals, addressing non-stationarity of the environment from a single agent's perspective by utilizing a deep net critic which depends on all observations and actions. The critic input concatenates agent observations and actions in a user-specified order. However, since deep nets aren't permutation invariant, a permuted input changes the critic output despite the environment remaining identical. To avoid this inefficiency, we propose a 'permutation invariant critic' (PIC), which yields identical output irrespective of the agent permutation. This consistent representation enables our model to scale to 30 times more agents and to achieve improvements of test episode reward between 15\% to 50\% on the challenging multi-agent particle environment (MPE).}
      \field{day}{31}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{10}
      \field{pubstate}{prepublished}
      \field{shorttitle}{{{PIC}}}
      \field{title}{{{PIC}}: {{Permutation Invariant Critic}} for {{Multi-Agent Deep Reinforcement Learning}}}
      \field{urlday}{14}
      \field{urlmonth}{5}
      \field{urlyear}{2025}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \verb{doi}
      \verb 10.48550/arXiv.1911.00025
      \endverb
      \verb{eprint}
      \verb 1911.00025
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/MJ4D8FTC/Liu et al_2019_PIC.pdf;/Users/brandonhosley/Zotero/storage/X6M69VAA/1911.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1911.00025
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1911.00025
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{liu2020b}{inproceedings}{}{}
      \name{author}{3}{}{%
        {{hash=1658eb9cc7f03d088ee134bd5c51d441}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Iou-Jen},
           giveni={I\bibinithyphendelim J\bibinitperiod}}}%
        {{hash=53b85b47b32486923f598b78fc90443f}{%
           family={Yeh},
           familyi={Y\bibinitperiod},
           given={Raymond\bibnamedelima A.},
           giveni={R\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=0c14478283bed70b2c487f3fba643a7b}{%
           family={Schwing},
           familyi={S\bibinitperiod},
           given={Alexander\bibnamedelima G.},
           giveni={A\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{79f4ccfabaa76925cf406dfb3b772aa1}
      \strng{fullhash}{bc7473e651fd56a500db06682bc9392a}
      \strng{fullhashraw}{bc7473e651fd56a500db06682bc9392a}
      \strng{bibnamehash}{bc7473e651fd56a500db06682bc9392a}
      \strng{authorbibnamehash}{bc7473e651fd56a500db06682bc9392a}
      \strng{authornamehash}{79f4ccfabaa76925cf406dfb3b772aa1}
      \strng{authorfullhash}{bc7473e651fd56a500db06682bc9392a}
      \strng{authorfullhashraw}{bc7473e651fd56a500db06682bc9392a}
      \field{extraname}{2}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Sample efficiency and scalability to a large number of agents are two important goals for multi-agent reinforcement learning systems. Recent works got us closer to those goals, addressing non-stationarity of the environment from a single agent’s perspective by utilizing a deep net critic which depends on all observations and actions. The critic input concatenates agent observations and actions in a user-specified order. However, since deep nets aren’t permutation invariant, a permuted input changes the critic output despite the environment remaining identical. To avoid this inefficiency, we propose a ‘permutation invariant critic’ (PIC), which yields identical output irrespective of the agent permutation. This consistent representation enables our model to scale to 30 times more agents and to achieve improvements of test episode reward between 15\% to 50\% on the challenging multi-agent particle environment (MPE).}
      \field{booktitle}{Proceedings of the {{Conference}} on {{Robot Learning}}}
      \field{day}{12}
      \field{eventtitle}{Conference on {{Robot Learning}}}
      \field{issn}{2640-3498}
      \field{langid}{english}
      \field{month}{5}
      \field{shorttitle}{{{PIC}}}
      \field{title}{{{PIC}}: {{Permutation Invariant Critic}} for {{Multi-Agent Deep Reinforcement Learning}}}
      \field{urlday}{25}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \field{pages}{590\bibrangedash 602}
      \range{pages}{13}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/C5YEN3SD/Liu et al_2020_PIC.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v100/liu20a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v100/liu20a.html
      \endverb
    \endentry
    \entry{liu2022light}{misc}{}{}
      \name{author}{3}{}{%
        {{hash=9828709dfc6cf78a895a3aeb50114ace}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Qihan},
           giveni={Q\bibinitperiod}}}%
        {{hash=6d52115aacda0a6321a63a344b3841de}{%
           family={Jiang},
           familyi={J\bibinitperiod},
           given={Yuhua},
           giveni={Y\bibinitperiod}}}%
        {{hash=b4f64492064746abd78453107aa07fff}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Xiaoteng},
           giveni={X\bibinitperiod}}}%
      }
      \list{organization}{1}{%
        {GitHub}%
      }
      \strng{namehash}{0ea7759f670ef2abb7fb919e7bc62198}
      \strng{fullhash}{acdcdfc7ef8acb66b53c942521e1d33a}
      \strng{fullhashraw}{acdcdfc7ef8acb66b53c942521e1d33a}
      \strng{bibnamehash}{acdcdfc7ef8acb66b53c942521e1d33a}
      \strng{authorbibnamehash}{acdcdfc7ef8acb66b53c942521e1d33a}
      \strng{authornamehash}{0ea7759f670ef2abb7fb919e7bc62198}
      \strng{authorfullhash}{acdcdfc7ef8acb66b53c942521e1d33a}
      \strng{authorfullhashraw}{acdcdfc7ef8acb66b53c942521e1d33a}
      \field{extraname}{3}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Light {{Aircraft Game}}: {{A}} Lightweight, Scalable, Gym-Wrapped Aircraft Competitive Environment with Baseline Reinforcement Learning Algorithms}
      \field{year}{2022}
      \field{dateera}{ce}
      \true{nocite}
      \verb{urlraw}
      \verb https://github.com/liuqh16/CloseAirCombat
      \endverb
      \verb{url}
      \verb https://github.com/liuqh16/CloseAirCombat
      \endverb
    \endentry
    \entry{liu2024a}{article}{}{}
      \name{author}{6}{}{%
        {{hash=fadae23b0b430577cae3409da740ff03}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Dingbang},
           giveni={D\bibinitperiod}}}%
        {{hash=f4f1bc4e9ca203a421edf1820a211539}{%
           family={Ren},
           familyi={R\bibinitperiod},
           given={Fenghui},
           giveni={F\bibinitperiod}}}%
        {{hash=934a0c3e849405255facd8113ba785b3}{%
           family={Yan},
           familyi={Y\bibinitperiod},
           given={Jun},
           giveni={J\bibinitperiod}}}%
        {{hash=e747cf0302cbafb9ecd3af4bd0fd7c32}{%
           family={Su},
           familyi={S\bibinitperiod},
           given={Guoxin},
           giveni={G\bibinitperiod}}}%
        {{hash=b54448d6134d1a5387209864568730e7}{%
           family={Gu},
           familyi={G\bibinitperiod},
           given={Wen},
           giveni={W\bibinitperiod}}}%
        {{hash=ad67f89af215c590242d7fcf75044526}{%
           family={Kato},
           familyi={K\bibinitperiod},
           given={Shohei},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{1d1cbc282514410f4c9281d245b03815}
      \strng{fullhash}{0fc138165549339499b001871bd4fe35}
      \strng{fullhashraw}{0fc138165549339499b001871bd4fe35}
      \strng{bibnamehash}{0fc138165549339499b001871bd4fe35}
      \strng{authorbibnamehash}{0fc138165549339499b001871bd4fe35}
      \strng{authornamehash}{1d1cbc282514410f4c9281d245b03815}
      \strng{authorfullhash}{0fc138165549339499b001871bd4fe35}
      \strng{authorfullhashraw}{0fc138165549339499b001871bd4fe35}
      \field{extraname}{4}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Multi-agent learning has made significant strides in recent years. Benefiting from deep learning, multi-agent deep reinforcement learning (MADRL) has transcended traditional limitations seen in tabular tasks, arousing tremendous research interest. However, compared to other challenges in MADRL, scalability remains underemphasized, impeding the application of MADRL in complex scenarios. Scalability stands as a foundational attribute of the multi-agent system (MAS), offering a potent approach to understand and improve collective learning among agents. It encompasses the capacity to handle the increasing state-action space which arises not only from a large number of agents but also from other factors related to agents and environment. In contrast to prior surveys, this work provides a comprehensive exposition of scalability concerns in MADRL. We first introduce foundational knowledge about deep reinforcement learning and MADRL to underscore the distinctiveness of scalability issues in this domain. Subsequently, we delve into the problems posed by scalability, examining agent complexity, environment complexity, and robustness against perturbation. We elaborate on the methods that demonstrate the evolution of scalable algorithms. To conclude this survey, we discuss challenges, identify trends, and outline possible directions for future work on scalability issues. It is our aspiration that this survey enhances the understanding of researchers in this field, providing a valuable resource for in-depth exploration.}
      \field{eventtitle}{{{IEEE Access}}}
      \field{issn}{2169-3536}
      \field{journaltitle}{IEEE Access}
      \field{shorttitle}{Scaling {{Up Multi-Agent Reinforcement Learning}}}
      \field{title}{Scaling {{Up Multi-Agent Reinforcement Learning}}: {{An Extensive Survey}} on {{Scalability Issues}}}
      \field{urlday}{20}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{volume}{12}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \field{pages}{94610\bibrangedash 94631}
      \range{pages}{22}
      \verb{doi}
      \verb 10.1109/ACCESS.2024.3410318
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/NU8WUBVH/Liu et al. - 2024 - Scaling Up Multi-Agent Reinforcement Learning An .pdf;/Users/brandonhosley/Zotero/storage/Q7NX42EU/10550936.html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/10550936/?arnumber=10550936
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/10550936/?arnumber=10550936
      \endverb
      \keyw{Collective intelligence,collective learning,Complexity theory,Deep reinforcement learning,Games,Multi-agent learning,Multi-agent systems,reinforcement learning,Robustness,scalability,Scalability,Surveys,Task analysis}
    \endentry
    \entry{mnih2013}{online}{}{}
      \name{author}{7}{}{%
        {{hash=f7d23cfe4ca0e6bf7a8c251bfa78aca6}{%
           family={Mnih},
           familyi={M\bibinitperiod},
           given={Volodymyr},
           giveni={V\bibinitperiod}}}%
        {{hash=481308b301166b521c74fde6566e97e6}{%
           family={Kavukcuoglu},
           familyi={K\bibinitperiod},
           given={Koray},
           giveni={K\bibinitperiod}}}%
        {{hash=89dbd30410c2085cd059f32c57d4593e}{%
           family={Silver},
           familyi={S\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=dfca94b0427da7f9088af596e23b46c0}{%
           family={Graves},
           familyi={G\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
        {{hash=af540e84ef1ecdaa70b1f7c90f59fd7d}{%
           family={Antonoglou},
           familyi={A\bibinitperiod},
           given={Ioannis},
           giveni={I\bibinitperiod}}}%
        {{hash=d7805381550fb5f8360345f7f72c0b49}{%
           family={Wierstra},
           familyi={W\bibinitperiod},
           given={Daan},
           giveni={D\bibinitperiod}}}%
        {{hash=9449802bcb467309c0ebced658096818}{%
           family={Riedmiller},
           familyi={R\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{aff24b211b34cdd515b2fd148ce0c920}
      \strng{fullhash}{2860d669102abb5e113bf4232d6d5997}
      \strng{fullhashraw}{2860d669102abb5e113bf4232d6d5997}
      \strng{bibnamehash}{aff24b211b34cdd515b2fd148ce0c920}
      \strng{authorbibnamehash}{aff24b211b34cdd515b2fd148ce0c920}
      \strng{authornamehash}{aff24b211b34cdd515b2fd148ce0c920}
      \strng{authorfullhash}{2860d669102abb5e113bf4232d6d5997}
      \strng{authorfullhashraw}{2860d669102abb5e113bf4232d6d5997}
      \field{extraname}{2}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.}
      \field{day}{19}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{12}
      \field{pubstate}{prepublished}
      \field{title}{Playing {{Atari}} with {{Deep Reinforcement Learning}}}
      \field{urlday}{21}
      \field{urlmonth}{7}
      \field{urlyear}{2024}
      \field{year}{2013}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \verb{doi}
      \verb 10.48550/arXiv.1312.5602
      \endverb
      \verb{eprint}
      \verb 1312.5602
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/6NYMSJY3/Mnih et al_2013_Playing Atari with Deep Reinforcement Learning.pdf;/Users/brandonhosley/Zotero/storage/PADLGVEM/1312.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1312.5602
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1312.5602
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
    \entry{mnih2015}{article}{}{}
      \name{author}{19}{}{%
        {{hash=f7d23cfe4ca0e6bf7a8c251bfa78aca6}{%
           family={Mnih},
           familyi={M\bibinitperiod},
           given={Volodymyr},
           giveni={V\bibinitperiod}}}%
        {{hash=481308b301166b521c74fde6566e97e6}{%
           family={Kavukcuoglu},
           familyi={K\bibinitperiod},
           given={Koray},
           giveni={K\bibinitperiod}}}%
        {{hash=89dbd30410c2085cd059f32c57d4593e}{%
           family={Silver},
           familyi={S\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=677dfee41a39b5ac7e138f5ce14467e9}{%
           family={Rusu},
           familyi={R\bibinitperiod},
           given={Andrei\bibnamedelima A.},
           giveni={A\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=1cbd91f7404b2298b46bb46c47c08251}{%
           family={Veness},
           familyi={V\bibinitperiod},
           given={Joel},
           giveni={J\bibinitperiod}}}%
        {{hash=eae3aa0ab9cb8b33a1ade80739e79e57}{%
           family={Bellemare},
           familyi={B\bibinitperiod},
           given={Marc\bibnamedelima G.},
           giveni={M\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=dfca94b0427da7f9088af596e23b46c0}{%
           family={Graves},
           familyi={G\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
        {{hash=9449802bcb467309c0ebced658096818}{%
           family={Riedmiller},
           familyi={R\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=be3d55306c5ab7ee716f96f137dddba6}{%
           family={Fidjeland},
           familyi={F\bibinitperiod},
           given={Andreas\bibnamedelima K.},
           giveni={A\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=46cc21fb5dec973c05ceb0f321e02ca0}{%
           family={Ostrovski},
           familyi={O\bibinitperiod},
           given={Georg},
           giveni={G\bibinitperiod}}}%
        {{hash=4e381e44037009b1cd834d794735c311}{%
           family={Petersen},
           familyi={P\bibinitperiod},
           given={Stig},
           giveni={S\bibinitperiod}}}%
        {{hash=03cf4f0976cc27112d267a8916a2d169}{%
           family={Beattie},
           familyi={B\bibinitperiod},
           given={Charles},
           giveni={C\bibinitperiod}}}%
        {{hash=b827d5121d467eeb30ccac8c61094591}{%
           family={Sadik},
           familyi={S\bibinitperiod},
           given={Amir},
           giveni={A\bibinitperiod}}}%
        {{hash=af540e84ef1ecdaa70b1f7c90f59fd7d}{%
           family={Antonoglou},
           familyi={A\bibinitperiod},
           given={Ioannis},
           giveni={I\bibinitperiod}}}%
        {{hash=59998a15386f62e4d2776176ab58d49c}{%
           family={King},
           familyi={K\bibinitperiod},
           given={Helen},
           giveni={H\bibinitperiod}}}%
        {{hash=a425c81c03d315597e3f92690763e24d}{%
           family={Kumaran},
           familyi={K\bibinitperiod},
           given={Dharshan},
           giveni={D\bibinitperiod}}}%
        {{hash=d7805381550fb5f8360345f7f72c0b49}{%
           family={Wierstra},
           familyi={W\bibinitperiod},
           given={Daan},
           giveni={D\bibinitperiod}}}%
        {{hash=afdf5ed50a24cdca0f42433e4f4848d5}{%
           family={Legg},
           familyi={L\bibinitperiod},
           given={Shane},
           giveni={S\bibinitperiod}}}%
        {{hash=b160026950ebb1e2286dfb40c15482f5}{%
           family={Hassabis},
           familyi={H\bibinitperiod},
           given={Demis},
           giveni={D\bibinitperiod}}}%
      }
      \strng{namehash}{aff24b211b34cdd515b2fd148ce0c920}
      \strng{fullhash}{29e1f598b04c2ec7e97aaeb02f70e0c8}
      \strng{fullhashraw}{29e1f598b04c2ec7e97aaeb02f70e0c8}
      \strng{bibnamehash}{aff24b211b34cdd515b2fd148ce0c920}
      \strng{authorbibnamehash}{aff24b211b34cdd515b2fd148ce0c920}
      \strng{authornamehash}{aff24b211b34cdd515b2fd148ce0c920}
      \strng{authorfullhash}{29e1f598b04c2ec7e97aaeb02f70e0c8}
      \strng{authorfullhashraw}{29e1f598b04c2ec7e97aaeb02f70e0c8}
      \field{extraname}{3}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{day}{26}
      \field{issn}{0028-0836, 1476-4687}
      \field{journaltitle}{Nature}
      \field{langid}{english}
      \field{month}{2}
      \field{number}{7540}
      \field{shortjournal}{Nature}
      \field{title}{Human-Level Control through Deep Reinforcement Learning}
      \field{urlday}{21}
      \field{urlmonth}{7}
      \field{urlyear}{2024}
      \field{volume}{518}
      \field{year}{2015}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \field{pages}{529\bibrangedash 533}
      \range{pages}{5}
      \verb{doi}
      \verb 10.1038/nature14236
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/MCMU6TN4/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/nature14236
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/nature14236
      \endverb
    \endentry
    \entry{mordatch2017emergence}{unpublished}{}{}
      \name{author}{2}{}{%
        {{hash=7570e7c3fc2c13d59af4d7cdb9962a4d}{%
           family={Mordatch},
           familyi={M\bibinitperiod},
           given={Igor},
           giveni={I\bibinitperiod}}}%
        {{hash=e28d4ee199593959d8c29980a64f1974}{%
           family={Abbeel},
           familyi={A\bibinitperiod},
           given={Pieter},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{0f4008604132ae725c2439b507375db1}
      \strng{fullhash}{0f4008604132ae725c2439b507375db1}
      \strng{fullhashraw}{0f4008604132ae725c2439b507375db1}
      \strng{bibnamehash}{0f4008604132ae725c2439b507375db1}
      \strng{authorbibnamehash}{0f4008604132ae725c2439b507375db1}
      \strng{authornamehash}{0f4008604132ae725c2439b507375db1}
      \strng{authorfullhash}{0f4008604132ae725c2439b507375db1}
      \strng{authorfullhashraw}{0f4008604132ae725c2439b507375db1}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{eprinttype}{arXiv}
      \field{title}{Emergence of Grounded Compositional Language in Multi-Agent Populations}
      \field{year}{2017}
      \field{dateera}{ce}
      \true{nocite}
      \verb{eprint}
      \verb 1703.04908
      \endverb
    \endentry
    \entry{nguyen2020}{article}{}{}
      \name{author}{3}{}{%
        {{hash=6c66ef72a75b2366367041cec372fe92}{%
           family={Nguyen},
           familyi={N\bibinitperiod},
           given={Thanh\bibnamedelima Thi},
           giveni={T\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
        {{hash=eb3b47df160ec1e0c12bced2d9a9437f}{%
           family={Nguyen},
           familyi={N\bibinitperiod},
           given={Ngoc\bibnamedelima Duy},
           giveni={N\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=5345efddc35011fcd0912e307fa90dab}{%
           family={Nahavandi},
           familyi={N\bibinitperiod},
           given={Saeid},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{7929bfac414f70f412dee9316697c210}
      \strng{fullhash}{208ee58a482e10888d772974a2d3a214}
      \strng{fullhashraw}{208ee58a482e10888d772974a2d3a214}
      \strng{bibnamehash}{208ee58a482e10888d772974a2d3a214}
      \strng{authorbibnamehash}{208ee58a482e10888d772974a2d3a214}
      \strng{authornamehash}{7929bfac414f70f412dee9316697c210}
      \strng{authorfullhash}{208ee58a482e10888d772974a2d3a214}
      \strng{authorfullhashraw}{208ee58a482e10888d772974a2d3a214}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Reinforcement learning (RL) algorithms have been around for decades and employed to solve various sequential decision-making problems. These algorithms however have faced great challenges when dealing with high-dimensional environments. The recent development of deep learning has enabled RL methods to drive optimal policies for sophisticated and capable agents, which can perform efficiently in these challenging environments. This paper addresses an important aspect of deep RL related to situations that require multiple agents to communicate and cooperate to solve complex tasks. A survey of different approaches to problems related to multi-agent deep RL (MADRL) is presented, including non-stationarity, partial observability, continuous state and action spaces, multi-agent training schemes, multi-agent transfer learning. The merits and demerits of the reviewed methods will be analyzed and discussed, with their corresponding applications explored. It is envisaged that this review provides insights about various MADRL methods and can lead to future development of more robust and highly useful multi-agent learning methods for solving real-world problems.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{issn}{2168-2267, 2168-2275}
      \field{journaltitle}{IEEE Transactions on Cybernetics}
      \field{month}{9}
      \field{number}{9}
      \field{shortjournal}{IEEE Trans. Cybern.}
      \field{shorttitle}{Deep {{Reinforcement Learning}} for {{Multi-Agent Systems}}}
      \field{title}{Deep {{Reinforcement Learning}} for {{Multi-Agent Systems}}: {{A Review}} of {{Challenges}}, {{Solutions}} and {{Applications}}}
      \field{urlday}{21}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{volume}{50}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \field{pages}{3826\bibrangedash 3839}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1109/TCYB.2020.2977374
      \endverb
      \verb{eprint}
      \verb 1812.11794
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/HS5GV8Y9/Nguyen et al_2020_Deep Reinforcement Learning for Multi-Agent Systems.pdf;/Users/brandonhosley/Zotero/storage/76BTD9SE/1812.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1812.11794
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1812.11794
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning}
    \endentry
    \entry{rutherford2023}{online}{useprefix=true}{}
      \name{author}{20}{}{%
        {{hash=f9e27c5b8951f846d92d37eb396da771}{%
           family={Rutherford},
           familyi={R\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
        {{hash=8013a1eb9ec691ea154fb9523884e3b2}{%
           family={Ellis},
           familyi={E\bibinitperiod},
           given={Benjamin},
           giveni={B\bibinitperiod}}}%
        {{hash=eb367d842390c5ef01bc6b3e451bab16}{%
           family={Gallici},
           familyi={G\bibinitperiod},
           given={Matteo},
           giveni={M\bibinitperiod}}}%
        {{hash=a071fde4f0800a3a8483eed7d8a4381f}{%
           family={Cook},
           familyi={C\bibinitperiod},
           given={Jonathan},
           giveni={J\bibinitperiod}}}%
        {{hash=89b81451635166fcb37197f8034c95d9}{%
           family={Lupu},
           familyi={L\bibinitperiod},
           given={Andrei},
           giveni={A\bibinitperiod}}}%
        {{hash=fadd9ba73f724547a12dcce977597545}{%
           family={Ingvarsson},
           familyi={I\bibinitperiod},
           given={Gardar},
           giveni={G\bibinitperiod}}}%
        {{hash=12fbe38b54197a785f62cdad8cc25489}{%
           family={Willi},
           familyi={W\bibinitperiod},
           given={Timon},
           giveni={T\bibinitperiod}}}%
        {{hash=88c18330092f6235ea218c58a5b0249f}{%
           family={Khan},
           familyi={K\bibinitperiod},
           given={Akbir},
           giveni={A\bibinitperiod}}}%
        {{hash=d056cdb8ed6bebb55e13c2246059af79}{%
           family={Witt},
           familyi={W\bibinitperiod},
           given={Christian\bibnamedelima Schroeder},
           giveni={C\bibinitperiod\bibinitdelim S\bibinitperiod},
           prefix={de},
           prefixi={d\bibinitperiod}}}%
        {{hash=cb15b86f80171d9bf956f535654e1be9}{%
           family={Souly},
           familyi={S\bibinitperiod},
           given={Alexandra},
           giveni={A\bibinitperiod}}}%
        {{hash=58dc57d6df21703f958727b114c0e6b9}{%
           family={Bandyopadhyay},
           familyi={B\bibinitperiod},
           given={Saptarashmi},
           giveni={S\bibinitperiod}}}%
        {{hash=c76172ee0c2ebda81796624b4a81ae54}{%
           family={Samvelyan},
           familyi={S\bibinitperiod},
           given={Mikayel},
           giveni={M\bibinitperiod}}}%
        {{hash=754eae2a0a1be7bc80aeaa872bb0154f}{%
           family={Jiang},
           familyi={J\bibinitperiod},
           given={Minqi},
           giveni={M\bibinitperiod}}}%
        {{hash=c8a31592280c5d36c233b1765b3bc813}{%
           family={Lange},
           familyi={L\bibinitperiod},
           given={Robert\bibnamedelima Tjarko},
           giveni={R\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
        {{hash=0d05819cf2b4fe22ba972c9b2b5d8c9d}{%
           family={Whiteson},
           familyi={W\bibinitperiod},
           given={Shimon},
           giveni={S\bibinitperiod}}}%
        {{hash=7191d5738229718fe891c52a6cd9102b}{%
           family={Lacerda},
           familyi={L\bibinitperiod},
           given={Bruno},
           giveni={B\bibinitperiod}}}%
        {{hash=ae82b1782be057db351b013d9858dade}{%
           family={Hawes},
           familyi={H\bibinitperiod},
           given={Nick},
           giveni={N\bibinitperiod}}}%
        {{hash=bf6bd1fe5980a054333a40bb94be2383}{%
           family={Rocktaschel},
           familyi={R\bibinitperiod},
           given={Tim},
           giveni={T\bibinitperiod}}}%
        {{hash=afa2a5ce23786b5a25addca9177bb418}{%
           family={Lu},
           familyi={L\bibinitperiod},
           given={Chris},
           giveni={C\bibinitperiod}}}%
        {{hash=265add78d7143f911d5b2a0b954ff3c2}{%
           family={Foerster},
           familyi={F\bibinitperiod},
           given={Jakob\bibnamedelima Nicolaus},
           giveni={J\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
      }
      \strng{namehash}{de0ef6d9285c0909f6269022001d8038}
      \strng{fullhash}{073e0f85784bdb84c982416c962ddb03}
      \strng{fullhashraw}{073e0f85784bdb84c982416c962ddb03}
      \strng{bibnamehash}{de0ef6d9285c0909f6269022001d8038}
      \strng{authorbibnamehash}{de0ef6d9285c0909f6269022001d8038}
      \strng{authornamehash}{de0ef6d9285c0909f6269022001d8038}
      \strng{authorfullhash}{073e0f85784bdb84c982416c962ddb03}
      \strng{authorfullhashraw}{073e0f85784bdb84c982416c962ddb03}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Benchmarks play an important role in the development of machine learning algorithms. For example, research in reinforcement learning (RL) has been heavily influenced by available environments and benchmarks. However, RL environments are traditionally run on the CPU, limiting their scalability with typical academic compute. Recent advancements in JAX have enabled the wider use of hardware acceleration to overcome these computational hurdles, enabling massively parallel RL training pipelines and environments. This is particularly useful for multi-agent reinforcement learning (MARL) research. First of all, multiple agents must be considered at each environment step, adding computational burden, and secondly, the sample complexity is increased due to non-stationarity, decentralised partial observability, or other MARL challenges. In this paper, we present JaxMARL, the first open-source code base that combines ease-of-use with GPU enabled efficiency, and supports a large number of commonly used MARL environments as well as popular baseline algorithms. When considering wall clock time, our experiments show that per-run our JAX-based training pipeline is up to 12500x faster than existing approaches. This enables efficient and thorough evaluations, with the potential to alleviate the evaluation crisis of the field. We also introduce and benchmark SMAX, a vectorised, simplified version of the popular StarCraft Multi-Agent Challenge, which removes the need to run the StarCraft II game engine. This not only enables GPU acceleration, but also provides a more flexible MARL environment, unlocking the potential for self-play, meta-learning, and other future applications in MARL. We provide code at https://github.com/flairox/jaxmarl.}
      \field{day}{19}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{12}
      \field{pubstate}{prepublished}
      \field{shorttitle}{{{JaxMARL}}}
      \field{title}{{{JaxMARL}}: {{Multi-Agent RL Environments}} in {{JAX}}}
      \field{urlday}{4}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \verb{eprint}
      \verb 2311.10090
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/DNRUI6S2/Rutherford et al. - 2023 - JaxMARL Multi-Agent RL Environments in JAX.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2311.10090
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2311.10090
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems}
    \endentry
    \entry{sonar2021}{inproceedings}{}{}
      \name{author}{3}{}{%
        {{hash=1f069cd18cd36d0848a197b9c628d3f7}{%
           family={Sonar},
           familyi={S\bibinitperiod},
           given={Anoopkumar},
           giveni={A\bibinitperiod}}}%
        {{hash=a1b4b7c50764fb4d6d8c1643c1b56f4a}{%
           family={Pacelli},
           familyi={P\bibinitperiod},
           given={Vincent},
           giveni={V\bibinitperiod}}}%
        {{hash=b4c715aefa330ae48f5662580997bafd}{%
           family={Majumdar},
           familyi={M\bibinitperiod},
           given={Anirudha},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{663dfe2246c373f0bcd8cf47e608f690}
      \strng{fullhash}{091a1520b68c31b4075f66cac083b82a}
      \strng{fullhashraw}{091a1520b68c31b4075f66cac083b82a}
      \strng{bibnamehash}{091a1520b68c31b4075f66cac083b82a}
      \strng{authorbibnamehash}{091a1520b68c31b4075f66cac083b82a}
      \strng{authornamehash}{663dfe2246c373f0bcd8cf47e608f690}
      \strng{authorfullhash}{091a1520b68c31b4075f66cac083b82a}
      \strng{authorfullhashraw}{091a1520b68c31b4075f66cac083b82a}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{A fundamental challenge in reinforcement learning is to learn policies that generalize beyond the operating domains experienced during training. In this paper, we approach this challenge through the following invariance principle: an agent must find a representation such that there exists an action-predictor built on top of this representation that is simultaneously optimal across all training domains. Intuitively, the resulting invariant policy enhances generalization by finding causes of successful actions. We propose a novel learning algorithm, Invariant Policy Optimization (IPO), that implements this principle and learns an invariant policy during training. We compare our approach with standard policy gradient methods and demonstrate significant improvements in generalization performance on unseen domains for linear quadratic regulator and grid-world problems, and an example where a robot must learn to open doors with varying physical properties.}
      \field{booktitle}{Proceedings of the 3rd {{Conference}} on {{Learning}} for {{Dynamics}} and {{Control}}}
      \field{day}{29}
      \field{eventtitle}{Learning for {{Dynamics}} and {{Control}}}
      \field{issn}{2640-3498}
      \field{langid}{english}
      \field{month}{5}
      \field{shorttitle}{Invariant {{Policy Optimization}}}
      \field{title}{Invariant {{Policy Optimization}}: {{Towards Stronger Generalization}} in {{Reinforcement Learning}}}
      \field{urlday}{25}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \field{pages}{21\bibrangedash 33}
      \range{pages}{13}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/GP2U977X/Sonar et al_2021_Invariant Policy Optimization.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v144/sonar21a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v144/sonar21a.html
      \endverb
    \endentry
    \entry{spooner2020}{online}{}{}
      \name{author}{2}{}{%
        {{hash=f027bdd62c183fa30552d3c62c25a167}{%
           family={Spooner},
           familyi={S\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
        {{hash=3e69e4694884dd066960e0f1d67c0f24}{%
           family={Savani},
           familyi={S\bibinitperiod},
           given={Rahul},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{1cbff6ce8dca60824cc004bbe9edb36a}
      \strng{fullhash}{1cbff6ce8dca60824cc004bbe9edb36a}
      \strng{fullhashraw}{1cbff6ce8dca60824cc004bbe9edb36a}
      \strng{bibnamehash}{1cbff6ce8dca60824cc004bbe9edb36a}
      \strng{authorbibnamehash}{1cbff6ce8dca60824cc004bbe9edb36a}
      \strng{authornamehash}{1cbff6ce8dca60824cc004bbe9edb36a}
      \strng{authorfullhash}{1cbff6ce8dca60824cc004bbe9edb36a}
      \strng{authorfullhashraw}{1cbff6ce8dca60824cc004bbe9edb36a}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We show that adversarial reinforcement learning (ARL) can be used to produce market marking agents that are robust to adversarial and adaptively-chosen market conditions. To apply ARL, we turn the well-studied single-agent model of Avellaneda and Stoikov [2008] into a discrete-time zero-sum game between a market maker and adversary. The adversary acts as a proxy for other market participants that would like to profit at the market maker's expense. We empirically compare two conventional single-agent RL agents with ARL, and show that our ARL approach leads to: 1) the emergence of risk-averse behaviour without constraints or domain-specific penalties; 2) significant improvements in performance across a set of standard metrics, evaluated with or without an adversary in the test environment, and; 3) improved robustness to model uncertainty. We empirically demonstrate that our ARL method consistently converges, and we prove for several special cases that the profiles that we converge to correspond to Nash equilibria in a simplified single-stage game.}
      \field{day}{8}
      \field{eprintclass}{q-fin}
      \field{eprinttype}{arXiv}
      \field{month}{7}
      \field{pubstate}{prepublished}
      \field{title}{Robust {{Market Making}} via {{Adversarial Reinforcement Learning}}}
      \field{urlday}{21}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \verb{doi}
      \verb 10.48550/arXiv.2003.01820
      \endverb
      \verb{eprint}
      \verb 2003.01820
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/D7Z8QB82/Spooner_Savani_2020_Robust Market Making via Adversarial Reinforcement Learning.pdf;/Users/brandonhosley/Zotero/storage/K6ACMENT/2003.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2003.01820
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2003.01820
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Finance - Trading and Market Microstructure,Statistics - Machine Learning}
    \endentry
    \entry{stone2010}{inproceedings}{}{}
      \name{author}{4}{}{%
        {{hash=3f42a499da2cddc0bae71b76c81f7c46}{%
           family={Stone},
           familyi={S\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
        {{hash=4e37bd02dddc9bc68003fc4a34722278}{%
           family={A.Kaminka},
           familyi={A\bibinitperiod},
           given={Gal},
           giveni={G\bibinitperiod}}}%
        {{hash=de674f6370185bbdaef21079761fcd7f}{%
           family={Kraus},
           familyi={K\bibinitperiod},
           given={Sarit},
           giveni={S\bibinitperiod}}}%
        {{hash=d02850850a6bc0a66d5cf3a3ac913fc1}{%
           family={S.Rosenschein},
           familyi={S\bibinitperiod},
           given={Jeffrey},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{769ed72fe9f19d387528207b8b28b398}
      \strng{fullhash}{e4ea7574310b3ea3f4ffa27829595dfe}
      \strng{fullhashraw}{e4ea7574310b3ea3f4ffa27829595dfe}
      \strng{bibnamehash}{e4ea7574310b3ea3f4ffa27829595dfe}
      \strng{authorbibnamehash}{e4ea7574310b3ea3f4ffa27829595dfe}
      \strng{authornamehash}{769ed72fe9f19d387528207b8b28b398}
      \strng{authorfullhash}{e4ea7574310b3ea3f4ffa27829595dfe}
      \strng{authorfullhashraw}{e4ea7574310b3ea3f4ffa27829595dfe}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{As autonomous agents proliferate in the real world, both in software and robotic settings, they will increasingly need to band together for cooperative activities with previously unfamiliar teammates. In such \mkbibemph{ad hoc team} settings, team strategies cannot be developed a priori. Rather, an agent must be prepared to cooperate with many types of teammates: it must collaborate without pre-coordination. This paper challenges the AI community to develop theory and to implement prototypes of ad hoc team agents. It defines the concept of ad hoc team agents, specifies an evaluation paradigm, and provides examples of possible theoretical and empirical approaches to challenge. The goal is to encourage progress towards this ambitious, newly realistic, and increasingly important research goal.}
      \field{booktitle}{Proceedings of the Twenty-Fourth Conference on Artificial Intelligence}
      \field{month}{7}
      \field{title}{Ad Hoc Autonomous Agent Teams: {{Collaboration}} without Pre-Coordination}
      \field{year}{2010}
      \field{dateera}{ce}
      \true{nocite}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/J9Q5IGN5/Stone et al_2010_Ad hoc autonomous agent teams.pdf
      \endverb
    \endentry
    \entry{tang2021}{inproceedings}{}{}
      \name{author}{2}{}{%
        {{hash=3b9f8cc01fc60b1f84fe81c4bf7e8f68}{%
           family={Tang},
           familyi={T\bibinitperiod},
           given={Yujin},
           giveni={Y\bibinitperiod}}}%
        {{hash=4467056eb9d0a44c1ec21bbb4d6152c5}{%
           family={Ha},
           familyi={H\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Red Hook, NY, USA}%
      }
      \list{publisher}{1}{%
        {Curran Associates Inc.}%
      }
      \strng{namehash}{04fceb828eb574062e2339f21e07b495}
      \strng{fullhash}{04fceb828eb574062e2339f21e07b495}
      \strng{fullhashraw}{04fceb828eb574062e2339f21e07b495}
      \strng{bibnamehash}{04fceb828eb574062e2339f21e07b495}
      \strng{authorbibnamehash}{04fceb828eb574062e2339f21e07b495}
      \strng{authornamehash}{04fceb828eb574062e2339f21e07b495}
      \strng{authorfullhash}{04fceb828eb574062e2339f21e07b495}
      \strng{authorfullhashraw}{04fceb828eb574062e2339f21e07b495}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{In complex systems, we often observe complex global behavior emerge from a collection of agents interacting with each other in their environment, with each individual agent acting only on locally available information, without knowing the full picture. Such systems have inspired development of artificial intelligence algorithms in areas such as swarm optimization and cellular automata. Motivated by the emergence of collective behavior from complex cellular systems, we build systems that feed each sensory input from the environment into distinct, but identical neural networks, each with no fixed relationship with one another. We show that these sensory networks can be trained to integrate information received locally, and through communication via an attention mechanism, can collectively produce a globally coherent policy. Moreover, the system can still perform its task even if the ordering of its inputs is randomly permuted several times during an episode. These permutation invariant systems also display useful robustness and generalization properties that are broadly applicable.}
      \field{booktitle}{Proceedings of the 35th {{International Conference}} on {{Neural Information Processing Systems}}}
      \field{day}{6}
      \field{isbn}{978-1-71384-539-3}
      \field{month}{12}
      \field{series}{{{NIPS}} '21}
      \field{shorttitle}{The Sensory Neuron as a Transformer}
      \field{title}{The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning}
      \field{year}{2021}
      \field{dateera}{ce}
      \true{nocite}
      \field{pages}{22574\bibrangedash 22587}
      \range{pages}{14}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/W3H4ZE5Y/Tang_Ha_2021_The sensory neuron as a transformer.pdf
      \endverb
    \endentry
    \entry{wen2021}{article}{}{}
      \name{author}{4}{}{%
        {{hash=b8ec91386447ce71e0fc5c08acec696d}{%
           family={Wen},
           familyi={W\bibinitperiod},
           given={Guanghui},
           giveni={G\bibinitperiod}}}%
        {{hash=54bc70bc3a080916e6684db86d108ef4}{%
           family={Fu},
           familyi={F\bibinitperiod},
           given={Junjie},
           giveni={J\bibinitperiod}}}%
        {{hash=568034ddbd3b890d5d2a84887c1d65f1}{%
           family={Dai},
           familyi={D\bibinitperiod},
           given={Pengcheng},
           giveni={P\bibinitperiod}}}%
        {{hash=5bdb6fd5a95da98db8e2bae3d00b52a1}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Jialing},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{b41b64b15d85dd3ca075baaa9168a488}
      \strng{fullhash}{7dab6e3c96c5defb9406d9e936e2569a}
      \strng{fullhashraw}{7dab6e3c96c5defb9406d9e936e2569a}
      \strng{bibnamehash}{7dab6e3c96c5defb9406d9e936e2569a}
      \strng{authorbibnamehash}{7dab6e3c96c5defb9406d9e936e2569a}
      \strng{authornamehash}{b41b64b15d85dd3ca075baaa9168a488}
      \strng{authorfullhash}{7dab6e3c96c5defb9406d9e936e2569a}
      \strng{authorfullhashraw}{7dab6e3c96c5defb9406d9e936e2569a}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{day}{1}
      \field{journaltitle}{The Innovation}
      \field{month}{9}
      \field{shortjournal}{The Innovation}
      \field{shorttitle}{{{DTDE}}}
      \field{title}{{{DTDE}}: {{A}} New Cooperative Multi-Agent Reinforcement Learning Framework}
      \field{volume}{2}
      \field{year}{2021}
      \field{dateera}{ce}
      \true{nocite}
      \field{pages}{100162}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1016/j.xinn.2021.100162
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/UA7JZT92/Wen et al_2021_DTDE.pdf
      \endverb
    \endentry
    \entry{williams1992}{article}{}{}
      \name{author}{1}{}{%
        {{hash=6cbc29ad7fd57ffdb9ed4728418fd988}{%
           family={Williams},
           familyi={W\bibinitperiod},
           given={Ronald\bibnamedelima J.},
           giveni={R\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{6cbc29ad7fd57ffdb9ed4728418fd988}
      \strng{fullhash}{6cbc29ad7fd57ffdb9ed4728418fd988}
      \strng{fullhashraw}{6cbc29ad7fd57ffdb9ed4728418fd988}
      \strng{bibnamehash}{6cbc29ad7fd57ffdb9ed4728418fd988}
      \strng{authorbibnamehash}{6cbc29ad7fd57ffdb9ed4728418fd988}
      \strng{authornamehash}{6cbc29ad7fd57ffdb9ed4728418fd988}
      \strng{authorfullhash}{6cbc29ad7fd57ffdb9ed4728418fd988}
      \strng{authorfullhashraw}{6cbc29ad7fd57ffdb9ed4728418fd988}
      \field{extraname}{1}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.}
      \field{day}{1}
      \field{issn}{1573-0565}
      \field{journaltitle}{Machine Learning}
      \field{langid}{english}
      \field{month}{5}
      \field{number}{3}
      \field{shortjournal}{Mach Learn}
      \field{title}{Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning}
      \field{urlday}{28}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{volume}{8}
      \field{year}{1992}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \field{pages}{229\bibrangedash 256}
      \range{pages}{28}
      \verb{doi}
      \verb 10.1007/BF00992696
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/ZP3SL2Z8/Williams_1992_Simple statistical gradient-following algorithms for connectionist.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/BF00992696
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/BF00992696
      \endverb
      \keyw{connectionist networks,gradient descent,mathematical analysis,Reinforcement learning}
    \endentry
    \entry{williams2025}{online}{}{}
      \name{author}{1}{}{%
        {{hash=449110a46fdfc7353862c50d39057c0e}{%
           family={Williams},
           familyi={W\bibinitperiod},
           given={Kylie},
           giveni={K\bibinitperiod}}}%
      }
      \list{organization}{1}{%
        {Vector Institute for Artificial Intelligence}%
      }
      \strng{namehash}{449110a46fdfc7353862c50d39057c0e}
      \strng{fullhash}{449110a46fdfc7353862c50d39057c0e}
      \strng{fullhashraw}{449110a46fdfc7353862c50d39057c0e}
      \strng{bibnamehash}{449110a46fdfc7353862c50d39057c0e}
      \strng{authorbibnamehash}{449110a46fdfc7353862c50d39057c0e}
      \strng{authornamehash}{449110a46fdfc7353862c50d39057c0e}
      \strng{authorfullhash}{449110a46fdfc7353862c50d39057c0e}
      \strng{authorfullhashraw}{449110a46fdfc7353862c50d39057c0e}
      \field{extraname}{2}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Explore the latest developments in Multi-Agent Reinforcement Learning (MARL) and its real-world applications. Learn how recent breakthroughs in sample efficiency and scalability are revolutionizing autonomous systems in wildfire fighting, healthcare, and autonomous driving.}
      \field{day}{31}
      \field{hour}{13}
      \field{langid}{canadian}
      \field{minute}{13}
      \field{month}{3}
      \field{second}{31}
      \field{timezone}{Z}
      \field{title}{Real {{World Multi-Agent Reinforcement Learning}} - {{Latest Developments}} and {{Applications}}}
      \field{urlday}{16}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2025}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/LB88BQXX/real-world-multi-agent-reinforcement-learning-latest-developments-and-applications.html
      \endverb
      \verb{urlraw}
      \verb https://vectorinstitute.ai/real-world-multi-agent-reinforcement-learning-latest-developments-and-applications/
      \endverb
      \verb{url}
      \verb https://vectorinstitute.ai/real-world-multi-agent-reinforcement-learning-latest-developments-and-applications/
      \endverb
    \endentry
    \entry{yang2018}{inproceedings}{}{}
      \name{author}{6}{}{%
        {{hash=616d3b0b19e3536f5c65dfab2a48a659}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Yaodong},
           giveni={Y\bibinitperiod}}}%
        {{hash=1292f67a9d739077371447300d6c740e}{%
           family={Luo},
           familyi={L\bibinitperiod},
           given={Rui},
           giveni={R\bibinitperiod}}}%
        {{hash=9dd3667af79a8395c0d82904c6d0d2fe}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Minne},
           giveni={M\bibinitperiod}}}%
        {{hash=2a72f304d2bc33c9c92301f2dc3063b2}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Ming},
           giveni={M\bibinitperiod}}}%
        {{hash=fbc51a6317f158547173b93086d9b1a2}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Weinan},
           giveni={W\bibinitperiod}}}%
        {{hash=2f3ea981fa5a715a69118b48e576a9f5}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Jun},
           giveni={J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{ff896b51edd840151184b0b4e3d67635}
      \strng{fullhash}{56bb03e096a8b0897ad24f05d22ba643}
      \strng{fullhashraw}{56bb03e096a8b0897ad24f05d22ba643}
      \strng{bibnamehash}{56bb03e096a8b0897ad24f05d22ba643}
      \strng{authorbibnamehash}{56bb03e096a8b0897ad24f05d22ba643}
      \strng{authornamehash}{ff896b51edd840151184b0b4e3d67635}
      \strng{authorfullhash}{56bb03e096a8b0897ad24f05d22ba643}
      \strng{authorfullhashraw}{56bb03e096a8b0897ad24f05d22ba643}
      \field{extraname}{2}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Existing multi-agent reinforcement learning methods are limited typically to a small number of agents. When the agent number increases largely, the learning becomes intractable due to the curse of the dimensionality and the exponential growth of agent interactions. In this paper, we present Mean Field Reinforcement Learning where the interactions within the population of agents are approximated by those between a single agent and the average effect from the overall population or neighboring agents; the interplay between the two entities is mutually reinforced: the learning of the individual agent’s optimal policy depends on the dynamics of the population, while the dynamics of the population change according to the collective patterns of the individual policies. We develop practical mean field Q-learning and mean field Actor-Critic algorithms and analyze the convergence of the solution to Nash equilibrium. Experiments on Gaussian squeeze, Ising model, and battle games justify the learning effectiveness of our mean field approaches. In addition, we report the first result to solve the Ising model via model-free reinforcement learning methods.}
      \field{booktitle}{Proceedings of the 35th {{International Conference}} on {{Machine Learning}}}
      \field{day}{3}
      \field{eventtitle}{International {{Conference}} on {{Machine Learning}}}
      \field{issn}{2640-3498}
      \field{langid}{english}
      \field{month}{7}
      \field{title}{Mean {{Field Multi-Agent Reinforcement Learning}}}
      \field{urlday}{14}
      \field{urlmonth}{5}
      \field{urlyear}{2025}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \field{pages}{5571\bibrangedash 5580}
      \range{pages}{10}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/UJ9GS4QT/Yang et al. - 2018 - Mean Field Multi-Agent Reinforcement Learning.pdf;/Users/brandonhosley/Zotero/storage/ZNHMG8P8/Yang et al_2018_Mean Field Multi-Agent Reinforcement Learning.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v80/yang18d.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v80/yang18d.html
      \endverb
    \endentry
    \entry{yang2020a}{online}{}{}
      \name{author}{8}{}{%
        {{hash=616d3b0b19e3536f5c65dfab2a48a659}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Yaodong},
           giveni={Y\bibinitperiod}}}%
        {{hash=8f534f1e9ccc15c7efd4e787cda9a148}{%
           family={Hao},
           familyi={H\bibinitperiod},
           given={Jianye},
           giveni={J\bibinitperiod}}}%
        {{hash=7e1094977f86278d4d351ff3e4806d7c}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Guangyong},
           giveni={G\bibinitperiod}}}%
        {{hash=c099d39cae22fd636f300a7fc3286de6}{%
           family={Tang},
           familyi={T\bibinitperiod},
           given={Hongyao},
           giveni={H\bibinitperiod}}}%
        {{hash=59e4de4878a29778955013d05e893404}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Yingfeng},
           giveni={Y\bibinitperiod}}}%
        {{hash=423c4bce1c44941767bcf539752b10bb}{%
           family={Hu},
           familyi={H\bibinitperiod},
           given={Yujing},
           giveni={Y\bibinitperiod}}}%
        {{hash=522f9824ad7abc272e15cfa4a81abf67}{%
           family={Fan},
           familyi={F\bibinitperiod},
           given={Changjie},
           giveni={C\bibinitperiod}}}%
        {{hash=d7db1b7daf525818abea6508831822f3}{%
           family={Wei},
           familyi={W\bibinitperiod},
           given={Zhongyu},
           giveni={Z\bibinitperiod}}}%
      }
      \strng{namehash}{ff896b51edd840151184b0b4e3d67635}
      \strng{fullhash}{5f8253bb52cedcc0e813511c5764df7b}
      \strng{fullhashraw}{5f8253bb52cedcc0e813511c5764df7b}
      \strng{bibnamehash}{ff896b51edd840151184b0b4e3d67635}
      \strng{authorbibnamehash}{ff896b51edd840151184b0b4e3d67635}
      \strng{authornamehash}{ff896b51edd840151184b0b4e3d67635}
      \strng{authorfullhash}{5f8253bb52cedcc0e813511c5764df7b}
      \strng{authorfullhashraw}{5f8253bb52cedcc0e813511c5764df7b}
      \field{extraname}{3}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recently, deep multiagent reinforcement learning (MARL) has become a highly active research area as many real-world problems can be inherently viewed as multiagent systems. A particularly interesting and widely applicable class of problems is the partially observable cooperative multiagent setting, in which a team of agents learns to coordinate their behaviors conditioning on their private observations and commonly shared global reward signals. One natural solution is to resort to the centralized training and decentralized execution paradigm. During centralized training, one key challenge is the multiagent credit assignment: how to allocate the global rewards for individual agent policies for better coordination towards maximizing system-level's benefits. In this paper, we propose a new method called Q-value Path Decomposition (QPD) to decompose the system's global Q-values into individual agents' Q-values. Unlike previous works which restrict the representation relation of the individual Q-values and the global one, we leverage the integrated gradient attribution technique into deep MARL to directly decompose global Q-values along trajectory paths to assign credits for agents. We evaluate QPD on the challenging StarCraft II micromanagement tasks and show that QPD achieves the state-of-the-art performance in both homogeneous and heterogeneous multiagent scenarios compared with existing cooperative MARL algorithms.}
      \field{day}{10}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{2}
      \field{pubstate}{prepublished}
      \field{title}{Q-Value {{Path Decomposition}} for {{Deep Multiagent Reinforcement Learning}}}
      \field{urlday}{14}
      \field{urlmonth}{2}
      \field{urlyear}{2025}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \verb{doi}
      \verb 10.48550/arXiv.2002.03950
      \endverb
      \verb{eprint}
      \verb 2002.03950
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/7DL3LJB6/Yang et al_2020_Q-value Path Decomposition for Deep Multiagent Reinforcement Learning.pdf;/Users/brandonhosley/Zotero/storage/HKCIMKS2/2002.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2002.03950
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2002.03950
      \endverb
      \keyw{Computer Science - Multiagent Systems}
    \endentry
    \entry{ye2020}{online}{}{}
      \name{author}{18}{}{%
        {{hash=1d5b87be3bb28836f94c41ae453c8c6b}{%
           family={Ye},
           familyi={Y\bibinitperiod},
           given={Deheng},
           giveni={D\bibinitperiod}}}%
        {{hash=1026adcc81f4d2ec8990768c6dddebdf}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Guibin},
           giveni={G\bibinitperiod}}}%
        {{hash=3fa0b98bdb0330cc0ea22e655fd964c2}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Wen},
           giveni={W\bibinitperiod}}}%
        {{hash=64288176f389838aa5917ebca5ce2455}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Sheng},
           giveni={S\bibinitperiod}}}%
        {{hash=1ce4eb84b8441cb4f47e328910e194f7}{%
           family={Yuan},
           familyi={Y\bibinitperiod},
           given={Bo},
           giveni={B\bibinitperiod}}}%
        {{hash=fa97927921a403cfe822973e31081dca}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Bo},
           giveni={B\bibinitperiod}}}%
        {{hash=95d25145789eb692784f0e434ed1b801}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Jia},
           giveni={J\bibinitperiod}}}%
        {{hash=d03d76110feb64538398f18452d1e0a1}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Zhao},
           giveni={Z\bibinitperiod}}}%
        {{hash=ddb226818556242515283f45fda1b366}{%
           family={Qiu},
           familyi={Q\bibinitperiod},
           given={Fuhao},
           giveni={F\bibinitperiod}}}%
        {{hash=5129af8dade2631e06d4ca9142d171b3}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Hongsheng},
           giveni={H\bibinitperiod}}}%
        {{hash=ed469e9dce0b9baad6b4077db569c15d}{%
           family={Yin},
           familyi={Y\bibinitperiod},
           given={Yinyuting},
           giveni={Y\bibinitperiod}}}%
        {{hash=98a550cc442226f68c889bf960831703}{%
           family={Shi},
           familyi={S\bibinitperiod},
           given={Bei},
           giveni={B\bibinitperiod}}}%
        {{hash=cce05cab2c437e9c128f853684ee3137}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Liang},
           giveni={L\bibinitperiod}}}%
        {{hash=9f8abb804804c3628a1da32eade6800a}{%
           family={Shi},
           familyi={S\bibinitperiod},
           given={Tengfei},
           giveni={T\bibinitperiod}}}%
        {{hash=68f17e7e60a79bb0606842a0ef03db5c}{%
           family={Fu},
           familyi={F\bibinitperiod},
           given={Qiang},
           giveni={Q\bibinitperiod}}}%
        {{hash=b32f61b294f8e4aa5306c6ea6859da26}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Wei},
           giveni={W\bibinitperiod}}}%
        {{hash=881c4a3798df325ab4e25330e80f9919}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Lanxiao},
           giveni={L\bibinitperiod}}}%
        {{hash=c0e0d23e2d09e45e6f51cc2bcea6d9f9}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Wei},
           giveni={W\bibinitperiod}}}%
      }
      \strng{namehash}{885e8d28c56c9adda7424a9cbc195903}
      \strng{fullhash}{2fd9c7c6a8d51ba2fc79496a257821ad}
      \strng{fullhashraw}{2fd9c7c6a8d51ba2fc79496a257821ad}
      \strng{bibnamehash}{885e8d28c56c9adda7424a9cbc195903}
      \strng{authorbibnamehash}{885e8d28c56c9adda7424a9cbc195903}
      \strng{authornamehash}{885e8d28c56c9adda7424a9cbc195903}
      \strng{authorfullhash}{2fd9c7c6a8d51ba2fc79496a257821ad}
      \strng{authorfullhashraw}{2fd9c7c6a8d51ba2fc79496a257821ad}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{MOBA games, e.g., Honor of Kings, League of Legends, and Dota 2, pose grand challenges to AI systems such as multi-agent, enormous state-action space, complex action control, etc. Developing AI for playing MOBA games has raised much attention accordingly. However, existing work falls short in handling the raw game complexity caused by the explosion of agent combinations, i.e., lineups, when expanding the hero pool in case that OpenAI's Dota AI limits the play to a pool of only 17 heroes. As a result, full MOBA games without restrictions are far from being mastered by any existing AI system. In this paper, we propose a MOBA AI learning paradigm that methodologically enables playing full MOBA games with deep reinforcement learning. Specifically, we develop a combination of novel and existing learning techniques, including curriculum self-play learning, policy distillation, off-policy adaption, multi-head value estimation, and Monte-Carlo tree-search, in training and playing a large pool of heroes, meanwhile addressing the scalability issue skillfully. Tested on Honor of Kings, a popular MOBA game, we show how to build superhuman AI agents that can defeat top esports players. The superiority of our AI is demonstrated by the first large-scale performance test of MOBA AI agent in the literature.}
      \field{day}{31}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{12}
      \field{pubstate}{prepublished}
      \field{title}{Towards {{Playing Full MOBA Games}} with {{Deep Reinforcement Learning}}}
      \field{urlday}{21}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \verb{doi}
      \verb 10.48550/arXiv.2011.12692
      \endverb
      \verb{eprint}
      \verb 2011.12692
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/WVJYK77L/Ye et al_2020_Towards Playing Full MOBA Games with Deep Reinforcement Learning.pdf;/Users/brandonhosley/Zotero/storage/XURB9XF3/2011.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2011.12692
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2011.12692
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
    \endentry
    \entry{yielding2023}{thesis}{}{}
      \name{author}{1}{}{%
        {{hash=69c0632857383c3ea48de15ebf7a464d}{%
           family={Yielding},
           familyi={Y\bibinitperiod},
           given={Nicholas},
           giveni={N\bibinitperiod}}}%
      }
      \list{institution}{1}{%
        {Air Force Institute of Technology}%
      }
      \strng{namehash}{69c0632857383c3ea48de15ebf7a464d}
      \strng{fullhash}{69c0632857383c3ea48de15ebf7a464d}
      \strng{fullhashraw}{69c0632857383c3ea48de15ebf7a464d}
      \strng{bibnamehash}{69c0632857383c3ea48de15ebf7a464d}
      \strng{authorbibnamehash}{69c0632857383c3ea48de15ebf7a464d}
      \strng{authornamehash}{69c0632857383c3ea48de15ebf7a464d}
      \strng{authorfullhash}{69c0632857383c3ea48de15ebf7a464d}
      \strng{authorfullhashraw}{69c0632857383c3ea48de15ebf7a464d}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{langid}{american}
      \field{month}{12}
      \field{pagetotal}{153}
      \field{title}{{{MARSS}}: {{Multi-Agent Reinforcement}} Learning for {{Satellite Swarms}}}
      \field{year}{2023}
      \field{dateera}{ce}
      \true{nocite}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/4GSNP4L7/Yielding_2023_MARSS.pdf
      \endverb
    \endentry
    \entry{zaheer2017}{inproceedings}{}{}
      \name{author}{6}{}{%
        {{hash=c4c46d2d2e84c4fd611f22d52ad6b782}{%
           family={Zaheer},
           familyi={Z\bibinitperiod},
           given={Manzil},
           giveni={M\bibinitperiod}}}%
        {{hash=cff8967849ae28fa040a2c31360f5f70}{%
           family={Kottur},
           familyi={K\bibinitperiod},
           given={Satwik},
           giveni={S\bibinitperiod}}}%
        {{hash=3b8f5a9a085bb7b59f95968faa4493b7}{%
           family={Ravanbakhsh},
           familyi={R\bibinitperiod},
           given={Siamak},
           giveni={S\bibinitperiod}}}%
        {{hash=1874a80c740f3a90f1959c842e8d1d10}{%
           family={Poczos},
           familyi={P\bibinitperiod},
           given={Barnabas},
           giveni={B\bibinitperiod}}}%
        {{hash=f0b8bc62189ed0fc13d8da5338458baa}{%
           family={Salakhutdinov},
           familyi={S\bibinitperiod},
           given={Russ\bibnamedelima R},
           giveni={R\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=82d61e31b4f7f82ad59ff887349bdfe3}{%
           family={Smola},
           familyi={S\bibinitperiod},
           given={Alexander\bibnamedelima J},
           giveni={A\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{7f3a8d6a5ab04df1d1a78a8f34f95ea4}
      \strng{fullhash}{44d13f88ae68ecdc99e23bbefb2bf0e5}
      \strng{fullhashraw}{44d13f88ae68ecdc99e23bbefb2bf0e5}
      \strng{bibnamehash}{44d13f88ae68ecdc99e23bbefb2bf0e5}
      \strng{authorbibnamehash}{44d13f88ae68ecdc99e23bbefb2bf0e5}
      \strng{authornamehash}{7f3a8d6a5ab04df1d1a78a8f34f95ea4}
      \strng{authorfullhash}{44d13f88ae68ecdc99e23bbefb2bf0e5}
      \strng{authorfullhashraw}{44d13f88ae68ecdc99e23bbefb2bf0e5}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We study the problem of designing models for machine learning tasks defined on sets. In contrast to the traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets and are invariant to permutations. Such problems are widespread, ranging from the estimation of population statistics, to anomaly detection in piezometer data of embankment dams, to cosmology. Our main theorem characterizes the permutation invariant objective functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{title}{Deep {{Sets}}}
      \field{urlday}{14}
      \field{urlmonth}{5}
      \field{urlyear}{2025}
      \field{volume}{30}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/N4X2YWWI/Zaheer et al_2017_Deep Sets.pdf
      \endverb
      \verb{urlraw}
      \verb https://papers.neurips.cc/paper_files/paper/2017/hash/f22e4747da1aa27e363d86d40ff442fe-Abstract.html
      \endverb
      \verb{url}
      \verb https://papers.neurips.cc/paper_files/paper/2017/hash/f22e4747da1aa27e363d86d40ff442fe-Abstract.html
      \endverb
    \endentry
    \entry{zambaldi2018}{inproceedings}{}{}
      \name{author}{16}{}{%
        {{hash=997657b08786dc45aaef854249d8af31}{%
           family={Zambaldi},
           familyi={Z\bibinitperiod},
           given={Vinicius},
           giveni={V\bibinitperiod}}}%
        {{hash=6087d2ea339b8f52276e883d86bd18f3}{%
           family={Raposo},
           familyi={R\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=0b3e9682fc5db39420c22d4267683326}{%
           family={Santoro},
           familyi={S\bibinitperiod},
           given={Adam},
           giveni={A\bibinitperiod}}}%
        {{hash=8158ca9415b0eb10ee17c8a8ae663bb3}{%
           family={Bapst},
           familyi={B\bibinitperiod},
           given={Victor},
           giveni={V\bibinitperiod}}}%
        {{hash=ccd1e47b05491aa37da5d040a0b406f6}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Yujia},
           giveni={Y\bibinitperiod}}}%
        {{hash=92efe2a8e13a9b7a3fb647951ee2391c}{%
           family={Babuschkin},
           familyi={B\bibinitperiod},
           given={Igor},
           giveni={I\bibinitperiod}}}%
        {{hash=abe873d21d37af0af9b6711c1efd9caa}{%
           family={Tuyls},
           familyi={T\bibinitperiod},
           given={Karl},
           giveni={K\bibinitperiod}}}%
        {{hash=ff36bcecacbac6382f6f3048b316d708}{%
           family={Reichert},
           familyi={R\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=3a6fdf4df9a25f1d2d506ad9e86e1f6c}{%
           family={Lillicrap},
           familyi={L\bibinitperiod},
           given={Timothy},
           giveni={T\bibinitperiod}}}%
        {{hash=67c36471603b6de0347c489b0f8b05b0}{%
           family={Lockhart},
           familyi={L\bibinitperiod},
           given={Edward},
           giveni={E\bibinitperiod}}}%
        {{hash=9c9a18dc9378af37452e312a622cd2ed}{%
           family={Shanahan},
           familyi={S\bibinitperiod},
           given={Murray},
           giveni={M\bibinitperiod}}}%
        {{hash=430fe30c5432d64aba2ec4198286db53}{%
           family={Langston},
           familyi={L\bibinitperiod},
           given={Victoria},
           giveni={V\bibinitperiod}}}%
        {{hash=7045b009b04d57bd2e19b5dfa0864d4f}{%
           family={Pascanu},
           familyi={P\bibinitperiod},
           given={Razvan},
           giveni={R\bibinitperiod}}}%
        {{hash=9ac5fb35edeb23736953b00e66bef926}{%
           family={Botvinick},
           familyi={B\bibinitperiod},
           given={Matthew},
           giveni={M\bibinitperiod}}}%
        {{hash=494b568c5dc85ba8f3f409635f9c5f25}{%
           family={Vinyals},
           familyi={V\bibinitperiod},
           given={Oriol},
           giveni={O\bibinitperiod}}}%
        {{hash=5a63429a6e1733e8f3f4dc71cbb6eec9}{%
           family={Battaglia},
           familyi={B\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{d83394116ab65180ade0d4d8a4e37ec3}
      \strng{fullhash}{3a10c200b2565beb12849f7fe41af222}
      \strng{fullhashraw}{3a10c200b2565beb12849f7fe41af222}
      \strng{bibnamehash}{d83394116ab65180ade0d4d8a4e37ec3}
      \strng{authorbibnamehash}{d83394116ab65180ade0d4d8a4e37ec3}
      \strng{authornamehash}{d83394116ab65180ade0d4d8a4e37ec3}
      \strng{authorfullhash}{3a10c200b2565beb12849f7fe41af222}
      \strng{authorfullhashraw}{3a10c200b2565beb12849f7fe41af222}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We introduce an approach for augmenting model-free deep reinforcement learning agents with a mechanism for relational reasoning over structured representations, which improves performance, learning efficiency, generalization, and interpretability. Our architecture encodes an image as a set of vectors, and applies an iterative message-passing procedure to discover and reason about relevant entities and relations in a scene. In six of seven StarCraft II Learning Environment mini-games, our agent achieved state-of-the-art performance, and surpassed human grandmaster-level on four. In a novel navigation and planning task, our agent's performance and learning efficiency far exceeded non-relational baselines, it was able to generalize to more complex scenes than it had experienced during training. Moreover, when we examined its learned internal representations, they reflected important structure about the problem and the agent's intentions. The main contribution of this work is to introduce techniques for representing and reasoning about states in model-free deep reinforcement learning agents via relational inductive biases. Our experiments show this approach can offer advantages in efficiency, generalization, and interpretability, and can scale up to meet some of the most challenging test environments in modern artificial intelligence.}
      \field{day}{27}
      \field{eventtitle}{International {{Conference}} on {{Learning Representations}}}
      \field{langid}{english}
      \field{month}{9}
      \field{title}{Deep Reinforcement Learning with Relational Inductive Biases}
      \field{urlday}{14}
      \field{urlmonth}{5}
      \field{urlyear}{2025}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/9UUADGCF/Zambaldi et al_2018_Deep reinforcement learning with relational inductive biases.pdf
      \endverb
      \verb{urlraw}
      \verb https://openreview.net/forum?id=HkxaFoC9KQ
      \endverb
      \verb{url}
      \verb https://openreview.net/forum?id=HkxaFoC9KQ
      \endverb
    \endentry
    \entry{zhang2021}{online}{}{}
      \name{author}{3}{}{%
        {{hash=d2333c98ffcc26f61ae2f6f998292935}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Kaiqing},
           giveni={K\bibinitperiod}}}%
        {{hash=139d2f7faea343c7833a9d50880d87c8}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Zhuoran},
           giveni={Z\bibinitperiod}}}%
        {{hash=70fa925cb5f5e3bd935b997f073ca47a}{%
           family={Başar},
           familyi={B\bibinitperiod},
           given={Tamer},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{ab4dcff937409d14f5d954a5ecca1cf6}
      \strng{fullhash}{e05bfaed45551f26949d1f199143629d}
      \strng{fullhashraw}{e05bfaed45551f26949d1f199143629d}
      \strng{bibnamehash}{e05bfaed45551f26949d1f199143629d}
      \strng{authorbibnamehash}{e05bfaed45551f26949d1f199143629d}
      \strng{authornamehash}{ab4dcff937409d14f5d954a5ecca1cf6}
      \strng{authorfullhash}{e05bfaed45551f26949d1f199143629d}
      \strng{authorfullhashraw}{e05bfaed45551f26949d1f199143629d}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Recent years have witnessed significant advances in reinforcement learning (RL), which has registered great success in solving various sequential decision-making problems in machine learning. Most of the successful RL applications, e.g., the games of Go and Poker, robotics, and autonomous driving, involve the participation of more than one single agent, which naturally fall into the realm of multi-agent RL (MARL), a domain with a relatively long history, and has recently re-emerged due to advances in single-agent RL techniques. Though empirically successful, theoretical foundations for MARL are relatively lacking in the literature. In this chapter, we provide a selective overview of MARL, with focus on algorithms backed by theoretical analysis. More specifically, we review the theoretical results of MARL algorithms mainly within two representative frameworks, Markov/stochastic games and extensive-form games, in accordance with the types of tasks they address, i.e., fully cooperative, fully competitive, and a mix of the two. We also introduce several significant but challenging applications of these algorithms. Orthogonal to the existing reviews on MARL, we highlight several new angles and taxonomies of MARL theory, including learning in extensive-form games, decentralized MARL with networked agents, MARL in the mean-field regime, (non-)convergence of policy-based methods for learning in games, etc. Some of the new angles extrapolate from our own research endeavors and interests. Our overall goal with this chapter is, beyond providing an assessment of the current state of the field on the mark, to identify fruitful future research directions on theoretical studies of MARL. We expect this chapter to serve as continuing stimulus for researchers interested in working on this exciting while challenging topic.}
      \field{day}{28}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{4}
      \field{pubstate}{prepublished}
      \field{shorttitle}{Multi-{{Agent Reinforcement Learning}}}
      \field{title}{Multi-{{Agent Reinforcement Learning}}: {{A Selective Overview}} of {{Theories}} and {{Algorithms}}}
      \field{urlday}{21}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \verb{doi}
      \verb 10.48550/arXiv.1911.10635
      \endverb
      \verb{eprint}
      \verb 1911.10635
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/8QLPXE8K/Zhang et al_2021_Multi-Agent Reinforcement Learning.pdf;/Users/brandonhosley/Zotero/storage/N8XASZ5K/1911.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1911.10635
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1911.10635
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning}
    \endentry
    \entry{zheng2020}{inproceedings}{}{}
      \name{author}{6}{}{%
        {{hash=6137596cdae8bca1bd5dd027b529c2da}{%
           family={Zheng},
           familyi={Z\bibinitperiod},
           given={Han},
           giveni={H\bibinitperiod}}}%
        {{hash=57cf8a4df07cf225eff490fcb3dd53ce}{%
           family={Wei},
           familyi={W\bibinitperiod},
           given={Pengfei},
           giveni={P\bibinitperiod}}}%
        {{hash=78f8ffda028131a2ad3f19fb3e42cdb7}{%
           family={Jiang},
           familyi={J\bibinitperiod},
           given={Jing},
           giveni={J\bibinitperiod}}}%
        {{hash=e7b6aa3c68f9430695b34429c0871856}{%
           family={Long},
           familyi={L\bibinitperiod},
           given={Guodong},
           giveni={G\bibinitperiod}}}%
        {{hash=1d1fb06f744e43890d04aa354a1016ed}{%
           family={Lu},
           familyi={L\bibinitperiod},
           given={Qinghua},
           giveni={Q\bibinitperiod}}}%
        {{hash=59f3e845390faff6c808a1556b2ac0ef}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Chengqi},
           giveni={C\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{5fd2dd1f59c4c5822a4ebda5c624e3aa}
      \strng{fullhash}{6fa65c0765cc77889fcdf88b9dae1669}
      \strng{fullhashraw}{6fa65c0765cc77889fcdf88b9dae1669}
      \strng{bibnamehash}{6fa65c0765cc77889fcdf88b9dae1669}
      \strng{authorbibnamehash}{6fa65c0765cc77889fcdf88b9dae1669}
      \strng{authornamehash}{5fd2dd1f59c4c5822a4ebda5c624e3aa}
      \strng{authorfullhash}{6fa65c0765cc77889fcdf88b9dae1669}
      \strng{authorfullhashraw}{6fa65c0765cc77889fcdf88b9dae1669}
      \field{extraname}{2}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Numerous deep reinforcement learning agents have been proposed, and each of them has its strengths and flaws. In this work, we present a Cooperative Heterogeneous Deep Reinforcement Learning (CHDRL) framework that can learn a policy by integrating the advantages of heterogeneous agents. Specifically, we propose a cooperative learning framework that classifies heterogeneous agents into two classes: global agents and local agents. Global agents are off-policy agents that can utilize experiences from the other agents. Local agents are either on-policy agents or population-based evolutionary algorithms (EAs) agents that can explore the local area effectively. We employ global agents, which are sample-efficient, to guide the learning of local agents so that local agents can benefit from the sample-efficient agents and simultaneously maintain their advantages, e.g., stability. Global agents also benefit from effective local searches. Experimental studies on a range of continuous control tasks from the Mujoco benchmark show that CHDRL achieves better performance compared with state-of-the-art baselines.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{title}{Cooperative {{Heterogeneous Deep Reinforcement Learning}}}
      \field{urlday}{14}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{volume}{33}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \true{nocite}
      \field{pages}{17455\bibrangedash 17465}
      \range{pages}{11}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/4XRVMVDD/Zheng et al_2020_Cooperative Heterogeneous Deep Reinforcement Learning.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2020/hash/ca3a9be77f7e88708afb20c8cdf44b60-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2020/hash/ca3a9be77f7e88708afb20c8cdf44b60-Abstract.html
      \endverb
    \endentry
    \entry{zotero-2599}{online}{}{}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labeltitlesource}{title}
      \field{title}{Overview — {{Ray}} 2.24.0}
      \field{urlday}{18}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{urldateera}{ce}
      \true{nocite}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/G8C3WP5D/index.html
      \endverb
      \verb{urlraw}
      \verb https://docs.ray.io/en/latest/ray-overview/index.html
      \endverb
      \verb{url}
      \verb https://docs.ray.io/en/latest/ray-overview/index.html
      \endverb
    \endentry
    \entry{zotero-2601}{online}{}{}
      \list{organization}{1}{%
        {TensorFlow}%
      }
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labeltitlesource}{title}
      \field{abstract}{A suite of visualization tools to understand, debug, and optimize TensorFlow programs for ML experimentation.}
      \field{langid}{english}
      \field{title}{{{TensorBoard}}}
      \field{urlday}{18}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{urldateera}{ce}
      \true{nocite}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/EJFXYETA/tensorboard.html
      \endverb
      \verb{urlraw}
      \verb https://www.tensorflow.org/tensorboard
      \endverb
      \verb{url}
      \verb https://www.tensorflow.org/tensorboard
      \endverb
    \endentry
    \entry{zotero-2603}{online}{}{}
      \list{organization}{1}{%
        {Weights \& Biases}%
      }
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The Weights \& Biases MLOps platform helps AI developers streamline their ML workflow from end-to-end.}
      \field{langid}{american}
      \field{shorttitle}{Weights \& {{Biases}}}
      \field{title}{Weights \& {{Biases}}: {{The AI Developer Platform}}}
      \field{urlday}{18}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{urldateera}{ce}
      \true{nocite}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/PW8WQCTU/site.html
      \endverb
      \verb{urlraw}
      \verb https://wandb.ai/site
      \endverb
      \verb{url}
      \verb https://wandb.ai/site
      \endverb
    \endentry
  \enddatalist
\endrefsection
\endinput

