---
title: 'Homework 6'
output:
  pdf_document: default
  html_document: default
date: "2023-05-11"
author: 'Brandon Hosley'
---
---
title: "Homework 6"
footer: 'HW 6'
output:
  pdf_document:
    df_print: kable
  html_document:
    df_print: paged
  slidy_presentation:
    code_folding: hide
    fig_caption: yes
    smart: no
keep_tex: yes
graphics: yes
---

```{r setup, include=FALSE}
source('../R/scripts/R/setup.R')
shiny::includeCSS('../R/scripts/css/flat-slidy.css')
shiny::includeScript("../R/scripts/js/jquery.min.js")
shiny::includeScript(system.file('../R/scripts','js','tpt-scroll.js'))
shiny::includeScript(system.file('../R/scripts','js','hideout.js'))
library(xlsx)
library(xtable)
library(MASS)
library(readxl)
options(xtable.comment = FALSE, xtable.type="latex")
options(rgl.useNULL=TRUE)
#.rs.restartR()
library(qpcR)
library(kableExtra)
library(modelsummary)
library(car)
```

```{r, include=FALSE}
kable <- function(data, ...) {
  knitr::kable(data, ..., booktabs = TRUE, digits = 4, align = 'c') %>% 
    kable_styling(latex_options =c("striped", "hold_position"))
}
```

*The purpose of this homework assignment is to assess your ability to perform and explain operations and concepts associated with lesson objectives for Lessons 4, 5, 11, and 13. The file 1989 NFL Season Data (Handout).xlsx contains team statistics from the 1989 National Football League (NFL) season.*

## Problem 1.

*Analyze the relationship between the nine numeric regressors* $(x_1, x_2, \ldots, x_9)$ *and total team wins.*

```{r, echo=TRUE}
df <- read_excel('../Homework 1/1989 NFL Season Data (Handout).xlsx', range = cell_rows(1:29))
```

### (a)

*Create the correlation matrix. What does this tell us about the multicollinearity risk in this dataset?*

```{r, echo=TRUE}
X <- data.matrix(df[c('x1','x2','x3','x4','x5','x6','x7','x8','x9')])
kable(cor(X))
```

The correlation matrix provides some insight into the risk of pairwise multicollinearity risk.
Using 0.5 as an arbitrary threshold we can identify the highest pairwise risks to be between x1, x7, and x8.

### (b)

*Calculate the relevant eigenvalues and condition number. Evaluate the evidence for multicollinearity.*

```{r, echo=T}
X.eig <- eigen(t(X) %*% X)
k <- max(X.eig$values) / min(X.eig$values)
sprintf('The condition is %f', k)
k_j <- max(X.eig$values) /X.eig$values
kable(k_j, col.names='$\\kappa_j$')
```

The condition number is large enough to indicate severe multicolinearity and upon further investigation,
the last five conditions indicate severe multicolinearity.

### (c)

*State and interpret the linear dependence relationship associated with the most relevant eigenvalue.*

```{r, echo=T}
kable(X.eig$vectors[,9], col.names='Eigenvector 9')
```

The relationship associated with the most significant relevant eigenvalue, after dropping the factors that that are $<0.01$ we get:
$$ 0 \approx -0.0111167\ x_3 + 0.0390763\ x_4 + 0.9991744\ x_7 $$

### (d)

*Create the full model, regressing y against all nine regressors. What do the variance inflation factors tell us about possible multicollinearity?*

```{r, echo=T}
mod = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9, data = df)
kable(t(vif(mod)))
```

From these results we see that the highest VIF values are associated with $x_1$ and $x_7$. Looking at the names of the variables in the original spread sheet, it seems fairly intuitive that percentage of rushing plays would be highly correlated to the number of rushing yards during the season.

## Problem 2.

*Explore the effects of centering variables on multicollinearity. Create a model containing linear, interaction, and quadratic terms for both x1 and x2. Create a second model by centering the two regressors first and then multiplying/squaring them to create the interaction/quadratic terms.*

```{r, echo=T}
# mod_quad <- lm(y~ polym(x1, x2, degree = 2, raw=TRUE), data=df )
# mod_cent_quad <- lm(y~ polym(scale(x1, scale = FALSE), scale(x2, scale = FALSE), 
#                              degree = 2, raw=TRUE), data=df )
mod_quad <- lm(y~ x1 + x2 + x1*x2 + I(x1^2) + I(x2^2), data=df )
x1s <- scale(df$x1, scale = FALSE)
x2s <- scale(df$x2, scale = FALSE)
mod_cent_quad <- lm(y~ x1s + x2s + x1s*x2s + I(x1s^2) + I(x2s^2), data=df )
```

### (a) 

*Compare the summary outputs of the two models. Is there a noticeable effect between the two models?*

```{r, echo=T}
summary(mod_quad)
summary(mod_cent_quad)
```

It appears that the two models are statistically (almost) identical. The evaluation statistics are the same and the non-linear term coefficients remain the same. The intercept and linear term coefficients do change. All of this seems to a translation of the features in an $\mathbb R^2$ space.

### (b) 

*Calculate and interpret the variance inflation factors for the two models.*

```{r, echo=T}
kable(cbind(vif(mod_quad), vif(mod_cent_quad)), col.names = c('Regular', 'Mean-centered'))
```

These results would seem to imply that mean-centering could address multicollinearity, which seems nonsensical.

### (c) 

*Create a table with two columns, one containing the predicted values from the first model and the other containing the predicted values from the second model. What conclusions can you draw about the effects of multicollinearity?*

```{r, echo=T}
kable(cbind(mod_quad$fitted.values, mod_cent_quad$fitted.values), 
      col.names = c('Regular', 'Mean-centered'))
```

This demonstrates that the two models predict identically. This makes sense, as a linear translation of the regressor values should have no effect on the model's prediction ability.

All of this seems very unusual, I had some difficulty in understanding why this would be the case, but the chapter at the site https://study.sagepub.com/sites/default/files/gurnsey_onlineapp17.pdf outlined some helpful clarifications about how changing the mean to 0 has the effect of data points on either side mostly balancing eachother out and resulting in a typically significantly lower VIF.
