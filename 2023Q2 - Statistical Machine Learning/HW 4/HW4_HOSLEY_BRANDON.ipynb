{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lAsyeOwFnoX"
   },
   "source": [
    "# CSCE 623 Homework Assignment 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "46d-KBtJJxN1"
   },
   "source": [
    "### Student Name:  HOSLEY, BRANDON"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Yna07rL4Jz43"
   },
   "source": [
    "### Date: May 9, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseball Salary Regression in multi-feature player dataset\n",
    "\n",
    "Instructions:\n",
    "* Review all provided code before starting your work - this instructor has provided hints and tips throughout the code\n",
    "* This assignment is composed of 2 parts\n",
    "    * Load, split, and explore the data\n",
    "    * Fit models and evaluate performance\n",
    "* Complete the numbered STEPS which contain (STUDENT CODE REQUIRED) and (STUDENT MARKDOWN RESPONSE REQUIRED) activities\n",
    "* Remember to restart the kernel and rerun all cells before submitting the assignment\n",
    "* Submit only the Jupyter Notebook (.ipynb) file - do not submit the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Note... not all of these are used...\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "import copy\n",
    "\n",
    "from math import factorial\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import cross_validate,  cross_val_score, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "#warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL STUDENT CODING: If you need any imports, code them below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### ------- EXTRA STUDENT IMPORTS ------------\n",
    "\n",
    "\n",
    "######### ------- END STUDENT IMPORTS ------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A:  Data setup and exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "This dataset contains information about various baseball players and their salaries.  \n",
    "\n",
    "### Load the 'ISLR_Hitters.csv' data using pandas (INSTRUCTOR CODE PROVIDED).  \n",
    "* set the `index_col` to 0\n",
    "* There are unknown salaries in the dataset for some players... drop any `na`s in the dataset (there will be 263 remaining rows after removing the nas)\n",
    "* Ensure the index is correctly identifying each of the 263 rows from the index 0 to 262 without gaps\n",
    "* Store the full dataset data in `df`\n",
    "* Make the name column of the player be `Player`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 263 entries, 0 to 262\n",
      "Data columns (total 21 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Player     263 non-null    object \n",
      " 1   AtBat      263 non-null    int64  \n",
      " 2   Hits       263 non-null    int64  \n",
      " 3   HmRun      263 non-null    int64  \n",
      " 4   Runs       263 non-null    int64  \n",
      " 5   RBI        263 non-null    int64  \n",
      " 6   Walks      263 non-null    int64  \n",
      " 7   Years      263 non-null    int64  \n",
      " 8   CAtBat     263 non-null    int64  \n",
      " 9   CHits      263 non-null    int64  \n",
      " 10  CHmRun     263 non-null    int64  \n",
      " 11  CRuns      263 non-null    int64  \n",
      " 12  CRBI       263 non-null    int64  \n",
      " 13  CWalks     263 non-null    int64  \n",
      " 14  League     263 non-null    object \n",
      " 15  Division   263 non-null    object \n",
      " 16  PutOuts    263 non-null    int64  \n",
      " 17  Assists    263 non-null    int64  \n",
      " 18  Errors     263 non-null    int64  \n",
      " 19  Salary     263 non-null    float64\n",
      " 20  NewLeague  263 non-null    object \n",
      "dtypes: float64(1), int64(16), object(4)\n",
      "memory usage: 43.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df = pd.read_csv('ISLR_Hitters.csv', index_col=False).dropna()\n",
    "pre_df = pd.read_csv('ISLR_Hitters.csv').dropna().reset_index(drop=True)\n",
    "pre_df.rename(columns={ pre_df.columns[0]: \"Player\" },inplace=True)\n",
    "display(pre_df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  STEP 1: (STUDENT CODE REQUIRED) Preprocess the categorical columns in the data using one-hot-encoding \n",
    "\n",
    "Notice that `League`, `Division` and `NewLeague` are non-numerical categories (stored as `object`s ) which should be converted to one-hot-encoded features.  Each of these categorical features only contains two categories, and if we created 2 one-hot features for each, using both columns would not provide any more information than using one.  Since we want to eliminate columns which provide redundant information, we only need one of the category columns to be 'hot' and if it is not (0 instead of 1), we know that the other category for that feature must be true.   \n",
    "\n",
    "For example, we create a one-hot-encoded column `League_N` such that if the original categorical value in `League` was `N` then the one-hot-encoded column `League_N` should contain a 1, however if the categorical value in `League` was `A` then the one-hot-encoded column `League_N` should contain a 0\n",
    "\n",
    "Create a new dataframe `df` to accomplish this for each of the three categorical features.  After completing this step, the dataset `df` should be such that\n",
    "* The categorical column `League` (`A`merican or `N`ational) is represented instead with column  `League_N` which contains a 1 if `League` was `N` or a 0 if `League` was `A`\n",
    "* The categorical column  `Division` (`E`ast or `W`est) is represented instead with column `Division_W` which contains a 1 if `Division` was `W` or a 0 if `Division` was `E`\n",
    "* The categorical column   `NewLeague` (`A`merican or `N`ational) is represented instead with column `NewLeague_N` which contains a 1 if `NewLeague` was `N` or a 0 if `NewLeague` was `A` \n",
    "* `df` doesn't contain the original 3 categorical variables (`League`, `Division` and `NewLeague`)\n",
    "\n",
    "Check to make sure everytyhing worked correctly.  After this step is complete there should still be 263 rows, 21 columns, and all columns except for `Player` will contain numeric information (no letters, strings, objects)\n",
    "\n",
    "\n",
    "Hint:  see https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html and the function's option `drop= 'first'` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1\n",
    "\n",
    "df = None #placeholder\n",
    "\n",
    "\n",
    "#----------------START STUDENT CODE -----------------------\n",
    "df = pre_df.copy().drop(columns = ['League','Division', 'NewLeague'])\n",
    "\n",
    "col_transformer = ColumnTransformer(transformers =[\n",
    "    ('enc', OneHotEncoder(drop ='first'), ['League','Division', 'NewLeague']),\n",
    "], remainder ='passthrough')\n",
    "\n",
    "df[['League_N', 'Division_W', 'NewLeague_N']] = pd.DataFrame( col_transformer.fit_transform(pre_df[['League','Division', 'NewLeague']]) ).apply(pd.to_numeric, downcast='integer')\n",
    "#df[['League_A', 'Division_E', 'NewLeague_A']] = df[['League_N', 'Division_W', 'NewLeague_N']].applymap(lambda x : [1,0][int(x)])\n",
    "#df.head()\n",
    "#----------------END STUDENT CODE -----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 263 entries, 0 to 262\n",
      "Data columns (total 21 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Player       263 non-null    object \n",
      " 1   AtBat        263 non-null    int64  \n",
      " 2   Hits         263 non-null    int64  \n",
      " 3   HmRun        263 non-null    int64  \n",
      " 4   Runs         263 non-null    int64  \n",
      " 5   RBI          263 non-null    int64  \n",
      " 6   Walks        263 non-null    int64  \n",
      " 7   Years        263 non-null    int64  \n",
      " 8   CAtBat       263 non-null    int64  \n",
      " 9   CHits        263 non-null    int64  \n",
      " 10  CHmRun       263 non-null    int64  \n",
      " 11  CRuns        263 non-null    int64  \n",
      " 12  CRBI         263 non-null    int64  \n",
      " 13  CWalks       263 non-null    int64  \n",
      " 14  PutOuts      263 non-null    int64  \n",
      " 15  Assists      263 non-null    int64  \n",
      " 16  Errors       263 non-null    int64  \n",
      " 17  Salary       263 non-null    float64\n",
      " 18  League_N     263 non-null    int8   \n",
      " 19  Division_W   263 non-null    int8   \n",
      " 20  NewLeague_N  263 non-null    int8   \n",
      "dtypes: float64(1), int64(16), int8(3), object(1)\n",
      "memory usage: 37.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()  #confirm existence of dummies and auto-generated names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating features and regression target labels & test/non-test split (INSTRUCTOR CODE PROVIDED)\n",
    "\n",
    "Next we will \n",
    "* Separate features and labels in the full dataset into X and y.\n",
    "* Partition the data into test and non-test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame( df.Salary)\n",
    "# Drop the column with the independent variable (Salary)\n",
    "# X = df.drop(['Salary'], axis=1).astype('float64')\n",
    "X = df.drop(['Salary'], axis=1)\n",
    "\n",
    "testfraction = 0.25\n",
    "randState = 42\n",
    "\n",
    "X_nonTest, X_test, y_nonTest, y_test = train_test_split(X, y, test_size=testfraction, random_state=randState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Player',\n",
       " 'AtBat',\n",
       " 'Hits',\n",
       " 'HmRun',\n",
       " 'Runs',\n",
       " 'RBI',\n",
       " 'Walks',\n",
       " 'Years',\n",
       " 'CAtBat',\n",
       " 'CHits',\n",
       " 'CHmRun',\n",
       " 'CRuns',\n",
       " 'CRBI',\n",
       " 'CWalks',\n",
       " 'PutOuts',\n",
       " 'Assists',\n",
       " 'Errors',\n",
       " 'League_N',\n",
       " 'Division_W',\n",
       " 'NewLeague_N']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the names of the features\n",
    "list(X_nonTest.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 197 entries, 143 to 102\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Salary  197 non-null    float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 3.1 KB\n"
     ]
    }
   ],
   "source": [
    "y_nonTest.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the value of the response variable (salary)   (INSTRUCTOR CODE PROVIDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 197 entries, 143 to 102\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Salary  197 non-null    float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 3.1 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>197.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>543.483442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>445.996188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>67.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>195.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>425.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>750.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2460.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Salary\n",
       "count   197.000000\n",
       "mean    543.483442\n",
       "std     445.996188\n",
       "min      67.500000\n",
       "25%     195.000000\n",
       "50%     425.000000\n",
       "75%     750.000000\n",
       "max    2460.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_nonTest.info())\n",
    "display(y_nonTest.describe())\n",
    "\n",
    "y_nonTest.hist(bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the histogram of the response variable reveals very few observations containing higher salaries (above $1.0M).  This may cause problems when trying to predict in this range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Create a new response variable by taking the log of the actual dollar salary value. (STUDENT CODE REQURIED)\n",
    "\n",
    "* Store the value of the base-10-log of the actual dollar sallary $($ `y_nonTest` $\\times1000)$ in a variable called `log_y_nonTest` using `numpy`'s  `log10` function.\n",
    "* show a histogram of `log_y_nonTest`\n",
    "\n",
    "Note that we will need to do this later for the test set y too\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2\n",
    "\n",
    "log_y_nonTest = None #placeholder\n",
    "\n",
    "#----------------START STUDENT CODE -----------------------\n",
    "\n",
    "#remember to use the function from numpy... not base python\n",
    "\n",
    "\n",
    "#----------------END STUDENT CODE -----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Explore the non-test data features & select relevant features (STUDENT CODE REQUIRED)\n",
    "\n",
    "1.  Using only the `X_nonTest` and `log_y_nonTest` data, conduct data exploration on the features\n",
    "2.  Using log salary labels, compute correlations with each possible feature\n",
    "3.  Select the top 7 features you think might work well for predicting player (log) salary.  Later you will be required to provide evidence supporting your beliefs \n",
    "2.  Store the column names of your selected features as a list in `best_features`.  \n",
    "\n",
    "Suggestions:  Use `.corr` will be helpful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3\n",
    "\n",
    "best_features = []  #placeholder\n",
    "\n",
    "# when exploring relationship between features and predictors, it might be useful to have a (re)combined set\n",
    "xy_nontest = pd.concat([X_nonTest, log_y_nonTest], axis=1)\n",
    "xy_nontest.rename(columns={\"Salary\":\"Log10_Salary\"},inplace=True)\n",
    "xy_nontest.info()\n",
    "\n",
    "#----------------START STUDENT CODE -----------------------\n",
    "\n",
    "\n",
    "#----------------END STUDENT CODE -----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instructor code \n",
    "print(\"STUDENT-SELECTED BEST FEATURES:\", best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: Evidence supporting selection of best Features  (STUDENT MARKDOWN REQUIRED)\n",
    "\n",
    "Provide descriptions of the evidence you used to make your decisions about best features\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "<font color='green'>STUDENT ANSWER HERE STEP 4</font>\n",
    "\n",
    "\n",
    "<font color='green'> \n",
    "\n",
    "<font color='green'>END STUDENT ANSWER</font>    \n",
    "    \n",
    "------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  Make a `seaborn` `pairplot` of the these 7 features that you think are most useful AND the log salary. (INSTRUCTOR-PROVIDED CODE)\n",
    "\n",
    "(note that pairplots make take time to render - the more features you include the longer they take)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE - THIS MAKE TAKE A FEW MOMENTS TO CALCULATE & DISPLAY ON YOUR COMPUTER\n",
    "\n",
    "#switch the below to True to show the pairsplot.\n",
    "if True:\n",
    "    g = sns.pairplot(data=xy_nontest, x_vars=best_features,y_vars=\"Log10_Salary\")\n",
    "    # g = sns.pairplot(data=xy_nontest, x_vars=best_features,y_vars=\"Salary\", height=1, aspect=1.0)\n",
    "    g.fig.set_size_inches(20,4)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5:  Scale all X data using nonTest data scaler  (STUDENT CODE REQUIRED)\n",
    "* Scale the data features since we dont want some features to affect the linear regressions differently just becasue they have different scales\n",
    "* Fit the `zscaler` to the non-test data and transform the non-test data into the variable called `X_nonTest_scaled`.  Remember that we should not be using test data for this!\n",
    "* then *apply* those (non-test) fitted parameters to the test data into the variable called `X_test_scaled` to preserve the scaling *without being influenced by any aspect of the test data*\n",
    "* Remember to not attempt to scale non-numerical features (like `Player`) or one-hot features liek `League_N`, `Division_W` and `NewLeague_N`\n",
    "\n",
    "Hint: when using `sklearn`'s `StandardScalar`, `.fit`, `.fit_transform`, and `.transform` will be useful here\n",
    "\n",
    "See:  https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?highlight=standardscaler#sklearn.preprocessing.StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5\n",
    "\n",
    "X_nonTest_scaled = None #placeholder\n",
    "\n",
    "cols = list(X_nonTest.columns.values)\n",
    "exclude_scale_cols = ['Player','League_N','Division_W','NewLeague_N']\n",
    "cols_to_scale = cols.copy()\n",
    "for r in exclude_scale_cols: \n",
    "    cols_to_scale.remove(r)\n",
    "\n",
    "zscaler = StandardScaler()  #use this object to conduct the scaling in the student code block\n",
    "\n",
    "X_nonTest_scaled = None  #placeholder\n",
    "X_test_scaled = None  #placeholder\n",
    "\n",
    "\n",
    "#----------------START STUDENT CODE -----------------------\n",
    "\n",
    "\n",
    "#----------------END STUDENT CODE -----------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following instructor eval code will report the means and standard deviations of both the nonTest data features and the testData features\n",
    "\n",
    "HINT:  What would you expect to see as the difference in the means and stdevs between these two partitions of the data if the previous step was completed properly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructor eval code\n",
    "print(\"-Before transformation, non-test column names:\\n\", cols)\n",
    "\n",
    "print(\"---After transformation, non-test column names:\\n\", list(X_nonTest_scaled.columns.values))\n",
    "print(\"---After transformation, test column names:\\n\", list(X_test_scaled.columns.values))\n",
    "\n",
    "\n",
    "scalingdf = pd.DataFrame({'actualNonTestMeans':zscaler.mean_ ,\n",
    "                          'actualNonTestStds':zscaler.scale_,\n",
    "                          'scaledMeansNonTest':X_nonTest_scaled[cols_to_scale].mean(axis=0),\n",
    "                          'scaledStdsNonTest':X_nonTest_scaled[cols_to_scale].std(axis=0),\n",
    "                          'scaledMeansTest':X_test_scaled[cols_to_scale].mean(axis=0),\n",
    "                          'scaledStdsTest':X_test_scaled[cols_to_scale].std(axis=0)})\n",
    "display(scalingdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a prediction evaluator that accounts for the log base 10 scaled salaries (INSTRUCTOR-PROVIDED CODE)\n",
    "\n",
    "Before we start fitting models there is one last thing we need to do.\n",
    "\n",
    "Although the new Y values after log-10 scaling should help our model fit, it means the models predictions will be of log-10 salary (not the salary value itself).  In order to compensate for this we will build a new helper function to compute MSE on predictions which were made in the logspace.  This MSE method will be used by our model while computing the optimization equiation in search for good models.\n",
    "\n",
    "A transform to calculate the $MSE$ in the original dataspace is shown below - and all $MSE$ values will be calculated in salary space instead of log-10 salary space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_rmse(y_true, y_pred):\n",
    "    ydiff = np.power(10,y_true) - np.power(10,y_pred)\n",
    "    mse = np.dot(ydiff.T, ydiff)/len(ydiff)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return -rmse\n",
    "\n",
    "dataspace_rmse = make_scorer(transform_rmse, greater_is_better=True)  #this scorer can be used by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B:  Building competing models with feature selection and regularization\n",
    "\n",
    "In this next part, we will build several 'best of type' linear regression models using feature selection & regularization techniques\n",
    "* Foward (stepwise) subset selection\n",
    "* Backwards (stepwise) subset selection\n",
    "* Ridge Regression \n",
    "* LASSO\n",
    "\n",
    "In each model type we will use k-fold crossvalidation to make the selection of the best model from the hyper-parameter settings witin the type (number of features to keep for forward and backwards subset selection, best alpha value for each regularization type - ridge regression and LASSO).  \n",
    "\n",
    "Once these four best of type models have been acquired and the crossval performances are known, we can pick the best model of these four types.\n",
    "\n",
    "Then, we will retrain a single model of that winning type using *all* the non-test data and the hyper-parameter settings of the type in preparation for the next part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6 (STUDENT CODE REQUIRED) Forward and reverse Subset Selection: Determining the greedy set of model features for each size linear regression model\n",
    "\n",
    "In this step, you will use `sklearn.feature_selection`'s `SequentialFeatureSelector` as a model trainer.   This trainer is a good choice becasue it can produce models usine either forward or backwards subset selection, and you can tell it to build a model containing a specific number of features.\n",
    "\n",
    "Inside a loop you will collect the models of each feasibile feature-count size to make the prediction of the `log_y_nonTest` data using all the numeric features avaialbel in the `X_nonTest_scaled` data as input for the model.  This process will involve:\n",
    "* Use LinearRegression() as set to `model` for the `estimator` in `SequentialFeatureSelector`\n",
    "* Use K-cross-fold validation where K is defined by the instructor in the `kfold_count` and is used to populate the `cv` parameter in `SequentialFeatureSelector`\n",
    "* Use the dataspace_rmse scorer (created above) to have the `scoring` parameter minimize the RMSE score in units of actual salary dollars\n",
    "* Create a separate instantiation of `SequentialFeatureSelector` to create both forward and backward models during each iteration of the loop\n",
    "* Fit both the forward and backwards instantiated `SequentialFeatureSelector` using `.fit` with using features in `X_nonTest_scaled` and labels in `log_y_nonTest`\n",
    "* Capture the resulting feature mask using `.get_support` \n",
    "* Use the feature mask to determine the dataframe column names to keep for each model\n",
    "* Because `SequentialFeatureSelector` just provides the best features (not a fit model) we still have to fit and score a model with these features\n",
    "    * Use the fit model to score the model fit on the non-test data using crossvalidation\n",
    "    * Keep track of the best fitting model as the loop iterates\n",
    "* Capture the overall best fit model of each type (fowards and backwards), and the cv score it achieved (RMSE) \n",
    "\n",
    "(Hint:  See https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html#sklearn.feature_selection.SequentialFeatureSelector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "kfold_count = 5\n",
    "\n",
    "#select only the numeric features to use\n",
    "numeric_features = X_nonTest_scaled.select_dtypes('number').columns\n",
    "print(len(numeric_features),\"numeric features:\", numeric_features)\n",
    "\n",
    "selected_features_forward = [] #placeholder to contain list of feature name text strings (column names)\n",
    "selected_features_backward = [] #placeholder to contain list of feature name text strings (column names)\n",
    "\n",
    "model_scores_forward = [] #placeholder to keep list of scores per fitted model (length = feature qty-1)\n",
    "model_scores_backward = [] #placeholder to keep list of scores per feature-count (length = feature qty-1)\n",
    "\n",
    "best_score_forward = np.inf #start as bad as possible, to be replaced with best score\n",
    "best_score_backward = np.inf #start as bad as possible, to be replaced with best score\n",
    "best_idx_forward = None #placeholder\n",
    "best_idx_backward = None #placeholder\n",
    "\n",
    "best_features_forward = None #placeholder to contain list of feature name text strings (column names)\n",
    "best_features_backward = None #placeholder\n",
    "\n",
    "\n",
    "for idx,num_feats in enumerate(range(1,len(numeric_features))):\n",
    "    \n",
    "    #----------------START STUDENT CODE -----------------------\n",
    "\n",
    "\n",
    "    #----------------END STUDENT CODE -----------------------\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instructor diagnostics \n",
    "\n",
    "print(model_scores_forward)\n",
    "print(model_scores_backward)\n",
    "\n",
    "feature_counts = range(1,len(model_scores_forward)+1)\n",
    "\n",
    "greedyscoresdf = pd.DataFrame({'forward':model_scores_forward, \n",
    "                               'backward':model_scores_backward},index=feature_counts)\n",
    "# greedyscoresdf.columns = ('foward','backward')\n",
    "display(greedyscoresdf)\n",
    "print(\"Best greedy feature selection models:\")\n",
    "print(\"forward:\", best_idx_forward+1,\"features, RMSE $\",best_score_forward)\n",
    "print(best_features_forward)\n",
    "print(\"backward:\",best_idx_backward+1,\"features, RMSE $\",best_score_backward)\n",
    "print(best_features_backward)\n",
    "\n",
    "\n",
    "\n",
    "#plot the RMSEs of the forward/backward models\n",
    "plt.figure()\n",
    "plt.plot(feature_counts,model_scores_forward,label='forward')\n",
    "plt.plot(feature_counts,model_scores_backward,label='backward')\n",
    "plt.xticks(feature_counts)\n",
    "ax = plt.gca()\n",
    "ax.ticklabel_format(useOffset=False, style='plain')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7 (STUDENT CODE REQUIRED) Find Best Ridge Regression Model\n",
    "\n",
    "Use RidgeCV to find the best alpha for the ridge regression of a models of  to make the prediction of the `log_y_nonTest` data using *all* the *numeric* features avaialble in the `X_nonTest_scaled` data as input for the model.  This process will involve:\n",
    "    * Use K-cross-fold validation where K is defined by the instructor in the `kfold_count` and is used to populate the `cv` parameter in `RidgeCV`\n",
    "    * Use the `dataspace_rmse` scorer (created above) to have the `scoring` parameter minimize the RMSE score in units of actual salary dollars\n",
    "    * set `store_cv_values=True` in order to save the CV scores obtained for each `alpha` evaluated during CV\n",
    "* Fit the instantiated `RidgeCV` using `.fit` with using features in `X_nonTest_scaled` and labels in `log_y_nonTest`\n",
    "\n",
    "* Use Ridge() as set to `model` for the `estimator` in `RidgeCV`\n",
    "* Capture the resulting best alpha using `.alpha_` \n",
    "* Fit a new model with this alpha to compute the RMSE score of the best `RidgeCV` model using the `dataspace_rmse` scorer\n",
    "\n",
    "See:  https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html?highlight=ridgecv#sklearn.linear_model.RidgeCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7\n",
    "\n",
    "kfold_count = 5\n",
    "minLogAlpha = -3\n",
    "maxLogAlpha = 7\n",
    "alphaCount = 1000\n",
    "\n",
    "alphagrid = np.zeros(alphaCount)  # placeholder for the alphas\n",
    "alphagrid = np.logspace(minLogAlpha,maxLogAlpha,num=alphaCount)\n",
    "\n",
    "best_ridge_alpha = None # placeholder\n",
    "best_ridge_score = None # placeholder\n",
    "\n",
    "\n",
    "#------------- START STUDENT CODE ------------------\n",
    "\n",
    "\n",
    "#------------- END STUDENT CODE ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instructor diagnostics\n",
    "\n",
    "print(\"Best Ridge alpha:\", best_ridge_alpha, \"; Best ridge RMSE $\", best_ridge_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8 (STUDENT CODE REQUIRED) Find Best LASSO Regression Model\n",
    "\n",
    "Lasso behaves differently from Ridge Regression, and if we want to use cross validation RMSE $ as a scoring function we have to accomplish crossvalidation in a different way - by using a generalized wrapper to conduct CV.   Using a generalized wrapper in this way provides some extra functionality in conducting CV over multiple hyperparameter searches and capturing the resulting best models, parameters and performance information.  It may be a wise choice for future efforts...\n",
    "\n",
    "Use Lasso within `GridSearchCV` to find the best alpha for the Lasso regression of a models of  to make the prediction of the `log_y_nonTest` data using *all* the *numeric* features avaialble in the `X_nonTest_scaled` data as input for the model.  This process will involve:\n",
    "* Wrapping a Lasso() estimator inside of a `GridSearchCV()` object\n",
    "    * Setting the `estimator` of GridSearchCV to be `Lasso()`\n",
    "    * Establishing a param_grid dictionary for `GridSearchCV` which contains the logspace set of `alpha` parameters (`alphagrid`)\n",
    "    * Using `dataspace_rmse` as the scoring function for the `GridSearchCV`\n",
    "    * Using K-cross-fold validation where K is defined by the instructor in the `kfold_count` and is used to populate the `cv` parameter in `GridSearchCV`\n",
    "* Fit the instantiated `GridSearchCV` using `.fit` with the numeric features in `X_nonTest_scaled` and labels in `log_y_nonTest`\n",
    "* Capture the best lasso regularization alpha using `best_params.alpha_` \n",
    "* Capture the resulting best score of that model using `best_score_` \n",
    "* Capture extended details of the CV process as a dataframe using `.cv_results_`\n",
    "\n",
    "(Hint:  See https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8\n",
    "\n",
    "minLogAlpha = -3\n",
    "maxLogAlpha = 7\n",
    "alphaCount = 1000\n",
    "\n",
    "alphagrid = np.zeros(alphaCount)  # placeholder for the alphas\n",
    "alphagrid = np.logspace(minLogAlpha,maxLogAlpha,num=alphaCount)\n",
    "\n",
    "lcv_model = None #placeholder for GridSearchCV() wrapper of Lasso() model\n",
    "best_lasso_alpha = None  #placeholder\n",
    "best_lasso_score = None  #placeholder\n",
    "\n",
    "lasso_cv_results = pd.DataFrame() #placeholder\n",
    "\n",
    "#------------- START STUDENT CODE ------------------\n",
    "\n",
    "\n",
    "#------------- END STUDENT CODE ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instructor diagnostics\n",
    "\n",
    "print(\"best Lasso alpha:\",best_lasso_alpha, \"; best Lasso score:\",best_lasso_score)\n",
    "display(lasso_cv_results)\n",
    "plt.figure()\n",
    "plt.semilogx(lasso_cv_results.param_alpha,-lasso_cv_results.mean_test_score)\n",
    "plt.xlabel('alpha (logscale)')\n",
    "plt.ylabel('RMSE $')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 9 (STUDENT CODE REQUIRED)  Select the best performing model of all the models and refit on *all non-test data*\n",
    "\n",
    "1. In the code block below, re-summarize the  CV performance and hyperparameter settings of each model (via prints or displaying dataframes) \n",
    "\n",
    "2.  Then, for the best performing model, store the fitted model in `best_model`  \n",
    "    * dont forget that if your best model is foward or backwards selection, you must subset the features when you use the test data for prediction (STEP 10)\n",
    "    * refit the model to *all* the non-test (scaled) data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 9\n",
    "\n",
    "#------------- START STUDENT CODE ------------------\n",
    "\n",
    "\n",
    "#------------- END STUDENT CODE ------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 10 (STUDENT CODE REQUIRED) Compute the RMSE performance of the best model on the test set data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 10\n",
    "\n",
    "log_y_test=np.log10(y_test*1000) #transform test data actual salaries into log form of $\n",
    "\n",
    "#------------- START STUDENT CODE ------------------\n",
    "\n",
    "\n",
    "#------------- END STUDENT CODE ------------------\n",
    "\n",
    "\n",
    "print(\"Test Set Salary RMSE in dollars:\",test_rmse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 11 (STUDENT CODE REQUIRED) Make a residuals scatterplot of the predictions \n",
    "\n",
    "Make 2 plots\n",
    "* A plot which shows Acutal Salary in dollars on the horizontal axis and predicted salary in dollars on the vertical axis.  Include a diagonal line showing ideal predictions\n",
    "* A residuals plot which shows actual salary in dollars on horizontal axis, and the residual dollars (actual dollars minus predicted dollars) on the vertical axis \n",
    "\n",
    "Remember that *actual* salary ranges from under $100K to ~$2.5M... your plots should reflect this range  (not the log of it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 11\n",
    "\n",
    "#------------- START STUDENT CODE ------------------\n",
    "\n",
    "\n",
    "#------------- END STUDENT CODE ------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 12 (STUDENT MARKDOWN REQUIRED - Impact of using this model\n",
    "\n",
    "In the markdown cell below, \n",
    "* Describe the residuals behavior as a function of actual salary (Hint: see section 3.3.3 in ISLR \"Potential Problems\"\n",
    "* Describe the pros and cons of the residuals with respect to things like \n",
    "    * Overpaying or underpaying salaries of prospective players with similar attributes\n",
    "    * What kinds of salary ranges are likely to get overpaid (or underpaid)\n",
    "\n",
    "Note:  Use the coding scrap area below to compute things if you wish that will support the markdown in the cell below\n",
    "\n",
    "OPTIONAL/FUN:  Assuming this model is correct, figure out which named players are getting severly underpaid or overpaid and see if you agree with the model instead of their actual salary!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coding Scrap area for addtional analysis for STEP 12 - optional\n",
    "\n",
    "# --------------START SCRAP -----------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------END SCRAP -----------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color='green'>STUDENT ANSWER HERE FOR STEP 12</font>\n",
    "\n",
    "--------------------------\n",
    "\n",
    "<font color='green'>...words go here....\n",
    "    \n",
    "<font color='green'>\n",
    "\n",
    "--------------------------\n",
    "\n",
    "<font color='green'>END STUDENT ANSWER </font>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL:  Build a better model and provide RMSE and residuals\n",
    "\n",
    "* Can you outperform the best model you provided above?\n",
    "* Can you get better behavior in residuals?\n",
    "\n",
    "Hint:  Maybe constraining ourselves to linear functions of the input features are limiting our performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
