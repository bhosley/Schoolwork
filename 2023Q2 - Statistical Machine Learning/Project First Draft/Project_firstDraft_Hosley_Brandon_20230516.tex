\documentclass[journal]{IEEEtran}

% Packages
\usepackage{graphicx}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{url}
\usepackage{tikz}
\usetikzlibrary{matrix,shapes.multipart, positioning}

% Drafting Utility Package
\usepackage{comment}

% Title
\title{A Comparative Study of Techniques for Addressing Class Imbalance in Machine Learning}

\author{Brandon Hosley, 1st Lt, \textit{AFIT}%
	\thanks{Manuscript received May 16, 2023%
		%		; revised Month DD, YYYY.
}}

%\keywords{class imbalance, data-level methods, algorithm-level methods, hybrid methods, machine learning, performance evaluation}

% Document
\begin{document}
	
	\maketitle
	
	
	% Abstract
	\begin{abstract}
		Class imbalance is a pervasive challenge in machine learning, occurring when the distribution of classes in a dataset is significantly skewed. Traditional learning algorithms often struggle to accurately classify minority classes due to the dominance of majority classes. To address this issue, various techniques have been proposed to mitigate the impact of class imbalance on model performance. This paper presents a comparative analysis of different methods for handling class imbalance, aiming to provide insights into their effectiveness and suitability across a range of domains and datasets.
	\end{abstract}
	
	% Introduction
	\section{Introduction}
	\label{sec:introduction}
	Class imbalance is a pervasive challenge in machine learning, where certain classes within a dataset are significantly underrepresented compared to others. This disparity often leads to biased models that are less accurate in predicting minority class instances, resulting in suboptimal overall performance. In various real-world applications, such as fraud detection, medical diagnosis, and rare event prediction, the minority class is often more significant than the majority class. In critical domains, such as medical diagnoses, where the cost of false negatives is extremely high, failing to correctly identify instances of the minority class can have severe implications. Consequently, it is crucial to develop effective strategies that mitigate the impact of class imbalance on model performance.
	
	In recent years, researchers have proposed numerous techniques to address class imbalance and improve model performance. These methods can be broadly categorized into three main approaches: data-level methods, algorithm-level methods, and hybrid methods. Data-level methods focus on modifying the dataset itself by oversampling the minority class, undersampling the majority class, or generating synthetic samples. Algorithm-level methods aim to adapt existing machine learning algorithms or design new ones that are more robust to class imbalance. Hybrid methods combine data-level and algorithm-level techniques to leverage the advantages of both.
	
	Given the abundance of class imbalance handling methods, it becomes crucial to conduct a comparative analysis to understand their relative strengths and weaknesses. 
	% We evaluate the effectiveness of each method using a diverse set of real-world and synthetic datasets, encompassing various levels of class imbalance and problem complexities. 
	% Our assessment includes a thorough analysis of performance metrics, such as accuracy, precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC-ROC), to provide a holistic understanding of the impact of each technique on model performance.
	By empirically evaluating these techniques on diverse datasets and considering different evaluation metrics, we can gain insights into their effectiveness, applicability, and trade-offs. This paper aims to bridge this gap by providing a comprehensive evaluation of 
	%state-of-the-art class imbalance handling methods.
	well-known class imbalance handling methods.
	
	By comparing and contrasting these methods, we aim to provide practitioners and researchers with valuable insights into the relative strengths and weaknesses of each technique. Moreover, our study intends to offer guidelines for selecting the most appropriate approach based on the specific characteristics of a given problem. Ultimately, we hope this work contributes to the development of more accurate and reliable selection of techniques to address disparate machine learning models in the face of class imbalance challenges.
	
	\begin{comment}
		The contributions of this study include:
		\begin{itemize}
			\item An in-depth review of existing class imbalance handling techniques, categorizing them into data-level, algorithm-level, and hybrid methods.
			\item A comparative analysis of these methods using benchmark datasets from various domains.
			\item Evaluation of the methods based on performance metrics such as accuracy, precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC-ROC).
			\item Insights into the strengths, limitations, and applicability of different class imbalance handling techniques.
			\item Recommendations for selecting the most suitable method(s) based on the characteristics of the dataset and the requirements of the application domain.
		\end{itemize}
	\end{comment}
	
	The remainder of this paper is organized as follows. Section 2 provides an overview of related work on class imbalance handling methods. Section 3 describes the dataset used for experimentation and the evaluation metrics employed. Section 4 presents a description of the methodology used to perform analysis of the different class imbalance handling methods. Section 5 discusses the experimental results, comparing the performance of the methods. Finally, Section 6 concludes the paper with a summary of the findings, limitations of the study, and potential directions for future research.
	
	% Literature Review
	\section{Related Work}
	\label{sec:related_work}

	Johnson et al. \cite{johnson2019} provides a comprehensive survey of the existing studies that focus on class imbalance and deep learning, detailing their implementation, outcomes, strengths, weaknesses, and applicability to various areas such as data complexity, performance interpretation, big data application, and more. Johnson et al. classify the techniques for addressing class imbalance into three categories; data-level, algorithm-level, and hybrid.
	The paper concludes by highlighting the gaps in the existing research related to class imbalance in deep learning, with an aim to guide future investigations in this area. The research underscores the need for more studies, given the limited scope of current literature, especially in relation to big data applications.
	
	In this paper we will examine data level methods and will not examine neural network based classification methods. In the remainder of this section we will further contextualize the general strategies for addressing imbalance.
	
	\subsection{Undersampling}
	
	The earliest and most straightforward approaches to deal with class imbalance involve resampling the dataset. Undersampling techniques, such as random undersampling (RUS) and Tomek Links, reduce the instances of the majority class to balance the class distribution Liu et al. \cite{liu2009}. 

	Tomek Links, as proposed by Tomek in 1976 \cite{tomek1976}, offer an enhancement over the preceding condensed nearest neighbor method. This improvement focuses on retaining only those majority class members that are proximate to the boundary with another class, rather than preserving redundant members within a cluster. The method strategically eliminates points distanced from the class boundaries, as these are less likely to be misclassified given that their nearest neighbors belong predominantly to the majority class

	The one-sided sampling technique \cite{kubat1997} offers an innovative approach to reconstructing the majority class. Initially, it creates a subset that consists solely of data from the minority class. Subsequently, sets of majority class members are reintroduced. Specifically, for those majority members whose nearest neighbor would lead to their misclassification, proximate members from the original dataset are incorporated into the newly formed set. This strategy effectively lessens the density in areas distanced from the minority class, thereby enhancing class balance.

	The general risk associated with undersampling techniques is the potential for important information to be lost.

	\subsection{Oversampling}

	Rather than reducing the size of a majority class, oversampling techniques such as random oversampling (ROS) and Synthetic Minority Over-sampling Technique (SMOTE) increase the instances of the minority class. 

	SMOTE was developed by Chawla et al. \cite{chawla2002} and is a technique in which addition minority members are synthesized. The synthetic data is interpolated between two or more minority class points. This technique provides a way to avoid the creation of redundant data in a manner that is likely to be more meaningful than something like a simple jitter. 

	Borderline SMOTE \cite{han2005} uses the principle method behind standard SMOTE, but with selection weighted toward the minority members closest to the boundary regions between classes. SVMSMOTE \cite{nguyen2011} accomplishes a very similar process, but uses an algorithm very similar to how a soft margin is calculated for a support vector machine.

	The general risk associated with oversampling techniques is overfitting and that sampling variance may be exaggerated through over-representation of duplicated minority members.

	\subsection{Cost-Sensitive Learning}
	
	Cost-sensitive learning (CSL) is an algorithm based strategy that assigns higher misclassification costs to the minority class Elkan et al. \cite{elkan2001}. This method avoids the overfitting problem of oversampling and the information loss of undersampling. However, the performance of CSL is dependent on the cost matrix, which is often hard to determine.
	
	\subsection{Ensemble Methods} 
	
	Ensemble methods, like Boosting and Bagging, have been extensively used to address class imbalance. While bagging techniques are generally data-level, boosting techniques are exclusively hybrid.
	AdaBoost Freund et al. \cite{freund1997} is a popular boosting algorithm, while Balanced Random Forests Chen et al. \cite{chen2004} is a well-known bagging technique. Both methods construct multiple classifiers and combine them to make the final decision. These techniques have been proven effective, but they can be computationally expensive.
	
	\subsection{Learning from Imbalanced Data Streams} 
	
	Imbalanced data streams are a more challenging scenario where the data distribution may change over time. Wang et al. \cite{wang2018} proposed a method that dynamically adjusts the class distribution by a resampling strategy. Gao et al. \cite{gao2015} presented a method that incrementally learns from the data stream by adjusting the decision boundaries.
	
	\subsection{Deep Learning Approaches}
	Within the area of deep learning, several methods have been proposed to handle class imbalance. Cui et al. \cite{cui2019} introduced a loss function, called Class-Balanced Loss, based on effective number of samples. This technique achieves a balance between classes by adjusting the weights in the loss function. Similarly, Khan et al. \cite{khan2017} proposed Cost-Sensitive Deep Learning, which incorporates the cost matrix into the loss function of the deep learning model.	
	
	% Dataset
	\section{Dataset}
	\label{sec:dataset}
	
	\subsection{CDC Diabetes Dataset}
	\cite{cdc2022}
	Diabetes is a prevalent chronic disease in the U.S. that affects millions of individuals, resulting in a significant economic impact. Complications include heart disease, vision loss, limb amputation, and kidney disease. Despite the absence of a cure, weight loss, healthy eating, physical activity, and medical treatments can manage the disease. Early diagnosis and predictive models are crucial for effective treatment and lifestyle modifications.
	
	The CDC conducts an annual health-focused telephone survey known as the Behavioral Risk Factor Surveillance System (BRFSS). Initiated in 1984, the survey garners responses from over 400,000 Americans each year, gathering data on health-related risk behaviors, chronic health conditions, and utilization of preventive services. For the project at hand, we used a dataset derived from the 2015 version survey, available on Kaggle as a csv file. The raw dataset comprised responses from 441,455 individuals and included 21 features derived from 330 questions, which are a combination of direct questions posed to participants and variables calculated based on their responses.

	The derived dataset was cleaned and balanced leaving 70,692 samples with 17 features and one response variable. 
	
	
	% Methodology
	\section{Methodology}
	\label{sec:methodology}
	
	\subsection{Data Exploration}

	In the final pre-processing phase of the diabetes dataset, we undertook further exploration of the remaining features to refine our selection.
	
	We initially eliminated features that exhibited extremely low variance. In this context, features related to heavy alcohol consumption and stroke were dropped due to their negligible number of positive cases, especially in scenarios with high $\rho$.
	
	Subsequently, we identified features such as 'Sex', 'Smoker', 'Fruits', and 'Veggies' that demonstrated a very low correlation with the response variable. After their removal, we were left with a reduced feature set comprising six elements.
	
	In the final step, we scaled these features. All were subjected to min-max normalization. Additionally, to mitigate the impact of skewness in each feature, 'BMI' and 'Physical Health' were further subjected to quantile normalization.


\subsection{Experiments}


	To describe the relative imbalance within a dataset a common convention is to use $\rho$ which is calculated as

	\begin{equation}
		\rho = \frac{\max[n_c]}{\min[n_c]}, \qquad \forall\ c \in C,
	\end{equation}
	
	where $n_c$ is the number of members in class $c$ and $C$ is the set of all classes in the dataset.

\begin{figure}
	\centering
	\begin{tikzpicture}
		\node[inner sep=0pt] (image) at (0,0)
		{\includegraphics[width=0.5\textwidth]{4Lrff}};
		\node[red!50!black, font=\Huge\bfseries, rotate=45] at (image.center) {Temporary Example};
	\end{tikzpicture}
	\caption{TikZ diagram is under construction. 
		This will be a visual aid of the down sample process.}
	\label{fig:4lrff}
\end{figure}

	To assess the influence of varying techniques on distinct values of $\rho$, we artificially induced imbalance by randomly diminishing the count of the positive class in the dataset until achieving the desired ratio. This newly imbalanced dataset was then subjected to the several of the techniques under investigation. The efficacy of these techniques was gauged using a collection of classifiers limited to a concise set of hyperparameters.
	
	To minimize the potential impact of sample variance originating from the down-sampling process, we iterated this process multiple times and then averaged the results.


\begin{figure}
	\centering
	\begin{tikzpicture}[
		roundnode/.style={rectangle,rounded corners=0.25cm, draw=black!60, fill=blue!5, very thick, minimum size=7mm},
		squanode/.style={rectangle, draw=black!60, fill=red!5, very thick, minimum size=5mm},
		]
		%
		%Nodes
		\node[squanode] (Maintable)     {Original Dataset};
		\node[squanode] (Subset1) [below=of Maintable] {Down-sampled Dataset};
		\node[roundnode] (Subset2) [below=of Subset1] {Imbalance Technique};
		\node[squanode] (Subset3) [below=of Subset2] {Augmented Dataset};
		
		\node[red!50!black, font=\Huge\bfseries, rotate=45] at (image.center) {Draft};
	\end{tikzpicture}
	\caption{TikZ diagram of the data trajectory/ experimental pipeline.}
	\label{fig:4}
\end{figure}



\subsection{Conditions}









\begin{comment}


\begin{tikzpicture}[
	roundnode/.style={circle, draw=black!60, fill=black!5, very thick, minimum size=7mm},
	squanode/.style={rectangle, draw=black!60, fill=black!5, very thick, minimum size=5mm},
	]
	
	%Nodes
	\node[squanode] (Maintable)     {Original Dataset};
	\node[squanode] (Subset1) [below=of Maintable] {Subset 1};
	\node[squanode] (Subset2) [right=of Subset1] {Subset 2};
	\node[squanode] (Subset3) [right=of Subset2] {Subset 3};
	\node[roundnode] (Model1) [below=of Subset1] {Model 1};
	\node[roundnode] (Model2) [below=of Subset2] {Model 2};
	\node[roundnode] (Model3) [below=of Subset3] {Model 3};
	\node[squanode] (FinalModel) [below=of Model2] {Final Model};
	
	%Lines
	\draw[->] (Maintable.south) -- (Subset1.north);
	\draw[->] (Maintable.south) -- (Subset2.north);
	\draw[->] (Maintable.south) -- (Subset3.north);
	\draw[->] (Subset1.south) -- (Model1.north);
	\draw[->] (Subset2.south) -- (Model2.north);
	\draw[->] (Subset3.south) -- (Model3.north);
	\draw[->] (Model1.south) -- (FinalModel.north);
	\draw[->] (Model2.south) -- (FinalModel.north);
	\draw[->] (Model3.south) -- (FinalModel.north);
	
\end{tikzpicture}

\begin{tikzpicture}
\matrix (M) [matrix of nodes,
nodes={minimum height = 7mm, minimum width = 2cm, outer sep=0, anchor=center, draw},
column 1/.style={nodes={draw=none}, minimum width = 4cm},
row sep=1mm, column sep=-\pgflinewidth, nodes in empty cells,
e/.style={fill=yellow!10}
]
{
	Experiment 1 & |[e]| & & & & \\
	Experiment 2 & & |[e]| & & & \\
	Experiment 3 & & & |[e]| & & \\
	Experiment 4 & & & & |[e]| & \\
	Experiment 5 & & & & & |[e]| \\
};
\draw (M-1-2.north west) ++(0,2mm) coordinate (LT) edge[|<->|, >= latex] node[above]{Total number of datasets} (LT-|M-1-6.north east);
\end{tikzpicture}

\end{comment}


	% Results and Discussion
	\section{Results and Discussion}
	\label{sec:results_discussion}
	SVMSMOTE was not used on the basis of the size and shape of the dataset.
	
	Preliminary Results
	
	
	% Conclusion
	\section{Conclusion}
	\label{sec:conclusion}
	This section summarizes the key findings, contributions, and implications of the project. It restates the project objectives, highlights the accomplishments, and discusses the significance of the work. It may also suggest areas for further research or improvements.
	
	\subsection{Future Work}
	\begin{itemize}
		\item Additional Classes
		\item Additional techniques
	\end{itemize}
	
	
	% References
	\label{sec:references}
	\bibliographystyle{IEEEtran}
	\bibliography{Project.bib}
	
\end{document}
