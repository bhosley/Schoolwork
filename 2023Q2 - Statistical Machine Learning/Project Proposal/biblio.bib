 @article{Abanay_Masmoudi_El Ansari_2022, title={A calibration method of 2D LIDAR-Visual sensors embedded on an agricultural robot}, volume={249}, ISSN={0030-4026}, DOI={10.1016/j.ijleo.2021.168254}, abstractNote={In the robotics field, mobile platforms typically combine data from multiple sensors and have been extensively researched. Estimating the homogeneous transformation between sensors is a critical step in associating their measurements data. In this paper, we present a calibration method of 2D LIDAR-Visual sensors embedded on an agricultural robot intended for strawberry greenhouse applications. The method is based on plan-to-plan homography approach and employs a set of point correspondences. The correspondences points are the measured intersection points of the laser range finder plane with the edges of the calibration pattern. The extracted points are used to estimate the homography matrix. The RANSAC algorithm is used with more points obtained from pairs scans/images by sensors to generate a robust homography. The estimated homography matrix is identified as the rigid transformation between the 2D LIDAR and the monocular camera. The results of the proposed method show that we can achieve accurate extrinsic parameters, avoiding the need for intrinsic parameters in advance or complex calibration objects. The system is composed of the Hokuyo URG-04LX-UG01 sensor and a monocular camera with an infrared filter. All of the software modules were created using Robot Operating System (ROS) and Python.}, journal={Optik}, author={Abanay, Abdelkrim and Masmoudi, Lhoussaine and El Ansari, Mohamed}, year={2022}, month={Jan}, pages={168254}, language={en} }
 @inproceedings{Bandyopadhyay_van Aardt_Cawse-Nicholson_2013, title={Enhancing classification accuracy via registration of discrete return LiDAR and aerial imagery using the Levenberg-Marquardt nonlinear optimization method}, ISSN={2153-7003}, DOI={10.1109/IGARSS.2013.6723561}, abstractNote={Description and quantification of a landscape or scene can be achieved by assessing its spectral and structural properties. Fusion of spectral information from aerial imagery and 3-D structural information from LiDAR point clouds allows us to integrate these two complementary characteristics. However, in any fusion method, alignment of data sets is crucial. We registered aerial color (RGB) imagery with LiDAR data by computing a homography matrix(H), using the Levenberg-Marquardt nonlinear optimization method. The root mean square error (RMSE) of registration was less than 0.5 m. The overall classification accuracy of our fusion based object extraction algorithm was also increased from 85% to 90%, when applied to a pre and post registered data set, respectively. In this paper, two different regions were selected to demonstrate the registration method and improved classification results.}, booktitle={2013 IEEE International Geoscience and Remote Sensing Symposium - IGARSS}, author={Bandyopadhyay, Madhurima and van Aardt, Jan A.N. and Cawse-Nicholson, Kerry}, year={2013}, month={Jul}, pages={3411–3414} }
 @article{Bybee_Budge_2019, title={Method for 3-D Scene Reconstruction Using Fused LiDAR and Imagery From a Texel Camera}, volume={57}, ISSN={1558-0644}, DOI={10.1109/TGRS.2019.2923551}, abstractNote={Reconstructing a 3-D scene from aerial sensor data creating a textured digital surface model (TDSM), consisting of a LiDAR point cloud and an overlaid image, is valuable in many applications including agriculture, military, surveying, and natural disaster response. When collecting LiDAR from an aircraft, the navigation system accuracy must exceed the LiDAR accuracy to properly reference returns in 3-D space. Precision navigation systems can be expensive and often require fullscale aircraft to house such systems. Synchronizing the LiDAR sensor and a camera, using a texel camera calibration, provides additional information that reduces the need for precision navigation equipment. This paper describes a bundle adjustment technique for aerial texel images that allows for relatively lowaccuracy navigation systems to be used with low-cost LiDAR and camera data to form higher fidelity terrain models. The bundle adjustment objective function utilizes matching image points, measured LiDAR distances, and the texel camera calibration and does not require overlapping LiDAR scans or ground control points. The utility of this method is proven using a simulated texel camera and unmanned aerial system (UAS) flight data created from aerial photographs and elevation data. A small UAS is chosen as the target vehicle due to its relatively inexpensive hardware and operating costs, illustrating the power of this method in accurately referencing the LiDAR and camera data. In the 3-D reconstruction, the 1-σ accuracy between LiDAR measurements across the scene is on the order of the digital camera pixel size.}, number={11}, journal={IEEE Transactions on Geoscience and Remote Sensing}, author={Bybee, Taylor C. and Budge, Scott E.}, year={2019}, month={Nov}, pages={8879-8889} }
 @inproceedings{Fouhey_Gupta_Hebert_2013, address={Sydney, Australia}, title={Data-Driven 3D Primitives for Single Image Understanding}, ISBN={978-1-4799-2840-8}, url={http://ieeexplore.ieee.org/document/6751533/}, DOI={10.1109/ICCV.2013.421}, booktitle={2013 IEEE International Conference on Computer Vision}, publisher={IEEE}, author={Fouhey, David F. and Gupta, Abhinav and Hebert, Martial}, year={2013}, month={Dec}, pages={3392–3399}, language={en} }
 @article{Han_Zhang_Hu_Guo_Ren_Wu_2015, title={Background Prior-Based Salient Object Detection via Deep Reconstruction Residual}, volume={25}, ISSN={1558-2205}, DOI={10.1109/TCSVT.2014.2381471}, abstractNote={Detection of salient objects from images is gaining increasing research interest in recent years as it can substantially facilitate a wide range of content-based multimedia applications. Based on the assumption that foreground salient regions are distinctive within a certain context, most conventional approaches rely on a number of hand-designed features and their distinctiveness is measured using local or global contrast. Although these approaches have been shown to be effective in dealing with simple images, their limited capability may cause difficulties when dealing with more complicated images. This paper proposes a novel framework for saliency detection by first modeling the background and then separating salient objects from the background. We develop stacked denoising autoencoders with deep learning architectures to model the background where latent patterns are explored and more powerful representations of data are learned in an unsupervised and bottom-up manner. Afterward, we formulate the separation of salient objects from the background as a problem of measuring reconstruction residuals of deep autoencoders. Comprehensive evaluations of three benchmark datasets and comparisons with nine state-of-the-art algorithms demonstrate the superiority of this paper.}, number={8}, journal={IEEE Transactions on Circuits and Systems for Video Technology}, author={Han, Junwei and Zhang, Dingwen and Hu, Xintao and Guo, Lei and Ren, Jinchang and Wu, Feng}, year={2015}, month={Aug}, pages={1309–1321} }
 @inproceedings{Han_Li_Huang_Kalogerakis_Yu_2017, address={Venice}, title={High-Resolution Shape Completion Using Deep Neural Networks for Global Structure and Local Geometry Inference}, ISBN={978-1-5386-1032-9}, url={http://ieeexplore.ieee.org/document/8237281/}, DOI={10.1109/ICCV.2017.19}, abstractNote={We propose a data-driven method for recovering missing parts of 3D shapes. Our method is based on a new deep learning architecture consisting of two sub-networks: a global structure inference network and a local geometry reﬁnement network. The global structure inference network incorporates a long short-term memorized context fusion module (LSTM-CF) that infers the global structure of the shape based on multi-view depth information provided as part of the input. It also includes a 3D fully convolutional (3DFCN) module that further enriches the global structure representation according to volumetric information in the input. Under the guidance of the global structure network, the local geometry reﬁnement network takes as input local 3D patches around missing regions, and progressively produces a high-resolution, complete surface through a volumetric encoder-decoder architecture. Our method jointly trains the global structure inference and local geometry reﬁnement networks in an end-to-end manner. We perform qualitative and quantitative evaluations on six object categories, demonstrating that our method outperforms existing state-of-the-art work on shape completion.}, booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, publisher={IEEE}, author={Han, Xiaoguang and Li, Zhen and Huang, Haibin and Kalogerakis, Evangelos and Yu, Yizhou}, year={2017}, month={Oct}, pages={85–93}, language={en} }
 @article{Hu_Han_Zwicker_2020, title={3D Shape Completion with Multi-View Consistent Inference}, volume={34}, rights={Copyright (c) 2020 Association for the Advancement of Artificial Intelligence}, ISSN={2374-3468}, DOI={10.1609/aaai.v34i07.6734}, abstractNote={3D shape completion is important to enable machines to perceive the complete geometry of objects from partial observations. To address this problem, view-based methods have been presented. These methods represent shapes as multiple depth images, which can be back-projected to yield corresponding 3D point clouds, and they perform shape completion by learning to complete each depth image using neural networks. While view-based methods lead to state-of-the-art results, they currently do not enforce geometric consistency among the completed views during the inference stage. To resolve this issue, we propose a multi-view consistent inference technique for 3D shape completion, which we express as an energy minimization problem including a data term and a regularization term. We formulate the regularization term as a consistency loss that encourages geometric consistency among multiple views, while the data term guarantees that the optimized views do not drift away too much from a learned shape descriptor. Experimental results demonstrate that our method completes shapes more accurately than previous techniques.}, number={0707}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Hu, Tao and Han, Zhizhong and Zwicker, Matthias}, year={2020}, month={Apr}, pages={10997–11004}, language={en} }
 @article{Isola_Zhu_Zhou_Efros_2018, title={Image-to-Image Translation with Conditional Adversarial Networks}, url={http://arxiv.org/abs/1611.07004}, DOI={10.48550/arXiv.1611.07004}, abstractNote={We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.}, note={arXiv:1611.07004 [cs]}, number={arXiv:1611.07004}, publisher={arXiv}, author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.}, year={2018}, month={Nov} }
 @inproceedings{Jenkins_Armstrong_Nelson_Gotad_Jenkins_Wilkey_Watts_2023, address={Waikoloa, HI, USA}, title={CountNet3D: A 3D Computer Vision Approach to Infer Counts of Occluded Objects}, ISBN={978-1-66549-346-8}, url={https://ieeexplore.ieee.org/document/10030990/}, DOI={10.1109/WACV56688.2023.00302}, booktitle={2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, publisher={IEEE}, author={Jenkins, Porter and Armstrong, Kyle and Nelson, Stephen and Gotad, Siddhesh and Jenkins, J. Stockton and Wilkey, Wade and Watts, Tanner}, year={2023}, month={Jan}, pages={3007–3016}, language={en} }
 @article{Kim_Chen_Alisafaee_2022, title={Imaging lidar prototype with homography and deep learning ranging methods}, volume={24}, ISSN={2040-8986}, DOI={10.1088/2040-8986/ac4870}, abstractNote={We report on developing a non-scanning laser-based imaging lidar system based on a diffractive optical element with potential applications in advanced driver assistance systems, autonomous vehicles, drone navigation, and mobile devices. Our proposed lidar utilizes image processing, homography, and deep learning. Our emphasis in the design approach is on the compactness and cost of the final system for it to be deployable both as standalone and complementary to existing lidar sensors, enabling fusion sensing in the applications. This work describes the basic elements of the proposed lidar system and presents two potential ranging mechanisms, along with their experimental results demonstrating the real-time performance of our first prototype.}, 
    number={3}, 
    journal={Journal of Optics}, 
    publisher={IOP Publishing}, 
    author={Kim, Sehyeon and Chen, Zhaowei and Alisafaee, Hossein}, 
    year={2022}, 
    month={Jan}, 
    pages={035701}, 
    language={en} }
 @inbook{Tylecek_Sara_2013, address={Berlin, Heidelberg}, series={Lecture Notes in Computer Science}, title={Spatial Pattern Templates for Recognition of Objects with Regular Structure}, volume={8142}, ISBN={978-3-642-40601-0}, url={http://link.springer.com/10.1007/978-3-642-40602-7_39}, DOI={10.1007/978-3-642-40602-7_39}, abstractNote={We propose a method for semantic parsing of images with regular structure. The structured objects are modeled in a densely connected CRF. The paper describes how to embody speciﬁc spatial relations in a representation called Spatial Pattern Templates (SPT), which allows us to capture regularity constraints of alignment and equal spacing in pairwise and ternary potentials.}, booktitle={Pattern Recognition}, publisher={Springer Berlin Heidelberg}, author={Tyleček, Radim and Šára, Radim}, editor={Weickert, Joachim and Hein, Matthias and Schiele, Bernt}, year={2013}, 
    pages={364–374}, 
    collection={Lecture Notes in Computer Science}, 
    language={en} }
 @inproceedings{Yang_Luo_Loy_Tang_2015, address={Boston, MA, USA}, title={A large-scale car dataset for fine-grained categorization and verification}, ISBN={978-1-4673-6964-0}, url={http://ieeexplore.ieee.org/document/7299023/}, DOI={10.1109/CVPR.2015.7299023}, booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Yang, Linjie and Luo, Ping and Loy, Chen Change and Tang, Xiaoou}, year={2015}, month={Jun}, pages={3973–3981}, language={en} }
 @article{Yohannes_Lin_Shih_Thaipisutikul_Enkhbat_Utaminingrum_2023, title={An Improved Speed Estimation Using Deep Homography Transformation Regression Network on Monocular Videos}, volume={11}, ISSN={2169-3536}, DOI={10.1109/ACCESS.2023.3236512}, abstractNote={Vehicle speed estimation is one of the most critical issues in intelligent transportation system (ITS) research, while defining distance and identifying direction have become an inseparable part of vehicle speed estimation. Despite the success of traditional and deep learning approaches in estimating vehicle speed, the high cost of deploying hardware devices to get all related sensor data, such as infrared/ultrasonic devices, Global Positioning Systems (GPS), Light Detection and Ranging (LiDAR systems), and magnetic devices, has become the key barrier to improvement in previous studies. In this paper, our proposed model consists of two main components: 1) a vehicle detection and tracking component – this module is designed for creating reliable detection and tracking every specific object without doing calibration; 2) homography transformation regression network – this module has a function to solve occlusion issues and estimate vehicle speed accurately and efficiently. Experimental results on two datasets show that the proposed method outperforms the state-of-the-art methods by reducing the mean square error (MSE) metric from 14.02 to 6.56 based on deep learning approaches. We have announced our test code and model on GitHub with https://github.com/ervinyo/Speed-Estimation-Using-Homography-Transformation-and-Regression-Network.}, journal={IEEE Access}, author={Yohannes, Ervin and Lin, Chih-Yang and Shih, Timothy K. and Thaipisutikul, Tipajin and Enkhbat, Avirmed and Utaminingrum, Fitri}, year={2023}, pages={5955–5965} }
 @inproceedings{Zhou_Deng_2014, title={Perspective distortion rectification for planar object based on LIDAR and camera data fusion}, ISSN={2153-0017}, DOI={10.1109/ITSC.2014.6957703}, abstractNote={Planar objects like road surface, traffic sign, and street sign, are ubiquitous in traffic scenes. Such objects provide important information for driving safety. But it is challenging to reliably detect and recognize them by using camera in complex environments. One of the main challenges comes from the perspective distortion. Substantial algorithms have been proposed to address this problem for specific planar objects by taking advantage of the characteristics of the problems at hand. In contrast to those target-specific algorithms, we propose a generic algorithm to rectify the perspective distortion of planar objects based on the data fusion of LIDAR and camera. The main idea of our algorithm is to use LIDAR data to recover the 3D geometric model of a planar object, and then pose a virtual camera to look orthogonally toward it. This virtual camera is capable of generating the desired fronto-parallel view of the planar target. Meanwhile, the rectifying homography between the virtual and real cameras only relies on the plane parameters and the relative pose between both cameras, irrespective of the category of the planar object. This enables our algorithm can be applied to various planar objects with arbitrary shapes. Our experimental results show that the proposed algorithm is effective and efficient to correct the perspective distortion of common planar objects in traffic scenarios.}, booktitle={17th International IEEE Conference on Intelligent Transportation Systems (ITSC)}, author={Zhou, Lipu and Deng, Zhidong}, year={2014}, month={Oct}, pages={270–275} }
 @misc{CompCars Dataset, url={http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/index.html} }
 @misc{SAMNet: Stereoscopically Attentive Multi-Scale Network for Lightweight Salient Object Detection | IEEE Journals & Magazine | IEEE Xplore, url={https://ieeexplore.ieee.org/abstract/document/9381668} }
