\documentclass[journal]{IEEEtran}
%\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Packages
\usepackage{amsmath}
\usepackage{graphicx}
\graphicspath{./Images/}
\usepackage[table,dvipsnames]{xcolor}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{cite}
\usepackage{url}
\usepackage{tikz}
\usetikzlibrary{matrix,shapes,positioning,arrows,backgrounds}

% Drafting Utility Package
\usepackage{comment}
\usepackage{blindtext}
\usepackage{hyperref}
%\usepackage[toc,page]{appendix}

% Title
\title{EENG 645 Final Project: \\Working Title for HMARL League Play}

\author{Brandon Hosley, Capt, \textit{AFIT}%
	\thanks{Manuscript received \today%
		%		; revised Month DD, YYYY.
}}

%\keywords{class imbalance, data-level methods, algorithm-level methods, hybrid methods, machine learning, performance evaluation}

% Document
\begin{document}
	
	\maketitle
	
	
	% Abstract
	\begin{abstract}
		
		In the realm of artificial intelligence, the domain of board games, particularly chess, 
		has long served as a benchmark for evaluating the capabilities of computational models.
		Recent advancements in reinforcement learning (RL) have enabled the development of 
		agents capable of achieving superhuman performance. 
		However, the majority of these models rely on a self-play mechanism 
		that may limit their strategic diversity and adaptability. 
		This study introduces a novel approach by training multiple agents with distinct 
		play styles to compete in a league format, akin to diverse ecosystems in natural competition. 
		Through the implementation of reinforcement learning algorithms, 
		we aim to investigate whether a heterogeneous pool of agents can enhance strategic depth 
		and performance compared to traditional self-play paradigms and if it simulates the benefits of 
		training against specifically tailored adversarial agents. 
		\begin{comment}
		We developed several agents, each with a unique play style—aggressive, defensive, positional, and tactical—trained within a multi-agent reinforcement learning framework. 
		The performance of these agents was evaluated based on their win rates, Elo ratings, Glicko ratings, and their ability to adapt and counter a variety of opponent strategies. 
		Our results indicate that the inclusion of diverse play styles not only elevates the overall performance of individual agents in league play but also encourages the emergence of innovative strategies and adaptability.
		\end{comment}
		This approach mirrors the complexity of human competition more closely than the conventional self-play model and suggests that diversity in training partners within reinforcement learning environments can significantly impact the development of artificial intelligence in competitive domains.
		
	\end{abstract}
	
	% Introduction
	\section{Introduction}
	\label{sec:introduction}
	Chess is a well established bench-marking challenge for artificial intelligence (AI),
	with its non-trivial strategy and vast number of state spaces providing a rich domain for exploring 
	computational intelligence, decision-making, and learning algorithms 
	\cite{silver2017, silver2017a, schrittwieser2020, 
	% Uncited elsewhere
	campbell2002, lai2015, hammersborg2023,bertram2022}.
	% Bertram and Hammersbourg is recon blind chess
	% Want to add more chess based agents,
	The success of AI in chess, culminating in programs capable of defeating human world champions
	\cite{campbell2002}, 
	is largely attributed to advances in machine learning, particularly reinforcement learning (RL), 
	where agents learn optimal behaviors through interactions with their environment. 
	Traditionally, the pinnacle of AI chess performance has been achieved through the self-play methodology
	\cite{silver2017, silver2017a}, 
	where a single agent improves by playing against versions of itself, gradually refining its strategy based on the outcomes of these games.
	
	While self-play has led to significant advancements, it inherently limits an agent's exposure to the diverse range of strategies 
	encountered in real-world play, potentially stunting the agent's ability to adapt to and counter novel strategies. 
	Recent research in reinforcement learning has begun exploring multi-agent systems and the benefits of diversity in training environments, 
	suggesting that exposure to a variety of opponents can lead to more robust and adaptable agents.
	In particular, Vinyals et. al.  had notable success in \cite{vinyals2019} (discussed further in section \ref{sec:related_work})
	utilizing a league-play approach that employed exploiters, specialists to assist primary agent training. 
	However, the potential of applying these insights to the domain of chess, 
	with deliberately diversified play styles during the training phase, remains underexplored.
	\begin{comment}
		I don't love this sentence, I think it implies too much that this is about chess in particular
	\end{comment}
	
	%
	%	Problem Statement is here
	%
	This study seeks to fill this gap by investigating the effects of training multiple chess agents, each with a distinct play style, 
	through reinforcement learning, and evaluating their performance in a league-based self-play architecture. 
	We hypothesize that a diversified training environment will foster the development of agents that are not only 
	proficient in their unique styles but are also more adaptable and capable of handling a wider array of opponent strategies. 
	
	It is important to acknowledge that we do not anticipate the rate or level of improvement 
	in our diverse agent pool to precisely match those achieved through training against dedicated adversarial agents, 
	which are specifically designed to challenge and refine the primary agent's strategies. 
	%
	Dedicated adversarial agents, by their very nature, are tailored to exploit weaknesses and force rapid adaptation, 
	potentially leading to steeper learning curves. However, our research aims to demonstrate that by incorporating a variety 
	of play styles within the same league, we can achieve a more nuanced and comprehensive training environment. 
	This approach seeks to balance the diversity of challenges faced by each agent, fostering a broader strategic competence 
	without the need for constructing and maintaining a suite of highly specialized adversarial agents. 
	We hypothesize that the efficiency of training can be significantly enhanced through this method,
	as it reduces the overhead associated with the development of 
	dedicated adversarial models while still promoting a competitive, diverse, and rich learning ecosystem. 
	In essence, our goal is to prove that a league of diverse agents can serve as a practical and effective alternative, 
	offering a pathway to robust and adaptable reinforcement learning pipeline through a more streamlined and accessible training process.
		
	To explore this hypothesis, we employ a range of reinforcement learning algorithms to train a cohort of chess agents, each encouraged to specialize in different strategic approaches to the game. 
	Our methodology involves designing tailored reward functions and training regimes to promote the development of unique play styles, 
	from aggressive to defensive and positional to tactical. 
	We assess the effectiveness of this approach through various metrics, 
	including win rates, ELO ratings,
	% Efficiency of training and execution
	and GLICKO ratings.
	% ,GLiCKO ratings, and a novel measure of strategic diversity.
		
	The contributions of this research are twofold. 
	Firstly, it advances our understanding of how diversity in training environments affects 
	the development and performance of AI agents in strategic games. 
	Secondly, it offers practical insights into the design of reinforcement learning systems, 
	providing a blueprint for creating more adaptable and strategically diverse agents. 
	By exploring the intersection of reinforcement learning, multi-agent systems, and game theory, 
	this study paves the way for new avenues in AI research, with implications extending well beyond the realm of chess.

	
	% Literature Review
	\section{Related Work}
	\label{sec:related_work}
	
	\subsection{RL Playing Games}
	
	A particularly noteworthy strand of research in the realm of reinforcement learning and game-playing AI has been led by DeepMind, culminating in their AlphaStar project. 
	DeepMind, a pioneer in the field since its acquisition by Alphabet Inc., has made remarkable strides in developing AI systems capable of mastering abstract strategy games. 
	The series of breakthroughs began with AlphaGo \cite{silver2016}, 
	which marked a significant milestone in AI by defeating several world champions in the game of Go.
	This was succeeded by AlphaGo Zero \cite{silver2017}, an even more powerful version that surpassed human-level performance in Go without relying on human game data, solely through the mechanism of self-play.
		
	The principles underlying AlphaGo Zero were further extended in the development of AlphaZero \cite{silver2017a}, 
	a generalized algorithm capable of achieving top-tier performance not only in Go but also in Chess and Shogi, 
	demonstrating the versatility and potential of self-play as a training method across various abstract strategy games. 
	DeepMind's trajectory of research then advanced towards more complex and dynamic games, 
	including the training of AlphaStar \cite{vinyals2019} to play Blizzard Entertainment's StarCraft II, 
	pushing the boundaries of AI's capabilities in games of increased complexity and strategic depth. 
	
	Building upon their established success, DeepMind has consistently progressed, applying their sophisticated techniques to equip agents for navigating the complexities of increasingly intricate games \cite{schrittwieser2020, perolat2022, fawzi2022}. 
	However, their pioneering efforts are met with formidable rivalry. 
	A notable contender, OpenAI, has made a significant foray into the esports domain with its OpenAI Five \cite{openai2019}, a system that has attained world-class proficiency in Dota 2, marking another milestone in the competitive landscape of AI-driven gaming performance.
	
	\subsection{League Play}	
	One of the critical challenges in training agents through self-play is the phenomenon known as 'Strategic Collapse' \cite{balduzzi2019},
	wherein the agent becomes overly specialized in strategies that are effective against itself but vulnerable to a broader spectrum of opponent tactics. 
	To mitigate this issue, several approaches have been devised. 
	Among these, the strategy employed by AlphaStar and its derivative algorithms\cite{wang2021, mathieu2023}, known as 'league play',
	stands out for its innovative expansion of conventional self-play methodologies. 
	
	Instead of limiting interactions to games against current or historical versions of itself, 
	the league play approach integrates a diverse array of agents into the training process. 
	This includes the incorporation of 'primary exploiters,' agents designed specifically to challenge the main agent by exploiting its weaknesses, 
	and 'league exploiters,' which are engineered to counter the strategies prevalent among the wider group of agents in the league. 
	This structure fosters a more dynamic and varied training environment, 
	encouraging the development of agents that are not only adept at navigating a wide range of strategic scenarios but also resilient against exploitative tactics.
	
	In considering the implementation of a league environment, the prospect of engaging with agents trained for complex games like Starcraft II, 
	akin to the AlphaStar project, would be more desirable. 
	However, the resources at our disposal for this research project are considerably more constrained compared to the 
	extensive capabilities utilized by DeepMind and its parent organization. 
	Consequently, we have chosen to apply our experimental training methodology within the context of a simpler game. 
	This strategic decision allows us to more comprehensively investigate the implications of our proposed adjustments, 
	ensuring a focused exploration of the effects without the limitations imposed by resource-intensive setups.
	
	\begin{figure}
		\centering
		\includegraphics[width=0.7\linewidth]{"Images/AlphaStar League"}
		\caption{\textcolor{blue}{(This was included in the class project, but I don't think that it is necessarily needed moving forward)} League play as implemented in AlphaStar.}
		\label{fig:alphastar-league}
	\end{figure}
	
	\begin{comment}		
	\end{comment}

	% Dataset
	\begin{comment}
	\section{Datasets}
	\label{sec:dataset}
	In the initial phase of our study, the agents underwent supervised learning pre-training to establish a foundational understanding of strategic play in chess. 
	To facilitate this, we utilized a comprehensive dataset of previously played games sourced from the Lichess Database \cite{zotero-2247}.
	Lichess is renowned for its extensive collection of chess games, played by users of varying skill levels from around the world, 
	and made freely available under an open database license. 
	The dataset includes millions of games, complete with player moves, timestamps, outcomes, and often, player ratings. 
	This diversity and volume of data provided a rich training ground for our agents, enabling them to learn from a wide array of game strategies, outcomes, and player behaviors.
	
	The Lichess game archive is structured to facilitate easy access and integration into machine learning pipelines, 
	offering monthly snapshots in standard chess notation formats. 
	For our purposes, this dataset was instrumental in the pre-training stage, 
	allowing our agents to mimic and understand a broad spectrum of human chess strategies before transitioning to the reinforcement learning phase. 
	The initial exposure to such a varied collection of games helped in seeding our agents with a balanced understanding of both conventional and unconventional plays, 
	setting a solid foundation for further specialization and strategic development within our diversified training environment.
	
	Additionally, we discovered that the Ficsgames archive\cite{zotero-2272} was invaluable for securing a collection of replays characterized by more distinct attributes. 
	While Lichess offered an extensive database of games for general analysis, 
	ficsgames provided the capability to acquire datasets tailored with specific criteria in mind, 
	such as particular openings, game speed, player ratings, and encounters involving titled players. 
	This differentiation allowed us to refine our analysis and training data, 
	enabling a targeted exploration of nuanced strategic elements and player behaviors within our research framework.
	
	The Stockfish Chess Engine \cite{romstad} represents a pinnacle of open-source achievement within the realm of computer chess, 
	offering an unparalleled blend of depth, accuracy, and computational efficiency. 
	Developed by a robust community of chess enthusiasts and programmers, Stockfish stands as a testament to collaborative innovation, 
	continuously evolving through contributions from around the globe. 
	It operates on advanced algorithms and evaluation techniques that simulate an expansive 
	understanding of chess strategies, tactics, and positions. 
	Notably, Stockfish employs a state-of-the-art search algorithm, optimized evaluation functions, 
	and machine learning techniques to adapt and refine its playing style, making it one of the strongest chess engines in the world.
		
	Stockfish's significance in the field of artificial intelligence and computer chess is multi-faceted. 
	Firstly, it serves as a benchmark for evaluating the performance of emerging chess engines and AI systems, 
	providing a consistent and challenging standard against which the capabilities of new algorithms are measured. 
	Secondly, it functions as a vital research tool, aiding in the development of 
	reinforcement learning agents by offering a rich source of high-level game play and strategic diversity. 
	This engine, with its open-source nature, not only democratizes access to high-quality chess analysis but also fosters an 
	environment of shared learning and development, encouraging innovation in AI research and the broader field of game theory. 
	The use of Stockfish in training and evaluating chess agents underlines the importance of leveraging 
	established knowledge bases and computational tools to advance the state of AI in strategic game playing.
	\end{comment}
	
	
	% Methodology
	\section{Methodology}
	\label{sec:methodology}
	
	First, each agent undergoes a preliminary phase of supervised pre-training, 
	during which they are exposed to a curated dataset of chess games. 
	This phase is distinct for each agent, with tailored constraints designed to encourage the development of their intended unique play styles. 
	This approach ensures that agents begin with a foundational understanding of chess that reflects their designated strategic focus.	
	Then, copies of these pre-trained agents will make up the cohort for each of the following self-play training regimes.
	
	\subsection{Experiments}
	
	\subsubsection{Self-Play Training in a Conventional League Format}
	Following the pre-training phase, agents engage in self-play within a traditional league format. 
	This stage allows agents to refine their strategies through iterative gameplay against a variety of past and present versions of themselves,
	facilitating a deepening of their tactical competencies within their specialized areas.
	
	\begin{figure}
		\centering
		\includegraphics[width=0.5\linewidth]{Images/Traditional}
		\caption{\textcolor{red}{(Temp image, to be replaced with a Tikz version)} Self-play in the traditional mode with arrows representing each agent's training priority.}
		\label{fig:traditional}
	\end{figure}

	
	\subsubsection{Integration into a Comprehensive League Environment} 
	Subsequently, agents participate in self-play within an enriched league environment that 
	integrates both current iterations and selectively chosen past versions of each agent.
	With all of the agents within a single pool.
	This environment is designed to mimic the dynamism and strategic diversity found in competitive chess, 
	promoting the evolution of more adaptable and robust playing styles.

\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{Images/New}
	\caption{\textcolor{red}{(Temp image, to be replaced with a Tikz version)} Integrated league with arrows representing each agent's training priority.}
	\label{fig:new}
\end{figure}

	\subsubsection{Inclusion of Unconstrained Agents Mimicking League Exploiters} 
	To further enhance the training environment, we introduce a set of unconstrained agents. 
	These agents are designed to emulate the role of league exploiters from traditional league models, 
	tasked with identifying and exploiting weaknesses not only in individual agents but across the league as a whole. 
	This addition aims to simulate the pressures of real-world competition, 
	pushing all agents to continuously adapt and refine their strategies in response to evolving challenges.
	
\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{"Images/New plus"}
	\caption{\textcolor{red}{(Temp image, to be replaced with a Tikz version)} Integrated league with supplemental exploiter and arrows representing each agent's training priority.}
	\label{fig:new-plus}
\end{figure}
	
	\subsection{Evaluation}
	
	Two key metrics stand at the forefront of our evaluation criteria. The first metric involves assessing the performance of each agent following a predetermined period of training, quantified in terms of both time and computational resources. The second metric examines the additional time and computational effort required to attain comparable performance levels under less efficient training frameworks relative to the most effective training regime.
	
	To measure performance, we will employ the win rate metric, calculated based on the outcomes of a series of games played against a computer opponent of fixed skill level. This approach allows us to quantitatively evaluate the effectiveness of the training received by each agent, providing a clear benchmark for comparison across different training strategies.
	
	\subsection{Agent Architecture}
	
	\textcolor{blue}{Should this section be included? I think that it may be useful as an appendix, but for anyone interested the code itself is published to Github publicly.}
	
	\begin{comment}		
		The board state is held between turns using the python-chess api.
		The state is passed to the agent as a pair of \(8\times8\times7\) tensors representing a copy of the board for each of the \(7\) types of pieces. 
		
	\end{comment}
	
	% Results and Discussion
	\section{Results and Discussion}
	\label{sec:results_discussion}
	
	\textcolor{lightgray}{\blindtext}
	\begin{comment}
		Significant performance in predicting the moves that the stockfish engine will make next.
	\end{comment}
	
	
	% Conclusion
	\section{Conclusion}
	\label{sec:conclusion}
	\textcolor{lightgray}{\blindtext}

	\subsection{Future Work}
		Looking ahead, our research will delve further into the training of multiple heterogeneous agents, 
		each tailored to operate within specific environments or under distinct constraints. 
		A particular focus will be on exploring the relationships between such specialized agents and the 
		possibility for composing and decomposing both reward functions and agent policies. 
		This exploration aims to enhance our understanding of how these mechanisms can be leveraged to 
		fine-tune agent behavior and learning processes.
	
		Moreover, we plan to extend our investigation to the application of our current methodologies to cooperative settings. 
		This expansion seeks to uncover how strategies developed for competitive play can inform and 
		improve the coordination and mutual support among agents working towards a common goal.
	
		Central to our future endeavors is the overarching goal of advancing reinforcement learning efficiency 
		and enhancing the explainability of agent behavior. 
		By exploring the composition and decomposition of agent functions, 
		we aim to shed light on the underlying mechanics of decision-making and learning in complex environments, 
		thereby contributing to the broader field of artificial intelligence with insights into more flexible and interpretable agent behaviors.
		
		The source code for this project can be found at  %\href{https://github.com/bhosley/MARL-chess-baseline}{https://github.com/bhosley/MARL-chess-baseline}.

	\begin{comment}
		
	\end{comment}

	Test Live Update?
	
		
	% References
	\label{sec:references}
	\bibliographystyle{IEEEtran}
	\bibliography{Project.bib}
	
	%\clearpage
	%\begin{appendices}
	%	
	%\section{Model Architecture}
	%\label{appendix:model}
	%\begin{center}
	%	\includegraphics[width=0.7\linewidth]{Images/Model}
	%\end{center}
	%
	%\end{appendices}
	
\end{document}