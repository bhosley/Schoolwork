\documentclass{article}
\usepackage[english]{babel}
\usepackage{csquotes}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{hyperref}

\usepackage{graphicx}
\graphicspath{{../images/}}

\usepackage{tikz}
\usetikzlibrary{positioning}

% \usepackage{cleveref}
\usepackage[backend=biber, style=ieee]{biblatex}
\usepackage[T1]{fontenc}
\usepackage[final]{microtype}
\addbibresource{../../2025Bibs/Prospectus.bib}

\title{Dimension Reduction of Shared Observations in Multi-Agent Reinforcement Learning}
\author{Brandon Hosley}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Heterogeneous multi-agent reinforcement learning (HARL) systems 
often rely on architectures that assume a fixed number of agents, 
embedding this assumption directly into each agent's observation space. 
While such encodings simplify coordination and benchmarking, 
they introduce rigidity that limits generalization to variable team 
sizes or dynamic environments. 
In this work, we focus on the Level-Based Foraging (LBF) environment, 
a common benchmark for cooperative HARL, 
and identify structural constraints in its default observation encoding. 
We propose two observation reduction strategies that mitigate the dependence 
on fixed agent populations while preserving access to task-relevant information. 
Our experimental results demonstrate that these reductions enable more 
flexible and efficient training while maintaining competitive 
performance across varying team sizes.
\end{abstract}

\section{Introduction}

Multi-agent reinforcement learning (MARL) presents a compelling approach for training agents to 
interact and cooperate in complex, dynamic environments. It has been applied to a wide range of 
domains, including autonomous driving \cite{yang2021a}, swarm robotics \cite{brambilla2013}, 
decentralized logistics, and multi-agent games \cite{vinyals2019,berner2019}.
 
In MARL, agents must learn to optimize both individual and collective behavior in the presence 
of other learning agents. A particularly challenging and increasingly relevant subfield is 
heterogeneous MARL (HARL), in which agents differ in roles, capabilities, or observations. Such 
heterogeneity arises in systems where specialized functions enable tailored coordination 
strategies. For instance, drone swarms performing complex tasks have demonstrated improved 
performance through specialized role allocation \cite{huttenrauch2019}, while multi-robot 
systems in warehouse operations showcase the benefits of leveraging diverse capabilities 
\cite{rizk2019}. Similarly, collaborative search-and-rescue operations, which require dynamic 
role assignment and real-time coordination, highlight the necessity for flexible learning 
strategies \cite{hoang2023}, with further challenges in heterogeneous settings detailed in 
\cite{kapetanakis2005,hernandez-leal2019}.

While many benchmark environments support large-scale homogeneous agent populations with
shared policies (e.g., MAgent2 \cite{zheng2017}), heterogeneous MARL (HARL)
settings often encode agent identity and role explicitly, leading to observation structures
that assume a fixed number of uniquely-indexed agents. This approach restricts the observation
space to a specific configuration, making it difficult to scale or generalize across variable
team sizes. This limitation is evident in environments such as Level-Based Foraging (LBF)
\cite{papoudakis2021}, Multiwalker and Waterworld in the PettingZoo SISL suite
\cite{terry2021}, the StarCraft Multi-Agent Challenge (SMAC) \cite{samvelyan2019}, 
and Google Research Football \cite{kurach2020}, 
all of which embed the agent count into fixed-size observation spaces.

A common consequence of this design choice is that each agent receives an observation vector
whose size and semantics are tied to the total number of agents in the environment. 
For example, in the Level-Based Foraging (LBF) environment \cite{papoudakis2021}, 
agents observe the state of all teammates and all food items in a flat, concatenated format. 
This encoding implicitly fixes the number of agents, as each additional teammate introduces 
a new segment in the observation vector. As a result, policies trained on one team size cannot 
be directly applied to another without architectural modifications or retraining.

This constraint poses a significant barrier to the development of flexible and scalable HARL 
systems. It limits the applicability of methods such as transfer learning, 
zero-shot generalization, and curriculum learning, all of which depend on the ability 
to vary team composition during training or deployment. 
Moreover, it complicates experimentation by necessitating separate network architectures and 
training loops for each agent count. Addressing this limitation is essential for building 
general-purpose, adaptable multi-agent policies.

In this paper, we examine structural constraints in the default observation design of the
LBF environment and introduce two strategies to decouple observation vector dimensionality 
from agent population size:
\begin{itemize}
    \item a minimal observation strategy that omits ally features entirely, and
    \item a structured reduction strategy that encodes ally information into a fixed-size,
    role-agnostic format.
\end{itemize}

We empirically evaluate these strategies against the default encoding across multiple team sizes.
Our results demonstrate that observation reduction can preserve, and in some cases even improve,
performance while enabling significantly more flexible training workflows.

% We retain the formal definition of the original observation structure here for reference:
% \[
% \underbrace{[\text{level}_{n_i},\ \text{row}_{n_i},\ \text{col}_{n_i}]}_{\text{Own Features}} ++
% \underbrace{[f \in F]}_{\text{Food Matrix}} ++
% \underbrace{[n \in N_{/i}]}_{\text{Agent Matrix}}
% \]


\section{Related Work}
\subsection{Observation Design in Multi-Agent Systems}

Observation space design plays a central role in the generalization, scalability, 
and transferability of learned policies in reinforcement learning more broadly, 
and presents unique challenges in the multi-agent setting that we focus on here. 
In many environments—such as SMAC \cite{samvelyan2019}, Google Research Football \cite{kurach2020},
and Level-Based Foraging (LBF) \cite{papoudakis2021}—the observation vector encodes fixed-size 
segments that correspond to all agents and nearby objects. 
This leads to observation dimensions that are tightly coupled to the number of agents
in the environment, which complicates transfer to variable team sizes and undermines architectural
modularity. 
Such design choices limit the feasibility of curriculum learning, zero-shot generalization,
and policy reuse across heterogeneous configurations.

While few works frame this issue directly as a problem of \textit{dimension reduction}, 
several methods indirectly address the challenge by compressing, pooling, 
or abstracting over agent-specific information.
Entity-centric approaches such as REFIL \cite{iqbal2021} and mean embedding strategies
\cite{huttenrauch2019} represent a collection of agents or objects using permutation-invariant
transformations, effectively decoupling policy input from team size. Similarly, attention-based methods
(e.g., top-$k$ pooling) and mean-field approximations \cite{yang2021a} reduce observation complexity
by focusing only on the most relevant entities. Role-conditioned architectures
\cite{gupta2017, kouzeghar2023} also compress observations by filtering agent features through
a low-dimensional role representation.

These approaches, however, often introduce architectural complexity or assume centralized 
training with shared state information \cite{foerster2017}. Moreover, they typically require 
retraining or re-encoding when the agent pool or spatial layout changes significantly. 
Our work takes a complementary approach: we treat the dimensionality of the observation vector 
itself as a constraint and propose lightweight observation-level reductions that preserve 
task-relevant structure while enabling flexible agent counts. 
Related efforts in high-dimensional robotics and policy compression similarly 
leverage latent representations to improve sample efficiency and structural 
adaptability \cite{bitzer2010, tangkaratt2016}.

% \todo{Consider adding citation for attention-based message pruning or top-k pooling.}

\subsection{Heterogeneous MARL and Policy Transferability}

Heterogeneous MARL (HARL) introduces additional complexity by requiring agents with differing
roles, capabilities, or observation modalities to coordinate effectively. This setting reflects
many real-world domains, including swarm robotics \cite{hoang2023}, traffic control \cite{calvo2018},
and decentralized logistics \cite{rizk2019}, where adaptability to team composition and role diversity
is essential.

Prior work has explored architectures that support specialization and flexibility. 
For example, Gupta et al. \cite{gupta2017a} and Hüttenrauch et al. \cite{huttenrauch2019} 
both demonstrate techniques for encoding structured input using single-agent policies that 
can generalize across role configurations, but do not address multi-agent coordination directly. 
Iqbal et al. \cite{iqbal2021} introduce entity-centric methods in a homogeneous MARL setting, 
which allow pooling across similar agents but still assume identical agent capabilities. 
While these methods improve generalization within their respective assumptions, they often 
maintain a fixed number of roles or homogeneous agent groups, limiting their applicability 
in dynamic heterogeneous settings.

Our work contributes to this space by focusing on observation-level design, offering strategies
that allow agent populations to vary without modifying the underlying network architecture.
This enables more efficient policy reuse and deployment across tasks with differing team sizes,
a capability not well supported in most existing HARL approaches.

\subsection{Benchmarks and Evaluation in HARL}

Evaluating HARL systems under varying team configurations remains underexplored in most
benchmark suites. While environments like LBF \cite{papoudakis2021} permit adjustable agent counts,
their default observation encodings entangle observation length with the number of agents,
hindering direct comparison across configurations. Other platforms, such as Melting Pot
\cite{leibo2021} and MAgent2 \cite{zheng2017}, support large-scale simulations but typically focus
on homogeneous populations with shared policies.

Recent benchmarks such as SMACv2 \cite{ellis2023} introduce generalization challenges via
procedural variation, and Google Research Football (GRF) \cite{kurach2020} provides rich agent
coordination scenarios. However, both maintain fixed observation schemas and assume static team
structures during training and evaluation.

Our work complements these efforts by demonstrating how reduced and agent-count-invariant
observation encodings can support variable-team evaluation within a consistent architecture.
This approach enhances the flexibility of HARL evaluation protocols without requiring environment- or
network-specific modifications.

\section{Methodology}
% Default observation space
% Reduced observation space
% Egocentric observation space

% Not ego-st (carried behavioral implication)

\subsection{Environment}

We use the Level-Based Foraging (LBF) environment \cite{papoudakis2021},
a grid-based reinforcement learning benchmark designed to evaluate 
coordination and cooperation in multi-agent settings,
that requires agents to jointly collect food items scattered across a map. 

Each agent and food item is assigned a discrete level, with the level representing 
the agent's foraging capability and the food's consumption requirement, respectively.
A food item can only be foraged when the sum of the levels of the agents 
occupying adjacent cells meets or exceeds the food's level.

The environment emphasizes multi-agent cooperation under spatial and temporal constraints. 
Agents must learn to navigate the grid, identify appropriate foraging opportunities, 
and coordinate with nearby teammates to successfully collect higher-level food items. 
Rewards are sparse and granted only upon successful foraging, 
making exploration and credit assignment particularly challenging. 
The environment can be configured for partial observability by limiting agent vision range, 
further increasing the need for implicit or explicit coordination.

\section{Results}

\section{Conclusion}

% \nocite{*}
\printbibliography

\end{document}