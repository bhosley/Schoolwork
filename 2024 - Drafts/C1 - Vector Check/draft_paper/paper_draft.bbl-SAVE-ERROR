% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.3 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{none/global//global/global/global}
    \entry{mohddaud2022}{article}{}{}
      \name{author}{7}{}{%
        {{hash=27b0c51140f71a45e7b1d28684b48920}{%
           family={Mohd\bibnamedelima Daud},
           familyi={M\bibinitperiod\bibinitdelim D\bibinitperiod},
           given={Sharifah\bibnamedelimb Mastura\bibnamedelima Syed},
           giveni={S\bibinitperiod\bibinitdelim M\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=5a23b028ddd68b282fe8d8bc2d0edb75}{%
           family={Mohd\bibnamedelima Yusof},
           familyi={M\bibinitperiod\bibinitdelim Y\bibinitperiod},
           given={Mohd\bibnamedelimb Yusmiaidil\bibnamedelima Putera},
           giveni={M\bibinitperiod\bibinitdelim Y\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=529324ed7ab598b3437467a720152e3b}{%
           family={Heo},
           familyi={H\bibinitperiod},
           given={Chong\bibnamedelima Chin},
           giveni={C\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
        {{hash=0927254108c11c377c69d7290f3513c2}{%
           family={Khoo},
           familyi={K\bibinitperiod},
           given={Lay\bibnamedelima See},
           giveni={L\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=bd159b60cc70242f4f80bc1f5d6bcdc9}{%
           family={Chainchel\bibnamedelima Singh},
           familyi={C\bibinitperiod\bibinitdelim S\bibinitperiod},
           given={Mansharan\bibnamedelima Kaur},
           giveni={M\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=4091835b73c7dc03a8d09418e070b5af}{%
           family={Mahmood},
           familyi={M\bibinitperiod},
           given={Mohd\bibnamedelima Shah},
           giveni={M\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=b0ef36e956f29b948a45125d0eb198ba}{%
           family={Nawawi},
           familyi={N\bibinitperiod},
           given={Hapizah},
           giveni={H\bibinitperiod}}}%
      }
      \strng{namehash}{50c55015234cd8812c36d70920db1c8c}
      \strng{fullhash}{fd2c5cc94f6fc748ca162ef88b7d5da6}
      \strng{fullhashraw}{fd2c5cc94f6fc748ca162ef88b7d5da6}
      \strng{bibnamehash}{50c55015234cd8812c36d70920db1c8c}
      \strng{authorbibnamehash}{50c55015234cd8812c36d70920db1c8c}
      \strng{authornamehash}{50c55015234cd8812c36d70920db1c8c}
      \strng{authorfullhash}{fd2c5cc94f6fc748ca162ef88b7d5da6}
      \strng{authorfullhashraw}{fd2c5cc94f6fc748ca162ef88b7d5da6}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The use of drones has rapidly evolved over the past decade involving a variety of fields ranging from agriculture, commercial and becoming increasingly used in disaster management or humanitarian aid. Unfortunately, the evidence of its use in mass disasters is still unclear and scarce. This article aims to evaluate the current drone feasibility projects and to discuss a number of challenges related to the deployment of drones in mass disasters in the hopes of empowering and inspiring possible future work. This research follows Arksey and O'Malley framework and updated by Joanna Briggs Institute Framework for Scoping Reviews methodology to summarise the results of 52 research papers over the past ten years, from 2009 to 2020, outlining the research trend of drone application in disaster. A literature search was performed in Medline, CINAHL, Scopus, individual journals, grey literature and google search with assessment based on their content and significance. Potential application of drones in disaster are broad. Based on articles identified, drone application in disasters are classified into four categories; (1) mapping or disaster management which has shown the highest contribution, (2) search and rescue, (3) transportation and (4) training. Although there is a significant increase in the number of publications on use of drone in disaster within the last five years, there is however limited discussion to address post-disaster healthcare situation especially with regards to disaster victim identification. It is evident that drone applications need to be further explored; to focus more on drone assistance to humans especially in victim identification. It is envisaged that with sufficient development, the application of drones appears to be promising and will improve their effectiveness especially in disaster management.}
      \field{day}{1}
      \field{issn}{1355-0306}
      \field{journaltitle}{Science \& Justice}
      \field{month}{1}
      \field{number}{1}
      \field{shortjournal}{Science \& Justice}
      \field{shorttitle}{Applications of Drone in Disaster Management}
      \field{title}{Applications of Drone in Disaster Management: {{A}} Scoping Review}
      \field{urlday}{16}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{62}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{30\bibrangedash 42}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1016/j.scijus.2021.11.002
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S1355030621001477
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S1355030621001477
      \endverb
      \keyw{Disaster,Drones,Humanitarian aid,UAV}
    \endentry
    \entry{hambling2021}{online}{}{}
      \name{author}{1}{}{%
        {{hash=48366d2d6dd4dcea134d9ea9b79a0268}{%
           family={Hambling},
           familyi={H\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
      }
      \list{organization}{1}{%
        {Forbes}%
      }
      \strng{namehash}{48366d2d6dd4dcea134d9ea9b79a0268}
      \strng{fullhash}{48366d2d6dd4dcea134d9ea9b79a0268}
      \strng{fullhashraw}{48366d2d6dd4dcea134d9ea9b79a0268}
      \strng{bibnamehash}{48366d2d6dd4dcea134d9ea9b79a0268}
      \strng{authorbibnamehash}{48366d2d6dd4dcea134d9ea9b79a0268}
      \strng{authornamehash}{48366d2d6dd4dcea134d9ea9b79a0268}
      \strng{authorfullhash}{48366d2d6dd4dcea134d9ea9b79a0268}
      \strng{authorfullhashraw}{48366d2d6dd4dcea134d9ea9b79a0268}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A slew of countries have announced military drone swarm projects in the last few weeks. Here's a primer on what swarms are, how they work and the advantages they bring.}
      \field{day}{1}
      \field{langid}{english}
      \field{month}{3}
      \field{title}{What {{Are Drone Swarms And Why Does Every Military Suddenly Want One}}?}
      \field{urlday}{22}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/N3AQWPT3/what-are-drone-swarms-and-why-does-everyone-suddenly-want-one.html
      \endverb
      \verb{urlraw}
      \verb https://www.forbes.com/sites/davidhambling/2021/03/01/what-are-drone-swarms-and-why-does-everyone-suddenly-want-one/
      \endverb
      \verb{url}
      \verb https://www.forbes.com/sites/davidhambling/2021/03/01/what-are-drone-swarms-and-why-does-everyone-suddenly-want-one/
      \endverb
    \endentry
    \entry{rogers2022}{online}{}{}
      \name{author}{1}{}{%
        {{hash=f5206ca06f64da7e9c2ba00f43b17659}{%
           family={Rogers},
           familyi={R\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod}}}%
      }
      \list{organization}{1}{%
        {Centre for International Governance Innovation}%
      }
      \strng{namehash}{f5206ca06f64da7e9c2ba00f43b17659}
      \strng{fullhash}{f5206ca06f64da7e9c2ba00f43b17659}
      \strng{fullhashraw}{f5206ca06f64da7e9c2ba00f43b17659}
      \strng{bibnamehash}{f5206ca06f64da7e9c2ba00f43b17659}
      \strng{authorbibnamehash}{f5206ca06f64da7e9c2ba00f43b17659}
      \strng{authornamehash}{f5206ca06f64da7e9c2ba00f43b17659}
      \strng{authorfullhash}{f5206ca06f64da7e9c2ba00f43b17659}
      \strng{authorfullhashraw}{f5206ca06f64da7e9c2ba00f43b17659}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The Third Drone Age is characterized by non-state actors using the latest advancements in drone technologies to pursue their political objectives.}
      \field{day}{28}
      \field{langid}{english}
      \field{month}{11}
      \field{shorttitle}{The {{Third Drone Age}}}
      \field{title}{The {{Third Drone Age}}: {{Visions Out}} to 2040}
      \field{urlday}{22}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/JYUJBM8F/the-third-drone-age-visions-out-to-2040.html
      \endverb
      \verb{urlraw}
      \verb https://www.cigionline.org/articles/the-third-drone-age-visions-out-to-2040/
      \endverb
      \verb{url}
      \verb https://www.cigionline.org/articles/the-third-drone-age-visions-out-to-2040/
      \endverb
    \endentry
    \entry{kallenborn2024}{online}{}{}
      \name{author}{1}{}{%
        {{hash=139eeea638a49da3badcc8cdab2c4217}{%
           family={Kallenborn},
           familyi={K\bibinitperiod},
           given={Zachary},
           giveni={Z\bibinitperiod}}}%
      }
      \strng{namehash}{139eeea638a49da3badcc8cdab2c4217}
      \strng{fullhash}{139eeea638a49da3badcc8cdab2c4217}
      \strng{fullhashraw}{139eeea638a49da3badcc8cdab2c4217}
      \strng{bibnamehash}{139eeea638a49da3badcc8cdab2c4217}
      \strng{authorbibnamehash}{139eeea638a49da3badcc8cdab2c4217}
      \strng{authornamehash}{139eeea638a49da3badcc8cdab2c4217}
      \strng{authorfullhash}{139eeea638a49da3badcc8cdab2c4217}
      \strng{authorfullhashraw}{139eeea638a49da3badcc8cdab2c4217}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{A 2020 New America report cataloged thirty-eight states with armed drone programs, twenty-eight with programs in development, and eleven that have used drones in combat. In less than four years since that report was published, drones’ rapidly growing influence on battlefields from Nagorno-Karabakh to Ukraine to Gaza has almost certainly increased states’ interest in developing}
      \field{day}{20}
      \field{hour}{10}
      \field{langid}{american}
      \field{minute}{29}
      \field{month}{3}
      \field{second}{33}
      \field{shorttitle}{Swarm {{Clouds}} on the {{Horizon}}?}
      \field{timezone}{Z}
      \field{title}{Swarm {{Clouds}} on the {{Horizon}}? {{Exploring}} the {{Future}} of {{Drone Swarm Proliferation}} - {{Modern War Institute}}}
      \field{urlday}{22}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/HA8V2ZYG/swarm-clouds-on-the-horizon-exploring-the-future-of-drone-swarm-proliferation.html
      \endverb
      \verb{urlraw}
      \verb https://mwi.westpoint.edu/swarm-clouds-on-the-horizon-exploring-the-future-of-drone-swarm-proliferation/, https://mwi.westpoint.edu/swarm-clouds-on-the-horizon-exploring-the-future-of-drone-swarm-proliferation/
      \endverb
      \verb{url}
      \verb https://mwi.westpoint.edu/swarm-clouds-on-the-horizon-exploring-the-future-of-drone-swarm-proliferation/,%20https://mwi.westpoint.edu/swarm-clouds-on-the-horizon-exploring-the-future-of-drone-swarm-proliferation/
      \endverb
    \endentry
    \entry{zotero-2835}{online}{}{}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labeltitlesource}{title}
      \field{title}{{{OFFSET Swarm Systems Integrators Demonstrate Tactics}} to {{Conduct Urban Raid}}}
      \field{urlday}{16}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/8ENUG5EW/offset-swarm-urban-raid.html
      \endverb
      \verb{urlraw}
      \verb https://www.darpa.mil/news/2020/offset-swarm-urban-raid
      \endverb
      \verb{url}
      \verb https://www.darpa.mil/news/2020/offset-swarm-urban-raid
      \endverb
    \endentry
    \entry{jin2025}{online}{}{}
      \name{author}{6}{}{%
        {{hash=e50468811cd22d27b53d5663a4310021}{%
           family={Jin},
           familyi={J\bibinitperiod},
           given={Weiqiang},
           giveni={W\bibinitperiod}}}%
        {{hash=0a33edcd9cf079959a9301994561b007}{%
           family={Du},
           familyi={D\bibinitperiod},
           given={Hongyang},
           giveni={H\bibinitperiod}}}%
        {{hash=e0a8441ea5e9cb783ba0bcb3fc549126}{%
           family={Zhao},
           familyi={Z\bibinitperiod},
           given={Biao},
           giveni={B\bibinitperiod}}}%
        {{hash=4ce2153d6b98c2c5f0c681ad4c0dbeeb}{%
           family={Tian},
           familyi={T\bibinitperiod},
           given={Xingwu},
           giveni={X\bibinitperiod}}}%
        {{hash=1e3d82466f2347b8ede6c970ceab13b5}{%
           family={Shi},
           familyi={S\bibinitperiod},
           given={Bohang},
           giveni={B\bibinitperiod}}}%
        {{hash=11a33b1aa9e89479e304318dff521b9c}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Guang},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{2f0d809d993de22304f454b21f857387}
      \strng{fullhash}{7dd07e5f7cc5a6b58a95c3de84648976}
      \strng{fullhashraw}{7dd07e5f7cc5a6b58a95c3de84648976}
      \strng{bibnamehash}{7dd07e5f7cc5a6b58a95c3de84648976}
      \strng{authorbibnamehash}{7dd07e5f7cc5a6b58a95c3de84648976}
      \strng{authornamehash}{2f0d809d993de22304f454b21f857387}
      \strng{authorfullhash}{7dd07e5f7cc5a6b58a95c3de84648976}
      \strng{authorfullhashraw}{7dd07e5f7cc5a6b58a95c3de84648976}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{With the rapid development of artificial intelligence, intelligent decision-making techniques have gradually surpassed human levels in various human-machine competitions, especially in complex multi-agent cooperative task scenarios. Multi-agent cooperative decision-making involves multiple agents working together to complete established tasks and achieve specific objectives. These techniques are widely applicable in real-world scenarios such as autonomous driving, drone navigation, disaster rescue, and simulated military confrontations. This paper begins with a comprehensive survey of the leading simulation environments and platforms used for multi-agent cooperative decision-making. Specifically, we provide an in-depth analysis for these simulation environments from various perspectives, including task formats, reward allocation, and the underlying technologies employed. Subsequently, we provide a comprehensive overview of the mainstream intelligent decision-making approaches, algorithms and models for multi-agent systems (MAS). Theseapproaches can be broadly categorized into five types: rule-based (primarily fuzzy logic), game theory-based, evolutionary algorithms-based, deep multi-agent reinforcement learning (MARL)-based, and large language models(LLMs)reasoning-based. Given the significant advantages of MARL andLLMs-baseddecision-making methods over the traditional rule, game theory, and evolutionary algorithms, this paper focuses on these multi-agent methods utilizing MARL and LLMs-based techniques. We provide an in-depth discussion of these approaches, highlighting their methodology taxonomies, advantages, and drawbacks. Further, several prominent research directions in the future and potential challenges of multi-agent cooperative decision-making are also detailed.}
      \field{day}{17}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{3}
      \field{pubstate}{prepublished}
      \field{shorttitle}{A {{Comprehensive Survey}} on {{Multi-Agent Cooperative Decision-Making}}}
      \field{title}{A {{Comprehensive Survey}} on {{Multi-Agent Cooperative Decision-Making}}: {{Scenarios}}, {{Approaches}}, {{Challenges}} and {{Perspectives}}}
      \field{urlday}{16}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{version}{1}
      \field{year}{2025}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2503.13415
      \endverb
      \verb{eprint}
      \verb 2503.13415
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/4VBMU7EK/Jin et al_2025_A Comprehensive Survey on Multi-Agent Cooperative Decision-Making.pdf;/Users/brandonhosley/Zotero/storage/NGAVTUPQ/2503.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2503.13415
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2503.13415
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems}
    \endentry
    \entry{silver2016}{article}{useprefix=true}{}
      \name{author}{20}{}{%
        {{hash=89dbd30410c2085cd059f32c57d4593e}{%
           family={Silver},
           familyi={S\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=ba4b200ce1412a2570cb113366cc9559}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Aja},
           giveni={A\bibinitperiod}}}%
        {{hash=7c3126321fcb4553dfda5ae96da928ce}{%
           family={Maddison},
           familyi={M\bibinitperiod},
           given={Chris\bibnamedelima J.},
           giveni={C\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=4131bd14e5ca890278ecd351e356dc34}{%
           family={Guez},
           familyi={G\bibinitperiod},
           given={Arthur},
           giveni={A\bibinitperiod}}}%
        {{hash=50d24de916599d306c5cb1a77156e4b9}{%
           family={Sifre},
           familyi={S\bibinitperiod},
           given={Laurent},
           giveni={L\bibinitperiod}}}%
        {{hash=54c5c572d4fdc5d822c240b59fcadad4}{%
           family={Driessche},
           familyi={D\bibinitperiod},
           given={George},
           giveni={G\bibinitperiod},
           prefix={van\bibnamedelima den},
           prefixi={v\bibinitperiod\bibinitdelim d\bibinitperiod}}}%
        {{hash=8fad8df927bc0014c0bd6a9feb7aa71d}{%
           family={Schrittwieser},
           familyi={S\bibinitperiod},
           given={Julian},
           giveni={J\bibinitperiod}}}%
        {{hash=af540e84ef1ecdaa70b1f7c90f59fd7d}{%
           family={Antonoglou},
           familyi={A\bibinitperiod},
           given={Ioannis},
           giveni={I\bibinitperiod}}}%
        {{hash=1f065703a4f7a60c69ecb59f499d3db3}{%
           family={Panneershelvam},
           familyi={P\bibinitperiod},
           given={Veda},
           giveni={V\bibinitperiod}}}%
        {{hash=3075d0e02c0833f6e5fe1addb880898f}{%
           family={Lanctot},
           familyi={L\bibinitperiod},
           given={Marc},
           giveni={M\bibinitperiod}}}%
        {{hash=a133b469b21b870786119a47b1b691eb}{%
           family={Dieleman},
           familyi={D\bibinitperiod},
           given={Sander},
           giveni={S\bibinitperiod}}}%
        {{hash=c19e654fe7f97a2862b0f06588d04c42}{%
           family={Grewe},
           familyi={G\bibinitperiod},
           given={Dominik},
           giveni={D\bibinitperiod}}}%
        {{hash=5560aef5a586c04b85130c3be44a3b6a}{%
           family={Nham},
           familyi={N\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod}}}%
        {{hash=d2d0778c1cdd451c75b874b58eec7564}{%
           family={Kalchbrenner},
           familyi={K\bibinitperiod},
           given={Nal},
           giveni={N\bibinitperiod}}}%
        {{hash=8d569d1d5b8b5a7836017a98b430f959}{%
           family={Sutskever},
           familyi={S\bibinitperiod},
           given={Ilya},
           giveni={I\bibinitperiod}}}%
        {{hash=3a6fdf4df9a25f1d2d506ad9e86e1f6c}{%
           family={Lillicrap},
           familyi={L\bibinitperiod},
           given={Timothy},
           giveni={T\bibinitperiod}}}%
        {{hash=958e13979b3920f7fb4793ed8dddcb33}{%
           family={Leach},
           familyi={L\bibinitperiod},
           given={Madeleine},
           giveni={M\bibinitperiod}}}%
        {{hash=481308b301166b521c74fde6566e97e6}{%
           family={Kavukcuoglu},
           familyi={K\bibinitperiod},
           given={Koray},
           giveni={K\bibinitperiod}}}%
        {{hash=368b9b2de627b852658c433b062d4e1e}{%
           family={Graepel},
           familyi={G\bibinitperiod},
           given={Thore},
           giveni={T\bibinitperiod}}}%
        {{hash=b160026950ebb1e2286dfb40c15482f5}{%
           family={Hassabis},
           familyi={H\bibinitperiod},
           given={Demis},
           giveni={D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Nature Publishing Group}%
      }
      \strng{namehash}{4ccacac34637df06f749428b6ec5052e}
      \strng{fullhash}{6ce4d589ddc1421afa003201a0a08ba0}
      \strng{fullhashraw}{6ce4d589ddc1421afa003201a0a08ba0}
      \strng{bibnamehash}{4ccacac34637df06f749428b6ec5052e}
      \strng{authorbibnamehash}{4ccacac34637df06f749428b6ec5052e}
      \strng{authornamehash}{4ccacac34637df06f749428b6ec5052e}
      \strng{authorfullhash}{6ce4d589ddc1421afa003201a0a08ba0}
      \strng{authorfullhashraw}{6ce4d589ddc1421afa003201a0a08ba0}
      \field{sortinit}{7}
      \field{sortinithash}{108d0be1b1bee9773a1173443802c0a3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.}
      \field{issn}{1476-4687}
      \field{journaltitle}{Nature}
      \field{langid}{english}
      \field{month}{1}
      \field{number}{7587}
      \field{title}{Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search}
      \field{urlday}{15}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{529}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{484\bibrangedash 489}
      \range{pages}{6}
      \verb{doi}
      \verb 10.1038/nature16961
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/F29T7VGP/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/nature16961
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/nature16961
      \endverb
      \keyw{Computational science,Computer science,Reward}
    \endentry
    \entry{vinyals2019}{article}{}{}
      \name{author}{42}{}{%
        {{hash=494b568c5dc85ba8f3f409635f9c5f25}{%
           family={Vinyals},
           familyi={V\bibinitperiod},
           given={Oriol},
           giveni={O\bibinitperiod}}}%
        {{hash=92efe2a8e13a9b7a3fb647951ee2391c}{%
           family={Babuschkin},
           familyi={B\bibinitperiod},
           given={Igor},
           giveni={I\bibinitperiod}}}%
        {{hash=5dc7de68ac7f0299a995822f38a3e705}{%
           family={Czarnecki},
           familyi={C\bibinitperiod},
           given={Wojciech\bibnamedelima M.},
           giveni={W\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=b3fbcff36ec4b37aed7c56f67c21038b}{%
           family={Mathieu},
           familyi={M\bibinitperiod},
           given={Michaël},
           giveni={M\bibinitperiod}}}%
        {{hash=76a7579d94d000dca5e0071fb3b69382}{%
           family={Dudzik},
           familyi={D\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
        {{hash=7a79e6bb4ca772c9b3b38f4e9f45b83c}{%
           family={Chung},
           familyi={C\bibinitperiod},
           given={Junyoung},
           giveni={J\bibinitperiod}}}%
        {{hash=17ad757032d822b4a43e828ae592c91e}{%
           family={Choi},
           familyi={C\bibinitperiod},
           given={David\bibnamedelima H.},
           giveni={D\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=2820daeb58d4d5a594fbdaf6db68f850}{%
           family={Powell},
           familyi={P\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
        {{hash=2687d58e59afed22249974d723704f56}{%
           family={Ewalds},
           familyi={E\bibinitperiod},
           given={Timo},
           giveni={T\bibinitperiod}}}%
        {{hash=1c5b83cefd7033a528994276b9c00e87}{%
           family={Georgiev},
           familyi={G\bibinitperiod},
           given={Petko},
           giveni={P\bibinitperiod}}}%
        {{hash=d6a1e759373b43eefda2aab3aec6b728}{%
           family={Oh},
           familyi={O\bibinitperiod},
           given={Junhyuk},
           giveni={J\bibinitperiod}}}%
        {{hash=c1a26cb7f963abdf071da408366e83a1}{%
           family={Horgan},
           familyi={H\bibinitperiod},
           given={Dan},
           giveni={D\bibinitperiod}}}%
        {{hash=be1b190734e6a891e14bf8c4adc4d1c3}{%
           family={Kroiss},
           familyi={K\bibinitperiod},
           given={Manuel},
           giveni={M\bibinitperiod}}}%
        {{hash=970650e9394fccd4144d4b829505d2b3}{%
           family={Danihelka},
           familyi={D\bibinitperiod},
           given={Ivo},
           giveni={I\bibinitperiod}}}%
        {{hash=ba4b200ce1412a2570cb113366cc9559}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Aja},
           giveni={A\bibinitperiod}}}%
        {{hash=50d24de916599d306c5cb1a77156e4b9}{%
           family={Sifre},
           familyi={S\bibinitperiod},
           given={Laurent},
           giveni={L\bibinitperiod}}}%
        {{hash=3d7a83ed6eb983ca17cec804631dc22e}{%
           family={Cai},
           familyi={C\bibinitperiod},
           given={Trevor},
           giveni={T\bibinitperiod}}}%
        {{hash=e4fa217e56ca1781ab11713ab27cf2b4}{%
           family={Agapiou},
           familyi={A\bibinitperiod},
           given={John\bibnamedelima P.},
           giveni={J\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=7dc1446dea7ff50b2b02fb83780cc9c6}{%
           family={Jaderberg},
           familyi={J\bibinitperiod},
           given={Max},
           giveni={M\bibinitperiod}}}%
        {{hash=06f6f348a3bb818c79944f71cf518f3f}{%
           family={Vezhnevets},
           familyi={V\bibinitperiod},
           given={Alexander\bibnamedelima S.},
           giveni={A\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=f64056fe9107fedd28b447d31bfc157b}{%
           family={Leblond},
           familyi={L\bibinitperiod},
           given={Rémi},
           giveni={R\bibinitperiod}}}%
        {{hash=b1bdbd9e1dcbed6591878a57d5058c54}{%
           family={Pohlen},
           familyi={P\bibinitperiod},
           given={Tobias},
           giveni={T\bibinitperiod}}}%
        {{hash=39809e64cffbcbfd42bd81da5153546a}{%
           family={Dalibard},
           familyi={D\bibinitperiod},
           given={Valentin},
           giveni={V\bibinitperiod}}}%
        {{hash=c23f8012677b40de82f339368c393522}{%
           family={Budden},
           familyi={B\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=22eaafdd6c3038933341b729e199e0ce}{%
           family={Sulsky},
           familyi={S\bibinitperiod},
           given={Yury},
           giveni={Y\bibinitperiod}}}%
        {{hash=88dca93fff01bbe55329c40c0891257d}{%
           family={Molloy},
           familyi={M\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod}}}%
        {{hash=f565c54b0935cdca35d3d90b831013cf}{%
           family={Paine},
           familyi={P\bibinitperiod},
           given={Tom\bibnamedelima L.},
           giveni={T\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=2adc0c92c308f233c731321d55efe58f}{%
           family={Gulcehre},
           familyi={G\bibinitperiod},
           given={Caglar},
           giveni={C\bibinitperiod}}}%
        {{hash=05bc8d503a2c310ef0976ace7f9d2734}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Ziyu},
           giveni={Z\bibinitperiod}}}%
        {{hash=a726408819b6f955652c3ffaaedef966}{%
           family={Pfaff},
           familyi={P\bibinitperiod},
           given={Tobias},
           giveni={T\bibinitperiod}}}%
        {{hash=954bef435a12336c9decd19360a640f5}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Yuhuai},
           giveni={Y\bibinitperiod}}}%
        {{hash=50349f5d4ef065e417356ee70e0f7069}{%
           family={Ring},
           familyi={R\bibinitperiod},
           given={Roman},
           giveni={R\bibinitperiod}}}%
        {{hash=14889c0769f78922934df82a671f0cf3}{%
           family={Yogatama},
           familyi={Y\bibinitperiod},
           given={Dani},
           giveni={D\bibinitperiod}}}%
        {{hash=616c76aced80eaed20688f6ab93db271}{%
           family={Wünsch},
           familyi={W\bibinitperiod},
           given={Dario},
           giveni={D\bibinitperiod}}}%
        {{hash=5f3f9f74e6c498a754b9a8a7b7a53311}{%
           family={McKinney},
           familyi={M\bibinitperiod},
           given={Katrina},
           giveni={K\bibinitperiod}}}%
        {{hash=e4c8c37e34209dd0f61d34723d9061fe}{%
           family={Smith},
           familyi={S\bibinitperiod},
           given={Oliver},
           giveni={O\bibinitperiod}}}%
        {{hash=a56e72b23778a835bdade7d0511e43a3}{%
           family={Schaul},
           familyi={S\bibinitperiod},
           given={Tom},
           giveni={T\bibinitperiod}}}%
        {{hash=3a6fdf4df9a25f1d2d506ad9e86e1f6c}{%
           family={Lillicrap},
           familyi={L\bibinitperiod},
           given={Timothy},
           giveni={T\bibinitperiod}}}%
        {{hash=481308b301166b521c74fde6566e97e6}{%
           family={Kavukcuoglu},
           familyi={K\bibinitperiod},
           given={Koray},
           giveni={K\bibinitperiod}}}%
        {{hash=b160026950ebb1e2286dfb40c15482f5}{%
           family={Hassabis},
           familyi={H\bibinitperiod},
           given={Demis},
           giveni={D\bibinitperiod}}}%
        {{hash=b7585cb31b9235c4a758d152b7f7e828}{%
           family={Apps},
           familyi={A\bibinitperiod},
           given={Chris},
           giveni={C\bibinitperiod}}}%
        {{hash=89dbd30410c2085cd059f32c57d4593e}{%
           family={Silver},
           familyi={S\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
      }
      \strng{namehash}{832d19b6ecbaa1d465342fe676ffb29a}
      \strng{fullhash}{e20baf229e73fd131dd91cbcd821c571}
      \strng{fullhashraw}{e20baf229e73fd131dd91cbcd821c571}
      \strng{bibnamehash}{832d19b6ecbaa1d465342fe676ffb29a}
      \strng{authorbibnamehash}{832d19b6ecbaa1d465342fe676ffb29a}
      \strng{authornamehash}{832d19b6ecbaa1d465342fe676ffb29a}
      \strng{authorfullhash}{e20baf229e73fd131dd91cbcd821c571}
      \strng{authorfullhashraw}{e20baf229e73fd131dd91cbcd821c571}
      \field{sortinit}{7}
      \field{sortinithash}{108d0be1b1bee9773a1173443802c0a3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{annotation}{shorttitle: AlphaStar}
      \field{day}{14}
      \field{issn}{0028-0836, 1476-4687}
      \field{journaltitle}{Nature}
      \field{langid}{english}
      \field{month}{11}
      \field{number}{7782}
      \field{shortjournal}{Nature}
      \field{shorttitle}{{{AlphaStar}}}
      \field{title}{Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning}
      \field{urlday}{24}
      \field{urlmonth}{12}
      \field{urlyear}{2023}
      \field{volume}{575}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{350\bibrangedash 354}
      \range{pages}{5}
      \verb{doi}
      \verb 10.1038/s41586-019-1724-z
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/97RK6RVS/Vinyals et al. - 2019 - Grandmaster level in StarCraft II using multi-agen.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/s41586-019-1724-z
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/s41586-019-1724-z
      \endverb
    \endentry
    \entry{berner2019}{online}{}{}
      \name{author}{25}{}{%
        {{hash=ca86811e7a0582a9e7cb8d33e7ab445d}{%
           family={Berner},
           familyi={B\bibinitperiod},
           given={Christopher},
           giveni={C\bibinitperiod}}}%
        {{hash=7ae2a0efeaf9c031f9b420bcb1c19e54}{%
           family={Brockman},
           familyi={B\bibinitperiod},
           given={Greg},
           giveni={G\bibinitperiod}}}%
        {{hash=bdcc84061540e15aac439cba59db6577}{%
           family={Chan},
           familyi={C\bibinitperiod},
           given={Brooke},
           giveni={B\bibinitperiod}}}%
        {{hash=ece7baf9b320c51ead8e24c1c6386dbf}{%
           family={Cheung},
           familyi={C\bibinitperiod},
           given={Vicki},
           giveni={V\bibinitperiod}}}%
        {{hash=5b9ef873a5cb802b4a76be3508f175b4}{%
           family={Dębiak},
           familyi={D\bibinitperiod},
           given={Przemysław},
           giveni={P\bibinitperiod}}}%
        {{hash=a509b9c62a0e81a58991a81caf48298d}{%
           family={Dennison},
           familyi={D\bibinitperiod},
           given={Christy},
           giveni={C\bibinitperiod}}}%
        {{hash=a562eb10ac2870de64b3956df2eb1896}{%
           family={Farhi},
           familyi={F\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=9077a5599d2a4f3aa8bb20cef3b6399a}{%
           family={Fischer},
           familyi={F\bibinitperiod},
           given={Quirin},
           giveni={Q\bibinitperiod}}}%
        {{hash=49e2023e4e856577f61450fc5149743d}{%
           family={Hashme},
           familyi={H\bibinitperiod},
           given={Shariq},
           giveni={S\bibinitperiod}}}%
        {{hash=07acc23f6ec051b64b82cd33255c0a69}{%
           family={Hesse},
           familyi={H\bibinitperiod},
           given={Chris},
           giveni={C\bibinitperiod}}}%
        {{hash=4eda39963a95c233a262d4191f198aa4}{%
           family={Józefowicz},
           familyi={J\bibinitperiod},
           given={Rafal},
           giveni={R\bibinitperiod}}}%
        {{hash=7006ca8c1ce969019b89de50fece60dd}{%
           family={Gray},
           familyi={G\bibinitperiod},
           given={Scott},
           giveni={S\bibinitperiod}}}%
        {{hash=bd24844b3abaa88e1f3c5c074ad37ab6}{%
           family={Olsson},
           familyi={O\bibinitperiod},
           given={Catherine},
           giveni={C\bibinitperiod}}}%
        {{hash=27d3d977b77156237cdeb5a7b7eaa560}{%
           family={Pachocki},
           familyi={P\bibinitperiod},
           given={Jakub},
           giveni={J\bibinitperiod}}}%
        {{hash=20bb006267a99bf6b2419ac00ec8decb}{%
           family={Petrov},
           familyi={P\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=f78abae164f9e9364dd3e0b31887fc08}{%
           family={Pinto},
           familyi={P\bibinitperiod},
           given={Henrique\bibnamedelimb P.\bibnamedelimi d\bibnamedelima O.},
           giveni={H\bibinitperiod\bibinitdelim P\bibinitperiod\bibinitdelim d\bibinitperiod\bibinitdelim O\bibinitperiod}}}%
        {{hash=9db43b2d2fa0f38a4051782eb9de8f87}{%
           family={Raiman},
           familyi={R\bibinitperiod},
           given={Jonathan},
           giveni={J\bibinitperiod}}}%
        {{hash=e6f76e1a4d058df028530916774ad3a7}{%
           family={Salimans},
           familyi={S\bibinitperiod},
           given={Tim},
           giveni={T\bibinitperiod}}}%
        {{hash=e3e581a0808055d754222c3b47b3a7ab}{%
           family={Schlatter},
           familyi={S\bibinitperiod},
           given={Jeremy},
           giveni={J\bibinitperiod}}}%
        {{hash=5167ef9d0b77bf68557730648baac9b7}{%
           family={Schneider},
           familyi={S\bibinitperiod},
           given={Jonas},
           giveni={J\bibinitperiod}}}%
        {{hash=6657859de18e844e4b1b815e03694c71}{%
           family={Sidor},
           familyi={S\bibinitperiod},
           given={Szymon},
           giveni={S\bibinitperiod}}}%
        {{hash=8d569d1d5b8b5a7836017a98b430f959}{%
           family={Sutskever},
           familyi={S\bibinitperiod},
           given={Ilya},
           giveni={I\bibinitperiod}}}%
        {{hash=d0b3008e85b1a8b38f46556abf1791c7}{%
           family={Tang},
           familyi={T\bibinitperiod},
           given={Jie},
           giveni={J\bibinitperiod}}}%
        {{hash=674ede0b9cd02a2bf5fc662972efb9f0}{%
           family={Wolski},
           familyi={W\bibinitperiod},
           given={Filip},
           giveni={F\bibinitperiod}}}%
        {{hash=24b5bdd4e149e4c9ed111f6051192cf8}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Susan},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{9d764cedbc33ef38116d07274416b052}
      \strng{fullhash}{841e942f61905cc71f1100c74388c62b}
      \strng{fullhashraw}{841e942f61905cc71f1100c74388c62b}
      \strng{bibnamehash}{9d764cedbc33ef38116d07274416b052}
      \strng{authorbibnamehash}{9d764cedbc33ef38116d07274416b052}
      \strng{authornamehash}{9d764cedbc33ef38116d07274416b052}
      \strng{authorfullhash}{841e942f61905cc71f1100c74388c62b}
      \strng{authorfullhashraw}{841e942f61905cc71f1100c74388c62b}
      \field{sortinit}{7}
      \field{sortinithash}{108d0be1b1bee9773a1173443802c0a3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.}
      \field{day}{13}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arXiv}
      \field{month}{12}
      \field{pubstate}{prepublished}
      \field{title}{Dota 2 with {{Large Scale Deep Reinforcement Learning}}}
      \field{urlday}{15}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1912.06680
      \endverb
      \verb{eprint}
      \verb 1912.06680
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/GIAX6G7H/OpenAI et al. - 2019 - Dota 2 with Large Scale Deep Reinforcement Learnin.pdf;/Users/brandonhosley/Zotero/storage/HM7FXNK8/1912.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1912.06680
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1912.06680
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{rizk2019}{book}{}{}
      \name{author}{3}{}{%
        {{hash=3bf0bdfcd1ad3a4d6ed3e1a1a5d94555}{%
           family={Rizk},
           familyi={R\bibinitperiod},
           given={Yara},
           giveni={Y\bibinitperiod}}}%
        {{hash=ab9ad0fd1072886327bd8fcf16882d22}{%
           family={Awad},
           familyi={A\bibinitperiod},
           given={Mariette},
           giveni={M\bibinitperiod}}}%
        {{hash=a696139e89c3f6d7152594d068cab433}{%
           family={Tunstel},
           familyi={T\bibinitperiod},
           given={E.},
           giveni={E\bibinitperiod}}}%
      }
      \strng{namehash}{63d6456a06899559626b710300422783}
      \strng{fullhash}{92270213c4e822e5c8cb023f26b5a06b}
      \strng{fullhashraw}{92270213c4e822e5c8cb023f26b5a06b}
      \strng{bibnamehash}{92270213c4e822e5c8cb023f26b5a06b}
      \strng{authorbibnamehash}{92270213c4e822e5c8cb023f26b5a06b}
      \strng{authornamehash}{63d6456a06899559626b710300422783}
      \strng{authorfullhash}{92270213c4e822e5c8cb023f26b5a06b}
      \strng{authorfullhashraw}{92270213c4e822e5c8cb023f26b5a06b}
      \field{sortinit}{8}
      \field{sortinithash}{a231b008ebf0ecbe0b4d96dcc159445f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The emergence of the Internet of things and the wide spread deployment of diverse computing systems have led to the formation of heterogeneous multi-agent systems (MAS) to complete a variety of tasks. Motivated to highlight the state of the art on existing MAS while identifying their limitations, remaining challenges and possible future directions, we survey recent contributions to the field. We focus on robot agents and emphasize the challenges of MAS sub-fields including task decomposition, coalition formation, task allocation, perception, and multi-agent planning and control. While some components have seen more advancements than others, more research is required before effective autonomous MAS can be deployed in real smart city settings that are less restrictive than the assumed validation environments of MAS. Specifically, more autonomous end-to-end solutions need to be experimentally tested and developed while incorporating natural language ontology and dictionaries to automate complex task decomposition and leveraging big data advancements to improve perception algorithms for robotics. Y. Rizk et al. 1 INTRODUCTION The dynamic and unpredictable nature of the world we live in makes it difficult to design one autonomous robot that can efficiently adapt to all circumstances. Therefore, robots of various shapes, sizes and capabilities such as unmanned aerial vehicles (UAVs), unmanned ground vehicles (UGVs), humanoids and others have been designed to cooperate with each other and humans, to successfully accomplish complex tasks. Allowing these diverse connected devices, expected to surpass \$20 billion by 2020 with the emergence of the Internet of things (IoT) [39], to cooperate will significantly increase the spectrum of automated tasks. Integrating these devices in areas such as health care, transportation systems, emergency response systems, household chores, and elderly care, among others will make smart cities even smarter. In this work, we discuss the literature on automating complex tasks using heterogeneous multi-robot systems (MRS), after a brief overview of the more general multi-agent system (MAS) field. We present the main components of a workflow to automate MRS: task decomposition, coalition formation, task allocation, perception, and MAS planning and control, survey existing work in each area and identify some remaining challenges and possible future research directions. However, many aspects of heterogeneous MAS are not covered in this survey. They include, but are not limited to, credit assignment which studies reward distribution among agents [226], consensus which investigates protocols that ensure agent agreement under different circumstances [102, 237], containment control which is a type of consensus in leader-follower models [236] and robot hardware design. Although communication protocols for robot-robot communication which enable agent cooperation through information exchange [30, 56] and the information flow problem which seeks to design efficient information exchange strategies [30] are an important component of MAS, we do not include them in this survey. Instead readers are invited to check the literature of [30, 56]. Few end-to-end frameworks have been presented for MRS, the most notable is swarmanoids [55] which accomplished search and retrieval tasks using a swarm of three types of robots. Many solutions still required significant human intervention to achieve complex tasks. Furthermore, individual aspects of MRS have been tackled, such as giving robots access to information on the cloud [175] and simultaneous coalition formation and task allocation [233]. However, more end-to-end testing on IoT-aided robotics [77] and MRS applications should be conducted to achieve more progress. Natural language ontology and dictionaries could help automate complex task decomposition and big data advancements could be leveraged to improve perception and consequently decision making. Two of the closest surveys to our work were published almost a decade ago; one covered MRS coordination including task allocation, decomposition and resource distribution [54]. However, it focused on market-based approaches and did not include work on coalition formation and decision making models. A more recent survey discussed existing MRS architectures, communication schemes, swarm robotics, task allocation, and learning [157] in applications like foraging, formation control, cooperative object manipulation and displacement, path planning and soccer [157]. Other surveys had a narrower scope than our work and focused on a specific research area such as cooperative MAS planning and control models and algorithms [155] and distributed consensus in MAS [235]. Some surveys focused on MRS and their assigned tasks; Arai et al. identified seven main research areas in MRS, including robot architectures, mapping and exploration, and motion coordination, and discussed state of the art research and challenges in each area [13]. Ota surveyed tasks assigned to MRS, classifying them into point reaching, region sweeping and compound tasks, in addition to one-time and many-time tasks, i.e. tasks that require multiple iterations for completion [152]. Murray surveyed cooperative control of multi-vehicle systems and their applications in the}
      \field{day}{7}
      \field{month}{1}
      \field{shorttitle}{Cooperative {{Heterogeneous Multi-Robot Systems}}}
      \field{title}{Cooperative {{Heterogeneous Multi-Robot Systems}}: {{A Survey}}}
      \field{year}{2019}
      \field{dateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/HNKQIAI6/Rizk et al_2019_Cooperative Heterogeneous Multi-Robot Systems.pdf
      \endverb
    \endentry
    \entry{liang2024}{inproceedings}{}{}
      \name{author}{6}{}{%
        {{hash=ea7a01cea17c40d48aee4d4188ec1f55}{%
           family={Liang},
           familyi={L\bibinitperiod},
           given={Zhiuxan},
           giveni={Z\bibinitperiod}}}%
        {{hash=cd59d436917c02c252d25762a26d0955}{%
           family={Cao},
           familyi={C\bibinitperiod},
           given={Jiannong},
           giveni={J\bibinitperiod}}}%
        {{hash=25ce97132dd55778e1dbf3c9bf2d3a37}{%
           family={Jiang},
           familyi={J\bibinitperiod},
           given={Shan},
           giveni={S\bibinitperiod}}}%
        {{hash=b5eaa8ef61a1b6fe783d35509459e28a}{%
           family={Saxena},
           familyi={S\bibinitperiod},
           given={Divya},
           giveni={D\bibinitperiod}}}%
        {{hash=e08dfef32541890832ffa41226eaac1b}{%
           family={Cao},
           familyi={C\bibinitperiod},
           given={Rui},
           giveni={R\bibinitperiod}}}%
        {{hash=afc4c59ddb7b37930593f3ae117e1d15}{%
           family={Xu},
           familyi={X\bibinitperiod},
           given={Huafeng},
           giveni={H\bibinitperiod}}}%
      }
      \strng{namehash}{4f5b8884406e78871e218a75968b0b16}
      \strng{fullhash}{7e548b150baf7a0ca32b7c250a67c5bb}
      \strng{fullhashraw}{7e548b150baf7a0ca32b7c250a67c5bb}
      \strng{bibnamehash}{7e548b150baf7a0ca32b7c250a67c5bb}
      \strng{authorbibnamehash}{7e548b150baf7a0ca32b7c250a67c5bb}
      \strng{authornamehash}{4f5b8884406e78871e218a75968b0b16}
      \strng{authorfullhash}{7e548b150baf7a0ca32b7c250a67c5bb}
      \strng{authorfullhashraw}{7e548b150baf7a0ca32b7c250a67c5bb}
      \field{sortinit}{8}
      \field{sortinithash}{a231b008ebf0ecbe0b4d96dcc159445f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Multi-robot reinforcement learning (MRRL) is a promising approach to solving cooperation problems and has been widely adopted in many applications. In the past decades, researchers have proposed various approaches to improve the efficiency of MRRL. However, most of them are trained and evaluated only in simulated environments with simple interaction scenarios. The problem of how these methods perform in the real-world environment with complex interaction scenarios remains unsolved. To meet this emergent need, we introduce a scalable multi-robot reinforcement learning platform (SMART) for training and evaluation. Specifically, SMART consists of two components: 1) a simulation environment with an uncertainty-aware social agent model that provides a variety of complex interaction scenarios for training and 2) a real-world multi-robot system for realistic performance evaluation. To evaluate the generalizability of MRRL baselines, we introduce a novel generalization metric that takes into account their performance across changes in the environment as well as the policies of other agents. Furthermore, we conduct a case study on the multi-vehicle cooperative lane change and summarize the unique challenges of MRRL, which are rarely considered previously. Finally, we open-source the simulation environments, associated benchmark tasks, and state-of-the-art baselines to encourage and empower MRRL research. Our code is available at https://github.com/Blackmamba-xuan/MRST.}
      \field{booktitle}{2024 {{IEEE}} 30th {{International Conference}} on {{Parallel}} and {{Distributed Systems}} ({{ICPADS}})}
      \field{eventtitle}{2024 {{IEEE}} 30th {{International Conference}} on {{Parallel}} and {{Distributed Systems}} ({{ICPADS}})}
      \field{issn}{2690-5965}
      \field{month}{10}
      \field{shorttitle}{From {{Agents}} to {{Robots}}}
      \field{title}{From {{Agents}} to {{Robots}}: {{A Training}} and {{Evaluation Platform}} for {{Multi-robot Reinforcement Learning}}}
      \field{urlday}{21}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{593\bibrangedash 600}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1109/ICPADS63350.2024.00083
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/VTJ38CWP/10763906.html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/10763906
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/10763906
      \endverb
      \keyw{Autonomous vehicles,Benchmark testing,Codes,Faces,Multi-robot Reinforcement Learning,Multi-robot Simulator,Multi-Robot System,Multi-robot systems,Performance evaluation,Reinforcement learning,Robots,Training}
    \endentry
    \entry{abeywickrama2022}{article}{}{}
      \name{author}{4}{}{%
        {{hash=4f884fa609977e1fa16831b5675b64c0}{%
           family={Abeywickrama},
           familyi={A\bibinitperiod},
           given={Dhaminda\bibnamedelima B.},
           giveni={D\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=17625f2a58799d3a11011e0fd098c2b6}{%
           family={Griffiths},
           familyi={G\bibinitperiod},
           given={Nathan},
           giveni={N\bibinitperiod}}}%
        {{hash=897528556e80e2d0207255647d606299}{%
           family={Xu},
           familyi={X\bibinitperiod},
           given={Zhou},
           giveni={Z\bibinitperiod}}}%
        {{hash=a94f9d5da349879782befc62a9f8ff16}{%
           family={Mouzakitis},
           familyi={M\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{55a8ee9ad9d90ed7e2d453b167c63cdd}
      \strng{fullhash}{2c5dc646b976e4fb07a3453fd95f80bb}
      \strng{fullhashraw}{2c5dc646b976e4fb07a3453fd95f80bb}
      \strng{bibnamehash}{2c5dc646b976e4fb07a3453fd95f80bb}
      \strng{authorbibnamehash}{2c5dc646b976e4fb07a3453fd95f80bb}
      \strng{authornamehash}{55a8ee9ad9d90ed7e2d453b167c63cdd}
      \strng{authorfullhash}{2c5dc646b976e4fb07a3453fd95f80bb}
      \strng{authorfullhashraw}{2c5dc646b976e4fb07a3453fd95f80bb}
      \field{sortinit}{9}
      \field{sortinithash}{0a5ebc79d83c96b6579069544c73c7d4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Autonomous agents are becoming increasingly ubiquitous and are playing an increasing role in wide range of safety-critical systems, such as driverless cars, exploration robots and unmanned aerial vehicles. These agents operate in highly dynamic and heterogeneous environments, resulting in complex behaviour and interactions. Therefore, the need arises to model and understand more complex and nuanced agent interactions than have previously been studied. In this paper, we propose a novel agent-based modelling approach to investigating norm emergence, in which such interactions can be investigated. To this end, while there may be an ideal set of optimally compatible actions there are also combinations that have positive rewards and are also compatible. Our approach provides a step towards identifying the conditions under which globally compatible norms are likely to emerge in the context of complex rewards. Our model is illustrated using the motivating example of self-driving cars, and we present the scenario of an autonomous vehicle performing a left-turn at a T-intersection.}
      \field{day}{26}
      \field{issn}{1573-7454}
      \field{journaltitle}{Autonomous Agents and Multi-Agent Systems}
      \field{langid}{english}
      \field{month}{10}
      \field{number}{1}
      \field{shortjournal}{Auton Agent Multi-Agent Syst}
      \field{title}{Emergence of Norms in Interactions with Complex Rewards}
      \field{urlday}{18}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{volume}{37}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{2}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1007/s10458-022-09585-3
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/YYXAD6N3/Abeywickrama et al_2022_Emergence of norms in interactions with complex rewards.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s10458-022-09585-3
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s10458-022-09585-3
      \endverb
      \keyw{Agent interactions,Agent-based modelling,Norm emergence,Reinforcement learning}
    \endentry
    \entry{yang2021a}{article}{}{}
      \name{author}{4}{}{%
        {{hash=7efa6952cff713d0987625215131d93a}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Shantian},
           giveni={S\bibinitperiod}}}%
        {{hash=d6f2b26e2b50e389f737db615a2d7427}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Bo},
           giveni={B\bibinitperiod}}}%
        {{hash=be20b75fce95276667b40fe95adc4dd6}{%
           family={Kang},
           familyi={K\bibinitperiod},
           given={Zhongfeng},
           giveni={Z\bibinitperiod}}}%
        {{hash=c9d4f94a15d098764eed5fe05f8dd346}{%
           family={Deng},
           familyi={D\bibinitperiod},
           given={Lihui},
           giveni={L\bibinitperiod}}}%
      }
      \strng{namehash}{aae906ef6d96c7f8b76f5f2b09d48742}
      \strng{fullhash}{a290b86273d352110a228c6f5f663f87}
      \strng{fullhashraw}{a290b86273d352110a228c6f5f663f87}
      \strng{bibnamehash}{a290b86273d352110a228c6f5f663f87}
      \strng{authorbibnamehash}{a290b86273d352110a228c6f5f663f87}
      \strng{authornamehash}{aae906ef6d96c7f8b76f5f2b09d48742}
      \strng{authorfullhash}{a290b86273d352110a228c6f5f663f87}
      \strng{authorfullhashraw}{a290b86273d352110a228c6f5f663f87}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Multi-agent deep reinforcement learning (MDRL) has been widely applied in multi-intersection traffic signal control. The MDRL algorithms produce the decentralized cooperative traffic-signal policies via specialized multi-agent settings in certain traffic networks. However, the state-of-the-art MDRL algorithms seem to have some drawbacks. (1) It is desirable that the traffic-signal policies can be smoothly transferred to diverse traffic networks, however, the adopted specialized multi-agent settings hinder the traffic-signal policies to transfer and generalize to new traffic networks. (2) Existing MDRL algorithms which are based on deep neural networks cannot flexibly tackle a time-varying number of vehicles traversing the traffic networks. (3) Existing MDRL algorithms which are based on homogeneous graph neural networks fail to capture the heterogeneous features of objects in traffic networks. Motivated by the above observations, in this paper, we propose an algorithm, referred to as Inductive Heterogeneous Graph Multi-agent Actor–critic (IHG-MA) algorithm, for multi-intersection traffic signal control. The proposed IHG-MA algorithm has two features: (1) It conducts representation learning using a proposed inductive heterogeneous graph neural network (IHG), which is an inductive algorithm. The proposed IHG algorithm can generate embeddings for previously unseen nodes (e.g., new entry vehicles) and new graphs (e.g., new traffic networks). But unlike the algorithms based on the homogeneous graph neural network, IHG algorithm not only encodes heterogeneous features of each node, but also encodes heterogeneous structural (graph) information. (2) It also conducts policy learning using a proposed multi-agent actor–critic(MA), which is a decentralized cooperative framework. The proposed MA framework employs the final embeddings to compute the Q-value and policy, and then optimizes the whole algorithm via the Q-value and policy loss. Experimental results on different traffic datasets illustrate that IHG-MA algorithm outperforms the state-of-the-art algorithms in terms of multiple traffic metrics, which seems to be a new promising algorithm for multi-intersection traffic signal control.}
      \field{day}{1}
      \field{issn}{0893-6080}
      \field{journaltitle}{Neural Networks}
      \field{month}{7}
      \field{shortjournal}{Neural Networks}
      \field{shorttitle}{{{IHG-MA}}}
      \field{title}{{{IHG-MA}}: {{Inductive}} Heterogeneous Graph Multi-Agent Reinforcement Learning for Multi-Intersection Traffic Signal Control}
      \field{urlday}{14}
      \field{urlmonth}{2}
      \field{urlyear}{2025}
      \field{volume}{139}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{265\bibrangedash 277}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1016/j.neunet.2021.03.015
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/BDL2X5JB/S0893608021000952.html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S0893608021000952
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S0893608021000952
      \endverb
      \keyw{Cooperative traffic signal control,Heterogeneous graph neural network,Inductive heterogeneous graph representation learning,Multi-agent reinforcement learning,Transfer learning}
    \endentry
    \entry{ackermann2019}{online}{}{}
      \name{author}{4}{}{%
        {{hash=d6773c2b5996cd51cdb0ab7775fb8fe8}{%
           family={Ackermann},
           familyi={A\bibinitperiod},
           given={Johannes},
           giveni={J\bibinitperiod}}}%
        {{hash=ac38577e4a1419b425cfacd667ad0716}{%
           family={Gabler},
           familyi={G\bibinitperiod},
           given={Volker},
           giveni={V\bibinitperiod}}}%
        {{hash=463fb8e057cd6542341e251ef86c6176}{%
           family={Osa},
           familyi={O\bibinitperiod},
           given={Takayuki},
           giveni={T\bibinitperiod}}}%
        {{hash=39334f77cbd67c8922133952b8095d89}{%
           family={Sugiyama},
           familyi={S\bibinitperiod},
           given={Masashi},
           giveni={M\bibinitperiod}}}%
      }
      \list{organization}{1}{%
        {arXiv.org}%
      }
      \strng{namehash}{97e03abbbb1513e13b9fc491ccfe70a8}
      \strng{fullhash}{5107ada9512eb732ac00b348d7e58d81}
      \strng{fullhashraw}{5107ada9512eb732ac00b348d7e58d81}
      \strng{bibnamehash}{5107ada9512eb732ac00b348d7e58d81}
      \strng{authorbibnamehash}{5107ada9512eb732ac00b348d7e58d81}
      \strng{authornamehash}{97e03abbbb1513e13b9fc491ccfe70a8}
      \strng{authorfullhash}{5107ada9512eb732ac00b348d7e58d81}
      \strng{authorfullhashraw}{5107ada9512eb732ac00b348d7e58d81}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Many real world tasks require multiple agents to work together. Multi-agent reinforcement learning (RL) methods have been proposed in recent years to solve these tasks, but current methods often fail to efficiently learn policies. We thus investigate the presence of a common weakness in single-agent RL, namely value function overestimation bias, in the multi-agent setting. Based on our findings, we propose an approach that reduces this bias by using double centralized critics. We evaluate it on six mixed cooperative-competitive tasks, showing a significant advantage over current methods. Finally, we investigate the application of multi-agent methods to high-dimensional robotic tasks and show that our approach can be used to learn decentralized policies in this domain.}
      \field{day}{3}
      \field{langid}{english}
      \field{month}{10}
      \field{title}{Reducing {{Overestimation Bias}} in {{Multi-Agent Domains Using Double Centralized Critics}}}
      \field{urlday}{26}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/3DUYWQRZ/Ackermann et al_2019_Reducing Overestimation Bias in Multi-Agent Domains Using Double Centralized.pdf
      \endverb
      \verb{urlraw}
      \verb https://arxiv.org/abs/1910.01465v2
      \endverb
      \verb{url}
      \verb https://arxiv.org/abs/1910.01465v2
      \endverb
    \endentry
    \entry{zhou2023}{online}{}{}
      \name{author}{8}{}{%
        {{hash=23a8d038a13e60e9376d006d3f9ff961}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Yihe},
           giveni={Y\bibinitperiod}}}%
        {{hash=9b0b8f748e5e6218072feb6d0738e877}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Shunyu},
           giveni={S\bibinitperiod}}}%
        {{hash=a21190ef9f56daf7961575f3c231915d}{%
           family={Qing},
           familyi={Q\bibinitperiod},
           given={Yunpeng},
           giveni={Y\bibinitperiod}}}%
        {{hash=3355e5c28ccf93453b5b4ad11b11f634}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Kaixuan},
           giveni={K\bibinitperiod}}}%
        {{hash=1149e9b799ef6e92e469da707fc20264}{%
           family={Zheng},
           familyi={Z\bibinitperiod},
           given={Tongya},
           giveni={T\bibinitperiod}}}%
        {{hash=0ba704ec82b8ef276a8876277de31250}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Yanhao},
           giveni={Y\bibinitperiod}}}%
        {{hash=f47cc1ff1ed8ea8025326fb247ba534f}{%
           family={Song},
           familyi={S\bibinitperiod},
           given={Jie},
           giveni={J\bibinitperiod}}}%
        {{hash=62f2f4ff0e18b5e38eaf0a72e3e88a86}{%
           family={Song},
           familyi={S\bibinitperiod},
           given={Mingli},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{f08d4fe69f49047305ab3f5ebd5b6599}
      \strng{fullhash}{e7a93bd74d81fe20d51425237dcc6806}
      \strng{fullhashraw}{e7a93bd74d81fe20d51425237dcc6806}
      \strng{bibnamehash}{f08d4fe69f49047305ab3f5ebd5b6599}
      \strng{authorbibnamehash}{f08d4fe69f49047305ab3f5ebd5b6599}
      \strng{authornamehash}{f08d4fe69f49047305ab3f5ebd5b6599}
      \strng{authorfullhash}{e7a93bd74d81fe20d51425237dcc6806}
      \strng{authorfullhashraw}{e7a93bd74d81fe20d51425237dcc6806}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Centralized Training with Decentralized Execution (CTDE) has recently emerged as a popular framework for cooperative Multi-Agent Reinforcement Learning (MARL), where agents can use additional global state information to guide training in a centralized way and make their own decisions only based on decentralized local policies. Despite the encouraging results achieved, CTDE makes an independence assumption on agent policies, which limits agents to adopt global cooperative information from each other during centralized training. Therefore, we argue that existing CTDE methods cannot fully utilize global information for training, leading to an inefficient joint-policy exploration and even suboptimal results. In this paper, we introduce a novel Centralized Advising and Decentralized Pruning (CADP) framework for multi-agent reinforcement learning, that not only enables an efficacious message exchange among agents during training but also guarantees the independent policies for execution. Firstly, CADP endows agents the explicit communication channel to seek and take advices from different agents for more centralized training. To further ensure the decentralized execution, we propose a smooth model pruning mechanism to progressively constraint the agent communication into a closed one without degradation in agent cooperation capability. Empirical evaluations on StarCraft II micromanagement and Google Research Football benchmarks demonstrate that the proposed framework achieves superior performance compared with the state-of-the-art counterparts. Our code will be made publicly available.}
      \field{day}{26}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{5}
      \field{pubstate}{prepublished}
      \field{title}{Is {{Centralized Training}} with {{Decentralized Execution Framework Centralized Enough}} for {{MARL}}?}
      \field{urlday}{25}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2305.17352
      \endverb
      \verb{eprint}
      \verb 2305.17352
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/EMWBIU4Q/Zhou et al_2023_Is Centralized Training with Decentralized Execution Framework Centralized.pdf;/Users/brandonhosley/Zotero/storage/MBC3AKQ9/2305.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2305.17352
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2305.17352
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems}
    \endentry
    \entry{hoang2023}{incollection}{}{}
      \name{author}{6}{}{%
        {{hash=519a2c85c408a53b30730d311071f6da}{%
           family={Hoang},
           familyi={H\bibinitperiod},
           given={Maria-Theresa\bibnamedelima Oanh},
           giveni={M\bibinithyphendelim T\bibinitperiod\bibinitdelim O\bibinitperiod}}}%
        {{hash=3833fbffd346f0912bc0ed96c6dc30b3}{%
           family={Grøntved},
           familyi={G\bibinitperiod},
           given={Kasper\bibnamedelimb Andreas\bibnamedelima Rømer},
           giveni={K\bibinitperiod\bibinitdelim A\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=685b9ea4bbe3cfd182b2f6de14392df7}{%
           family={Van\bibnamedelima Berkel},
           familyi={V\bibinitperiod\bibinitdelim B\bibinitperiod},
           given={Niels},
           giveni={N\bibinitperiod}}}%
        {{hash=9adb34bb3d94b6b03a5e7aeae989aca7}{%
           family={Skov},
           familyi={S\bibinitperiod},
           given={Mikael\bibnamedelima B},
           giveni={M\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=9bcfcdaddcb7ce07a993c8896270eecd}{%
           family={Christensen},
           familyi={C\bibinitperiod},
           given={Anders\bibnamedelima Lyhne},
           giveni={A\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=dc5a47d852a6263c511e4ba4f52ebc1b}{%
           family={Merritt},
           familyi={M\bibinitperiod},
           given={Timothy},
           giveni={T\bibinitperiod}}}%
      }
      \name{editor}{4}{}{%
        {{hash=a234b822a6d76c8a9cb5f078e801d411}{%
           family={Dunstan},
           familyi={D\bibinitperiod},
           given={Belinda\bibnamedelima J.},
           giveni={B\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=3e8eb7293272c2087e88443048b77198}{%
           family={Koh},
           familyi={K\bibinitperiod},
           given={Jeffrey\bibnamedelimb T.\bibnamedelimi K.\bibnamedelimi V.},
           giveni={J\bibinitperiod\bibinitdelim T\bibinitperiod\bibinitdelim K\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
        {{hash=0dbb722262c73e12749c4022266a4c7c}{%
           family={Turnbull\bibnamedelima Tillman},
           familyi={T\bibinitperiod\bibinitdelim T\bibinitperiod},
           given={Deborah},
           giveni={D\bibinitperiod}}}%
        {{hash=a8d06e1e090867d306cf81c9bdd48e85}{%
           family={Brown},
           familyi={B\bibinitperiod},
           given={Scott\bibnamedelima Andrew},
           giveni={S\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cham}%
      }
      \list{publisher}{1}{%
        {Springer International Publishing}%
      }
      \strng{namehash}{bba7e2d066a0f511a3ef8adea78073d2}
      \strng{fullhash}{9cb03b966aadaffa0ad4a6c8f281f01c}
      \strng{fullhashraw}{9cb03b966aadaffa0ad4a6c8f281f01c}
      \strng{bibnamehash}{9cb03b966aadaffa0ad4a6c8f281f01c}
      \strng{authorbibnamehash}{9cb03b966aadaffa0ad4a6c8f281f01c}
      \strng{authornamehash}{bba7e2d066a0f511a3ef8adea78073d2}
      \strng{authorfullhash}{9cb03b966aadaffa0ad4a6c8f281f01c}
      \strng{authorfullhashraw}{9cb03b966aadaffa0ad4a6c8f281f01c}
      \strng{editorbibnamehash}{a1cf2c6552df80185d8e46469a5a17b8}
      \strng{editornamehash}{b310fd650be253b5989b532de2bab857}
      \strng{editorfullhash}{a1cf2c6552df80185d8e46469a5a17b8}
      \strng{editorfullhashraw}{a1cf2c6552df80185d8e46469a5a17b8}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{booktitle}{Cultural {{Robotics}}: {{Social Robots}} and {{Their Emergent Cultural Ecologies}}}
      \field{isbn}{978-3-031-28137-2 978-3-031-28138-9}
      \field{langid}{english}
      \field{shorttitle}{Drone {{Swarms}} to {{Support Search}} and {{Rescue Operations}}}
      \field{title}{Drone {{Swarms}} to {{Support Search}} and {{Rescue Operations}}: {{Opportunities}} and {{Challenges}}}
      \field{urlday}{12}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{163\bibrangedash 176}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1007/978-3-031-28138-9_11
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/EWGATS8U/Hoang et al. - 2023 - Drone Swarms to Support Search and Rescue Operatio.pdf
      \endverb
      \verb{urlraw}
      \verb https://link.springer.com/10.1007/978-3-031-28138-9_11
      \endverb
      \verb{url}
      \verb https://link.springer.com/10.1007/978-3-031-28138-9_11
      \endverb
    \endentry
    \entry{zhong2024}{article}{}{}
      \name{author}{6}{}{%
        {{hash=b95d1a2055e09b96f430caf3d4d35616}{%
           family={Zhong},
           familyi={Z\bibinitperiod},
           given={Yifan},
           giveni={Y\bibinitperiod}}}%
        {{hash=0fb5c75e0d48bc9eacbbc94daa482547}{%
           family={Kuba},
           familyi={K\bibinitperiod},
           given={Jakub\bibnamedelima Grudzien},
           giveni={J\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=0e1c640014bac9108baa5a3600006e86}{%
           family={Feng},
           familyi={F\bibinitperiod},
           given={Xidong},
           giveni={X\bibinitperiod}}}%
        {{hash=ee936fa0fa279c00e808275482941636}{%
           family={Hu},
           familyi={H\bibinitperiod},
           given={Siyi},
           giveni={S\bibinitperiod}}}%
        {{hash=f08383385e5e86d608bf053e10579e54}{%
           family={Ji},
           familyi={J\bibinitperiod},
           given={Jiaming},
           giveni={J\bibinitperiod}}}%
        {{hash=616d3b0b19e3536f5c65dfab2a48a659}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Yaodong},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{2d1f4bb5b8a73da311fb4b059b2b8b41}
      \strng{fullhash}{811d441c2dcb8ddce046201441d67105}
      \strng{fullhashraw}{811d441c2dcb8ddce046201441d67105}
      \strng{bibnamehash}{811d441c2dcb8ddce046201441d67105}
      \strng{authorbibnamehash}{811d441c2dcb8ddce046201441d67105}
      \strng{authornamehash}{2d1f4bb5b8a73da311fb4b059b2b8b41}
      \strng{authorfullhash}{811d441c2dcb8ddce046201441d67105}
      \strng{authorfullhashraw}{811d441c2dcb8ddce046201441d67105}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Journal of Machine Learning Research}
      \field{langid}{english}
      \field{number}{32}
      \field{shortjournal}{JMLR}
      \field{title}{Heterogeneous-{{Agent Reinforcement Learning}}}
      \field{volume}{25}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{pages}{1\bibrangedash 67}
      \range{pages}{67}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/RP4HSFZU/Zhong et al. - Heterogeneous-Agent Reinforcement Learning.pdf
      \endverb
      \verb{urlraw}
      \verb http://jmlr.org/papers/v25/23-0488.html
      \endverb
      \verb{url}
      \verb http://jmlr.org/papers/v25/23-0488.html
      \endverb
    \endentry
    \entry{smit2023}{article}{}{}
      \name{author}{4}{}{%
        {{hash=4b49a4ae46f23bdc584e1f6576d46f1d}{%
           family={Smit},
           familyi={S\bibinitperiod},
           given={Andries},
           giveni={A\bibinitperiod}}}%
        {{hash=1ad14539972beb801f05fa896097879e}{%
           family={Engelbrecht},
           familyi={E\bibinitperiod},
           given={Herman\bibnamedelima A.},
           giveni={H\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=03ceb98a1190ebc6471f95bf82e2c060}{%
           family={Brink},
           familyi={B\bibinitperiod},
           given={Willie},
           giveni={W\bibinitperiod}}}%
        {{hash=eb6e9b62595b1500b93f495d875bb6c2}{%
           family={Pretorius},
           familyi={P\bibinitperiod},
           given={Arnu},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{e57bd41a1b18a599096ba2b49b452d55}
      \strng{fullhash}{34a9e177b906a01ef2c329e82775bf13}
      \strng{fullhashraw}{34a9e177b906a01ef2c329e82775bf13}
      \strng{bibnamehash}{34a9e177b906a01ef2c329e82775bf13}
      \strng{authorbibnamehash}{34a9e177b906a01ef2c329e82775bf13}
      \strng{authornamehash}{e57bd41a1b18a599096ba2b49b452d55}
      \strng{authorfullhash}{34a9e177b906a01ef2c329e82775bf13}
      \strng{authorfullhashraw}{34a9e177b906a01ef2c329e82775bf13}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Robotic football has long been seen as a grand challenge in artificial intelligence. Despite recent success of learned policies over heuristics and handcrafted rules in general, current teams in the simulated RoboCup football leagues, where autonomous agents compete against each other, still rely on handcrafted strategies with only a few using reinforcement learning directly. This limits a learning agent’s ability to find stronger high-level strategies for the full game. In this paper, we show that it is possible for agents to learn competent football strategies on a full 22 player setting using limited computation resources (one GPU and one CPU), from tabula rasa through self-play. To do this, we build a 2D football simulator with faster simulation times than the RoboCup simulator. We propose various improvements to the standard single-agent PPO training algorithm which help it scale to our multi-agent setting. These improvements include (1) using a policy and critic network with an attention mechanism that scales linearly in the number of agents, (2) sharing networks between agents which allow for faster throughput using batching, and (3) using Polyak averaged opponents, league opponents and freezing the opponent team when necessary. We show through experimental results that stable training in the full 22 player setting is possible. Agents trained in the 22 player setting learn to defeat a variety of handcrafted strategies, and also achieve a higher win rate compared to agents trained in the 4 player setting and evaluated in the full game.}
      \field{annotation}{https://github.com/DriesSmit/MARL2DSoccer}
      \field{day}{24}
      \field{issn}{1573-7454}
      \field{journaltitle}{Autonomous Agents and Multi-Agent Systems}
      \field{langid}{english}
      \field{month}{3}
      \field{number}{1}
      \field{shortjournal}{Auton Agent Multi-Agent Syst}
      \field{title}{Scaling Multi-Agent Reinforcement Learning to Full 11 versus 11 Simulated Robotic Football}
      \field{urlday}{16}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{37}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{20}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1007/s10458-023-09603-y
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/5UPXMZ63/Smit et al. - 2023 - Scaling multi-agent reinforcement learning to full.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s10458-023-09603-y
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s10458-023-09603-y
      \endverb
      \keyw{11 versus 11 football,Multi-agent reinforcement learning,RoboCup,RoboCup 2D,Simulated robotic football,Soccer}
    \endentry
    \entry{papoudakis2021}{online}{}{}
      \name{author}{4}{}{%
        {{hash=d0631db9b5ca2e6150a04082663ca08d}{%
           family={Papoudakis},
           familyi={P\bibinitperiod},
           given={Georgios},
           giveni={G\bibinitperiod}}}%
        {{hash=f355421623a9ce0d54965817f05f6ad0}{%
           family={Christianos},
           familyi={C\bibinitperiod},
           given={Filippos},
           giveni={F\bibinitperiod}}}%
        {{hash=c32fcecebc45af27f83fbcfea1c93b31}{%
           family={Schäfer},
           familyi={S\bibinitperiod},
           given={Lukas},
           giveni={L\bibinitperiod}}}%
        {{hash=9da4f3f413d514d731efe34067976909}{%
           family={Albrecht},
           familyi={A\bibinitperiod},
           given={Stefano\bibnamedelima V.},
           giveni={S\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
      }
      \strng{namehash}{f2473647f2bd5df0777ba1def078b0af}
      \strng{fullhash}{fb48708063b34a78b84029004ce47bdb}
      \strng{fullhashraw}{fb48708063b34a78b84029004ce47bdb}
      \strng{bibnamehash}{fb48708063b34a78b84029004ce47bdb}
      \strng{authorbibnamehash}{fb48708063b34a78b84029004ce47bdb}
      \strng{authornamehash}{f2473647f2bd5df0777ba1def078b0af}
      \strng{authorfullhash}{fb48708063b34a78b84029004ce47bdb}
      \strng{authorfullhashraw}{fb48708063b34a78b84029004ce47bdb}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Multi-agent deep reinforcement learning (MARL) suffers from a lack of commonly-used evaluation tasks and criteria, making comparisons between approaches difficult. In this work, we provide a systematic evaluation and comparison of three different classes of MARL algorithms (independent learning, centralised multi-agent policy gradient, value decomposition) in a diverse range of cooperative multi-agent learning tasks. Our experiments serve as a reference for the expected performance of algorithms across different learning tasks, and we provide insights regarding the effectiveness of different learning approaches. We open-source EPyMARL, which extends the PyMARL codebase to include additional algorithms and allow for flexible configuration of algorithm implementation details such as parameter sharing. Finally, we open-source two environments for multi-agent research which focus on coordination under sparse rewards.}
      \field{day}{9}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arXiv}
      \field{month}{11}
      \field{pubstate}{prepublished}
      \field{title}{Benchmarking {{Multi-Agent Deep Reinforcement Learning Algorithms}} in {{Cooperative Tasks}}}
      \field{urlday}{28}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2006.07869
      \endverb
      \verb{eprint}
      \verb 2006.07869
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/X55NBJGK/Papoudakis et al_2021_Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative.pdf;/Users/brandonhosley/Zotero/storage/9S95S7UZ/2006.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2006.07869
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2006.07869
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning}
    \endentry
    \entry{iqbal2021}{online}{}{}
      \name{author}{6}{}{%
        {{hash=7fb7c7ca1c2dd528601fdff6327cb704}{%
           family={Iqbal},
           familyi={I\bibinitperiod},
           given={Shariq},
           giveni={S\bibinitperiod}}}%
        {{hash=33ca447eeab5880cac6e91d2f93b47c1}{%
           family={Witt},
           familyi={W\bibinitperiod},
           given={Christian\bibnamedelimb A.\bibnamedelimi Schroeder},
           giveni={C\bibinitperiod\bibinitdelim A\bibinitperiod\bibinitdelim S\bibinitperiod},
           prefix={de},
           prefixi={d\bibinitperiod}}}%
        {{hash=09355ad427f98680e1f04f5266d4d479}{%
           family={Peng},
           familyi={P\bibinitperiod},
           given={Bei},
           giveni={B\bibinitperiod}}}%
        {{hash=c0cf2fea367eee2f127eef13920b442b}{%
           family={Böhmer},
           familyi={B\bibinitperiod},
           given={Wendelin},
           giveni={W\bibinitperiod}}}%
        {{hash=0d05819cf2b4fe22ba972c9b2b5d8c9d}{%
           family={Whiteson},
           familyi={W\bibinitperiod},
           given={Shimon},
           giveni={S\bibinitperiod}}}%
        {{hash=86e3e396d826309a838ac02c93e21550}{%
           family={Sha},
           familyi={S\bibinitperiod},
           given={Fei},
           giveni={F\bibinitperiod}}}%
      }
      \strng{namehash}{f19915e7ec1063c149806dd45d1f01df}
      \strng{fullhash}{4a46a7ad96a79c0a2f091b13cd89351c}
      \strng{fullhashraw}{4a46a7ad96a79c0a2f091b13cd89351c}
      \strng{bibnamehash}{4a46a7ad96a79c0a2f091b13cd89351c}
      \strng{authorbibnamehash}{4a46a7ad96a79c0a2f091b13cd89351c}
      \strng{authornamehash}{f19915e7ec1063c149806dd45d1f01df}
      \strng{authorfullhash}{4a46a7ad96a79c0a2f091b13cd89351c}
      \strng{authorfullhashraw}{4a46a7ad96a79c0a2f091b13cd89351c}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Multi-agent settings in the real world often involve tasks with varying types and quantities of agents and non-agent entities; however, common patterns of behavior often emerge among these agents/entities. Our method aims to leverage these commonalities by asking the question: ``What is the expected utility of each agent when only considering a randomly selected sub-group of its observed entities?'' By posing this counterfactual question, we can recognize state-action trajectories within sub-groups of entities that we may have encountered in another task and use what we learned in that task to inform our prediction in the current one. We then reconstruct a prediction of the full returns as a combination of factors considering these disjoint groups of entities and train this ``randomly factorized" value function as an auxiliary objective for value-based multi-agent reinforcement learning. By doing so, our model can recognize and leverage similarities across tasks to improve learning efficiency in a multi-task setting. Our approach, Randomized Entity-wise Factorization for Imagined Learning (REFIL), outperforms all strong baselines by a significant margin in challenging multi-task StarCraft micromanagement settings.}
      \field{day}{11}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{6}
      \field{pubstate}{prepublished}
      \field{title}{Randomized {{Entity-wise Factorization}} for {{Multi-Agent Reinforcement Learning}}}
      \field{urlday}{14}
      \field{urlmonth}{2}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2006.04222
      \endverb
      \verb{eprint}
      \verb 2006.04222
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/K3BBXYGS/Iqbal et al_2021_Randomized Entity-wise Factorization for Multi-Agent Reinforcement Learning.pdf;/Users/brandonhosley/Zotero/storage/H46GEC6S/2006.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2006.04222
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2006.04222
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning}
    \endentry
    \entry{foerster2017}{online}{}{}
      \name{author}{5}{}{%
        {{hash=d62c75690ab7c754f6e7217a242a4318}{%
           family={Foerster},
           familyi={F\bibinitperiod},
           given={Jakob},
           giveni={J\bibinitperiod}}}%
        {{hash=de7fc27bb6e6105440cd3039a5c9b684}{%
           family={Farquhar},
           familyi={F\bibinitperiod},
           given={Gregory},
           giveni={G\bibinitperiod}}}%
        {{hash=c15e1ba2ced206763fad02b814aa569e}{%
           family={Afouras},
           familyi={A\bibinitperiod},
           given={Triantafyllos},
           giveni={T\bibinitperiod}}}%
        {{hash=a07891d2d6d3da5f1827c01269cc6da6}{%
           family={Nardelli},
           familyi={N\bibinitperiod},
           given={Nantas},
           giveni={N\bibinitperiod}}}%
        {{hash=0d05819cf2b4fe22ba972c9b2b5d8c9d}{%
           family={Whiteson},
           familyi={W\bibinitperiod},
           given={Shimon},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{bacd311c6a08d138cde55b969b44b0f3}
      \strng{fullhash}{9b69be1387a30ea5191e83c832c30ed5}
      \strng{fullhashraw}{9b69be1387a30ea5191e83c832c30ed5}
      \strng{bibnamehash}{9b69be1387a30ea5191e83c832c30ed5}
      \strng{authorbibnamehash}{9b69be1387a30ea5191e83c832c30ed5}
      \strng{authornamehash}{bacd311c6a08d138cde55b969b44b0f3}
      \strng{authorfullhash}{9b69be1387a30ea5191e83c832c30ed5}
      \strng{authorfullhashraw}{9b69be1387a30ea5191e83c832c30ed5}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Cooperative multi-agent systems can be naturally used to model many real world problems, such as network packet routing and the coordination of autonomous vehicles. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.}
      \field{day}{14}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{12}
      \field{pubstate}{prepublished}
      \field{title}{Counterfactual {{Multi-Agent Policy Gradients}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1705.08926
      \endverb
      \verb{eprint}
      \verb 1705.08926
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/2XKJVLZB/Foerster et al_2017_Counterfactual Multi-Agent Policy Gradients.pdf;/Users/brandonhosley/Zotero/storage/CUA5ZIGJ/1705.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1705.08926
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1705.08926
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems}
    \endentry
    \entry{shoham2007}{article}{}{}
      \name{author}{3}{}{%
        {{hash=774e2e16526f33f383bed004b13bf326}{%
           family={Shoham},
           familyi={S\bibinitperiod},
           given={Yoav},
           giveni={Y\bibinitperiod}}}%
        {{hash=999a732f5e7d7aca84c5ff18e80f648d}{%
           family={Powers},
           familyi={P\bibinitperiod},
           given={Rob},
           giveni={R\bibinitperiod}}}%
        {{hash=978fa1abbba10679dc3d70d791eb1b43}{%
           family={Grenager},
           familyi={G\bibinitperiod},
           given={Trond},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{5b9f6f0e7739f6b355afc40ad1447f10}
      \strng{fullhash}{dd593b12b6198960022535ccb3413a01}
      \strng{fullhashraw}{dd593b12b6198960022535ccb3413a01}
      \strng{bibnamehash}{dd593b12b6198960022535ccb3413a01}
      \strng{authorbibnamehash}{dd593b12b6198960022535ccb3413a01}
      \strng{authornamehash}{5b9f6f0e7739f6b355afc40ad1447f10}
      \strng{authorfullhash}{dd593b12b6198960022535ccb3413a01}
      \strng{authorfullhashraw}{dd593b12b6198960022535ccb3413a01}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The area of learning in multi-agent systems is today one of the most fertile grounds for interaction between game theory and artificial intelligence. We focus on the foundational questions in this interdisciplinary area, and identify several distinct agendas that ought to, we argue, be separated. The goal of this article is to start a discussion in the research community that will result in firmer foundations for the area.1 © 2007 Published by Elsevier B.V.}
      \field{issn}{00043702}
      \field{journaltitle}{Artificial Intelligence}
      \field{langid}{english}
      \field{month}{5}
      \field{number}{7}
      \field{shortjournal}{Artificial Intelligence}
      \field{title}{If Multi-Agent Learning Is the Answer, What Is the Question?}
      \field{urlday}{12}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{volume}{171}
      \field{year}{2007}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{365\bibrangedash 377}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1016/j.artint.2006.02.006
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/722AZMMT/Shoham et al. - 2007 - If multi-agent learning is the answer, what is the.pdf
      \endverb
      \verb{urlraw}
      \verb https://linkinghub.elsevier.com/retrieve/pii/S0004370207000495
      \endverb
      \verb{url}
      \verb https://linkinghub.elsevier.com/retrieve/pii/S0004370207000495
      \endverb
    \endentry
    \entry{gupta2017}{inproceedings}{}{}
      \name{author}{3}{}{%
        {{hash=fe52f7f3365727a86a07b7c519c958b1}{%
           family={Gupta},
           familyi={G\bibinitperiod},
           given={Jayesh\bibnamedelima K},
           giveni={J\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=f54d2cf50019ddff2faf8a3ff0b086fd}{%
           family={Egorov},
           familyi={E\bibinitperiod},
           given={Maxim},
           giveni={M\bibinitperiod}}}%
        {{hash=e36ad022497396d3ca41ea1594ac09ec}{%
           family={Kochenderfer},
           familyi={K\bibinitperiod},
           given={Mykel},
           giveni={M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{d9d07e34ca39a7a3342b5a69a6e64a11}
      \strng{fullhash}{e96796c55326b509d6061753f49c4b82}
      \strng{fullhashraw}{e96796c55326b509d6061753f49c4b82}
      \strng{bibnamehash}{e96796c55326b509d6061753f49c4b82}
      \strng{authorbibnamehash}{e96796c55326b509d6061753f49c4b82}
      \strng{authornamehash}{d9d07e34ca39a7a3342b5a69a6e64a11}
      \strng{authorfullhash}{e96796c55326b509d6061753f49c4b82}
      \strng{authorfullhashraw}{e96796c55326b509d6061753f49c4b82}
      \field{extraname}{1}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{International Conference on Autonomous Agents and Multiagent Systems}
      \field{title}{Cooperative Multi-Agent Control Using Deep Reinforcement Learning}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{pages}{66\bibrangedash 83}
      \range{pages}{18}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/2FJVAMV9/Gupta et al_2017_Cooperative multi-agent control using deep reinforcement learning.pdf
      \endverb
    \endentry
    \entry{zheng2017}{online}{}{}
      \name{author}{6}{}{%
        {{hash=0beb8f75088bfac1e81ca3840d0e89f0}{%
           family={Zheng},
           familyi={Z\bibinitperiod},
           given={Lianmin},
           giveni={L\bibinitperiod}}}%
        {{hash=8b073ea281f7b7362369e25115e52b8f}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Jiacheng},
           giveni={J\bibinitperiod}}}%
        {{hash=2b7464d9067c81c55766d572c1785250}{%
           family={Cai},
           familyi={C\bibinitperiod},
           given={Han},
           giveni={H\bibinitperiod}}}%
        {{hash=fbc51a6317f158547173b93086d9b1a2}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Weinan},
           giveni={W\bibinitperiod}}}%
        {{hash=2f3ea981fa5a715a69118b48e576a9f5}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Jun},
           giveni={J\bibinitperiod}}}%
        {{hash=c447a713c11f58a40c2a899040774a98}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Yong},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{293dcbfb5b9446f7f90cf8d346a542fe}
      \strng{fullhash}{2b6ef308e477b824f568f49af12aca7b}
      \strng{fullhashraw}{2b6ef308e477b824f568f49af12aca7b}
      \strng{bibnamehash}{2b6ef308e477b824f568f49af12aca7b}
      \strng{authorbibnamehash}{2b6ef308e477b824f568f49af12aca7b}
      \strng{authornamehash}{293dcbfb5b9446f7f90cf8d346a542fe}
      \strng{authorfullhash}{2b6ef308e477b824f568f49af12aca7b}
      \strng{authorfullhashraw}{2b6ef308e477b824f568f49af12aca7b}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We introduce MAgent, a platform to support research and development of many-agent reinforcement learning. Unlike previous research platforms on single or multi-agent reinforcement learning, MAgent focuses on supporting the tasks and the applications that require hundreds to millions of agents. Within the interactions among a population of agents, it enables not only the study of learning algorithms for agents' optimal polices, but more importantly, the observation and understanding of individual agent's behaviors and social phenomena emerging from the AI society, including communication languages, leaderships, altruism. MAgent is highly scalable and can host up to one million agents on a single GPU server. MAgent also provides flexible configurations for AI researchers to design their customized environments and agents. In this demo, we present three environments designed on MAgent and show emerged collective intelligence by learning from scratch.}
      \field{day}{2}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{12}
      \field{pubstate}{prepublished}
      \field{shorttitle}{{{MAgent}}}
      \field{title}{{{MAgent}}: {{A Many-Agent Reinforcement Learning Platform}} for {{Artificial Collective Intelligence}}}
      \field{urlday}{25}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1712.00600
      \endverb
      \verb{eprint}
      \verb 1712.00600
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/6KKHUIZ4/Zheng et al_2017_MAgent.pdf;/Users/brandonhosley/Zotero/storage/UU2EXIJC/1712.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1712.00600
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1712.00600
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems}
    \endentry
    \entry{terry2021}{online}{}{}
      \name{author}{13}{}{%
        {{hash=6227c82f39a2c06c345e7fafd8e8e6cc}{%
           family={Terry},
           familyi={T\bibinitperiod},
           given={J.\bibnamedelimi K.},
           giveni={J\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=bfd91affe76615f3a790a36f0715a769}{%
           family={Black},
           familyi={B\bibinitperiod},
           given={Benjamin},
           giveni={B\bibinitperiod}}}%
        {{hash=562720b3b3dc0da7beff80f5a9658a96}{%
           family={Grammel},
           familyi={G\bibinitperiod},
           given={Nathaniel},
           giveni={N\bibinitperiod}}}%
        {{hash=c6b7ed7facca2921b67761e33e954c30}{%
           family={Jayakumar},
           familyi={J\bibinitperiod},
           given={Mario},
           giveni={M\bibinitperiod}}}%
        {{hash=ea644330b231fbdbe9a3f5ad956fb601}{%
           family={Hari},
           familyi={H\bibinitperiod},
           given={Ananth},
           giveni={A\bibinitperiod}}}%
        {{hash=da9fc58d71c244b06f2afe82f94430a7}{%
           family={Sullivan},
           familyi={S\bibinitperiod},
           given={Ryan},
           giveni={R\bibinitperiod}}}%
        {{hash=7d8a57fad01c681de6134f0135f6336c}{%
           family={Santos},
           familyi={S\bibinitperiod},
           given={Luis},
           giveni={L\bibinitperiod}}}%
        {{hash=356a6213ea7de064fa570072df596cd1}{%
           family={Perez},
           familyi={P\bibinitperiod},
           given={Rodrigo},
           giveni={R\bibinitperiod}}}%
        {{hash=cbb7c6298f0b856d468231591ac8bc87}{%
           family={Horsch},
           familyi={H\bibinitperiod},
           given={Caroline},
           giveni={C\bibinitperiod}}}%
        {{hash=66cae0bed81aff7faa72e799a98883e8}{%
           family={Dieffendahl},
           familyi={D\bibinitperiod},
           given={Clemens},
           giveni={C\bibinitperiod}}}%
        {{hash=15a75a6608ae12ea4a04455ea8348c5d}{%
           family={Williams},
           familyi={W\bibinitperiod},
           given={Niall\bibnamedelima L.},
           giveni={N\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=97e09e24277306103d594812cbf50ea2}{%
           family={Lokesh},
           familyi={L\bibinitperiod},
           given={Yashas},
           giveni={Y\bibinitperiod}}}%
        {{hash=5be170706e483e6984f1f174c31fbc59}{%
           family={Ravi},
           familyi={R\bibinitperiod},
           given={Praveen},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{68cf4dcac41c5e149332a5e77e1ffb3a}
      \strng{fullhash}{54a94de3c57b2fbf7c47f6bf069b96a4}
      \strng{fullhashraw}{54a94de3c57b2fbf7c47f6bf069b96a4}
      \strng{bibnamehash}{68cf4dcac41c5e149332a5e77e1ffb3a}
      \strng{authorbibnamehash}{68cf4dcac41c5e149332a5e77e1ffb3a}
      \strng{authornamehash}{68cf4dcac41c5e149332a5e77e1ffb3a}
      \strng{authorfullhash}{54a94de3c57b2fbf7c47f6bf069b96a4}
      \strng{authorfullhashraw}{54a94de3c57b2fbf7c47f6bf069b96a4}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{This paper introduces the PettingZoo library and the accompanying Agent Environment Cycle ("AEC") games model. PettingZoo is a library of diverse sets of multi-agent environments with a universal, elegant Python API. PettingZoo was developed with the goal of accelerating research in Multi-Agent Reinforcement Learning ("MARL"), by making work more interchangeable, accessible and reproducible akin to what OpenAI's Gym library did for single-agent reinforcement learning. PettingZoo's API, while inheriting many features of Gym, is unique amongst MARL APIs in that it's based around the novel AEC games model. We argue, in part through case studies on major problems in popular MARL environments, that the popular game models are poor conceptual models of games commonly used in MARL and accordingly can promote confusing bugs that are hard to detect, and that the AEC games model addresses these problems.}
      \field{day}{26}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{10}
      \field{pubstate}{prepublished}
      \field{shorttitle}{{{PettingZoo}}}
      \field{title}{{{PettingZoo}}: {{Gym}} for {{Multi-Agent Reinforcement Learning}}}
      \field{urlday}{25}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2009.14471
      \endverb
      \verb{eprint}
      \verb 2009.14471
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/BUBMKBB8/Terry et al. - 2021 - PettingZoo Gym for Multi-Agent Reinforcement Lear.pdf;/Users/brandonhosley/Zotero/storage/YVZNGRAH/2009.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2009.14471
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2009.14471
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning}
    \endentry
    \entry{samvelyan2019}{online}{useprefix=true}{}
      \name{author}{10}{}{%
        {{hash=c76172ee0c2ebda81796624b4a81ae54}{%
           family={Samvelyan},
           familyi={S\bibinitperiod},
           given={Mikayel},
           giveni={M\bibinitperiod}}}%
        {{hash=8eb7df23daba9c7bf22402026effdae7}{%
           family={Rashid},
           familyi={R\bibinitperiod},
           given={Tabish},
           giveni={T\bibinitperiod}}}%
        {{hash=d056cdb8ed6bebb55e13c2246059af79}{%
           family={Witt},
           familyi={W\bibinitperiod},
           given={Christian\bibnamedelima Schroeder},
           giveni={C\bibinitperiod\bibinitdelim S\bibinitperiod},
           prefix={de},
           prefixi={d\bibinitperiod}}}%
        {{hash=de7fc27bb6e6105440cd3039a5c9b684}{%
           family={Farquhar},
           familyi={F\bibinitperiod},
           given={Gregory},
           giveni={G\bibinitperiod}}}%
        {{hash=a07891d2d6d3da5f1827c01269cc6da6}{%
           family={Nardelli},
           familyi={N\bibinitperiod},
           given={Nantas},
           giveni={N\bibinitperiod}}}%
        {{hash=2b8a7b8bb154710457a1fe68adc9cb07}{%
           family={Rudner},
           familyi={R\bibinitperiod},
           given={Tim\bibnamedelimb G.\bibnamedelimi J.},
           giveni={T\bibinitperiod\bibinitdelim G\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=ca083f59b63a6ee2837027775301af6b}{%
           family={Hung},
           familyi={H\bibinitperiod},
           given={Chia-Man},
           giveni={C\bibinithyphendelim M\bibinitperiod}}}%
        {{hash=3269ee96fb4b61a52f4c01b02fc0751c}{%
           family={Torr},
           familyi={T\bibinitperiod},
           given={Philip\bibnamedelimb H.\bibnamedelimi S.},
           giveni={P\bibinitperiod\bibinitdelim H\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=d62c75690ab7c754f6e7217a242a4318}{%
           family={Foerster},
           familyi={F\bibinitperiod},
           given={Jakob},
           giveni={J\bibinitperiod}}}%
        {{hash=0d05819cf2b4fe22ba972c9b2b5d8c9d}{%
           family={Whiteson},
           familyi={W\bibinitperiod},
           given={Shimon},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{db017d19c7fbc59e66933da4bbd4deeb}
      \strng{fullhash}{439111c8508bc2979f9a0c48a59c96da}
      \strng{fullhashraw}{439111c8508bc2979f9a0c48a59c96da}
      \strng{bibnamehash}{db017d19c7fbc59e66933da4bbd4deeb}
      \strng{authorbibnamehash}{db017d19c7fbc59e66933da4bbd4deeb}
      \strng{authornamehash}{db017d19c7fbc59e66933da4bbd4deeb}
      \strng{authorfullhash}{439111c8508bc2979f9a0c48a59c96da}
      \strng{authorfullhashraw}{439111c8508bc2979f9a0c48a59c96da}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In the last few years, deep multi-agent reinforcement learning (RL) has become a highly active area of research. A particularly challenging class of problems in this area is partially observable, cooperative, multi-agent learning, in which teams of agents must learn to coordinate their behaviour while conditioning only on their private observations. This is an attractive research area since such problems are relevant to a large number of real-world systems and are also more amenable to evaluation than general-sum problems. Standardised environments such as the ALE and MuJoCo have allowed single-agent RL to move beyond toy domains, such as grid worlds. However, there is no comparable benchmark for cooperative multi-agent RL. As a result, most papers in this field use one-off toy problems, making it difficult to measure real progress. In this paper, we propose the StarCraft Multi-Agent Challenge (SMAC) as a benchmark problem to fill this gap. SMAC is based on the popular real-time strategy game StarCraft II and focuses on micromanagement challenges where each unit is controlled by an independent agent that must act based on local observations. We offer a diverse set of challenge maps and recommendations for best practices in benchmarking and evaluations. We also open-source a deep multi-agent RL learning framework including state-of-the-art algorithms. We believe that SMAC can provide a standard benchmark environment for years to come. Videos of our best agents for several SMAC scenarios are available at: https://youtu.be/VZ7zmQ\_obZ0.}
      \field{day}{9}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arXiv}
      \field{month}{12}
      \field{pubstate}{prepublished}
      \field{title}{The {{StarCraft Multi-Agent Challenge}}}
      \field{urlday}{26}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1902.04043
      \endverb
      \verb{eprint}
      \verb 1902.04043
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/TJCL63RQ/Samvelyan et al_2019_The StarCraft Multi-Agent Challenge.pdf;/Users/brandonhosley/Zotero/storage/N4RKHKPB/1902.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1902.04043
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1902.04043
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning}
    \endentry
    \entry{kurach2020}{online}{}{}
      \name{author}{11}{}{%
        {{hash=5de3bfed29969b7034c3e7108031b1c7}{%
           family={Kurach},
           familyi={K\bibinitperiod},
           given={Karol},
           giveni={K\bibinitperiod}}}%
        {{hash=b456173e93f42e0bb093f27668a968f7}{%
           family={Raichuk},
           familyi={R\bibinitperiod},
           given={Anton},
           giveni={A\bibinitperiod}}}%
        {{hash=b20f72a4478d4b8236071f03c27d1867}{%
           family={Stańczyk},
           familyi={S\bibinitperiod},
           given={Piotr},
           giveni={P\bibinitperiod}}}%
        {{hash=7cf4267d81dee550aa8b02092fb5afa3}{%
           family={Zając},
           familyi={Z\bibinitperiod},
           given={Michał},
           giveni={M\bibinitperiod}}}%
        {{hash=b8d6ee5b98a48b04ccf119b64508d63c}{%
           family={Bachem},
           familyi={B\bibinitperiod},
           given={Olivier},
           giveni={O\bibinitperiod}}}%
        {{hash=31f15faeab280fcda1466b9710381955}{%
           family={Espeholt},
           familyi={E\bibinitperiod},
           given={Lasse},
           giveni={L\bibinitperiod}}}%
        {{hash=cee59af5363f5cb5e5e95d437dd91534}{%
           family={Riquelme},
           familyi={R\bibinitperiod},
           given={Carlos},
           giveni={C\bibinitperiod}}}%
        {{hash=917cc9d59f6ad3c51ab0377febd20143}{%
           family={Vincent},
           familyi={V\bibinitperiod},
           given={Damien},
           giveni={D\bibinitperiod}}}%
        {{hash=508b11a7934e1a9be0357f29469ea2f4}{%
           family={Michalski},
           familyi={M\bibinitperiod},
           given={Marcin},
           giveni={M\bibinitperiod}}}%
        {{hash=cf4fea95321213e7dc3532fd33b85112}{%
           family={Bousquet},
           familyi={B\bibinitperiod},
           given={Olivier},
           giveni={O\bibinitperiod}}}%
        {{hash=450963f966620925cb8fecd32f7a6ee1}{%
           family={Gelly},
           familyi={G\bibinitperiod},
           given={Sylvain},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{e36a6e812ea86429ee245c524b240c2d}
      \strng{fullhash}{8a887d04fcd8fe1bed1cd5ea3773dfc8}
      \strng{fullhashraw}{8a887d04fcd8fe1bed1cd5ea3773dfc8}
      \strng{bibnamehash}{e36a6e812ea86429ee245c524b240c2d}
      \strng{authorbibnamehash}{e36a6e812ea86429ee245c524b240c2d}
      \strng{authornamehash}{e36a6e812ea86429ee245c524b240c2d}
      \strng{authorfullhash}{8a887d04fcd8fe1bed1cd5ea3773dfc8}
      \strng{authorfullhashraw}{8a887d04fcd8fe1bed1cd5ea3773dfc8}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Recent progress in the field of reinforcement learning has been accelerated by virtual learning environments such as video games, where novel algorithms and ideas can be quickly tested in a safe and reproducible manner. We introduce the Google Research Football Environment, a new reinforcement learning environment where agents are trained to play football in an advanced, physics-based 3D simulator. The resulting environment is challenging, easy to use and customize, and it is available under a permissive open-source license. In addition, it provides support for multiplayer and multi-agent experiments. We propose three full-game scenarios of varying difficulty with the Football Benchmarks and report baseline results for three commonly used reinforcement algorithms (IMPALA, PPO, and Ape-X DQN). We also provide a diverse set of simpler scenarios with the Football Academy and showcase several promising research directions.}
      \field{day}{14}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arXiv}
      \field{month}{4}
      \field{pubstate}{prepublished}
      \field{shorttitle}{Google {{Research Football}}}
      \field{title}{Google {{Research Football}}: {{A Novel Reinforcement Learning Environment}}}
      \field{urlday}{18}
      \field{urlmonth}{6}
      \field{urlyear}{2024}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1907.11180
      \endverb
      \verb{eprint}
      \verb 1907.11180
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/DNBMDCA2/Kurach et al_2020_Google Research Football.pdf;/Users/brandonhosley/Zotero/storage/3X73G4DX/1907.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1907.11180
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1907.11180
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{canese2021}{article}{}{}
      \name{author}{7}{}{%
        {{hash=1e9cd968439d1ce1a4811ac247d39bf0}{%
           family={Canese},
           familyi={C\bibinitperiod},
           given={Lorenzo},
           giveni={L\bibinitperiod}}}%
        {{hash=38d35bd3858486c4ed1e58f9ab8d07c6}{%
           family={Cardarilli},
           familyi={C\bibinitperiod},
           given={Gian\bibnamedelima Carlo},
           giveni={G\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
        {{hash=8a94f6be09a7243131fb661f3b9ca83f}{%
           family={Di\bibnamedelima Nunzio},
           familyi={D\bibinitperiod\bibinitdelim N\bibinitperiod},
           given={Luca},
           giveni={L\bibinitperiod}}}%
        {{hash=95606a870779f6783427c94bf660365a}{%
           family={Fazzolari},
           familyi={F\bibinitperiod},
           given={Rocco},
           giveni={R\bibinitperiod}}}%
        {{hash=46d05bf67ac60105ff28759467e84253}{%
           family={Giardino},
           familyi={G\bibinitperiod},
           given={Daniele},
           giveni={D\bibinitperiod}}}%
        {{hash=51a48be033404de5c2027a90f13ada68}{%
           family={Re},
           familyi={R\bibinitperiod},
           given={Marco},
           giveni={M\bibinitperiod}}}%
        {{hash=fcc3435f64a45f95f6b120c69b02ae26}{%
           family={Spanò},
           familyi={S\bibinitperiod},
           given={Sergio},
           giveni={S\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Multidisciplinary Digital Publishing Institute}%
      }
      \strng{namehash}{ed78b4c350aab6f69347e265227fcfb8}
      \strng{fullhash}{a012182fbc452022ce8b21bd0659ccd6}
      \strng{fullhashraw}{a012182fbc452022ce8b21bd0659ccd6}
      \strng{bibnamehash}{ed78b4c350aab6f69347e265227fcfb8}
      \strng{authorbibnamehash}{ed78b4c350aab6f69347e265227fcfb8}
      \strng{authornamehash}{ed78b4c350aab6f69347e265227fcfb8}
      \strng{authorfullhash}{a012182fbc452022ce8b21bd0659ccd6}
      \strng{authorfullhashraw}{a012182fbc452022ce8b21bd0659ccd6}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{In this review, we present an analysis of the most used multi-agent reinforcement learning algorithms. Starting with the single-agent reinforcement learning algorithms, we focus on the most critical issues that must be taken into account in their extension to multi-agent scenarios. The analyzed algorithms were grouped according to their features. We present a detailed taxonomy of the main multi-agent approaches proposed in the literature, focusing on their related mathematical models. For each algorithm, we describe the possible application fields, while pointing out its pros and cons. The described multi-agent algorithms are compared in terms of the most important characteristics for multi-agent reinforcement learning applications—namely, nonstationarity, scalability, and observability. We also describe the most common benchmark environments used to evaluate the performances of the considered methods.}
      \field{issn}{2076-3417}
      \field{issue}{11}
      \field{journaltitle}{Applied Sciences}
      \field{langid}{english}
      \field{month}{1}
      \field{number}{11}
      \field{shorttitle}{Multi-{{Agent Reinforcement Learning}}}
      \field{title}{Multi-{{Agent Reinforcement Learning}}: {{A Review}} of {{Challenges}} and {{Applications}}}
      \field{urlday}{16}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{11}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{4948}
      \range{pages}{1}
      \verb{doi}
      \verb 10.3390/app11114948
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/565YJNJJ/Canese et al_2021_Multi-Agent Reinforcement Learning.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.mdpi.com/2076-3417/11/11/4948
      \endverb
      \verb{url}
      \verb https://www.mdpi.com/2076-3417/11/11/4948
      \endverb
      \keyw{machine learning,multi-agent,reinforcement learning,swarm}
    \endentry
    \entry{krouka2022}{article}{}{}
      \name{author}{4}{}{%
        {{hash=616e6d8961032bfb45ba2d908c5bd579}{%
           family={Krouka},
           familyi={K\bibinitperiod},
           given={Mounssif},
           giveni={M\bibinitperiod}}}%
        {{hash=3a3569c9399ccbb0e5a839392163d978}{%
           family={Elgabli},
           familyi={E\bibinitperiod},
           given={Anis},
           giveni={A\bibinitperiod}}}%
        {{hash=dc57afe6da11419e9f5baa24fc305874}{%
           family={Issaid},
           familyi={I\bibinitperiod},
           given={Chaouki\bibnamedelima Ben},
           giveni={C\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=eaf666d93aee903b2bbd8a40853a4243}{%
           family={Bennis},
           familyi={B\bibinitperiod},
           given={Mehdi},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{7824fd2537fe178ac791ce5b3523e4d1}
      \strng{fullhash}{1a92a0bbb9600f65ab789efffe6fce9f}
      \strng{fullhashraw}{1a92a0bbb9600f65ab789efffe6fce9f}
      \strng{bibnamehash}{1a92a0bbb9600f65ab789efffe6fce9f}
      \strng{authorbibnamehash}{1a92a0bbb9600f65ab789efffe6fce9f}
      \strng{authornamehash}{7824fd2537fe178ac791ce5b3523e4d1}
      \strng{authorfullhash}{1a92a0bbb9600f65ab789efffe6fce9f}
      \strng{authorfullhashraw}{1a92a0bbb9600f65ab789efffe6fce9f}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper, we consider a distributed reinforcement learning setting where agents are communicating with a central entity in a shared environment to maximize a global reward. A main challenge in this setting is that the randomness of the wireless channel perturbs each agent’s model update while multiple agents’ updates may cause interference when communicating under limited bandwidth. To address this issue, we propose a novel distributed reinforcement learning algorithm based on the alternating direction method of multipliers (ADMM) and “over air aggregation” using analog transmission scheme, referred to as A-RLADMM. Our algorithm incorporates the wireless channel into the formulation of the ADMM method, which enables agents to transmit each element of their updated models over the same channel using analog communication. Numerical experiments on a multi-agent collaborative navigation task show that our proposed algorithm significantly outperforms the digital communication baseline of A-RLADMM (D-RLADMM), the lazily aggregated policy gradient (RL-LAPG), as well as the analog and the digital communication versions of the vanilla FL, (A-FRL) and (D-FRL) respectively.}
      \field{issn}{2332-7731}
      \field{journaltitle}{IEEE Transactions on Cognitive Communications and Networking}
      \field{month}{3}
      \field{number}{1}
      \field{title}{Communication-{{Efficient}} and {{Federated Multi-Agent Reinforcement Learning}}}
      \field{urlday}{16}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{8}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{311\bibrangedash 320}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1109/TCCN.2021.3130993
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/YXIAE97H/Krouka et al_2022_Communication-Efficient and Federated Multi-Agent Reinforcement Learning.pdf
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/abstract/document/9627728
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/abstract/document/9627728
      \endverb
      \keyw{ADMM,analog communications,Collaboration,Computational modeling,Convergence,Digital communication,distributed optimization,Numerical models,policy gradient,Privacy,Reinforcement learning}
    \endentry
    \entry{busoniu2008}{article}{}{}
      \name{author}{3}{}{%
        {{hash=7fb304698f0da43de984f81db7bf7a9a}{%
           family={Busoniu},
           familyi={B\bibinitperiod},
           given={Lucian},
           giveni={L\bibinitperiod}}}%
        {{hash=0156cb9902c793bb153564c4b86cf076}{%
           family={Babuska},
           familyi={B\bibinitperiod},
           given={Robert},
           giveni={R\bibinitperiod}}}%
        {{hash=3aa279196551f6d4f6429776156250b6}{%
           family={De\bibnamedelima Schutter},
           familyi={D\bibinitperiod\bibinitdelim S\bibinitperiod},
           given={Bart},
           giveni={B\bibinitperiod}}}%
      }
      \strng{namehash}{7480bfd69f596bbd28ce921862d8a55c}
      \strng{fullhash}{a78496e0aaf836592260a1445c99bad8}
      \strng{fullhashraw}{a78496e0aaf836592260a1445c99bad8}
      \strng{bibnamehash}{a78496e0aaf836592260a1445c99bad8}
      \strng{authorbibnamehash}{a78496e0aaf836592260a1445c99bad8}
      \strng{authornamehash}{7480bfd69f596bbd28ce921862d8a55c}
      \strng{authorfullhash}{a78496e0aaf836592260a1445c99bad8}
      \strng{authorfullhashraw}{a78496e0aaf836592260a1445c99bad8}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Multiagent systems are rapidly finding applications in a variety of domains, including robotics, distributed control, telecommunications, and economics. The complexity of many tasks arising in these domains makes them difficult to solve with preprogrammed agent behaviors. The agents must, instead, discover a solution on their own, using learning. A significant part of the research on multiagent learning concerns reinforcement learning techniques. This paper provides a comprehensive survey of multiagent reinforcement learning (MARL). A central issue in the field is the formal statement of the multiagent learning goal. Different viewpoints on this issue have led to the proposal of many different goals, among which two focal points can be distinguished: stability of the agents’ learning dynamics, and adaptation to the changing behavior of the other agents. The MARL algorithms described in the literature aim—either explicitly or implicitly—at one of these two goals or at a combination of both, in a fully cooperative, fully competitive, or more general setting. A representative selection of these algorithms is discussed in detail in this paper, together with the specific issues that arise in each category. Additionally, the benefits and challenges of MARL are described along with some of the problem domains where the MARL techniques have been applied. Finally, an outlook for the field is provided.}
      \field{issn}{1094-6977, 1558-2442}
      \field{journaltitle}{IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)}
      \field{langid}{english}
      \field{month}{3}
      \field{number}{2}
      \field{shortjournal}{IEEE Trans. Syst., Man, Cybern. C}
      \field{title}{A {{Comprehensive Survey}} of {{Multiagent Reinforcement Learning}}}
      \field{urlday}{12}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{volume}{38}
      \field{year}{2008}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{156\bibrangedash 172}
      \range{pages}{17}
      \verb{doi}
      \verb 10.1109/TSMCC.2007.913919
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/VC8GVJJT/Busoniu et al. - 2008 - A Comprehensive Survey of Multiagent Reinforcement.pdf
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/4445757/
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/4445757/
      \endverb
    \endentry
    \entry{lowe2020}{online}{}{}
      \name{author}{6}{}{%
        {{hash=7be01fa6277ef22b804ff6cc54c87b72}{%
           family={Lowe},
           familyi={L\bibinitperiod},
           given={Ryan},
           giveni={R\bibinitperiod}}}%
        {{hash=e2101a0f6a72a2fb022cd3e2d45461e1}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Yi},
           giveni={Y\bibinitperiod}}}%
        {{hash=41ea42ab7e09607e91466632ce419d8c}{%
           family={Tamar},
           familyi={T\bibinitperiod},
           given={Aviv},
           giveni={A\bibinitperiod}}}%
        {{hash=5de1025172ba2cd90c9f04ae0ba17006}{%
           family={Harb},
           familyi={H\bibinitperiod},
           given={Jean},
           giveni={J\bibinitperiod}}}%
        {{hash=e28d4ee199593959d8c29980a64f1974}{%
           family={Abbeel},
           familyi={A\bibinitperiod},
           given={Pieter},
           giveni={P\bibinitperiod}}}%
        {{hash=7570e7c3fc2c13d59af4d7cdb9962a4d}{%
           family={Mordatch},
           familyi={M\bibinitperiod},
           given={Igor},
           giveni={I\bibinitperiod}}}%
      }
      \strng{namehash}{dde2b308cb02077d5b0dcf690cd4ea7e}
      \strng{fullhash}{d649a4bfdfab1502cd7ae45ec8e10355}
      \strng{fullhashraw}{d649a4bfdfab1502cd7ae45ec8e10355}
      \strng{bibnamehash}{d649a4bfdfab1502cd7ae45ec8e10355}
      \strng{authorbibnamehash}{d649a4bfdfab1502cd7ae45ec8e10355}
      \strng{authornamehash}{dde2b308cb02077d5b0dcf690cd4ea7e}
      \strng{authorfullhash}{d649a4bfdfab1502cd7ae45ec8e10355}
      \strng{authorfullhashraw}{d649a4bfdfab1502cd7ae45ec8e10355}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.}
      \field{day}{14}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{3}
      \field{pubstate}{prepublished}
      \field{title}{Multi-{{Agent Actor-Critic}} for {{Mixed Cooperative-Competitive Environments}}}
      \field{urlday}{12}
      \field{urlmonth}{2}
      \field{urlyear}{2023}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1706.02275
      \endverb
      \verb{eprint}
      \verb 1706.02275
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/Q54TTJQQ/Lowe et al. - 2020 - Multi-Agent Actor-Critic for Mixed Cooperative-Com.pdf;/Users/brandonhosley/Zotero/storage/F3DU3EG3/1706.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1706.02275
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1706.02275
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
    \endentry
    \entry{sukhbaatar2016}{online}{}{}
      \name{author}{3}{}{%
        {{hash=72c54cfe76db45d84a3413df9e903ff4}{%
           family={Sukhbaatar},
           familyi={S\bibinitperiod},
           given={Sainbayar},
           giveni={S\bibinitperiod}}}%
        {{hash=b31b3f03b1e1ee0eec6ff58f9a9df960}{%
           family={Szlam},
           familyi={S\bibinitperiod},
           given={Arthur},
           giveni={A\bibinitperiod}}}%
        {{hash=a6784304d1cc890b2cb6c6c7f2f3fd76}{%
           family={Fergus},
           familyi={F\bibinitperiod},
           given={Rob},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{140eda2c6b05153d13195f7f9b9bcfed}
      \strng{fullhash}{a9cb358542c5136a8832a15c37b6233d}
      \strng{fullhashraw}{a9cb358542c5136a8832a15c37b6233d}
      \strng{bibnamehash}{a9cb358542c5136a8832a15c37b6233d}
      \strng{authorbibnamehash}{a9cb358542c5136a8832a15c37b6233d}
      \strng{authornamehash}{140eda2c6b05153d13195f7f9b9bcfed}
      \strng{authorfullhash}{a9cb358542c5136a8832a15c37b6233d}
      \strng{authorfullhashraw}{a9cb358542c5136a8832a15c37b6233d}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Many tasks in AI require the collaboration of multiple agents. Typically, the communication protocol between agents is manually specified and not altered during training. In this paper we explore a simple neural model, called CommNet, that uses continuous communication for fully cooperative tasks. The model consists of multiple agents and the communication between them is learned alongside their policy. We apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines. In some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand.}
      \field{day}{31}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{10}
      \field{pubstate}{prepublished}
      \field{title}{Learning {{Multiagent Communication}} with {{Backpropagation}}}
      \field{urlday}{28}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1605.07736
      \endverb
      \verb{eprint}
      \verb 1605.07736
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/LFWPCTDS/Sukhbaatar et al_2016_Learning Multiagent Communication with Backpropagation.pdf;/Users/brandonhosley/Zotero/storage/R7FJJZ2M/1605.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1605.07736
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1605.07736
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
    \endentry
    \entry{wei2022}{article}{}{}
      \name{author}{8}{}{%
        {{hash=33004e7f04179d2de636a44fe0130abc}{%
           family={Wei},
           familyi={W\bibinitperiod},
           given={Kang},
           giveni={K\bibinitperiod}}}%
        {{hash=bc20e54573d2db853852f86802ab83de}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Jun},
           giveni={J\bibinitperiod}}}%
        {{hash=28c2f8feae8557397170c998fd2e3b47}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Chuan},
           giveni={C\bibinitperiod}}}%
        {{hash=46a3f0b5c048254bc7f6910cd7e8cd63}{%
           family={Ding},
           familyi={D\bibinitperiod},
           given={Ming},
           giveni={M\bibinitperiod}}}%
        {{hash=185fc3c4c509ddb27c3370e69064bd91}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Cailian},
           giveni={C\bibinitperiod}}}%
        {{hash=c59d8dd54aa89c2a33d932d96e6f4bbd}{%
           family={Jin},
           familyi={J\bibinitperiod},
           given={Shi},
           giveni={S\bibinitperiod}}}%
        {{hash=21502e7d52af09e49d31a5bc40073591}{%
           family={Han},
           familyi={H\bibinitperiod},
           given={Zhu},
           giveni={Z\bibinitperiod}}}%
        {{hash=de7b0e512fcdd087f5750837f7c7cb9f}{%
           family={Poor},
           familyi={P\bibinitperiod},
           given={H.\bibnamedelimi Vincent},
           giveni={H\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
      }
      \strng{namehash}{6a74074103e01caaba0403c70ffc229c}
      \strng{fullhash}{071d3123f29f727b2d5f0b555bea5b89}
      \strng{fullhashraw}{071d3123f29f727b2d5f0b555bea5b89}
      \strng{bibnamehash}{6a74074103e01caaba0403c70ffc229c}
      \strng{authorbibnamehash}{6a74074103e01caaba0403c70ffc229c}
      \strng{authornamehash}{6a74074103e01caaba0403c70ffc229c}
      \strng{authorfullhash}{071d3123f29f727b2d5f0b555bea5b89}
      \strng{authorfullhashraw}{071d3123f29f727b2d5f0b555bea5b89}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In federated learning (FL), model training is distributed over clients and local models are aggregated by a central server. The performance of uploaded models in such situations can vary widely due to imbalanced data distributions, potential demands on privacy protections, and quality of transmissions. In this paper, we aim to minimize FL training delay over wireless channels, constrained by overall training performance as well as each client’s differential privacy (DP) requirement. We solve this problem in a multi-agent multi-armed bandit (MAMAB) framework to deal with the situation where there are multiple clients confronting different unknown transmission environments, e.g., channel fading and interference. Specifically, we first transform long-term constraints on both training performance and each client’s DP into a virtual queue based on the Lyapunov drift technique. Then, we convert the MAMAB to a max-min bipartite matching problem at each communication round, by estimating rewards with the upper confidence bound (UCB) approach. More importantly, we propose two efficient solutions to this matching problem, i.e., a modified Hungarian algorithm and greedy matching with a better alternative (GMBA), of which the former can achieve the optimal solution with high complexity while the latter approaches a better trade-off by enabling verified low-complexity with little performance loss. In addition, we develop an upper bound on the expected regret of this MAMAB based FL framework, which shows a linear growth over the logarithm of communication rounds, justifying its theoretical feasibility. Extensive experimental results are conducted to validate the effectiveness of our proposed algorithms, and the impacts of various parameters on the FL performance over wireless edge networks are also discussed.}
      \field{eventtitle}{{{IEEE Journal}} on {{Selected Areas}} in {{Communications}}}
      \field{issn}{1558-0008}
      \field{journaltitle}{IEEE Journal on Selected Areas in Communications}
      \field{month}{1}
      \field{number}{1}
      \field{title}{Low-{{Latency Federated Learning Over Wireless Channels With Differential Privacy}}}
      \field{volume}{40}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{pages}{290\bibrangedash 307}
      \range{pages}{18}
      \verb{doi}
      \verb 10.1109/JSAC.2021.3126052
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Library/Mobile Documents/com~apple~CloudDocs/ZotFile/Wei et al_2022_Low-Latency Federated Learning Over Wireless Channels With Differential Privacy.pdf
      \endverb
      \keyw{Computational modeling,Data models,differential privacy,Federated learning,Interference,max-min bipartite matching,multi-agent multi-armed bandit,Servers,Training,Wireless communication,Wireless sensor networks}
    \endentry
    \entry{shukla2022}{inproceedings}{}{}
      \name{author}{5}{}{%
        {{hash=7b1f0c483cb560ca440334f258326043}{%
           family={Shukla},
           familyi={S\bibinitperiod},
           given={Yash},
           giveni={Y\bibinitperiod}}}%
        {{hash=adf45752398fa062c4a8793be1085a2b}{%
           family={Thierauf},
           familyi={T\bibinitperiod},
           given={Christopher},
           giveni={C\bibinitperiod}}}%
        {{hash=bc6d51ee57ea806361549ea3cbae3bfe}{%
           family={Hosseini},
           familyi={H\bibinitperiod},
           given={Ramtin},
           giveni={R\bibinitperiod}}}%
        {{hash=5a338d60fbeeaf0b3883fd08d660e474}{%
           family={Tatiya},
           familyi={T\bibinitperiod},
           given={Gyan},
           giveni={G\bibinitperiod}}}%
        {{hash=3e3700e7eb6c746805a17e22eda92536}{%
           family={Sinapov},
           familyi={S\bibinitperiod},
           given={Jivko},
           giveni={J\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Richland, SC}%
      }
      \list{publisher}{1}{%
        {International Foundation for Autonomous Agents and Multiagent Systems}%
      }
      \strng{namehash}{048b71bb0b1a5f0160027e981395f8ba}
      \strng{fullhash}{78857cfb4a2382092d8571369af5df90}
      \strng{fullhashraw}{78857cfb4a2382092d8571369af5df90}
      \strng{bibnamehash}{78857cfb4a2382092d8571369af5df90}
      \strng{authorbibnamehash}{78857cfb4a2382092d8571369af5df90}
      \strng{authornamehash}{048b71bb0b1a5f0160027e981395f8ba}
      \strng{authorfullhash}{78857cfb4a2382092d8571369af5df90}
      \strng{authorfullhashraw}{78857cfb4a2382092d8571369af5df90}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Despite recent advances in Reinforcement Learning (RL), many problems, especially real-world tasks, remain prohibitively expensive to learn. To address this issue, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum to learn a problem that may otherwise be too difficult to learn from scratch. However, generating and optimizing a curriculum in a realistic scenario still requires extensive interactions with the environment. To address this challenge, we formulate the curriculum transfer problem, in which the schema of a curriculum optimized in a simpler, easy-to-solve environment (e.g., a grid world) is transferred to a complex, realistic scenario (e.g., a physics-based robotics simulation or the real world). We present "ACuTE", Automatic Curriculum Transfer from Simple to Complex Environments, a novel framework to solve this problem, and evaluate our proposed method by comparing it to other baseline approaches (e.g., domain adaptation) designed to speed up learning. We observe that our approach produces improved jumpstart and time-to-threshold performance even when adding task elements that further increase the difficulty of the realistic scenario. Finally, we demonstrate that our approach is independent of the learning algorithm used for curriculum generation, and is Sim2Real transferable to a real world scenario using a physical robot.}
      \field{booktitle}{Proceedings of the 21st {{International Conference}} on {{Autonomous Agents}} and {{Multiagent Systems}}}
      \field{day}{9}
      \field{isbn}{978-1-4503-9213-6}
      \field{month}{5}
      \field{series}{{{AAMAS}} '22}
      \field{shorttitle}{{{ACuTE}}}
      \field{title}{{{ACuTE}}: {{Automatic Curriculum Transfer}} from {{Simple}} to {{Complex Environments}}}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{pages}{1192\bibrangedash 1200}
      \range{pages}{9}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/WNDIQVTP/Shukla et al_2022_ACuTE.pdf
      \endverb
    \endentry
    \entry{shi2023}{article}{}{}
      \name{author}{4}{}{%
        {{hash=367ad0815e91f7dab9b88f6a423e0b0a}{%
           family={Shi},
           familyi={S\bibinitperiod},
           given={Haobin},
           giveni={H\bibinitperiod}}}%
        {{hash=9b9b6cc94f3eacfcc32c837d182deb36}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Jingchen},
           giveni={J\bibinitperiod}}}%
        {{hash=7367ca91f0da6760e96432392109c59d}{%
           family={Mao},
           familyi={M\bibinitperiod},
           given={Jiahui},
           giveni={J\bibinitperiod}}}%
        {{hash=a973ebce3a15a8b4d5484b771dc26bdc}{%
           family={Hwang},
           familyi={H\bibinitperiod},
           given={Kao-Shing},
           giveni={K\bibinithyphendelim S\bibinitperiod}}}%
      }
      \strng{namehash}{e64bd603ce97f095c01d75b758264727}
      \strng{fullhash}{c4873a4e11fd0ce6ac97e2e0a87cdaf8}
      \strng{fullhashraw}{c4873a4e11fd0ce6ac97e2e0a87cdaf8}
      \strng{bibnamehash}{c4873a4e11fd0ce6ac97e2e0a87cdaf8}
      \strng{authorbibnamehash}{c4873a4e11fd0ce6ac97e2e0a87cdaf8}
      \strng{authornamehash}{e64bd603ce97f095c01d75b758264727}
      \strng{authorfullhash}{c4873a4e11fd0ce6ac97e2e0a87cdaf8}
      \strng{authorfullhashraw}{c4873a4e11fd0ce6ac97e2e0a87cdaf8}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Some researchers have introduced transfer learning mechanisms to multiagent reinforcement learning (MARL). However, the existing works devoted to cross-task transfer for multiagent systems were designed just for homogeneous agents or similar domains. This work proposes an all-purpose cross-transfer method, called multiagent lateral transfer (MALT), assisting MARL with alleviating the training burden. We discuss several challenges in developing an all-purpose multiagent cross-task transfer learning method and provide a feasible way of reusing knowledge for MARL. In the developed method, we take features as the transfer object rather than policies or experiences, inspired by the progressive network. To achieve more efficient transfer, we assign pretrained policy networks for agents based on clustering, while an attention module is introduced to enhance the transfer framework. The proposed method has no strict requirements for the source task and target task. Compared with the existing works, our method can transfer knowledge among heterogeneous agents and also avoid negative transfer in the case of fully different tasks. As far as we know, this article is the first work denoted to all-purpose cross-task transfer for MARL. Several experiments in various scenarios have been conducted to compare the performance of the proposed method with baselines. The results demonstrate that the method is sufficiently flexible for most settings, including cooperative, competitive, homogeneous, and heterogeneous configurations.}
      \field{eventtitle}{{{IEEE Transactions}} on {{Cybernetics}}}
      \field{issn}{2168-2275}
      \field{journaltitle}{IEEE Transactions on Cybernetics}
      \field{month}{3}
      \field{number}{3}
      \field{title}{Lateral {{Transfer Learning}} for {{Multiagent Reinforcement Learning}}}
      \field{urlday}{21}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{volume}{53}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1699\bibrangedash 1711}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1109/TCYB.2021.3108237
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/6GRL3NSF/9535269.html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/9535269
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/9535269
      \endverb
      \keyw{Attention mechanism,Costs,Multi-agent systems,multiagent reinforcement learning (MARL),Neural networks,Reinforcement learning,Task analysis,Training,transfer learning,Transfer learning}
    \endentry
    \entry{albrecht2024}{book}{}{}
      \name{author}{3}{}{%
        {{hash=9da4f3f413d514d731efe34067976909}{%
           family={Albrecht},
           familyi={A\bibinitperiod},
           given={Stefano\bibnamedelima V.},
           giveni={S\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
        {{hash=f355421623a9ce0d54965817f05f6ad0}{%
           family={Christianos},
           familyi={C\bibinitperiod},
           given={Filippos},
           giveni={F\bibinitperiod}}}%
        {{hash=c32fcecebc45af27f83fbcfea1c93b31}{%
           family={Schäfer},
           familyi={S\bibinitperiod},
           given={Lukas},
           giveni={L\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cambridge, Massachusetts}%
      }
      \list{publisher}{1}{%
        {The MIT Press}%
      }
      \strng{namehash}{2399b7bc1ddbf337ba5d505312bf70c2}
      \strng{fullhash}{d9adbde549d4078b80f9846f86320d2e}
      \strng{fullhashraw}{d9adbde549d4078b80f9846f86320d2e}
      \strng{bibnamehash}{d9adbde549d4078b80f9846f86320d2e}
      \strng{authorbibnamehash}{d9adbde549d4078b80f9846f86320d2e}
      \strng{authornamehash}{2399b7bc1ddbf337ba5d505312bf70c2}
      \strng{authorfullhash}{d9adbde549d4078b80f9846f86320d2e}
      \strng{authorfullhashraw}{d9adbde549d4078b80f9846f86320d2e}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{"This book provides an accessible technical introduction to the field of Multi-Agent Reinforcement Learning (MARL)"--}
      \field{isbn}{978-0-262-04937-5}
      \field{langid}{english}
      \field{shorttitle}{Multi-Agent Reinforcement Learning}
      \field{title}{Multi-Agent Reinforcement Learning: Foundations and Modern Approaches}
      \field{year}{2024}
      \field{dateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/BNWH6I6A/Albrecht et al. - 2024 - Multi-agent reinforcement learning foundations an.pdf
      \endverb
      \keyw{Intelligent agents (Computer software),Reinforcement learning}
    \endentry
    \entry{cui2022}{online}{}{}
      \name{author}{7}{}{%
        {{hash=ae9f731739b88b9963c4f9ea7295d961}{%
           family={Cui},
           familyi={C\bibinitperiod},
           given={Kai},
           giveni={K\bibinitperiod}}}%
        {{hash=e8582bf631f0f40a64f82023f2b83a74}{%
           family={Tahir},
           familyi={T\bibinitperiod},
           given={Anam},
           giveni={A\bibinitperiod}}}%
        {{hash=68c1cafc9ec95b3f3f88f6cca23bf157}{%
           family={Ekinci},
           familyi={E\bibinitperiod},
           given={Gizem},
           giveni={G\bibinitperiod}}}%
        {{hash=9dbcf8624ee66dea23c946a8d82eee59}{%
           family={Elshamanhory},
           familyi={E\bibinitperiod},
           given={Ahmed},
           giveni={A\bibinitperiod}}}%
        {{hash=3e34c81db85d4a070fdf0035898f480d}{%
           family={Eich},
           familyi={E\bibinitperiod},
           given={Yannick},
           giveni={Y\bibinitperiod}}}%
        {{hash=58c0f51a031f07e3e346957de1340fda}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Mengguang},
           giveni={M\bibinitperiod}}}%
        {{hash=836aadfafb99a418e0308c121a0ac626}{%
           family={Koeppl},
           familyi={K\bibinitperiod},
           given={Heinz},
           giveni={H\bibinitperiod}}}%
      }
      \strng{namehash}{6404d2b3ac97f6dca0707439520ff9c8}
      \strng{fullhash}{6997df192fa15d8f62a6176bf4b4d112}
      \strng{fullhashraw}{6997df192fa15d8f62a6176bf4b4d112}
      \strng{bibnamehash}{6404d2b3ac97f6dca0707439520ff9c8}
      \strng{authorbibnamehash}{6404d2b3ac97f6dca0707439520ff9c8}
      \strng{authornamehash}{6404d2b3ac97f6dca0707439520ff9c8}
      \strng{authorfullhash}{6997df192fa15d8f62a6176bf4b4d112}
      \strng{authorfullhashraw}{6997df192fa15d8f62a6176bf4b4d112}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The analysis and control of large-population systems is of great interest to diverse areas of research and engineering, ranging from epidemiology over robotic swarms to economics and finance. An increasingly popular and effective approach to realizing sequential decision-making in multi-agent systems is through multi-agent reinforcement learning, as it allows for an automatic and model-free analysis of highly complex systems. However, the key issue of scalability complicates the design of control and reinforcement learning algorithms particularly in systems with large populations of agents. While reinforcement learning has found resounding empirical success in many scenarios with few agents, problems with many agents quickly become intractable and necessitate special consideration. In this survey, we will shed light on current approaches to tractably understanding and analyzing large-population systems, both through multi-agent reinforcement learning and through adjacent areas of research such as mean-field games, collective intelligence, or complex network theory. These classically independent subject areas offer a variety of approaches to understanding or modeling large-population systems, which may be of great use for the formulation of tractable MARL algorithms in the future. Finally, we survey potential areas of application for large-scale control and identify fruitful future applications of learning algorithms in practical systems. We hope that our survey could provide insight and future directions to junior and senior researchers in theoretical and applied sciences alike.}
      \field{day}{8}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{9}
      \field{pubstate}{prepublished}
      \field{title}{A {{Survey}} on {{Large-Population Systems}} and {{Scalable Multi-Agent Reinforcement Learning}}}
      \field{urlday}{21}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2209.03859
      \endverb
      \verb{eprint}
      \verb 2209.03859
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/AM9XH9WA/Cui et al_2022_A Survey on Large-Population Systems and Scalable Multi-Agent Reinforcement.pdf;/Users/brandonhosley/Zotero/storage/UQDSHI2V/2209.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2209.03859
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2209.03859
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems}
    \endentry
    \entry{calvo2018}{article}{}{}
      \name{author}{2}{}{%
        {{hash=714301dd2a30055a734f8f8ddf9f18af}{%
           family={Calvo},
           familyi={C\bibinitperiod},
           given={Jeancarlo\bibnamedelima Arguello},
           giveni={J\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=d9cfb32feeafc29e489d22e64a37a8a9}{%
           family={Dusparic},
           familyi={D\bibinitperiod},
           given={Ivana},
           giveni={I\bibinitperiod}}}%
      }
      \strng{namehash}{7401ae94bd6c78bee9b0583a6cf64bd1}
      \strng{fullhash}{7401ae94bd6c78bee9b0583a6cf64bd1}
      \strng{fullhashraw}{7401ae94bd6c78bee9b0583a6cf64bd1}
      \strng{bibnamehash}{7401ae94bd6c78bee9b0583a6cf64bd1}
      \strng{authorbibnamehash}{7401ae94bd6c78bee9b0583a6cf64bd1}
      \strng{authornamehash}{7401ae94bd6c78bee9b0583a6cf64bd1}
      \strng{authorfullhash}{7401ae94bd6c78bee9b0583a6cf64bd1}
      \strng{authorfullhashraw}{7401ae94bd6c78bee9b0583a6cf64bd1}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Reinforcement Learning (RL) has been extensively used in Urban Traffic Control (UTC) optimization due its capability to learn the dynamics of complex problems from interactions with the environment. Recent advances in Deep Reinforcement Learning (DRL) have opened up the possibilities for extending this work to more complex situations due to it overcoming the curse of dimensionality resulting from the exponential growth of the state and action spaces when incorporating fine-grained information. DRL has been shown to work very well for UTC on a single intersection, however, due to large training times, multi-junction implementations have been limited to training a single agent and replicating behaviour to other junctions, assuming homogeneity of all agents.}
      \field{journaltitle}{AICS}
      \field{langid}{english}
      \field{month}{12}
      \field{title}{Heterogeneous {{Multi-Agent Deep Reinforcement Learning}} for {{Traﬃc Lights Control}}}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{pages}{2\bibrangedash 13}
      \range{pages}{12}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/KIDZZFHR/Calvo and Dusparic - Heterogeneous Multi-Agent Deep Reinforcement Learn.pdf
      \endverb
    \endentry
    \entry{gupta2017a}{online}{}{}
      \name{author}{5}{}{%
        {{hash=d0792eb04b685020eafedbc871aeb84f}{%
           family={Gupta},
           familyi={G\bibinitperiod},
           given={Abhishek},
           giveni={A\bibinitperiod}}}%
        {{hash=a8f9ab847e5d07be6c35edcb4da8f0c6}{%
           family={Devin},
           familyi={D\bibinitperiod},
           given={Coline},
           giveni={C\bibinitperiod}}}%
        {{hash=8dbb6d17d807c3708f0ea6bf371527f6}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={YuXuan},
           giveni={Y\bibinitperiod}}}%
        {{hash=e28d4ee199593959d8c29980a64f1974}{%
           family={Abbeel},
           familyi={A\bibinitperiod},
           given={Pieter},
           giveni={P\bibinitperiod}}}%
        {{hash=c9545da54b33744da25943cdf66eadac}{%
           family={Levine},
           familyi={L\bibinitperiod},
           given={Sergey},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{d2f56625487873e41edcfcb7ea5206af}
      \strng{fullhash}{0e726b9fe38266db9b6585e1c7308b86}
      \strng{fullhashraw}{0e726b9fe38266db9b6585e1c7308b86}
      \strng{bibnamehash}{0e726b9fe38266db9b6585e1c7308b86}
      \strng{authorbibnamehash}{0e726b9fe38266db9b6585e1c7308b86}
      \strng{authornamehash}{d2f56625487873e41edcfcb7ea5206af}
      \strng{authorfullhash}{0e726b9fe38266db9b6585e1c7308b86}
      \strng{authorfullhashraw}{0e726b9fe38266db9b6585e1c7308b86}
      \field{extraname}{2}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{People can learn a wide range of tasks from their own experience, but can also learn from observing other creatures. This can accelerate acquisition of new skills even when the observed agent differs substantially from the learning agent in terms of morphology. In this paper, we examine how reinforcement learning algorithms can transfer knowledge between morphologically different agents (e.g., different robots). We introduce a problem formulation where two agents are tasked with learning multiple skills by sharing information. Our method uses the skills that were learned by both agents to train invariant feature spaces that can then be used to transfer other skills from one agent to another. The process of learning these invariant feature spaces can be viewed as a kind of "analogy making", or implicit learning of partial correspondences between two distinct domains. We evaluate our transfer learning algorithm in two simulated robotic manipulation skills, and illustrate that we can transfer knowledge between simulated robotic arms with different numbers of links, as well as simulated arms with different actuation mechanisms, where one robot is torque-driven while the other is tendon-driven.}
      \field{day}{8}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{3}
      \field{pubstate}{prepublished}
      \field{title}{Learning {{Invariant Feature Spaces}} to {{Transfer Skills}} with {{Reinforcement Learning}}}
      \field{urlday}{25}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1703.02949
      \endverb
      \verb{eprint}
      \verb 1703.02949
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/SC4QV825/Gupta et al_2017_Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning.pdf;/Users/brandonhosley/Zotero/storage/FPAI6T9H/1703.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1703.02949
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1703.02949
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Robotics}
    \endentry
    \entry{huttenrauch2019}{online}{}{}
      \name{author}{3}{}{%
        {{hash=57c5488cca6508b0a6dc29ab3c9afe54}{%
           family={Hüttenrauch},
           familyi={H\bibinitperiod},
           given={Maximilian},
           giveni={M\bibinitperiod}}}%
        {{hash=a41b551ba01941f0498ae8a8ab62e712}{%
           family={Šošić},
           familyi={Š\bibinitperiod},
           given={Adrian},
           giveni={A\bibinitperiod}}}%
        {{hash=1896ab7f5b0b179c9ff54fbf39b31106}{%
           family={Neumann},
           familyi={N\bibinitperiod},
           given={Gerhard},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{612856197b008899a144ad44593a6e5a}
      \strng{fullhash}{5409ec807d10bb1eb53ea4cf718d77d0}
      \strng{fullhashraw}{5409ec807d10bb1eb53ea4cf718d77d0}
      \strng{bibnamehash}{5409ec807d10bb1eb53ea4cf718d77d0}
      \strng{authorbibnamehash}{5409ec807d10bb1eb53ea4cf718d77d0}
      \strng{authornamehash}{612856197b008899a144ad44593a6e5a}
      \strng{authorfullhash}{5409ec807d10bb1eb53ea4cf718d77d0}
      \strng{authorfullhashraw}{5409ec807d10bb1eb53ea4cf718d77d0}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recently, deep reinforcement learning (RL) methods have been applied successfully to multi-agent scenarios. Typically, these methods rely on a concatenation of agent states to represent the information content required for decentralized decision making. However, concatenation scales poorly to swarm systems with a large number of homogeneous agents as it does not exploit the fundamental properties inherent to these systems: (i) the agents in the swarm are interchangeable and (ii) the exact number of agents in the swarm is irrelevant. Therefore, we propose a new state representation for deep multi-agent RL based on mean embeddings of distributions. We treat the agents as samples of a distribution and use the empirical mean embedding as input for a decentralized policy. We define different feature spaces of the mean embedding using histograms, radial basis functions and a neural network learned end-to-end. We evaluate the representation on two well known problems from the swarm literature (rendezvous and pursuit evasion), in a globally and locally observable setup. For the local setup we furthermore introduce simple communication protocols. Of all approaches, the mean embedding representation using neural network features enables the richest information exchange between neighboring agents facilitating the development of more complex collective strategies.}
      \field{day}{6}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{6}
      \field{pubstate}{prepublished}
      \field{title}{Deep {{Reinforcement Learning}} for {{Swarm Systems}}}
      \field{urlday}{25}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1807.06613
      \endverb
      \verb{eprint}
      \verb 1807.06613
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/NNR2UM6G/Hüttenrauch et al. - 2019 - Deep Reinforcement Learning for Swarm Systems.pdf;/Users/brandonhosley/Zotero/storage/5WTS988E/1807.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1807.06613
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1807.06613
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Computer Science - Systems and Control,Statistics - Machine Learning}
    \endentry
    \entry{kouzeghar2023}{online}{}{}
      \name{author}{4}{}{%
        {{hash=4f23240b0d55c7c46f5581cde138d239}{%
           family={Kouzeghar},
           familyi={K\bibinitperiod},
           given={Maryam},
           giveni={M\bibinitperiod}}}%
        {{hash=ea5a794151c4e50774de681b15695182}{%
           family={Song},
           familyi={S\bibinitperiod},
           given={Youngbin},
           giveni={Y\bibinitperiod}}}%
        {{hash=017c38c8d02eaf393174f99f1f2b7e09}{%
           family={Meghjani},
           familyi={M\bibinitperiod},
           given={Malika},
           giveni={M\bibinitperiod}}}%
        {{hash=0ef768a0f70c70e07476a88f403f2051}{%
           family={Bouffanais},
           familyi={B\bibinitperiod},
           given={Roland},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{4b6401a457970de28baeac6eb530d0f2}
      \strng{fullhash}{d049bfce8677e5f6f0770a1b929f6360}
      \strng{fullhashraw}{d049bfce8677e5f6f0770a1b929f6360}
      \strng{bibnamehash}{d049bfce8677e5f6f0770a1b929f6360}
      \strng{authorbibnamehash}{d049bfce8677e5f6f0770a1b929f6360}
      \strng{authornamehash}{4b6401a457970de28baeac6eb530d0f2}
      \strng{authorfullhash}{d049bfce8677e5f6f0770a1b929f6360}
      \strng{authorfullhashraw}{d049bfce8677e5f6f0770a1b929f6360}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Multi-agent pursuit-evasion tasks involving intelligent targets are notoriously challenging coordination problems. In this paper, we investigate new ways to learn such coordinated behaviors of unmanned aerial vehicles (UAVs) aimed at keeping track of multiple evasive targets. Within a Multi-Agent Reinforcement Learning (MARL) framework, we specifically propose a variant of the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) method. Our approach addresses multi-target pursuit-evasion scenarios within non-stationary and unknown environments with random obstacles. In addition, given the critical role played by collective exploration in terms of detecting possible targets, we implement heterogeneous roles for the pursuers for enhanced exploratory actions balanced by exploitation (i.e. tracking) of previously identified targets. Our proposed role-based MADDPG algorithm is not only able to track multiple targets, but also is able to explore for possible targets by means of the proposed Voronoi-based rewarding policy. We implemented, tested and validated our approach in a simulation environment prior to deploying a real-world multi-robot system comprising of Crazyflie drones. Our results demonstrate that a multi-agent pursuit team has the ability to learn highly efficient coordinated control policies in terms of target tracking and exploration even when confronted with multiple fast evasive targets in complex environments.}
      \field{day}{3}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{3}
      \field{pubstate}{prepublished}
      \field{title}{Multi-{{Target Pursuit}} by a {{Decentralized Heterogeneous UAV Swarm}} Using {{Deep Multi-Agent Reinforcement Learning}}}
      \field{urlday}{12}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2303.01799
      \endverb
      \verb{eprint}
      \verb 2303.01799
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/N93MAVU3/Kouzeghar et al_2023_Multi-Target Pursuit by a Decentralized Heterogeneous UAV Swarm using Deep.pdf;/Users/brandonhosley/Zotero/storage/LF8UKJHP/2303.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2303.01799
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2303.01799
      \endverb
      \keyw{Computer Science - Robotics}
    \endentry
    \entry{bitzer2010}{inproceedings}{}{}
      \name{author}{3}{}{%
        {{hash=9d87f64c55110b4e8516dc9cc4fa35c9}{%
           family={Bitzer},
           familyi={B\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
        {{hash=dc744818d7b192b3af78600e7a85d196}{%
           family={Howard},
           familyi={H\bibinitperiod},
           given={Matthew},
           giveni={M\bibinitperiod}}}%
        {{hash=7b2c17552ac9061afc4606fc8fc554b9}{%
           family={Vijayakumar},
           familyi={V\bibinitperiod},
           given={Sethu},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{a12c7d4101ce7a1d2e2bc6ab62b56d6e}
      \strng{fullhash}{8c28210a51b6434ee1abc2548da2962b}
      \strng{fullhashraw}{8c28210a51b6434ee1abc2548da2962b}
      \strng{bibnamehash}{8c28210a51b6434ee1abc2548da2962b}
      \strng{authorbibnamehash}{8c28210a51b6434ee1abc2548da2962b}
      \strng{authornamehash}{a12c7d4101ce7a1d2e2bc6ab62b56d6e}
      \strng{authorfullhash}{8c28210a51b6434ee1abc2548da2962b}
      \strng{authorfullhashraw}{8c28210a51b6434ee1abc2548da2962b}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Reinforcement learning in the high-dimensional, continuous spaces typical in robotics, remains a challenging problem. To overcome this challenge, a popular approach has been to use demonstrations to find an appropriate initialisation of the policy in an attempt to reduce the number of iterations needed to find a solution. Here, we present an alternative way to incorporate prior knowledge from demonstrations of individual postures into learning, by extracting the inherent problem structure to find an efficient state representation. In particular, we use probabilistic, nonlinear dimensionality reduction to capture latent constraints present in the data. By learning policies in the learnt latent space, we are able to solve the planning problem in a reduced space that automatically satisfies task constraints. As shown in our experiments, this reduces the exploration needed and greatly accelerates the learning. We demonstrate our approach for learning a bimanual reaching task on the 19-DOF KHR-1HV humanoid.}
      \field{booktitle}{2010 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}}
      \field{eventtitle}{2010 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}}
      \field{issn}{2153-0866}
      \field{month}{10}
      \field{title}{Using Dimensionality Reduction to Exploit Constraints in Reinforcement Learning}
      \field{urlday}{25}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{year}{2010}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{3219\bibrangedash 3225}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1109/IROS.2010.5650243
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/X35W5R32/Bitzer et al_2010_Using dimensionality reduction to exploit constraints in reinforcement learning.pdf;/Users/brandonhosley/Zotero/storage/8MZPUUAV/5650243.html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/5650243
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/5650243
      \endverb
      \keyw{Aerospace electronics,Joints,Learning,Manifolds,Principal component analysis,Robots,Trajectory}
    \endentry
    \entry{tangkaratt2016}{article}{}{}
      \name{author}{3}{}{%
        {{hash=add71ed9a02b60c00333454974b85f9d}{%
           family={Tangkaratt},
           familyi={T\bibinitperiod},
           given={Voot},
           giveni={V\bibinitperiod}}}%
        {{hash=27c6cae8e17b3c38ffcd03efd6861ffd}{%
           family={Morimoto},
           familyi={M\bibinitperiod},
           given={Jun},
           giveni={J\bibinitperiod}}}%
        {{hash=39334f77cbd67c8922133952b8095d89}{%
           family={Sugiyama},
           familyi={S\bibinitperiod},
           given={Masashi},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{537028dd3632877043f4b739baa0e6fc}
      \strng{fullhash}{8bceb1c909d1101cadce18bf0ffda77e}
      \strng{fullhashraw}{8bceb1c909d1101cadce18bf0ffda77e}
      \strng{bibnamehash}{8bceb1c909d1101cadce18bf0ffda77e}
      \strng{authorbibnamehash}{8bceb1c909d1101cadce18bf0ffda77e}
      \strng{authornamehash}{537028dd3632877043f4b739baa0e6fc}
      \strng{authorfullhash}{8bceb1c909d1101cadce18bf0ffda77e}
      \strng{authorfullhashraw}{8bceb1c909d1101cadce18bf0ffda77e}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The goal of reinforcement learning is to learn an optimal policy which controls an agent to acquire the maximum cumulative reward. The model-based reinforcement learning approach learns a transition model of the environment from data, and then derives the optimal policy using the transition model. However, learning an accurate transition model in high-dimensional environments requires a large amount of data which is difficult to obtain. To overcome this difficulty, in this paper, we propose to combine model-based reinforcement learning with the recently developed least-squares conditional entropy (LSCE) method, which simultaneously performs transition model estimation and dimension reduction. We also further extend the proposed method to imitation learning scenarios. The experimental results show that policy search combined with LSCE performs well for high-dimensional control tasks including real humanoid robot control.}
      \field{day}{1}
      \field{issn}{0893-6080}
      \field{journaltitle}{Neural Networks}
      \field{month}{12}
      \field{shortjournal}{Neural Networks}
      \field{title}{Model-Based Reinforcement Learning with Dimension Reduction}
      \field{urlday}{25}
      \field{urlmonth}{3}
      \field{urlyear}{2025}
      \field{volume}{84}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 16}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1016/j.neunet.2016.08.005
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/LDFDBMSV/S0893608016301095.html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S0893608016301095
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S0893608016301095
      \endverb
      \keyw{Model-based reinforcement learning,Sufficient dimension reduction,Transition model estimation}
    \endentry
    \entry{wakilpoor2020}{online}{}{}
      \name{author}{4}{}{%
        {{hash=5b27318667ca33a8d2e71093a7748ccc}{%
           family={Wakilpoor},
           familyi={W\bibinitperiod},
           given={Ceyer},
           giveni={C\bibinitperiod}}}%
        {{hash=f904e82926fc93b2cbe045456925de2b}{%
           family={Martin},
           familyi={M\bibinitperiod},
           given={Patrick\bibnamedelima J.},
           giveni={P\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=cb641d3c18344237680a5d49f6103964}{%
           family={Rebhuhn},
           familyi={R\bibinitperiod},
           given={Carrie},
           giveni={C\bibinitperiod}}}%
        {{hash=96962575233d78cc7ac45b135169ec24}{%
           family={Vu},
           familyi={V\bibinitperiod},
           given={Amanda},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{9a39042ad597fc22648c12c305fe1cd1}
      \strng{fullhash}{e954e02accfc9b5cfcc9f60513bd9022}
      \strng{fullhashraw}{e954e02accfc9b5cfcc9f60513bd9022}
      \strng{bibnamehash}{e954e02accfc9b5cfcc9f60513bd9022}
      \strng{authorbibnamehash}{e954e02accfc9b5cfcc9f60513bd9022}
      \strng{authornamehash}{9a39042ad597fc22648c12c305fe1cd1}
      \strng{authorfullhash}{e954e02accfc9b5cfcc9f60513bd9022}
      \strng{authorfullhashraw}{e954e02accfc9b5cfcc9f60513bd9022}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Reinforcement learning in heterogeneous multi-agent scenarios is important for real-world applications but presents challenges beyond those seen in homogeneous settings and simple benchmarks. In this work, we present an actor-critic algorithm that allows a team of heterogeneous agents to learn decentralized control policies for covering an unknown environment. This task is of interest to national security and emergency response organizations that would like to enhance situational awareness in hazardous areas by deploying teams of unmanned aerial vehicles. To solve this multi-agent coverage path planning problem in unknown environments, we augment a multi-agent actor-critic architecture with a new state encoding structure and triplet learning loss to support heterogeneous agent learning. We developed a simulation environment that includes real-world environmental factors such as turbulence, delayed communication, and agent loss, to train teams of agents as well as probe their robustness and flexibility to such disturbances.}
      \field{day}{6}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{10}
      \field{pubstate}{prepublished}
      \field{title}{Heterogeneous {{Multi-Agent Reinforcement Learning}} for {{Unknown Environment Mapping}}}
      \field{urlday}{14}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2010.02663
      \endverb
      \verb{eprint}
      \verb 2010.02663
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/JRDBJMX5/Wakilpoor et al_2020_Heterogeneous Multi-Agent Reinforcement Learning for Unknown Environment Mapping.pdf;/Users/brandonhosley/Zotero/storage/G975SLIL/2010.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2010.02663
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2010.02663
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems}
    \endentry
    \entry{koster2020}{online}{}{}
      \name{author}{4}{}{%
        {{hash=1c6b022b3dc230f7672b476207a85bd2}{%
           family={Köster},
           familyi={K\bibinitperiod},
           given={Raphael},
           giveni={R\bibinitperiod}}}%
        {{hash=d39f872255b10943403c9e34b5f85174}{%
           family={Hadfield-Menell},
           familyi={H\bibinithyphendelim M\bibinitperiod},
           given={Dylan},
           giveni={D\bibinitperiod}}}%
        {{hash=84bd71cc4f1432d2228fcdb7544b5409}{%
           family={Hadfield},
           familyi={H\bibinitperiod},
           given={Gillian\bibnamedelima K.},
           giveni={G\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=698896c85bec4f93adca09e2a21cdf78}{%
           family={Leibo},
           familyi={L\bibinitperiod},
           given={Joel\bibnamedelima Z.},
           giveni={J\bibinitperiod\bibinitdelim Z\bibinitperiod}}}%
      }
      \strng{namehash}{2b752cb042477c8d0f0286a4aedd5392}
      \strng{fullhash}{5761ea1dfb7340a5704172a44b89133d}
      \strng{fullhashraw}{5761ea1dfb7340a5704172a44b89133d}
      \strng{bibnamehash}{5761ea1dfb7340a5704172a44b89133d}
      \strng{authorbibnamehash}{5761ea1dfb7340a5704172a44b89133d}
      \strng{authornamehash}{2b752cb042477c8d0f0286a4aedd5392}
      \strng{authorfullhash}{5761ea1dfb7340a5704172a44b89133d}
      \strng{authorfullhashraw}{5761ea1dfb7340a5704172a44b89133d}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{How can societies learn to enforce and comply with social norms? Here we investigate the learning dynamics and emergence of compliance and enforcement of social norms in a foraging game, implemented in a multi-agent reinforcement learning setting. In this spatiotemporally extended game, individuals are incentivized to implement complex berry-foraging policies and punish transgressions against social taboos covering specific berry types. We show that agents benefit when eating poisonous berries is taboo, meaning the behavior is punished by other agents, as this helps overcome a credit-assignment problem in discovering delayed health effects. Critically, however, we also show that introducing an additional taboo, which results in punishment for eating a harmless berry, improves the rate and stability with which agents learn to punish taboo violations and comply with taboos. Counterintuitively, our results show that an arbitrary taboo (a "silly rule") can enhance social learning dynamics and achieve better outcomes in the middle stages of learning. We discuss the results in the context of studying normativity as a group-level emergent phenomenon.}
      \field{day}{25}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{1}
      \field{pubstate}{prepublished}
      \field{title}{Silly Rules Improve the Capacity of Agents to Learn Stable Enforcement and Compliance Behaviors}
      \field{urlday}{17}
      \field{urlmonth}{8}
      \field{urlyear}{2023}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2001.09318
      \endverb
      \verb{eprint}
      \verb 2001.09318
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/E86J9BM9/Köster et al_2020_Silly rules improve the capacity of agents to learn stable enforcement and.pdf;/Users/brandonhosley/Zotero/storage/5RUU4NII/2001.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2001.09318
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2001.09318
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems}
    \endentry
    \entry{leibo2021}{inproceedings}{}{}
      \name{author}{10}{}{%
        {{hash=698896c85bec4f93adca09e2a21cdf78}{%
           family={Leibo},
           familyi={L\bibinitperiod},
           given={Joel\bibnamedelima Z.},
           giveni={J\bibinitperiod\bibinitdelim Z\bibinitperiod}}}%
        {{hash=9cdb350d500215943fb341d35562135f}{%
           family={Dueñez-Guzman},
           familyi={D\bibinithyphendelim G\bibinitperiod},
           given={Edgar\bibnamedelima A.},
           giveni={E\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=c1e61cb8b50d4e339d37abb4716fed2f}{%
           family={Vezhnevets},
           familyi={V\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
        {{hash=e4fa217e56ca1781ab11713ab27cf2b4}{%
           family={Agapiou},
           familyi={A\bibinitperiod},
           given={John\bibnamedelima P.},
           giveni={J\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=daf6cbed703aa44e31d950c3c2720056}{%
           family={Sunehag},
           familyi={S\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
        {{hash=b7e696bb21d8792432e4ddbb1668839e}{%
           family={Koster},
           familyi={K\bibinitperiod},
           given={Raphael},
           giveni={R\bibinitperiod}}}%
        {{hash=c83c8bcac45e46ecbae55ea990780c0a}{%
           family={Matyas},
           familyi={M\bibinitperiod},
           given={Jayd},
           giveni={J\bibinitperiod}}}%
        {{hash=c6ccf15676d3236596232dfadad38ac6}{%
           family={Beattie},
           familyi={B\bibinitperiod},
           given={Charlie},
           giveni={C\bibinitperiod}}}%
        {{hash=7570e7c3fc2c13d59af4d7cdb9962a4d}{%
           family={Mordatch},
           familyi={M\bibinitperiod},
           given={Igor},
           giveni={I\bibinitperiod}}}%
        {{hash=368b9b2de627b852658c433b062d4e1e}{%
           family={Graepel},
           familyi={G\bibinitperiod},
           given={Thore},
           giveni={T\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{01a06d76ef2b39d070e5b4e6e26e7f0c}
      \strng{fullhash}{f1a737a9866a7d3cd9c695e38313725a}
      \strng{fullhashraw}{f1a737a9866a7d3cd9c695e38313725a}
      \strng{bibnamehash}{01a06d76ef2b39d070e5b4e6e26e7f0c}
      \strng{authorbibnamehash}{01a06d76ef2b39d070e5b4e6e26e7f0c}
      \strng{authornamehash}{01a06d76ef2b39d070e5b4e6e26e7f0c}
      \strng{authorfullhash}{f1a737a9866a7d3cd9c695e38313725a}
      \strng{authorfullhashraw}{f1a737a9866a7d3cd9c695e38313725a}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Existing evaluation suites for multi-agent reinforcement learning (MARL) do not assess generalization to novel situations as their primary objective (unlike supervised learning benchmarks). Our contribution, Melting Pot, is a MARL evaluation suite that fills this gap and uses reinforcement learning to reduce the human labor required to create novel test scenarios. This works because one agent’s behavior constitutes (part of) another agent’s environment. To demonstrate scalability, we have created over 80 unique test scenarios covering a broad range of research topics such as social dilemmas, reciprocity, resource sharing, and task partitioning. We apply these test scenarios to standard MARL training algorithms, and demonstrate how Melting Pot reveals weaknesses not apparent from training performance alone.}
      \field{booktitle}{Proceedings of the 38th {{International Conference}} on {{Machine Learning}}}
      \field{day}{1}
      \field{eventtitle}{International {{Conference}} on {{Machine Learning}}}
      \field{issn}{2640-3498}
      \field{langid}{english}
      \field{month}{7}
      \field{title}{Scalable {{Evaluation}} of {{Multi-Agent Reinforcement Learning}} with {{Melting Pot}}}
      \field{urlday}{21}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{6187\bibrangedash 6199}
      \range{pages}{13}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/3JIXPLFK/Leibo et al. - 2021 - Scalable Evaluation of Multi-Agent Reinforcement L.pdf;/Users/brandonhosley/Zotero/storage/RTMNFLZN/Leibo et al_2021_Scalable Evaluation of Multi-Agent Reinforcement Learning with Melting Pot.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v139/leibo21a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v139/leibo21a.html
      \endverb
    \endentry
    \entry{ellis2023}{inproceedings}{}{}
      \name{author}{8}{}{%
        {{hash=8013a1eb9ec691ea154fb9523884e3b2}{%
           family={Ellis},
           familyi={E\bibinitperiod},
           given={Benjamin},
           giveni={B\bibinitperiod}}}%
        {{hash=a071fde4f0800a3a8483eed7d8a4381f}{%
           family={Cook},
           familyi={C\bibinitperiod},
           given={Jonathan},
           giveni={J\bibinitperiod}}}%
        {{hash=f701e0d41900a34dfdf2a9e51698dde7}{%
           family={Moalla},
           familyi={M\bibinitperiod},
           given={Skander},
           giveni={S\bibinitperiod}}}%
        {{hash=c76172ee0c2ebda81796624b4a81ae54}{%
           family={Samvelyan},
           familyi={S\bibinitperiod},
           given={Mikayel},
           giveni={M\bibinitperiod}}}%
        {{hash=709c3aa92386ace0c21c1eec7c2ab736}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Mingfei},
           giveni={M\bibinitperiod}}}%
        {{hash=ac63a3aecba18da9492226922d51b6ae}{%
           family={Mahajan},
           familyi={M\bibinitperiod},
           given={Anuj},
           giveni={A\bibinitperiod}}}%
        {{hash=265add78d7143f911d5b2a0b954ff3c2}{%
           family={Foerster},
           familyi={F\bibinitperiod},
           given={Jakob\bibnamedelima Nicolaus},
           giveni={J\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
        {{hash=0d05819cf2b4fe22ba972c9b2b5d8c9d}{%
           family={Whiteson},
           familyi={W\bibinitperiod},
           given={Shimon},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{473e2569a38c6a23206dba318d2462f6}
      \strng{fullhash}{cf3af7798e5cdeca6f6c23b21529b92e}
      \strng{fullhashraw}{cf3af7798e5cdeca6f6c23b21529b92e}
      \strng{bibnamehash}{473e2569a38c6a23206dba318d2462f6}
      \strng{authorbibnamehash}{473e2569a38c6a23206dba318d2462f6}
      \strng{authornamehash}{473e2569a38c6a23206dba318d2462f6}
      \strng{authorfullhash}{cf3af7798e5cdeca6f6c23b21529b92e}
      \strng{authorfullhashraw}{cf3af7798e5cdeca6f6c23b21529b92e}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The availability of challenging benchmarks has played a key role in the recent progress of machine learning. In cooperative multi-agent reinforcement learning, the StarCraft Multi-Agent Challenge (SMAC) has become a popular testbed for centralised training with decentralised execution. However, after years of sustained improvement on SMAC, algorithms now achieve near-perfect performance. In this work, we conduct new analysis demonstrating that SMAC lacks the stochasticity and partial observability to require complex *closed-loop* policies. In particular, we show that an *open-loop* policy conditioned only on the timestep can achieve non-trivial win rates for many SMAC scenarios. To address this limitation, we introduce SMACv2, a new version of the benchmark where scenarios are procedurally generated and require agents to generalise to previously unseen settings (from the same distribution) during evaluation. We also introduce the extended partial observability challenge (EPO), which augments SMACv2 to ensure meaningful partial observability. We show that these changes ensure the benchmark requires the use of *closed-loop* policies. We evaluate state-of-the-art algorithms on SMACv2 and show that it presents significant challenges not present in the original benchmark. Our analysis illustrates that SMACv2 addresses the discovered deficiencies of SMAC and can help benchmark the next generation of MARL methods. Videos of training are available on our [website](https://sites.google.com/view/smacv2).}
      \field{day}{2}
      \field{eventtitle}{Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}}}
      \field{langid}{english}
      \field{month}{11}
      \field{shorttitle}{{{SMACv2}}}
      \field{title}{{{SMACv2}}: {{An Improved Benchmark}} for {{Cooperative Multi-Agent Reinforcement Learning}}}
      \field{urlday}{24}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/JQSY6GHU/Ellis et al_2023_SMACv2.pdf
      \endverb
      \verb{urlraw}
      \verb https://openreview.net/forum?id=5OjLGiJW3u
      \endverb
      \verb{url}
      \verb https://openreview.net/forum?id=5OjLGiJW3u
      \endverb
    \endentry
    \entry{guo2024}{online}{}{}
      \name{author}{4}{}{%
        {{hash=1c2bf81b5c6942ce905c399b610f0139}{%
           family={Guo},
           familyi={G\bibinitperiod},
           given={Xudong},
           giveni={X\bibinitperiod}}}%
        {{hash=17e62ce42a4dcfea4157418890de2a16}{%
           family={Shi},
           familyi={S\bibinitperiod},
           given={Daming},
           giveni={D\bibinitperiod}}}%
        {{hash=516d3f191d8c8a0f30577af4d59ac160}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Junjie},
           giveni={J\bibinitperiod}}}%
        {{hash=58e9a114f77a9bbce3d73aac6ab241fa}{%
           family={Fan},
           familyi={F\bibinitperiod},
           given={Wenhui},
           giveni={W\bibinitperiod}}}%
      }
      \strng{namehash}{c4d6c31c6af9a6a12169a8745f840974}
      \strng{fullhash}{4d61092ffb2b98a47397f05e62b77284}
      \strng{fullhashraw}{4d61092ffb2b98a47397f05e62b77284}
      \strng{bibnamehash}{4d61092ffb2b98a47397f05e62b77284}
      \strng{authorbibnamehash}{4d61092ffb2b98a47397f05e62b77284}
      \strng{authornamehash}{c4d6c31c6af9a6a12169a8745f840974}
      \strng{authorfullhash}{4d61092ffb2b98a47397f05e62b77284}
      \strng{authorfullhashraw}{4d61092ffb2b98a47397f05e62b77284}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The emergence of multi-agent reinforcement learning (MARL) is significantly transforming various fields like autonomous vehicle networks. However, real-world multi-agent systems typically contain multiple roles, and the scale of these systems dynamically fluctuates. Consequently, in order to achieve zero-shot scalable collaboration, it is essential that strategies for different roles can be updated flexibly according to the scales, which is still a challenge for current MARL frameworks. To address this, we propose a novel MARL framework named Scalable and Heterogeneous Proximal Policy Optimization (SHPPO), integrating heterogeneity into parameter-shared PPO-based MARL networks. We first leverage a latent network to learn strategy patterns for each agent adaptively. Second, we introduce a heterogeneous layer to be inserted into decision-making networks, whose parameters are specifically generated by the learned latent variables. Our approach is scalable as all the parameters are shared except for the heterogeneous layer, and gains both inter-individual and temporal heterogeneity, allowing SHPPO to adapt effectively to varying scales. SHPPO exhibits superior performance in classic MARL environments like Starcraft Multi-Agent Challenge (SMAC) and Google Research Football (GRF), showcasing enhanced zero-shot scalability, and offering insights into the learned latent variables' impact on team performance by visualization.}
      \field{day}{2}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{10}
      \field{pubstate}{prepublished}
      \field{title}{Heterogeneous {{Multi-Agent Reinforcement Learning}} for {{Zero-Shot Scalable Collaboration}}}
      \field{urlday}{20}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2404.03869
      \endverb
      \verb{eprint}
      \verb 2404.03869
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/PV5L5KGI/Guo et al. - 2024 - Heterogeneous Multi-Agent Reinforcement Learning f.pdf;/Users/brandonhosley/Zotero/storage/ITWRM7CS/2404.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2404.03869
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2404.03869
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Computer Science - Robotics,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control}
    \endentry
    \entry{schulman2017}{online}{}{}
      \name{author}{5}{}{%
        {{hash=3e09bd2a25d237ebdaed560a07c0451e}{%
           family={Schulman},
           familyi={S\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod}}}%
        {{hash=c9545da54b33744da25943cdf66eadac}{%
           family={Levine},
           familyi={L\bibinitperiod},
           given={Sergey},
           giveni={S\bibinitperiod}}}%
        {{hash=a901fd78fe1108cfa7d11129644967c7}{%
           family={Moritz},
           familyi={M\bibinitperiod},
           given={Philipp},
           giveni={P\bibinitperiod}}}%
        {{hash=8a36116840c7ee55901618c95fd08a58}{%
           family={Jordan},
           familyi={J\bibinitperiod},
           given={Michael\bibnamedelima I.},
           giveni={M\bibinitperiod\bibinitdelim I\bibinitperiod}}}%
        {{hash=e28d4ee199593959d8c29980a64f1974}{%
           family={Abbeel},
           familyi={A\bibinitperiod},
           given={Pieter},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{8d6613ae59595910252a86a61babf6c8}
      \strng{fullhash}{945251de1d41bc2767a541a8148f51da}
      \strng{fullhashraw}{945251de1d41bc2767a541a8148f51da}
      \strng{bibnamehash}{945251de1d41bc2767a541a8148f51da}
      \strng{authorbibnamehash}{945251de1d41bc2767a541a8148f51da}
      \strng{authornamehash}{8d6613ae59595910252a86a61babf6c8}
      \strng{authorfullhash}{945251de1d41bc2767a541a8148f51da}
      \strng{authorfullhashraw}{945251de1d41bc2767a541a8148f51da}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.}
      \field{day}{20}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{4}
      \field{pubstate}{prepublished}
      \field{title}{Trust {{Region Policy Optimization}}}
      \field{urlday}{25}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1502.05477
      \endverb
      \verb{eprint}
      \verb 1502.05477
      \endverb
      \verb{file}
      \verb /Users/brandonhosley/Zotero/storage/WB9X428C/Schulman et al_2017_Trust Region Policy Optimization.pdf;/Users/brandonhosley/Zotero/storage/N4BJ2EUW/1502.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1502.05477
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1502.05477
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
  \enddatalist
\endrefsection
\endinput

