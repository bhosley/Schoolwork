\documentclass[journal]{IEEEtran}

% Packages
\usepackage{amsmath}
\usepackage{graphicx}
\graphicspath{./Images/}
\usepackage[table,dvipsnames]{xcolor}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{cite}
\usepackage{url}
\usepackage{tikz}
\usetikzlibrary{matrix,shapes,positioning,arrows,backgrounds}

% Drafting Utility Package
\usepackage{comment}
\usepackage{blindtext}

% Title
\title{Working Title for HMARL League Play}

\author{Brandon Hosley, Capt, \textit{AFIT}%
	\thanks{Manuscript received \today%
		%		; revised Month DD, YYYY.
}}

%\keywords{class imbalance, data-level methods, algorithm-level methods, hybrid methods, machine learning, performance evaluation}

% Document
\begin{document}
	
	\maketitle
	
	
	% Abstract
	\begin{abstract}
		
		In the realm of artificial intelligence, the domain of board games, particularly chess, 
		has long served as a benchmark for evaluating the capabilities of computational models. 
		Recent advancements in reinforcement learning (RL) have enabled the development of 
		agents capable of achieving superhuman performance. 
		However, the majority of these models rely on a self-play mechanism 
		that may limit their strategic diversity and adaptability. 
		This study introduces a novel approach by training multiple agents with distinct 
		play styles to compete in a league format, akin to diverse ecosystems in natural competition. 
		Through the implementation of reinforcement learning algorithms, 
		we aim to investigate whether a heterogeneous pool of agents can enhance strategic depth 
		and performance compared to traditional self-play paradigms and if it simulates the benefits of 
		training against specifically tailored adversarial agents. 
		\begin{comment}
		We developed several agents, each with a unique play style—aggressive, defensive, positional, and tactical—trained within a multi-agent reinforcement learning framework. 
		The performance of these agents was evaluated based on their win rates, Elo ratings, Glicko ratings, and their ability to adapt and counter a variety of opponent strategies. 
		Our results indicate that the inclusion of diverse play styles not only elevates the overall performance of individual agents in league play but also encourages the emergence of innovative strategies and adaptability.
		\end{comment}
		This approach mirrors the complexity of human competition more closely than the conventional self-play model and suggests that diversity in training partners within reinforcement learning environments can significantly impact the development of artificial intelligence in competitive domains.
		
	\end{abstract}
	
	% Introduction
	\section{Introduction}
	\label{sec:introduction}
	Chess has long served as a bench-mark challenge for artificial intelligence (AI),
	with its non-trivial strategy and vast number of state spaces providing a rich domain for exploring 
	computational intelligence, decision-making, and learning algorithms. 
	The success of AI in chess, culminating in programs capable of defeating human world champions, 
	is largely attributed to advances in machine learning, particularly reinforcement learning (RL), 
	where agents learn optimal behaviors through interactions with their environment. 
	Traditionally, the pinnacle of AI chess performance has been achieved through the self-play methodology, 
	where a single agent improves by playing against versions of itself, gradually refining its strategy based on the outcomes of these games.
	
	While self-play has led to significant advancements, it inherently limits an agent's exposure to the diverse range of strategies 
	encountered in real-world play, potentially stunting the agent's ability to adapt to and counter novel strategies. 
	Recent research in reinforcement learning has begun exploring multi-agent systems and the benefits of diversity in training environments, 
	suggesting that exposure to a variety of opponents can lead to more robust and adaptable agents. 
	However, the potential of applying these insights to the domain of chess, 
	with deliberately diversified play styles during the training phase, remains underexplored.
		
	This study seeks to fill this gap by investigating the effects of training multiple chess agents, each with a distinct play style, 
	through reinforcement learning, and evaluating their performance in a league-based self-play architecture. 
	We hypothesize that a diversified training environment will foster the development of agents that are not only 
	proficient in their unique styles but are also more adaptable and capable of handling a wider array of opponent strategies. 
		
	It is important to acknowledge that we do not anticipate the rate or level of improvement 
	in our diverse agent pool to precisely match those achieved through training against dedicated adversarial agents, 
	which are specifically designed to challenge and refine the primary agent's strategies. 
	Dedicated adversarial agents, by their very nature, are tailored to exploit weaknesses and force rapid adaptation, 
	potentially leading to steeper learning curves. However, our research aims to demonstrate that by incorporating a variety 
	of play styles within the same league, we can achieve a more nuanced and comprehensive training environment. 
	This approach seeks to balance the diversity of challenges faced by each agent, fostering a broader strategic competence 
	without the need for constructing and maintaining a suite of highly specialized adversarial agents. 
	We hypothesize that the efficiency of training can be significantly enhanced through this method,
	as it reduces the overhead associated with the development of 
	dedicated adversarial models while still promoting a competitive, diverse, and rich learning ecosystem. 
	In essence, our goal is to prove that a league of diverse agents can serve as a practical and effective alternative, 
	offering a pathway to robust and adaptable reinforcement learning pipeline through a more streamlined and accessible training process.
		
	To explore this hypothesis, we employ a range of reinforcement learning algorithms to train a cohort of chess agents, each encouraged to specialize in different strategic approaches to the game. 
	Our methodology involves designing tailored reward functions and training regimes to promote the development of unique play styles, 
	from aggressive to defensive and positional to tactical. 
	We assess the effectiveness of this approach through various metrics, including win rates, ELO ratings,
	and GLICKO ratings.
	% ,GLiCKO ratings, and a novel measure of strategic diversity.
		
	The contributions of this research are twofold. 
	Firstly, it advances our understanding of how diversity in training environments affects 
	the development and performance of AI agents in strategic games. 
	Secondly, it offers practical insights into the design of reinforcement learning systems, 
	providing a blueprint for creating more adaptable and strategically diverse agents. 
	By exploring the intersection of reinforcement learning, multi-agent systems, and game theory, 
	this study paves the way for new avenues in AI research, with implications extending well beyond the realm of chess.

	
	% Literature Review
	\section{Related Work}
	\label{sec:related_work}
	
	
	\begin{comment}
		content...
	\end{comment}
	

	% Dataset
	\section{Datasets}
	\label{sec:dataset}
	
	
	\begin{comment}
		content...
	\end{comment}

	
	% Methodology
	\section{Methodology}
	\label{sec:methodology}
	
	
	\begin{comment}
		content...
	\end{comment}
	
	\subsection{Experiments}
	

	
	% Results and Discussion
	\section{Results and Discussion}
	\label{sec:results_discussion}
	
	
	\begin{comment}
		content...
	\end{comment}
	
	
	% Conclusion
	\section{Conclusion}
	\label{sec:conclusion}
	
	
	\begin{comment}
		content...
	\end{comment}
	
	
	\subsection{Future Work}
	
	\begin{comment}
		content...
	\end{comment}
	
		
	% References
	\label{sec:references}
	%\bibliographystyle{IEEEtran}
	%\bibliography{Project.bib}
	
\end{document}
