% Introduction %
\section{Motivation}%
\label{sec:motivation}

In the rapidly evolving field of artificial intelligence, \gls{rl} has become a 
cornerstone for developing autonomous decision-making systems~\cite{sutton2018}.
However, the complexity of real-world challenges often surpasses the capabilities of solitary 
agents, necessitating a shift towards more collaborative solutions~\cite{cao2012}.
\Gls{marl} extends beyond single-agent \gls{rl} to provide solutions to many such problems,
even significantly exceeding the capabilities of the latter 
within certain domains that do not necessitate multiple agents. \Gls{harl}, is an extension of 
\gls{marl} characterized by agents having distinct roles, capabilities, policies, or objectives.
This \printdoctype is driven by the potential of such systems
to significantly improve problem-solving efficiency and adaptability, 
potentially surpassing the performance of homogeneous agent groups~\cite{calvo2018}.

The concept of heterogeneity among agents
—featuring diverse skills, perspectives, and problem-solving strategies—
reflects the complexity of collaborative human endeavors, 
providing a robust framework for navigating complex, dynamic environments. 
The practical applications of this approach are vast, 
particularly in scenarios where coordination and cooperation are crucial. 
For example, drone swarms in search and rescue missions can leverage diverse capabilities, 
such as various sensory equipment or distinct maneuverability traits, 
enabling more thorough area coverage and expedited victim detection~\cite{hoang2023,kouzeghar2023}.
Similarly, agricultural robots outfitted with different sensors and tools can concurrently 
execute multiple tasks—ranging from harvesting to soil analysis and pest control—
thereby substantially enhancing efficiency and increasing crop yields~%
\cite{carbone2018,amarasinghe2019}.

To fully realize the potential benefits of \gls{harl}, it is crucial to understand the trade-offs 
involved. This \printdoctype will survey current approaches for training multiple agents, 
identifying the strengths and limitations of each. 
In \cref{sec:problem_statement,sec:research_question}
we will highlight several under-explored areas that warrant further investigation. 
By addressing these gaps, we aim to advance the field and enhance the effectiveness of 
multi-agent systems in real-world applications.

\section{Background}%
\label{sec:background}

    \subsection*{Reinforcement Learning: From Deep Blue to AlphaStar}%

\Gls{rl} has marked a number of significant milestones in outperforming humans in 
competitive domains. One of the most pivotal events occurred in 1997 
when IBM's Deep Blue defeated world chess champion Garry Kasparov. 
Though powered mainly by brute force computation and hand-tuned algorithms
rather than learning-based approaches~\cite{campbell2002}, Deep Blue's victory set 
the stage for the broader application of \gls{ai} in complex strategic games.

The field progressed significantly with DeepMind's introduction of AlphaGo in 2015. AlphaGo 
employed a combination of deep neural networks and Monte Carlo tree search~\cite{silver2016},
initially trained on human expert games and further improved through self-play.
This method enabled AlphaGo to defeat Lee Sedol, one of the world's top Go players, 
illustrating \gls{rl}'s potential to tackle challenges in games with vast state spaces and 
decisions typically driven by human intuition.

This breakthrough was quickly followed by the development of AlphaZero~\cite{silver2017},
which revolutionized the field by mastering chess, Go, and Shogi through self-play alone,
without any human-derived data~\cite{silver2017a}.
The method of self-play demonstrated not only versatility across different games but also the
capacity of RL systems to develop domain-independent strategies.

A subsequent major advancement was achieved with DeepMind's AlphaStar~\cite{vinyals2019},
which demonstrated that advanced RL models could handle complex strategies,
real-time decision-making, and intricate player interactions. AlphaStar's success in 
defeating professional StarCraft II players was particularly notable due to the game's demand 
for long-term strategic planning and quick tactical responses in an open-ended scenario.

To achieve the level of proficiency demonstrated in AlphaStar, Vinyals et al.~employed a 
multifaceted approach that integrated deep learning, imitation learning, reinforcement learning, 
and multi-agent learning. The specifics of these contributions will be explored in 
detail in~\Cref{ch:literature_review}.

    \subsection*{Multi-agent Reinforcement Learning}%: Learning to Work Together}

As early as 1951, Brown proposed a method for calculating \gls{nash} in two-player games 
through a process he termed fictitious play, which involves iteratively updating strategies. 
Unlike simultaneous strategy updates, Brown's method applies updates sequentially—a condition 
that Berger later proved to be sufficient for guaranteed convergence to \gls{nash} in 
nondegenerate ordinal games~\cite{brown1951iterative, berger2005, berger2007}. 
This foundational concept paved the way for later developments in game-theoretical 
approaches to multi-agent systems.

The development in this area remained comparatively stunted until significant 
strides were made in single-agent methods. Traditional Bellman-Equation-style solutions, 
while effective in single-agent settings and certain types of multi-agent games like zero-sum and 
common-payoff games, faced greater difficulty in stochastic or degenerate games~\cite{shoham2007a}.
These challenges highlighted the limitations of extending single-agent frameworks directly to 
multi-agent environments without modifications.

The introduction of multiple independent agents in an environment introduces additional complexity;
the game becomes non-stationary from the perspective of any single agent~\cite{busoniu2008}. 
This non-stationarity poses unique challenges as each agent must adapt to the actions of others 
whose strategies are also evolving, significantly complicating the learning process.

In this realm, the extension into \gls{marl} allows for the consideration of a wide spectrum of 
interactions as described in game theory, ranging from purely competitive to purely cooperative. 
\gls{marl} addresses the multitude of challenges associated with these diverse styles of 
interaction, offering frameworks and strategies that are adaptable to varying degrees of 
cooperation and competition among agents~\cite{lowe2020}.

In some cases, the interactions of interest in \gls{marl} are asymmetrical, 
adding another layer of complexity to strategy formulation and execution~\cite*{sun2023}.
Among the most notable successes in handling mixed modes of cooperation and competition is 
OpenAI's achievement with OpenAI Five. In this project, a team of agents reached superhuman 
performance in the multiplayer game Dota 2, utilizing a blend of techniques including a unique 
method of skill transfer known as ``surgery'' and extensive use of self-play~\cite{berner2019}.
This milestone not only demonstrated the capability of \gls{marl} systems to manage and excel in 
intricate, dynamically shifting competitive environments but also showcased the potential for 
these systems to develop and refine collaborative strategies among heterogeneous agents.

    \subsection*{The Game Theoretical Concerns}%:

In both papers describing AlphaStar~\cite{vinyals2019} and OpenAI Five~\cite{berner2019}, 
the authors mention in sparse detail the ``game theoretic'' concerns their respective frameworks 
seek to address. These concerns are primarily attributed to the potential pitfalls of self-play. 
Two major problems are highlighted.

The first problem, often called strategic collapse~\cite{berner2019, vinyals2019}, 
describes a phenomenon where an agent overfits to a self-defeating strategy, 
resulting in a feedback loop and a counter-intuitive observation where cumulative 
rewards per episode may suddenly drop and become unrecoverable during continued training.

The second problem is cyclic strategy chasing, where multiple agents converge on a set of 
strategies that balance wins against one strategy with losses to another. 
An example of this is the game rock-paper-scissors. 
Balduzzi et al. (2019) discuss this phenomenon in~\cite{balduzzi2019}.

To mitigate these risks, AlphaStar implemented a structured league-play schema that continuously 
pitted different agent policies against each other. OpenAI Five, on the other hand, 
used a simpler approach by maintaining a pool of previous milestone agents for ongoing 
comparison and refinement.

\section{Problem statement}%
\label{sec:problem_statement}%

While significant achievements in competitive environments have been demonstrated by 
AlphaStar~\cite{vinyals2019} and OpenAI Five~\cite{berner2019}, 
these projects have relied on substantial resources, which may not be accessible to all 
researchers or practitioners, to implement their extensive frameworks and achieve unique successes.
These systems have competed and achieved top rankings in online matchmaking against numerous human 
players.

This \printdoctype aims to examine several key aspects that contributed to the success of those 
projects, with the overarching goal of identifying how these methods can be applied not only to 
address the game-theoretic challenges impacting \gls{marl} but also to improve scalability and 
flexibility in other contexts. Understanding how different components of these training frameworks 
contribute to the effectiveness of the resulting agents and their relative costs is crucial for 
enabling further research and broader applications of \gls{marl} technologies. 
Additionally, we hypothesize that \gls{harl} techniques will offer observable benefits 
during the training process, not only in addressing game-theoretic problems but also in 
enhancing the resulting agents' flexibility when deployed in novel configurations.

\section{Research Questions}%
\label{sec:research_question}%
\label{sec:relevance_and_importance}

%\begin{tcolorbox}[colback=red!5,colframe=red!50!black,title=Note to Self]
%    Re-write after doing some more work on \cref{ch:methodology}
%    %For example,~\cite{smit2023} was unable to train agents in 4v4 that generalized 
%    %well to 11v11 (for Football/American Soccer).
%\end{tcolorbox}

\begin{description}
    % Q1 Which of the current SOTA algos are best suited?
    \item[Question 1.1:] 
    How do \gls{marl} and \gls{harl} algorithms compare in terms of 
    generalizability when agents trained under a given policy are required to 
    cooperate with teammates trained under separate instances?
    \item[Question 1.2:] 
    What impact does non-stationarity introduced by adversarial agents have on the 
    performance of \gls{marl} and \gls{harl} algorithms in competitive settings?
    %
    \item[Question 2:] 
    Given a small set of trained agents, can they be further trained to effectively 
    operate as a larger team? Conversely, can a multi-agent system be effectively 
    trained using only a subset of agents?
    %
    \item[Question 3:] 
    What methods and features can be leveraged to enhance the scalability and adaptability of 
    training methodologies in complex, variable-scale \gls{marl} environments?
    %
\end{description}

\section{Outline}%

\begin{comment} %%%% Dissertation Version %%%%
The remainder of this document is designed to systematically explore the complex field of 
multi-agent reinforcement learning, particularly focusing on heterogeneous-agent systems.
Following this introductory chapter, \ref{ch:literature_review}: Literature Review provides a 
comprehensive analysis of the seminal and recent literature pertinent to our research focus. 
\Cref{ch:methodology} details the experimental and analytical techniques employed. 
\Cref{ch:results} presents the data and findings from our research, followed by 
\cref{ch:discussion}, where these results are interpreted in the context of existing 
knowledge and their implications for future research are explored.
The \emph{dissertation} will conclude with~\cref{ch:conclusion}, 
which summarizes the research and suggests avenues for further investigation. 
Each chapter builds upon the previous to provide a comprehensive understanding of the topic, 
aiming to contribute valuable insights to the field of \gls{harl}.
\end{comment}

%%%% Prospectus Version %%%%
The remainder of this document is organized into two main sections. 
In \cref{ch:literature_review}, we conduct a comprehensive literature review, 
maintaining a scholarly tone that aligns with the style of the eventual dissertation. 
This section will examine existing research, identify gaps, 
and establish the foundational knowledge necessary for our study. 
In \cref{ch:methodology}, we outline a proposed research plan, 
shifting to a more proposal-oriented tone appropriate for a prospectus. 
This section will detail the research methods, experimental design, and analytical approaches 
we intend to employ to investigate the identified challenges and hypotheses. 
By clearly delineating these sections, we aim to provide a structured and 
coherent roadmap for our research journey.