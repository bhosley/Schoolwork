% Introduction %%%%%####################%%%%%%%%%%%%%%%%%%%%####################%%%%%%%%%%%%%%%%%%%%
\section{Motivation}%
\label{sec:motivation}

In the rapidly evolving field of artificial intelligence, reinforcement learning has become a 
cornerstone for developing autonomous decision-making systems~\cite{sutton2018}.
However, the complexity of real-world challenges often surpasses the capabilities of solitary agents,
necessitating a shift towards more collaborative solutions~\cite{cao2012}.
This dissertation is driven by the potential of \gls{harl}, an extension of \gls{marl}
wherein multiple agents with varied capabilities work together to accomplish common objectives. 
Such systems are anticipated to significantly improve problem-solving efficiency and adaptability, 
surpassing the performance of homogeneous agent groups~\cite{calvo2018}.

The concept of heterogeneity among agents—featuring diverse skills, perspectives, 
and problem-solving strategies—reflects the complexity of collaborative human endeavors, 
providing a robust framework for navigating complex, dynamic environments. 
The practical applications of this approach are vast, 
particularly in scenarios where coordination and cooperation are crucial. 
For example, drone swarms in search and rescue missions can leverage diverse capabilities, 
such as various sensory equipment or distinct maneuverability traits, 
enabling more thorough area coverage and expedited victim detection~\cite{hoang2023,kouzeghar2023}.
Similarly, agricultural robots outfitted with different sensors and tools can concurrently 
execute multiple tasks—ranging from harvesting to soil analysis and pest control—
thereby substantially enhancing efficiency and increasing crop yields~%
\cite{carbone2018,amarasinghe2019}.

%This dissertation seeks to advance the growing body of research in this field by developing 
%methodologies and frameworks that enable effective collaboration among heterogeneous agents. 
%By exploring the synergy between diverse agents, this work aims to harness their collective 
%potential to address challenges currently unmanageable by homogeneous systems. 
%Specifically, it will explore novel strategies for coordination and decision-making 
%that enhance adaptability and efficiency in complex scenarios.

\section{Background}%
\label{sec:background}

    \subsection*{Reinforcement Learning: From Deep Blue to AlphaStar}

\gls{rl} has marked a number of significant milestones in outperforming humans in competitive domains.
One of the most pivotal events occurred in 1997 when IBM's Deep Blue defeated world chess champion
Garry Kasparov. Though powered mainly by brute force computation and hand-tuned algorithms rather
than learning-based approaches~\cite{campbell2002},
Deep Blue's victory set the stage for the broader application of \gls{ai} in complex strategic games.

The field progressed significantly with DeepMind's introduction of AlphaGo in 2015.
AlphaGo employed a combination of deep neural networks and Monte Carlo tree search~\cite{silver2016},
initially trained on human expert games and further improved through self-play.
This method enabled AlphaGo to defeat one of the world's top Go players, illustrating \gls{rl}'s
potential to tackle challenges in games with vast state spaces and decisions typically driven by
human intuition.

This breakthrough was quickly followed by the development of AlphaZero~\cite{silver2017},
which revolutionized the field by mastering chess, Go, and Shogi through self-play alone,
without any human-derived data~\cite{silver2017a}.
The method of self-play demonstrated not only versatility across different games but also the
capacity of RL systems to develop domain-independent strategies.

A subsequent major advancement was achieved with DeepMind's AlphaStar~\cite{vinyals2019},
which demonstrated that advanced RL models could handle complex strategies,
real-time decision-making, and intricate player interactions. AlphaStar's success in 
defeating professional StarCraft II players was particularly notable due to the game's demand 
for long-term strategic planning and quick tactical responses in an open-ended scenario.

To achieve the level of proficiency demonstrated in AlphaStar, Vinyals et al.~employed a 
multifaceted approach that integrated deep learning, imitation learning, reinforcement learning, 
and multi-agent learning. The specifics of these contributions will be explored in 
detail in~\Cref{ch:literature_review}.


    \subsection*{Multi-agent Reinforcement Learning}%: Learning to Work Together}

As early as 1951, Brown proposed a method for calculating \gls{nash} in two-player games 
through a process he termed fictitious play, which involves iteratively updating strategies. 
Unlike simultaneous strategy updates, Brown's method applies updates sequentially—a condition 
that Berger later proved to be sufficient for guaranteed convergence to \gls{nash} in 
nondegenerate ordinal games~\cite{brown1951iterative, berger2005, berger2007}. 
This foundational concept paved the way for later developments in game-theoretical 
approaches to multi-agent systems.

The development in this area remained comparatively stunted until significant 
strides were made in single-agent methods. Traditional Bellman-Equation-style solutions, 
while effective in single-agent settings and certain types of multi-agent games like zero-sum and 
common-payoff games, faced greater difficulty in stochastic or degenerate games~\cite{shoham2007a}.
These challenges highlighted the limitations of extending single-agent frameworks directly to 
multi-agent environments without modifications.

The introduction of multiple independent agents in an environment introduces additional complexity;
the game becomes non-stationary from the perspective of any single agent~\cite{busoniu2008}. 
This non-stationarity poses unique challenges as each agent must adapt to the actions of others 
whose strategies are also evolving, significantly complicating the learning process.

In this realm, the extension into \gls{marl} allows for the consideration of a wide spectrum of 
interactions as described in game theory, ranging from purely competitive to purely cooperative. 
\gls{marl} addresses the multitude of challenges associated with these diverse styles of interaction,
offering frameworks and strategies that are adaptable to varying degrees of cooperation and 
competition among agents~\cite{lowe2020}.

In some cases, the interactions of interest in \gls{marl} are asymmetrical, 
adding another layer of complexity to strategy formulation and execution~\cite*{sun2023}.
Among the most notable successes in handling mixed modes of cooperation and competition is 
OpenAI's achievement with OpenAI Five. In this project, a team of agents reached superhuman 
performance in the multiplayer game Dota 2, utilizing a blend of techniques including a unique 
method of skill transfer known as ``surgery'' and extensive use of self-play~\cite{berner2019}.
This milestone not only demonstrated the capability of \gls{marl} systems to manage and excel in 
intricate, dynamically shifting competitive environments but also showcased the potential for 
these systems to develop and refine collaborative strategies among heterogeneous agents.


    \subsection*{Strategic Collapse}%:

Balduzzi et al. (2019) discuss the phenomenon of cyclic strategy chasing, 
a form of strategic collapse where a policy becomes overly specialized. 
This issue often becomes apparent in self-play environments, 
where the cumulative rewards per episode may suddenly drop if the strategy overfit is 
inherently self-defeating~\cite{balduzzi2019}. 
When a policy is not directly self-defeating, its flaws may remain hidden until the 
agent encounters another with a different policy.

Both OpenAI Five and AlphaStar have encountered and addressed the potential pitfalls of 
self-play in their development. These systems, which heavily utilized self-play, 
were particularly vulnerable to strategic collapse due to their deployment in competitive 
environments with skilled human players~\cite{berner2019, vinyals2019}. 
To mitigate these risks, AlphaStar implemented a structured league-play schema that 
continuously pitted different agent policies against each other. 
OpenAI Five, on the other hand, used a simpler approach by maintaining a pool of 
previous milestone agents for ongoing comparison and refinement.


\section{Problem statement}%
\label{sec:problem_statement}

The field of \gls*{rl} in heterogeneous agents, while growing, remains narrowly focused. 
Current research predominantly addresses the cooperative dynamics of agents working together to 
achieve objectives within a given environment, as evidenced by studies such as those by 
~\cite*{zhong2024, zheng2020, wakilpoor2020, kapetanakis2005}. 
Conversely, the competitive application of these agents is considerably less developed. 
While significant achievements in competitive environments have been demonstrated by 
AlphaStar~\cite{vinyals2019} and OpenAI Five~\cite{berner2019}, 
these projects have relied on extensive resources, which may not be accessible to all 
researchers or practitioners.

This dissertation seeks to conduct a deeper examination of the conditions that lead to strategic
collapse in \gls*{harl} and to identify effective and efficient methods to address these issues. 
There is a critical need to understand how competitive strategies can fail or succeed in diverse 
settings, which will be crucial for enabling broader applications of \gls*{harl} technologies.

Additionally, the identification and understanding of pitfall strategies in \gls*{harl} 
could be immensely beneficial. Being able to recognize and enumerate strategies that are prone 
to risks during training and before deployment could greatly enhance the trust and 
reliability of these systems for future users. This study aims to fill these gaps, 
contributing to a more robust understanding of competitive heterogeneous-agent environments.


\section{Research Questions}%
\label{sec:research_question}%
\label{sec:relevance_and_importance}

\begin{description}
    \item[Question 1:] 
    What are the key factors contributing to strategic collapse in \gls*{harl} environments?
    % This question aims to identify and analyze the conditions under which strategies fail, 
    % particularly in \emph{competitive} \gls*{harl} settings. 
    % Understanding these factors is crucial for developing robust and resilient AI systems.
    \item[Question 1.1:] 
    What methods can prevent or mitigate strategic collapse in \gls*{harl} without compromising the 
    learning efficiency and effectiveness of the agents?
    % Focusing on solutions, this question investigates the techniques and approaches that can be 
    % employed to address the issues of strategic collapse, 
    % ensuring that agents remain effective in their tasks while avoiding over-specialization 
    % and other pitfalls.
    \item[Question 2:] 
    How can competitive \gls*{harl} be effectively managed with limited resources compared to the 
    extensive resources used in projects like AlphaStar and OpenAI Five?
    % This question seeks to explore methods and strategies that can optimize resource use in \gls*{harl}, 
    % making competitive applications more accessible and feasible for a wider range of researchers and 
    % practitioners.
    %%  Question 2.1:
    %%  What role does the interaction between cooperation and competition play in the strategic 
    %%  dynamics of \gls*{harl}, and how can this balance be optimized?
    %%  % By examining the interplay between cooperative and competitive behaviors, 
    %%  % this question aims to understand how these dynamics influence overall system 
    %%  % performance and strategy development in heterogeneous-agent settings.
    \item[Question 3:] 
    Can pitfall strategies in \gls*{harl} be identified and enumerated during the training phase to 
    prevent potential failures post-deployment?
    % This question addresses the predictive aspects of HARL, exploring whether it is possible 
    % to foresee and correct risky strategies during the development phase of AI systems,
    % thereby enhancing their reliability and performance in real-world applications.
\end{description}

\begin{comment}
    
\end{comment}

\begin{tcolorbox}[colback=blue!5,colframe=blue!50!black,title=Note to Editor]
    Among the current questions I don't believe that I have effectively enumerated the
    objective where it comes to examining training that is robust to various team sizes/
    compositions. For example,~\cite*{smit2023} was unable to train agents in 4v4 that generalized
    well to 11v11 (for Football/American Soccer).

    I think that a rewrite on Q2 is the appropriate place. 
    And would be most consistent with previous plans.
\end{tcolorbox}

\section{Outline}

The remainder of this document is designed to systematically explore the complex field of 
multi-agent reinforcement learning, particularly focusing on heterogeneous-agent systems.
Following this introductory chapter,~\ref*{ch:literature_review}: Literature Review provides a 
comprehensive analysis of the seminal and recent literature pertinent to our research focus.%
%\ref*{ch:methodology} outlines the essential theories that underpin our methodologies.
\ref*{ch:methodology} details the experimental and analytical techniques employed.
~\ref*{ch:results} presents the data and findings from our research, followed by 
~\ref*{ch:discussion}, where these results are interpreted in the context of existing 
knowledge and their implications for future research are explored.
The \emph{dissertation} will conclude with
~\ref*{ch:conclusion}, which summarizes the research and suggests avenues for further investigation. 
Each chapter builds upon the previous to provide a comprehensive understanding of the topic, 
aiming to contribute valuable insights to the field of \gls*{harl}.

\begin{comment}
    The structure of this dissertation is designed to systematically explore the complex field of 
    multi-agent reinforcement learning, particularly focusing on heterogeneous-agent systems. 
    Following this introductory chapter,~\ref*{ch:literature_review}: Literature Review provides a 
    comprehensive analysis of the seminal and recent literature pertinent to our research focus. 
    Chapter 3: Theoretical Foundations lays out the essential theories that underpin our methodologies. 
    Chapter 4: Methodology details the experimental and analytical techniques employed. 
    ~\ref*{ch:results} presents the data and findings from our research, followed by 
    Chapter 6: Discussion, where these results are interpreted in the context of existing knowledge and 
    their implications for future research are explored. The dissertation concludes with
    ~\ref*{ch:conclusion}, which summarizes the research and suggests avenues for further investigation. 
    Each chapter builds upon the previous to provide a comprehensive understanding of the topic, 
    aiming to contribute valuable insights to the field of multi-agent reinforcement learning.
\end{comment}
