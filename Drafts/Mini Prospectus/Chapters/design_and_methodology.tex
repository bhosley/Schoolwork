

\section{Contribution 1}
\subsection{Motivation}
Contribution 1 is motivated by establishing a broader baseline evaluation of relevant 
\gls{marl} and \gls{harl} algorithms, particularly those presented and referenced in 
\cite{zhong2024}, the former of which have not been rigorously examined. 
The existing evaluations primarily rely on learning curves compared to the algorithms they are 
based on. Moreover, studies such as \cite{zhong2024,yu2022,papoudakis2021,zheng2020} have 
tested exclusively on cooperative tasks, while \cite{sun2023,lowe2020} did some testing
on competitive environments but did not evaluate agents against novel opponents.

\subsection{Methodology}
We will train a set of \gls{marl} and \gls{harl} algorithms across various scenarios multiple times, 
with the number of iterations (X) determined by the available time and computational resources on 
our cluster. This phase will provide data on training time, consistency of performance, and 
variance in training curves.

Next, we will evaluate the algorithms in purely cooperative tasks using teams constructed from 
agents drawn from separate training instances. We expect \gls{marl} training instances to produce 
agents with sufficiently similar policies, resulting in teams that demonstrate similar, 
albeit slightly reduced, levels of effectiveness. In contrast, \gls{harl} allows 
agents to converge on distinct policies, potentially leading to diverse team behaviors. 
The absence of existing research on this scenario suggests a wide range of possible outcomes.

For tasks where training instances converge on distinct agent roles, we hypothesize that evaluation
teams composed of similarly specialized agents will significantly under-perform compared to teams 
with appropriately diverse agents. Finally, we will assess the algorithms in competitive settings, 
introducing non-stationarity from adversarial agents. 
This allows for testing team mixing in a manner similar to the cooperative tasks and 
evaluating performance against adversarial agents trained in separate instances.

\subsection{Resources}

To perform this battery of tests we intend to use a framework that will ease the application 
of each algorithm to the environments, improving interoperability. 
We believe that RLlib~\cite{liang2018}, a growing framework that exists within the greater
Ray-project~\cite{zotero-2599} is the ideal tool for this line of research.
RLlib provides a number of important features to streamline the construction of the appropriate
pipeline.
The framework integrates well with the \glsplural{api} of several other more mature 
frameworks that offer a lot of beneficial functionalities.

There are multiple options for data collection, \cite{zotero-2601,zotero-2603}.
The most common metrics seen in the papers that we reviewed for this \printdoctype
were episode returns and win rates, typically with intervals~
\cite{zhong2024,yu2022,papoudakis2021,lowe2020,zheng2020}.

There are a number of popular algorithms implemented already within RLlib,
but not all of the algorithms of interest. Some may need to be written within
the existing framework standards.

There are a large number of environments currently supported in the framework.
The majority of the environments used in the papers discussed in this section are 
already implemented within the framework. 


\begin{description}
    \item[Data Collection:] integrations with Tensorboard~\cite{zotero-2601} 
    and WandB.ai~\cite{zotero-2603}
    \item[Algorithms:] 
    \item[Environments:] 
\end{description}









\subsection{Anticipated Obstacles}



\subsection{Expected Contributions}
\begin{description}
    \item[Baseline Evaluation:] 
    Establishing a comprehensive baseline for \gls{marl} and \gls{harl} algorithms, 
    particularly those presented in \cite{zhong2024}, by rigorously examining their 
    performance across various scenarios.
    \item[Training Consistency:] 
    Providing insights into the relationship between training time, consistency of performance, 
    and variance in training curves for \gls{marl} and \gls{harl} algorithms.
    \item[Cooperative Task Performance:] 
    Evaluating the performance of agents in cooperative tasks when teams are constructed from 
    agents trained in separate instances, highlighting the differences between 
    \gls{marl} and \gls{harl} in terms of policy convergence and team effectiveness.
    \item[Competitive Task Evaluation:] 
    Assessing the performance of algorithms in competitive settings, examining the impact of 
    non-stationarity introduced by adversarial agents, and the effectiveness of team mixing.
    \item[Novel Insights:] 
    Generating new insights into the behavior of \gls{harl} agents when deployed in novel 
    configurations, contributing to the broader understanding of multi-agent reinforcement 
    learning and its practical applications.
    \item[] 
\end{description}





\begin{comment}
For each proposed contribution:
\begin{itemize}
    \item Motivation and the specific problem. (Call back to the appropriate references.)
    \item Specify the academic contributions (Bulleted format makes it easier and clearer)
    \item Summary of specific methodology to be employed
    \begin{itemize}
        \item Expected obstacles
        \item Sources
    \end{itemize}
    \item Expected results. That is, the expected implications of whatever the results will be. 
    We aren't predicting the answer to the questions, just what the answers will tell us.
    \begin{itemize}
        \item Practical implications
        \item Theoretical implications
    \end{itemize}
\end{itemize}
\end{comment}

\section{Contribution 2}
An ablation of league play methods



\section{Contribution 3}
Addressing the problem mentioned in \cite{smit2023}
