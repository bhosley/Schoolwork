@inproceedings{amarasinghe2019,
  title = {A Swarm of Crop Spraying Drones Solution for Optimising Safe Pesticide Usage in Arable Lands: Poster Abstract},
  shorttitle = {A Swarm of Crop Spraying Drones Solution for Optimising Safe Pesticide Usage in Arable Lands},
  booktitle = {Proceedings of the 17th {{Conference}} on {{Embedded Networked Sensor Systems}}},
  author = {Amarasinghe, Akarshani and Wijesuriya, Viraj B. and Ganepola, Dilshan and Jayaratne, Lakshman},
  date = {2019-11-10},
  series = {{{SenSys}} '19},
  pages = {410--411},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3356250.3361948},
  url = {https://doi.org/10.1145/3356250.3361948},
  urldate = {2024-05-12},
  abstract = {Pesticides are detrimental to the well-being of all living beings. Inappropriate pesticide usage has been identified as a major health hazard. Therefore, it is important to devise solutions that enable agricultural pest control without excessive use of pesticides. At present, drone technology is being increasingly applied in agriculture through soil and field analysis, planting, crop spraying, crop monitoring, irrigation and health assessment. The primary focus of this ongoing work is to leverage a swarm of drones solution that includes precision agriculture techniques, to efficiently spray pesticides in arable lands with optimal pesticides usage and minimum human intervention.},
  isbn = {978-1-4503-6950-3},
  keywords = {drone systems,swarm of drones}
}

@online{balduzzi2019,
  title = {Open-Ended {{Learning}} in {{Symmetric Zero-sum Games}}},
  author = {Balduzzi, David and Garnelo, Marta and Bachrach, Yoram and Czarnecki, Wojciech M. and Perolat, Julien and Jaderberg, Max and Graepel, Thore},
  date = {2019-05-13},
  eprint = {1901.08106},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1901.08106},
  url = {http://arxiv.org/abs/1901.08106},
  urldate = {2024-03-15},
  abstract = {Zero-sum games such as chess and poker are, abstractly, functions that evaluate pairs of agents, for example labeling them `winner' and `loser'. If the game is approximately transitive, then self-play generates sequences of agents of increasing strength. However, nontransitive games, such as rock-paper-scissors, can exhibit strategic cycles, and there is no longer a clear objective -- we want agents to increase in strength, but against whom is unclear. In this paper, we introduce a geometric framework for formulating agent objectives in zero-sum games, in order to construct adaptive sequences of objectives that yield open-ended learning. The framework allows us to reason about population performance in nontransitive games, and enables the development of a new algorithm (rectified Nash response, PSRO\_rN) that uses game-theoretic niching to construct diverse populations of effective agents, producing a stronger set of agents than existing algorithms. We apply PSRO\_rN to two highly nontransitive resource allocation games and find that PSRO\_rN consistently outperforms the existing alternatives.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning},
  file = {/Users/brandonhosley/Zotero/storage/JIHZMJL2/Balduzzi et al. - 2019 - Open-ended Learning in Symmetric Zero-sum Games.pdf;/Users/brandonhosley/Zotero/storage/NWLIW2GL/1901.html}
}

@article{berger2005,
  title = {Fictitious Play in 2×n Games},
  author = {Berger, Ulrich},
  date = {2005-02},
  journaltitle = {Journal of Economic Theory},
  shortjournal = {Journal of Economic Theory},
  volume = {120},
  number = {2},
  pages = {139--154},
  issn = {00220531},
  doi = {10.1016/j.jet.2004.02.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0022053104000626},
  urldate = {2024-05-13},
  abstract = {It is known that every discrete-time fictitious play process approaches equilibrium in nondegenerate 2 Â 2 games, and that every continuous-time fictitious play process approaches equilibrium in nondegenerate 2 Â 2 and 2 Â 3 games. It has also been conjectured that convergence to the set of equilibria holds generally for nondegenerate 2 Â n games. We give a simple geometric proof of this for the continuous-time process, and also extend the result to discrete-time fictitious play.},
  langid = {english},
  file = {/Users/brandonhosley/Zotero/storage/2QRP9GVP/Berger - 2005 - Fictitious play in 2×n games.pdf}
}

@article{berger2007,
  title = {Brown's {{Original Fictitious Play}}},
  author = {Berger, Ulrich},
  date = {2007},
  journaltitle = {Journal of Economic Theory},
  volume = {135},
  number = {1},
  pages = {572--578},
  issn = {0022-0531},
  doi = {10.1016/j.jet.2005.12.010},
  abstract = {What modern game theorists describe as fictitious play is not the learning process George W. Brown defined in his 1951 paper. Brown's original version differs in a subtle detail, namely the order of belief updating. In this note we revive Brown's original fictitious play process and demonstrate that this seemingly innocent detail allows for an extremely simple and intuitive proof of convergence in an interesting and large class of games: nondegenerate ordinal potential games.},
  file = {/Users/brandonhosley/Zotero/storage/XLUDA35I/Berger_2007_Brown's Original Fictitious Play.pdf}
}

@online{berner2019,
  title = {Dota 2 with {{Large Scale Deep Reinforcement Learning}}},
  author = {Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Dębiak, Przemysław and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and Józefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pinto, Henrique P. d O. and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
  date = {2019-12-13},
  eprint = {1912.06680},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1912.06680},
  url = {http://arxiv.org/abs/1912.06680},
  urldate = {2024-05-13},
  abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/brandonhosley/Zotero/storage/IS2XA9JP/OpenAI et al_2019_Dota 2 with Large Scale Deep Reinforcement Learning.pdf;/Users/brandonhosley/Zotero/storage/V8XZWULV/1912.html}
}

@article{brown1951iterative,
  title = {Iterative Solution of Games by Fictitious Play},
  author = {Brown, George W},
  date = {1951},
  journaltitle = {Act. Anal. Prod Allocation},
  volume = {13},
  number = {1},
  pages = {374}
}

@article{busoniu2008,
  title = {A {{Comprehensive Survey}} of {{Multiagent Reinforcement Learning}}},
  author = {Busoniu, Lucian and Babuska, Robert and De Schutter, Bart},
  date = {2008-03},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  shortjournal = {IEEE Trans. Syst., Man, Cybern. C},
  volume = {38},
  number = {2},
  pages = {156--172},
  issn = {1094-6977, 1558-2442},
  doi = {10.1109/TSMCC.2007.913919},
  url = {https://ieeexplore.ieee.org/document/4445757/},
  urldate = {2024-05-12},
  abstract = {Multiagent systems are rapidly finding applications in a variety of domains, including robotics, distributed control, telecommunications, and economics. The complexity of many tasks arising in these domains makes them difficult to solve with preprogrammed agent behaviors. The agents must, instead, discover a solution on their own, using learning. A significant part of the research on multiagent learning concerns reinforcement learning techniques. This paper provides a comprehensive survey of multiagent reinforcement learning (MARL). A central issue in the field is the formal statement of the multiagent learning goal. Different viewpoints on this issue have led to the proposal of many different goals, among which two focal points can be distinguished: stability of the agents’ learning dynamics, and adaptation to the changing behavior of the other agents. The MARL algorithms described in the literature aim—either explicitly or implicitly—at one of these two goals or at a combination of both, in a fully cooperative, fully competitive, or more general setting. A representative selection of these algorithms is discussed in detail in this paper, together with the specific issues that arise in each category. Additionally, the benefits and challenges of MARL are described along with some of the problem domains where the MARL techniques have been applied. Finally, an outlook for the field is provided.},
  langid = {english},
  file = {/Users/brandonhosley/Zotero/storage/VC8GVJJT/Busoniu et al. - 2008 - A Comprehensive Survey of Multiagent Reinforcement.pdf}
}

@article{calvo2018,
  title = {Heterogeneous {{Multi-Agent Deep Reinforcement Learning}} for {{Traﬃc Lights Control}}},
  author = {Calvo, Jeancarlo Arguello and Dusparic, Ivana},
  date = {2018-12},
  journaltitle = {AICS},
  pages = {2--13},
  abstract = {Reinforcement Learning (RL) has been extensively used in Urban Traffic Control (UTC) optimization due its capability to learn the dynamics of complex problems from interactions with the environment. Recent advances in Deep Reinforcement Learning (DRL) have opened up the possibilities for extending this work to more complex situations due to it overcoming the curse of dimensionality resulting from the exponential growth of the state and action spaces when incorporating fine-grained information. DRL has been shown to work very well for UTC on a single intersection, however, due to large training times, multi-junction implementations have been limited to training a single agent and replicating behaviour to other junctions, assuming homogeneity of all agents.},
  langid = {english},
  file = {/Users/brandonhosley/Zotero/storage/KIDZZFHR/Calvo and Dusparic - Heterogeneous Multi-Agent Deep Reinforcement Learn.pdf}
}

@article{campbell2002,
  title = {Deep {{Blue}}},
  author = {Campbell, Murray and Hoane, A. Joseph and Hsu, Feng-hsiung},
  date = {2002-01-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {134},
  number = {1},
  pages = {57--83},
  issn = {0004-3702},
  doi = {10.1016/S0004-3702(01)00129-1},
  url = {https://www.sciencedirect.com/science/article/pii/S0004370201001291},
  urldate = {2024-04-01},
  abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: •a single-chip chess search engine,•a massively parallel system with multiple levels of parallelism,•a strong emphasis on search extensions,•a complex evaluation function, and•effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.},
  keywords = {Computer chess,Evaluation function,Game tree search,Parallel search,Search extensions,Selective search},
  file = {/Users/brandonhosley/Zotero/storage/4HB7XQYF/S0004370201001291.html}
}

@online{cao2012,
  title = {An {{Overview}} of {{Recent Progress}} in the {{Study}} of {{Distributed Multi-agent Coordination}}},
  author = {Cao, Yongcan and Yu, Wenwu and Ren, Wei and Chen, Guanrong},
  date = {2012-09-04},
  eprint = {1207.3231},
  eprinttype = {arxiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1207.3231},
  url = {http://arxiv.org/abs/1207.3231},
  urldate = {2024-05-12},
  abstract = {This article reviews some main results and progress in distributed multi-agent coordination, focusing on papers published in major control systems and robotics journals since 2006. Distributed coordination of multiple vehicles, including unmanned aerial vehicles, unmanned ground vehicles and unmanned underwater vehicles, has been a very active research subject studied extensively by the systems and control community. The recent results in this area are categorized into several directions, such as consensus, formation control, optimization, task assignment, and estimation. After the review, a short discussion section is included to summarize the existing research and to propose several promising research directions along with some open problems that are deemed important for further investigations.},
  pubstate = {preprint},
  keywords = {Mathematics - Optimization and Control},
  file = {/Users/brandonhosley/Zotero/storage/WDMRVX7F/Cao et al_2012_An Overview of Recent Progress in the Study of Distributed Multi-agent.pdf;/Users/brandonhosley/Zotero/storage/IICE6PDC/1207.html}
}

@article{carbone2018,
  title = {Swarm {{Robotics}} as a {{Solution}} to {{Crops Inspection}} for {{Precision Agriculture}}},
  author = {Carbone, Carlos and Garibaldi, Oscar and Kurt, Zohre},
  date = {2018-02-11},
  journaltitle = {KnE Engineering},
  pages = {552--562},
  issn = {2518-6841},
  doi = {10.18502/keg.v3i1.1459},
  url = {https://knepublishing.com/index.php/KnE-Engineering/article/view/1459},
  urldate = {2024-05-12},
  abstract = {This paper summarizes the concept of swarm robotics and its applicability to crop inspections. To increase the agricultural yield it is essential to monitor the crop health. Hence, precision agriculture is becoming a common practice for farmers providing a system that can inspect the state of the plants (Khosla and others, 2010). One of the rising technologies used for agricultural inspections is the use of unmaned air vehicles (UAVs) which are used to take aerial pictures of the farms so that the images could be processed to extract data about the state of the crops (Das et al., 2015). For this process both fixed wings and quadrotors UAVs are used with a preference over the quadrotor since it’s easier to operate and has a milder learning curve compared to fixed wings (Kolodny, 2017). UAVs require battery replacement especially when the environmental conditions result in longer inspection times (“Agriculture - Maximize Yields with Aerial Imaging,” n.d., “Matrice 100 - DJI Wiki,” n.d.). As a result, inspection systems for crops using commercial quadrotors are limited by the quadrotor´s maximum flight speed, maximum flight height, quadrotor´s battery time, crops area, wind conditions, etc. (“Mission Estimates,” n.d.).Keywords: Swarm Robotics, Precision Agriculture, Unmanned Air Vehicle, Quadrotor, inspection.},
  langid = {english},
  file = {/Users/brandonhosley/Zotero/storage/ABHP2XRH/Carbone et al_2018_Swarm Robotics as a Solution to Crops Inspection for Precision Agriculture.pdf}
}

@incollection{hoang2023,
  title = {Drone {{Swarms}} to {{Support Search}} and {{Rescue Operations}}: {{Opportunities}} and {{Challenges}}},
  shorttitle = {Drone {{Swarms}} to {{Support Search}} and {{Rescue Operations}}},
  booktitle = {Cultural {{Robotics}}: {{Social Robots}} and {{Their Emergent Cultural Ecologies}}},
  author = {Hoang, Maria-Theresa Oanh and Grøntved, Kasper Andreas Rømer and Van Berkel, Niels and Skov, Mikael B and Christensen, Anders Lyhne and Merritt, Timothy},
  editor = {Dunstan, Belinda J. and Koh, Jeffrey T. K. V. and Turnbull Tillman, Deborah and Brown, Scott Andrew},
  date = {2023},
  pages = {163--176},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-28138-9_11},
  url = {https://link.springer.com/10.1007/978-3-031-28138-9_11},
  urldate = {2024-05-12},
  isbn = {978-3-031-28137-2 978-3-031-28138-9},
  langid = {english},
  file = {/Users/brandonhosley/Zotero/storage/EWGATS8U/Hoang et al. - 2023 - Drone Swarms to Support Search and Rescue Operatio.pdf}
}

@inproceedings{kapetanakis2005,
  title = {Reinforcement {{Learning}} of {{Coordination}} in {{Heterogeneous Cooperative Multi-agent Systems}}},
  booktitle = {Adaptive {{Agents}} and {{Multi-Agent Systems II}}},
  author = {Kapetanakis, Spiros and Kudenko, Daniel},
  editor = {Kudenko, Daniel and Kazakov, Dimitar and Alonso, Eduardo},
  date = {2005},
  pages = {119--131},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-32274-0_8},
  abstract = {Most approaches to the learning of coordination in multi-agent systems (MAS) to date require all agents to use the same learning algorithm with similar (or even the same) parameter settings. In today’s open networks and high inter-connectivity such an assumption becomes increasingly unrealistic. Developers are starting to have less control over the agents that join the system and the learning algorithms they employ. This makes effective coordination and good learning performance extremely difficult to achieve, especially in the absence of learning agent standards. In this paper we investigate the problem of learning to coordinate with heterogeneous agents. We show that an agent employing the FMQ algorithm, a recently developed multi-agent learning method, has the ability to converge towards the optimal joint action when teamed-up with one or more simple Q-learners. Specifically, we show such convergence in scenarios where simple Q-learners alone are unable to converge towards an optimum. Our results show that system designers may improve learning and coordination performance by adding a “smart” agent to the MAS.},
  isbn = {978-3-540-32274-0},
  langid = {english}
}

@online{kouzeghar2023,
  title = {Multi-{{Target Pursuit}} by a {{Decentralized Heterogeneous UAV Swarm}} Using {{Deep Multi-Agent Reinforcement Learning}}},
  author = {Kouzeghar, Maryam and Song, Youngbin and Meghjani, Malika and Bouffanais, Roland},
  date = {2023-03-03},
  eprint = {2303.01799},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.01799},
  url = {http://arxiv.org/abs/2303.01799},
  urldate = {2024-05-12},
  abstract = {Multi-agent pursuit-evasion tasks involving intelligent targets are notoriously challenging coordination problems. In this paper, we investigate new ways to learn such coordinated behaviors of unmanned aerial vehicles (UAVs) aimed at keeping track of multiple evasive targets. Within a Multi-Agent Reinforcement Learning (MARL) framework, we specifically propose a variant of the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) method. Our approach addresses multi-target pursuit-evasion scenarios within non-stationary and unknown environments with random obstacles. In addition, given the critical role played by collective exploration in terms of detecting possible targets, we implement heterogeneous roles for the pursuers for enhanced exploratory actions balanced by exploitation (i.e. tracking) of previously identified targets. Our proposed role-based MADDPG algorithm is not only able to track multiple targets, but also is able to explore for possible targets by means of the proposed Voronoi-based rewarding policy. We implemented, tested and validated our approach in a simulation environment prior to deploying a real-world multi-robot system comprising of Crazyflie drones. Our results demonstrate that a multi-agent pursuit team has the ability to learn highly efficient coordinated control policies in terms of target tracking and exploration even when confronted with multiple fast evasive targets in complex environments.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics},
  file = {/Users/brandonhosley/Zotero/storage/N93MAVU3/Kouzeghar et al_2023_Multi-Target Pursuit by a Decentralized Heterogeneous UAV Swarm using Deep.pdf;/Users/brandonhosley/Zotero/storage/LF8UKJHP/2303.html}
}

@online{lowe2020,
  title = {Multi-{{Agent Actor-Critic}} for {{Mixed Cooperative-Competitive Environments}}},
  author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
  date = {2020-03-14},
  eprint = {1706.02275},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.02275},
  url = {http://arxiv.org/abs/1706.02275},
  urldate = {2023-02-12},
  abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/brandonhosley/Zotero/storage/Q54TTJQQ/Lowe et al. - 2020 - Multi-Agent Actor-Critic for Mixed Cooperative-Com.pdf;/Users/brandonhosley/Zotero/storage/F3DU3EG3/1706.html}
}

@article{shoham2007a,
  title = {If Multi-Agent Learning Is the Answer, What Is the Question?},
  author = {Shoham, Yoav and Powers, Rob and Grenager, Trond},
  date = {2007-05},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {171},
  number = {7},
  pages = {365--377},
  issn = {00043702},
  doi = {10.1016/j.artint.2006.02.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370207000495},
  urldate = {2024-05-12},
  abstract = {The area of learning in multi-agent systems is today one of the most fertile grounds for interaction between game theory and artificial intelligence. We focus on the foundational questions in this interdisciplinary area, and identify several distinct agendas that ought to, we argue, be separated. The goal of this article is to start a discussion in the research community that will result in firmer foundations for the area.1 © 2007 Published by Elsevier B.V.},
  langid = {english},
  file = {/Users/brandonhosley/Zotero/storage/722AZMMT/Shoham et al. - 2007 - If multi-agent learning is the answer, what is the.pdf}
}

@article{silver2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  options = {useprefix=true},
  date = {2016-01},
  journaltitle = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature16961},
  url = {https://www.nature.com/articles/nature16961},
  urldate = {2024-03-15},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  langid = {english},
  keywords = {Computational science,Computer science,Reward},
  file = {/Users/brandonhosley/Zotero/storage/F29T7VGP/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf}
}

@article{silver2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
  options = {useprefix=true},
  date = {2017-10},
  journaltitle = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature24270},
  url = {https://www.nature.com/articles/nature24270},
  urldate = {2024-03-15},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo. Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games. To beat world champions at the game of Go, the computer program AlphaGo has relied largely on supervised learning from millions of human expert moves. David Silver and colleagues have now produced a system called AlphaGo Zero, which is based purely on reinforcement learning and learns solely from self-play. Starting from random moves, it can reach superhuman level in just a couple of days of training and five million games of self-play, and can now beat all previous versions of AlphaGo. Because the machine independently discovers the same fundamental principles of the game that took humans millennia to conceptualize, the work suggests that such principles have some universal character, beyond human bias.},
  langid = {english},
  keywords = {Computational science,Computer science,Reward},
  file = {/Users/brandonhosley/Zotero/storage/RCPCCGV2/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf}
}

@online{silver2017a,
  title = {Mastering {{Chess}} and {{Shogi}} by {{Self-Play}} with a {{General Reinforcement Learning Algorithm}}},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  date = {2017-12-05},
  eprint = {1712.01815},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1712.01815},
  url = {http://arxiv.org/abs/1712.01815},
  urldate = {2024-03-15},
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/brandonhosley/Zotero/storage/PFGFAPGW/Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a Gene.pdf;/Users/brandonhosley/Zotero/storage/SR3IRUXR/1712.html}
}

@article{smit2023,
  title = {Scaling Multi-Agent Reinforcement Learning to Full 11 versus 11 Simulated Robotic Football},
  author = {Smit, Andries and Engelbrecht, Herman A. and Brink, Willie and Pretorius, Arnu},
  date = {2023-03-24},
  journaltitle = {Autonomous Agents and Multi-Agent Systems},
  shortjournal = {Auton Agent Multi-Agent Syst},
  volume = {37},
  number = {1},
  pages = {20},
  issn = {1573-7454},
  doi = {10.1007/s10458-023-09603-y},
  url = {https://doi.org/10.1007/s10458-023-09603-y},
  urldate = {2024-03-16},
  abstract = {Robotic football has long been seen as a grand challenge in artificial intelligence. Despite recent success of learned policies over heuristics and handcrafted rules in general, current teams in the simulated RoboCup football leagues, where autonomous agents compete against each other, still rely on handcrafted strategies with only a few using reinforcement learning directly. This limits a learning agent’s ability to find stronger high-level strategies for the full game. In this paper, we show that it is possible for agents to learn competent football strategies on a full 22 player setting using limited computation resources (one GPU and one CPU), from tabula rasa through self-play. To do this, we build a 2D football simulator with faster simulation times than the RoboCup simulator. We propose various improvements to the standard single-agent PPO training algorithm which help it scale to our multi-agent setting. These improvements include (1) using a policy and critic network with an attention mechanism that scales linearly in the number of agents, (2) sharing networks between agents which allow for faster throughput using batching, and (3) using Polyak averaged opponents, league opponents and freezing the opponent team when necessary. We show through experimental results that stable training in the full 22 player setting is possible. Agents trained in the 22 player setting learn to defeat a variety of handcrafted strategies, and also achieve a higher win rate compared to agents trained in the 4 player setting and evaluated in the full game.},
  langid = {english},
  keywords = {11 versus 11 football,Multi-agent reinforcement learning,RoboCup,RoboCup 2D,Simulated robotic football,Soccer},
  file = {/Users/brandonhosley/Zotero/storage/5UPXMZ63/Smit et al. - 2023 - Scaling multi-agent reinforcement learning to full.pdf}
}

@online{sun2023,
  title = {Mastering {{Asymmetrical Multiplayer Game}} with {{Multi-Agent Asymmetric-Evolution Reinforcement Learning}}},
  author = {Sun, Chenglu and Zhang, Yichi and Zhang, Yu and Lu, Ziling and Liu, Jingbin and Xu, Sijia and Zhang, Weidong},
  date = {2023-04-20},
  eprint = {2304.10124},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.10124},
  url = {http://arxiv.org/abs/2304.10124},
  urldate = {2024-05-13},
  abstract = {Asymmetrical multiplayer (AMP) game is a popular game genre which involves multiple types of agents competing or collaborating with each other in the game. It is difficult to train powerful agents that can defeat top human players in AMP games by typical self-play training method because of unbalancing characteristics in their asymmetrical environments. We propose asymmetric-evolution training (AET), a novel multi-agent reinforcement learning framework that can train multiple kinds of agents simultaneously in AMP game. We designed adaptive data adjustment (ADA) and environment randomization (ER) to optimize the AET process. We tested our method in a complex AMP game named Tom \textbackslash\& Jerry, and our AIs trained without using any human data can achieve a win rate of 98.5\% against top human players over 65 matches. The ablation experiments indicated that the proposed modules are beneficial to the framework.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/Users/brandonhosley/Zotero/storage/SCS6KMAT/Sun et al_2023_Mastering Asymmetrical Multiplayer Game with Multi-Agent Asymmetric-Evolution.pdf;/Users/brandonhosley/Zotero/storage/IQIGRGZG/2304.html}
}

@book{sutton2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  date = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {The MIT Press},
  location = {Cambridge, Massachusetts},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  langid = {english},
  pagetotal = {526},
  keywords = {Reinforcement learning},
  file = {/Users/brandonhosley/Zotero/storage/WBG2QEG6/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf}
}

@article{vinyals2019,
  title = {Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning},
  author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and Wünsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  date = {2019-11-14},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {575},
  number = {7782},
  pages = {350--354},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1724-z},
  url = {https://www.nature.com/articles/s41586-019-1724-z},
  urldate = {2023-12-24},
  langid = {english},
  file = {/Users/brandonhosley/Zotero/storage/97RK6RVS/Vinyals et al. - 2019 - Grandmaster level in StarCraft II using multi-agen.pdf}
}

@online{wakilpoor2020,
  title = {Heterogeneous {{Multi-Agent Reinforcement Learning}} for {{Unknown Environment Mapping}}},
  author = {Wakilpoor, Ceyer and Martin, Patrick J. and Rebhuhn, Carrie and Vu, Amanda},
  date = {2020-10-06},
  eprint = {2010.02663},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.02663},
  url = {http://arxiv.org/abs/2010.02663},
  urldate = {2024-05-14},
  abstract = {Reinforcement learning in heterogeneous multi-agent scenarios is important for real-world applications but presents challenges beyond those seen in homogeneous settings and simple benchmarks. In this work, we present an actor-critic algorithm that allows a team of heterogeneous agents to learn decentralized control policies for covering an unknown environment. This task is of interest to national security and emergency response organizations that would like to enhance situational awareness in hazardous areas by deploying teams of unmanned aerial vehicles. To solve this multi-agent coverage path planning problem in unknown environments, we augment a multi-agent actor-critic architecture with a new state encoding structure and triplet learning loss to support heterogeneous agent learning. We developed a simulation environment that includes real-world environmental factors such as turbulence, delayed communication, and agent loss, to train teams of agents as well as probe their robustness and flexibility to such disturbances.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems},
  file = {/Users/brandonhosley/Zotero/storage/JRDBJMX5/Wakilpoor et al_2020_Heterogeneous Multi-Agent Reinforcement Learning for Unknown Environment Mapping.pdf;/Users/brandonhosley/Zotero/storage/G975SLIL/2010.html}
}

@inproceedings{zheng2020,
  title = {Cooperative {{Heterogeneous Deep Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zheng, Han and Wei, Pengfei and Jiang, Jing and Long, Guodong and Lu, Qinghua and Zhang, Chengqi},
  date = {2020},
  volume = {33},
  pages = {17455--17465},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/ca3a9be77f7e88708afb20c8cdf44b60-Abstract.html},
  urldate = {2024-05-14},
  abstract = {Numerous deep reinforcement learning agents have been proposed, and each of them has its strengths and flaws. In this work, we present a Cooperative Heterogeneous Deep Reinforcement Learning (CHDRL) framework that can learn a policy by integrating the advantages of heterogeneous agents. Specifically, we propose a cooperative learning framework that classifies heterogeneous agents into two classes: global agents and local agents. Global agents are off-policy agents that can utilize experiences from the other agents. Local agents are either on-policy agents or population-based evolutionary algorithms (EAs) agents that can explore the local area effectively. We employ global agents, which are sample-efficient, to guide the learning of local agents so that local agents can benefit from the sample-efficient agents and simultaneously maintain their advantages, e.g., stability. Global agents also benefit from effective local searches. Experimental studies on a range of continuous control tasks from the Mujoco benchmark show that CHDRL achieves better performance compared with state-of-the-art baselines.},
  file = {/Users/brandonhosley/Zotero/storage/4XRVMVDD/Zheng et al_2020_Cooperative Heterogeneous Deep Reinforcement Learning.pdf}
}

@article{zhong2024,
  title = {Heterogeneous-{{Agent Reinforcement Learning}}},
  author = {Zhong, Yifan and Kuba, Jakub Grudzien and Feng, Xidong and Hu, Siyi and Ji, Jiaming and Yang, Yaodong},
  date = {2024},
  journaltitle = {Journal of Machine Learning Research},
  shortjournal = {JMLR},
  volume = {25},
  number = {32},
  pages = {1--67},
  url = {http://jmlr.org/papers/v25/23-0488.html},
  langid = {english},
  file = {/Users/brandonhosley/Zotero/storage/RP4HSFZU/Zhong et al. - Heterogeneous-Agent Reinforcement Learning.pdf}
}
