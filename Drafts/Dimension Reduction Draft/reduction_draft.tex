\documentclass{article}
\usepackage[english]{babel}
\usepackage{csquotes}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{hyperref}

\usepackage{graphicx}
\graphicspath{{../images/}}

\usepackage{tikz}
\usetikzlibrary{positioning}

% \usepackage{cleveref}
\usepackage[backend=biber, style=ieee]{biblatex}
\usepackage[T1]{fontenc}
\usepackage[final]{microtype}
\addbibresource{../../2025Bibs/Prospectus.bib}

\title{Dimension Reduction of Shared Observations in Multi-Agent Reinforcement Learning}
\author{Brandon Hosley}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Heterogeneous multi-agent reinforcement learning (HARL) systems 
often rely on architectures that assume a fixed number of agents, 
embedding this assumption directly into each agent's observation space. 
While such encodings simplify coordination and benchmarking, 
they introduce rigidity that limits generalization to variable team 
sizes or dynamic environments. 
In this work, we focus on the Level-Based Foraging (LBF) environment, 
a common benchmark for cooperative HARL, 
and identify structural constraints in its default observation encoding. 
We propose two observation reduction strategies that mitigate the dependence 
on fixed agent populations while preserving access to task-relevant information. 
Our experimental results demonstrate that these reductions enable more 
flexible and efficient training while maintaining competitive 
performance across varying team sizes.
\end{abstract}

\section{Introduction}

Multi-agent reinforcement learning (MARL) presents a compelling approach for training agents to 
interact and cooperate in complex, dynamic environments. It has been applied to a wide range of 
domains, including autonomous driving \cite{yang2021a}, swarm robotics, \cite{brambilla2013} 
decentralized logistics, and multi-agent games \cite{vinyals2019,berner2019}.
 
In MARL, agents must learn to optimize both individual and collective behavior in the presence 
of other learning agents. A particularly challenging and increasingly relevant subfield is 
heterogeneous MARL (HARL), in which agents differ in roles, capabilities, or observations. Such 
heterogeneity arises in systems where specialized functions enable tailored coordination 
strategies. For instance, drone swarms performing complex tasks have demonstrated improved 
performance through specialized role allocation \cite{huttenrauch2019}, while multi-robot 
systems in warehouse operations showcase the benefits of leveraging diverse capabilities 
\cite{rizk2019}. Similarly, collaborative search-and-rescue operations, which require dynamic 
role assignment and real-time coordination, highlight the necessity for flexible learning 
strategies \cite{hoang2023}. Additional challenges in heterogeneous settings are discussed in 
\cite{kapetanakis2005,hernandez-leal2019}.

While many benchmark environments support large-scale homogeneous agent populations with
shared policies (e.g., MAgent2 % [FLAG: Add MAgent2 reference]), heterogeneous MARL (HARL)
settings often encode agent identity and role explicitly, leading to observation structures
that assume a fixed number of uniquely-indexed agents. This approach restricts the observation
space to a specific configuration, making it difficult to scale or generalize across variable
team sizes. This limitation is evident in environments such as Level-Based Foraging (LBF)
\cite{papoudakis2021}, Multiwalker and Waterworld in the PettingZoo SISL suite
\cite{terry2021}, the StarCraft Multi-Agent Challenge (SMAC) \cite{samvelyan2019}, 
and Google Research Football \cite{kurach2020}, 
all of which embed the agent count into fixed-size observation spaces.

A common consequence of this design choice is that each agent receives an observation vector
whose size and semantics are tied to the total number of agents in the environment. 
For example, in the Level-Based Foraging (LBF) environment \cite{papoudakis2021}, 
agents observe the state of all teammates and all food items in a flat, concatenated format. 
This encoding implicitly fixes the number of agents, as each additional teammate introduces 
a new segment in the observation vector. As a result, policies trained on one team size cannot 
be directly applied to another without architectural modifications or retraining.

This constraint poses a significant barrier to the development of flexible and scalable HARL 
systems. It limits the applicability of methods such as transfer learning, 
zero-shot generalization, and curriculum learning, all of which depend on the ability 
to vary team composition during training or deployment. 
Moreover, it complicates experimentation by necessitating separate network architectures and 
training loops for each agent count. Addressing this limitation is essential for building 
general-purpose, adaptable multi-agent policies.

In this paper, we examine structural constraints in the default observation design of the
LBF environment and introduce two strategies to decouple observation size from agent count:
\begin{itemize}
    \item a minimal observation strategy that omits ally features entirely, and
    \item a structured reduction strategy that encodes ally information into a fixed-size,
    role-agnostic format.
\end{itemize}

We empirically evaluate these strategies against the default encoding across multiple team sizes.
Our results demonstrate that observation reduction can preserve, and in some cases even improve,
performance while enabling significantly more flexible training workflows.

% We retain the formal definition of the original observation structure here for reference:
% \[
% \underbrace{[\text{level}_{n_i},\ \text{row}_{n_i},\ \text{col}_{n_i}]}_{\text{Own Features}} ++
% \underbrace{[f \in F]}_{\text{Food Matrix}} ++
% \underbrace{[n \in N_{/i}]}_{\text{Agent Matrix}}
% \]


\section{Related Work}

\cite{busoniu2008} % An early survey of MARL

\section{Methodology}
% Default observation space
% Reduced observation space
% Egocentric observation space

% Not ego-st (carried behavioral implication)


% The environment: LBF

\subsection{Environment}

We use the Level-Based Foraging (LBF) environment \cite{papoudakis2021},
a grid-based reinforcement learning benchmark designed to evaluate 
coordination and cooperation in multi-agent settings,
that requires agents to jointly collect food items scattered across a map. 

Each agent and food item is assigned a discrete level, with the level representing 
the agent's foraging capability and the food's consumption requirement, respectively.
A food item can only be foraged when the sum of the levels of the agents 
occupying adjacent cells meets or exceeds the food's level.

The environment emphasizes multi-agent cooperation under spatial and temporal constraints. 
Agents must learn to navigate the grid, identify appropriate foraging opportunities, 
and coordinate with nearby teammates to successfully collect higher-level food items. 
Rewards are sparse and granted only upon successful foraging, 
making exploration and credit assignment particularly challenging. 
The environment can be configured for partial observability by limiting agent vision range, 
further increasing the need for implicit or explicit coordination.

\section{Results}

\section{Conclusion}

% \nocite{*}
\printbibliography

\end{document}