\documentclass{article}
\usepackage[english]{babel}
\usepackage{csquotes}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{hyperref}

\usepackage{graphicx}
\graphicspath{{../images/}}

\usepackage{tikz}
\usetikzlibrary{positioning}

% \usepackage{cleveref}
\usepackage[backend=biber, style=ieee]{biblatex}
\usepackage[T1]{fontenc}
\usepackage[final]{microtype}
\addbibresource{../../../2025Bibs/Prospectus.bib}

\title{Investigating Training Efficiency of Direct Scaling in Multi-Agent Reinforcement Learning}
\author{Brandon Hosley}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    This paper investigates the feasibility of training multi-agent reinforcement learning (MARL) 
    systems with a reduced number of agents before scaling up to full team sizes. 
    Inspired by prior work, particularly Smit et al. (2023), 
    we analyze whether pretraining smaller agent groups can improve training efficiency 
    without sacrificing final performance. 
    We introduce an agent-steps metric, which provides a standardized measure of 
    total training effort across different agent counts.
    Experiments conducted in the Waterworld, Multiwalker, and Level-based Foraging environments 
    reveal that the effectiveness of this approach appears to be inversely related 
    to the diversity required among agents in the final team. 
    When tasks allow agents to adopt similar roles, 
    pretraining on smaller groups accelerates learning; 
    however, in environments where agents must specialize into distinct roles, 
    the benefits of early training are diminished. 
    These findings inform future work in curriculum learning and scalable 
    heterogeneous-agent reinforcement learning (HARL).
\end{abstract}

\section{Introduction}

Multi-agent reinforcement learning (MARL) has shown significant promise in solving complex 
decision-making tasks across diverse domains, including strategic gameplay, robotics, 
and autonomous systems \cite{silver2016, vinyals2019, berner2019}. 
However, training MARL models at scale remains a computationally expensive process, 
often requiring extensive simulation time and large numbers of training episodes. 
The challenge is further compounded in heterogeneous-agent reinforcement learning (HARL), 
where agents may possess differing roles, observation spaces, or action capabilities, 
leading to an increase in learning complexity \cite{rizk2019, yang2021a}.
Efficient training methodologies that reduce computational costs while maintaining 
performance are crucial for enabling broader adoption of MARL and HARL systems.

Beyond differences in architecture, HARL also encompasses what we refer to as 
\emph{behavioral heterogeneity}, % a subtype of emergent heterogeneity
where agents may be structurally identical but their policies are updated independently 
during training, allowing their roles and behaviors to diverge over time.
This framing captures a range of realistic deployments—such as drone swarms, 
robotic warehouses \cite{rizk2019}, and cooperative games—where interchangeable agents 
can evolve distinct roles through interaction and policy drift. 
In these settings, the coordination burden increases even when agents are nominally homogeneous.

One proposed strategy to improve training efficiency involves pretraining a reduced subset of agents 
before scaling to the full target configuration. The intuition is that policies learned in smaller-team 
scenarios may converge more quickly and subsequently transfer to larger-scale tasks with lower 
overall cost. Smit et al.\ \cite{smit2023} evaluated this approach in cooperative football environments, 
but observed mixed results when scaling to full-team configurations. Their findings suggest that 
the effectiveness of such curricula is sensitive to task structure—particularly the degree to which 
role specialization is required.

We investigate the feasibility of a direct scaling strategy in environments with varying 
demands for coordination and role specialization. Our experiments employ an agent-steps 
metric that standardizes training cost by accounting for both agent count and time, 
enabling direct comparisons across curricula. To evaluate the generality of this approach, 
we test it on three benchmark environments—Waterworld \cite{gupta2017}, Multiwalker 
\cite{gupta2017}, and Level-Based Foraging (LBF) \cite{papoudakis2021}—selected for their 
differing degrees of behavioral symmetry and cooperative complexity. Building on prior work, 
we extend the basic strategy with an additional retraining phase: policies trained with 
fewer agents are used to initialize larger teams, which are then further adapted in the 
target configuration. This approach enables evaluation of transferability across 
configurations with varying degrees of agent specialization.

While environments such as MAgent2 \cite{zheng2017} support large agent populations through 
shared policies, most HARL benchmarks fix the number of agents in their observation structures. 
This entanglement of team size with input dimensionality restricts policy transfer across 
configurations and complicates the use of curriculum learning or zero-shot generalization. 
Environments like LBF and Multiwalker from PettingZoo \cite{terry2021}, as well as 
SMAC \cite{samvelyan2019} and Google Research Football (GRF) \cite{kurach2020}, 
encode team size directly into observation formats, making policy reuse across scales nontrivial.

To enable fair comparisons, we adapted the observation structure in the Level-Based Foraging (LBF) 
environment to decouple the number of agents from the observation dimensionality. 
Each agent in LBF receives a fully observable state, which includes a structured array of 
features for all teammates—an input that scales with the size of the team. As a result, changing 
team size alters the observation dimensionality, making policy transfer across configurations 
incompatible with fixed architectures. To address this, we implemented two lightweight 
observation modifications: (1) a restricted mode that omits all ally-specific information, 
maintaining a constant observation size across configurations, 
and (2) an abbreviated format that retains a fixed number of teammate feature slots, 
randomly sampling allies when the team exceeds the pretraining configuration. 
These adaptations preserve architectural compatibility and facilitate systematic 
evaluation of policy transferability across team sizes.


\section{Related Work}

\subsection{Training Efficiency and Curriculum Learning in MARL}

Improving training efficiency in multi-agent reinforcement learning (MARL) has become a research 
priority, especially given the computational costs and sample complexity that scale with team 
size and task difficulty. Prior research has explored MARL scalability and efficiency in 
large-scale environments, investigating methods to reduce overestimation bias, 
support policy decentralization, and optimize communication overheads 
\cite{shoham2007a, busoniu2008, foerster2017, ackermann2019, lowe2020}.
Curriculum learning has emerged as a promising approach, where tasks are presented in a structured 
sequence to facilitate progressive skill acquisition. This strategy can take many forms, 
including environment simplification \cite{shukla2022}, task decomposition \cite{shi2023}, 
or gradually increasing the number of agents during training \cite{smit2023, albrecht2024}.

Transfer learning approaches have also been used to mitigate the cost of retraining agents 
tabula rasa (i.e., without prior initialization) in each new configuration. 
In typical applications, knowledge learned in a source task—often with reduced 
complexity or smaller scale—is used to initialize agents, 
which are then fine-tuned to adapt to a target task \cite{cui2022}. 
These methods have shown success in domains where skills are composable 
or generalize well across task variants. However, their effectiveness often 
depends on architectural compatibility and the degree of agent or task homogeneity.

Smit et al.\ \cite{smit2023} investigated curriculum learning in a simplified version of the 
Google Research Football (GRF) environment, focused on player positioning rather than 
fine-grained actions or physics simulation. Their approach trained agents in 2v2 scenarios 
before scaling to full 11v11 teams, evaluating transfer performance using multiple baselines. 
While conceptually appealing, their curriculum yielded mixed results: agents trained directly 
in 11v11 settings consistently outperformed those pretrained in smaller games, exhibiting 
stronger coordination and role differentiation. These findings suggest that when behavioral 
specialization is critical, small-scale pretraining may not expose agents to sufficiently 
diverse interactions to foster generalizable cooperation.

Our work extends this line of research by evaluating how smaller-team pretraining followed 
by retraining in the full agent configuration can accelerate learning. 
Distinct from earlier efforts, we prioritize structural compatibility between 
training phases through deliberate observation design. 
We also examine the effects of this strategy in environments with varying degrees of 
behavioral heterogeneity, offering insight into how task structure influences 
transferability and efficiency.

\subsection{Heterogeneous MARL and Policy Transferability}

Heterogeneous MARL (HARL) introduces additional complexity by requiring agents with differing
roles, capabilities, or observation modalities to coordinate effectively. This setting reflects
many real-world domains, including swarm robotics \cite{hoang2023}, traffic control \cite{calvo2018},
and decentralized logistics \cite{rizk2019}, where adaptability to team composition and role diversity
is essential.

Prior work has explored architectures that support specialization and flexibility. 
For example, Gupta et al. \cite{gupta2017a} and H{\"u}ttenrauch et al. \cite{huttenrauch2019} 
both demonstrate techniques for encoding structured input using single-agent policies that 
can generalize across role configurations, but do not address multi-agent coordination directly. 
Iqbal et al. \cite{iqbal2021} introduce entity-centric methods in a homogeneous MARL setting, 
which allow pooling across similar agents but still assume identical agent capabilities. 
While these methods improve generalization within their respective assumptions, they often 
maintain a fixed number of roles or homogeneous agent groups, limiting their applicability 
in dynamic heterogeneous settings.

Our work contributes to this space by focusing on observation-level design, offering strategies
that allow agent populations to vary without modifying the underlying network architecture.
This enables more efficient policy reuse and deployment across tasks with differing team sizes,
a capability not well supported in most existing HARL approaches.

\subsection{Observation Design in Multi-Agent Systems}

Observation space design plays a central role in the generalization, scalability, 
and transferability of learned policies in reinforcement learning, 
particularly in the multi-agent setting. 
Many environments—such as SMAC \cite{samvelyan2019}, 
Google Research Football (GRF) \cite{kurach2020}, 
and Level-Based Foraging (LBF) \cite{papoudakis2021}—encode observations as fixed-size vectors, 
with dedicated segments for each teammate or nearby object. 
As a result, the dimensionality of the observation space scales with the 
number of agents present in the environment. 
This tight coupling between team size and observation format restricts the 
reuse of trained policies across varying team configurations and undermines 
modular architectural design. These limitations pose challenges for curriculum learning, 
zero-shot generalization, and transfer learning—scenarios that require robust generalization 
to variable input structures.

% \includegraphics[width=0.75\linewidth]{2d_football.png}

While few works frame this issue directly as a problem of \textit{dimension reduction}, 
several methods address the challenge indirectly by compressing, pooling, 
or abstracting over agent-specific information. 
Entity-centric approaches such as REFIL \cite{iqbal2021} and mean embedding strategies 
\cite{huttenrauch2019} represent collections of agents or objects using permutation-invariant 
transformations, effectively decoupling policy input from team size. 
Similarly, attention-based strategies (e.g., top-$k$ pooling) and mean-field approximations 
\cite{yang2021a} reduce observation complexity by emphasizing only the most relevant entities.

Some works explore architectural modifications to better accommodate agent specialization 
or variability. Kouzeghar et al.~\cite{kouzeghar2023}, for example, adopt a decentralized 
approach to UAV swarm coordination by training distinct policies for different drone types, 
without employing role embeddings or shared parameterization. 
Gupta et al.~\cite{gupta2017a}, in contrast, employ an embedding-based representation to 
enable knowledge transfer across morphologically distinct robots, though their setting 
involves single-agent learning. These studies reflect broader architectural strategies that 
address variability in agent capabilities or embodiment, even if not in a MARL context.

Despite their promise, these strategies often come with architectural overhead or assume 
centralized training with shared state information \cite{foerster2017}. 
Moreover, they typically require retraining or re-encoding when the agent pool or spatial 
layout changes significantly. Our work takes a complementary approach: 
we treat the dimensionality of the observation vector itself as a constraint and propose 
lightweight observation-level reductions that preserve task-relevant structure 
while enabling flexible agent counts. 
Related efforts in high-dimensional robotics and policy compression similarly leverage 
latent representations to improve sample efficiency and structural adaptability 
\cite{bitzer2010, tangkaratt2016}.


% Anchor 4: Benchmark Design and Evaluation Frameworks

\subsection{Benchmarks and Evaluation}

Prior literature has explored alternative approaches to HARL evaluation. 
Wakilpoor et al.~\cite{wakilpoor2020} introduce an attention-guided multi-agent actor-critic 
that enables efficient coordination between agents with complementary abilities. 
Koster et al.~\cite{koster2020} similarly propose an attention-based method for centralized 
training of heterogeneous agents, enabling shared learning across distinct roles. 
These works demonstrate scalable performance in specialized evaluation domains, 
yet the broader field still lacks general-purpose benchmarks that support systematic 
study of heterogeneity across configurations.

Benchmarking HARL systems under varying team configurations presents persistent
challenges for experimental design and policy generalization. While many benchmarks 
such as LBF \cite{papoudakis2021}, Melting Pot \cite{leibo2021}, and MAgent2 \cite{zheng2017} 
support multi-agent setups, they are primarily designed for homogeneous MARL 
and often rely on shared policies or fixed observation formats. This limits 
their utility in studying systems with intrinsic heterogeneity—where agents possess 
distinct capabilities or behaviors by design. As a result, dedicated HARL benchmarks 
remain sparse, and evaluating emergent or role-specific behaviors is often constrained 
by the assumptions and architectures of existing platforms.

Recent benchmarks such as SMACv2 \cite{ellis2023} and 
Google Research Football (GRF) \cite{kurach2020} offer rich multi-agent coordination scenarios 
and introduce generalization challenges through procedural variation and strategic diversity. 
While both environments often employ fixed team sizes, 
they can be adapted to support varying agent counts.
Our work complements these efforts by demonstrating how a simple, practical approach to observation 
restructuring can enable policy evaluation across varying team sizes. By preserving a fixed 
observation format through lightweight modifications, we support compatibility across configurations 
without requiring architectural changes or complex preprocessing.
This approach contributes toward scalable benchmarking practices and supports 
more flexible evaluation of heterogeneous-agent reinforcement learning (HARL) systems.

% %% sub c1 - hetero rl
% % argue confusion
% % define terms
% % b. het
% % i. het

\section{Methodology}\label{sec:methodology}

\subsection{Training Framework for Heterogeneous MARL} 
We formulate each task as a cooperative multi-agent partially observable Markov 
decision process (MA-POMDP). At each timestep, each of the $N$ agents receives 
an observation of the environment $o_n$ and selects an action $a_n$ from its action space. 
Although the agents receive individual rewards $r_n$, these reward functions are aligned 
toward a shared team objective, reflecting the cooperative nature of the task.
The objective function is such that policies \(\pi_n(a_n|o_n)\)
that maximizes the expected cumulative team reward
\[\sum_{k=t+1}^{T} \sum_{n\in N} r_{n,k}.\]
These cooperative settings benefit the multi-agent problem as a single-team optimization, 
encouraging agents to work together. 

Our training procedure follows the centralized training with 
decentralized execution (CTDE) paradigm, which is a standard 
framework for MARL in cooperative tasks 
\cite{guo2024}
We employ an actor-critic approach wherein a centralized critic (value function) 
is trained to estimate joint state-value, while each agent has its own actor (policy) 
operating solely on local observations during execution. 
This leverages the strengths of actor-critic methods in MARL. 



\paragraph{Shared and Specialized Policy Representations.} 
We design a **multi-agent PPO (MAPPO)** training regimen that promotes 
both knowledge sharing and agent specialization. 
In scenarios where agents are homogeneous (i.e., identical observation and action spaces), 
we \emph{share policy parameters} 
across all agents 
Parameter sharing is highly scalable and has proven effective in speeding up learning for teams of identical agents 

It allows experience to be pooled, so that behaviors learned by one agent can immediately benefit others. However, naively sharing one policy for all agents can limit the emergence of diverse behaviors and is problematic if agents have different roles or observations 

To handle **heterogeneous agents** (differences in capabilities or observation/action dimensions), we incorporate agent-specific information into the policy input and, where necessary, use separate policy networks for distinct agent types. In practice, this means each agent’s observation $o^i$ is augmented with features encoding its identity or capabilities (e.g., a skill level or role indicator). This technique enables a single neural policy to condition on the agent’s attributes, effectively allowing specialization within a shared policy architecture 

Through this design, we strike a balance between the efficiency of a shared representation and the flexibility for agents to exhibit different strategies. Past research has shown that such partially shared representations can handle heterogeneous inputs/outputs and still facilitate knowledge transfer among agents 

All policy networks are trained with PPO's clipped surrogate objective and generalized advantage estimation, using minibatch updates after each training epoch.

% Another implementation detail for transferability is maintaining a consistent observation structure across varying team sizes. As the number of agents $N$ changes, the raw observation for an agent (e.g., positions or states of other agents) could vary in dimension. We address this by using a fixed-size observation encoding for each agent. Specifically, we cap the number of other agents or entities considered in each agent’s observation and pad with dummy values or mask inputs beyond this cap [oai_citation_attribution:11‡arxiv.org](https://arxiv.org/html/2404.03869v2#:~:text=In%20order%20to%20directly%20zero,Hence%2C%20we%20can%20concentrate%20on). This way, the input dimension of the policy network remains constant even if the team size changes, eliminating the need to redesign network architecture for different $N$. This approach, inspired by prior works on zero-shot scalable MARL, ensures that a learned policy can be \emph{directly reused} in scenarios with a different number of agents [oai_citation_attribution:12‡arxiv.org](https://arxiv.org/html/2404.03869v2#:~:text=In%20order%20to%20directly%20zero,Hence%2C%20we%20can%20concentrate%20on). In summary, our training framework yields a policy (or set of policies for heterogeneous roles) that is compatible with varying team sizes and agent types, which is crucial for our transfer learning objectives.

% \subsection{Curriculum-Based Team Expansion and Transfer Learning} 
% A core contribution of our methodology is a \textbf{curriculum learning strategy} for progressively increasing the team size during training. Rather than training from scratch on the largest, most complex team configuration, we start with a smaller number of agents and gradually scale up. This staged training follows the intuition of easing the learning problem by first mastering simpler scenarios [oai_citation_attribution:13‡rohanpaleja.com](https://www.rohanpaleja.com/assets/pdf/RSS22_SRL_Workshop_Paper_final_accepted.pdf#:~:text=proposed%20techniques%20have%20included%20Intra,based%20repre%02sentations%20are%20ideal%20for). Concretely, we begin with the minimum team size that meaningfully exercises cooperation in each environment (e.g., two agents in the Level-Based Foraging task), and train the agent policies until convergence. We then **transfer** the learned policy parameters to initialize training for a slightly larger team (e.g., three agents), and continue this process until reaching the maximum team size targeted in our experiments. At each stage of this curriculum, the policy is fine-tuned in the new team size environment. By reusing knowledge acquired in simpler tasks, the agents adapt more quickly to the increased complexity [oai_citation_attribution:14‡rohanpaleja.com](https://www.rohanpaleja.com/assets/pdf/RSS22_SRL_Workshop_Paper_final_accepted.pdf#:~:text=but%20related%20task%20,based%20repre%02sentations%20are%20ideal%20for) [oai_citation_attribution:15‡arxiv.org](https://arxiv.org/abs/2204.12937#:~:text=and%20transfer%20across%20team%20sizes,Management%20benchmark). This approach can be seen as a form of multi-task transfer learning, where the source tasks are the smaller-team games and the target task is the larger-team game [oai_citation_attribution:16‡rohanpaleja.com](https://www.rohanpaleja.com/assets/pdf/RSS22_SRL_Workshop_Paper_final_accepted.pdf#:~:text=easier%20task%20and%20then%20transferred,Han). We classify it as \emph{policy reuse} because the same underlying policy is iteratively adapted for new team sizes, rather than learning a separate policy from scratch for each team configuration [oai_citation_attribution:17‡rohanpaleja.com](https://www.rohanpaleja.com/assets/pdf/RSS22_SRL_Workshop_Paper_final_accepted.pdf#:~:text=easier%20task%20and%20then%20transferred,Han). 

% The curriculum is designed to promote the emergence of coordinated strategies and \textbf{role specialization} in a manner that generalizes across scales. Early in training (with fewer agents), the agents discover basic cooperative tactics. As new agents are added, the previously learned behaviors serve as a foundation — for example, agents can recognize and assume complementary roles that were effective in smaller teams and extend them to larger teams. Recent studies have shown that transferring role structures from small teams to larger teams can significantly accelerate learning and improve performance in the larger teams [oai_citation_attribution:18‡arxiv.org](https://arxiv.org/abs/2204.12937#:~:text=and%20transfer%20across%20team%20sizes,Management%20benchmark). By incrementally increasing $N$, our method allows the policy to adjust to new **role assignments** required for bigger teams without losing the core coordination mechanisms learned earlier. This fosters a robust policy that implicitly covers a spectrum of team sizes. In effect, the policy learns to be \emph{adaptive} to team composition, which is essential for scalable MARL [oai_citation_attribution:19‡arxiv.org](https://arxiv.org/html/2404.03869v2#:~:text=required%20for%20heterogeneous%20strategies,a%20more%20practical%20MARL%20algorithm) [oai_citation_attribution:20‡arxiv.org](https://arxiv.org/html/2404.03869v2#:~:text=Some%20works%C2%A0,Motivated%20by%20these). Distinct from traditional MARL training confined to a fixed team, our approach directly addresses scalability by ensuring that both the learning process and the learned behaviors remain effective as the team grows [oai_citation_attribution:21‡arxiv.org](https://arxiv.org/html/2404.03869v2#:~:text=Despite%20its%20successes%2C%20traditional%20MARL,the%20role%20assignment%20and%20the).

% It is worth noting that our curriculum-based transfer does \textbf{not require additional training} when deploying the policy on intermediate team sizes that were not explicitly seen during training. Because of the parameter-sharing design and unified observation space, an agent policy trained with, say, $k$ agents can be applied \emph{zero-shot} to an environment with $k+1$ or $k-1$ agents. We evaluate such zero-shot transferability to confirm that our learned policies are indeed scalable without retraining [oai_citation_attribution:22‡arxiv.org](https://arxiv.org/html/2404.03869v2#:~:text=Specifically%2C%20in%20this%20paper%2C%20we,with%20varying%20numbers%20of%20vehicles). In cases where fine-tuning is desirable (for significantly larger teams), the curriculum provides an excellent initialization that significantly speeds up convergence on the new task [oai_citation_attribution:23‡arxiv.org](https://arxiv.org/abs/2204.12937#:~:text=teams%2C%20which%20continue%20to%20learn,proposal%20outperforms%20competing%20techniques%20in). Overall, this training paradigm is aligned with how humans might learn a multi-agent task: start in a small group and gradually adapt to larger groups, reusing prior knowledge at each step. By the end of training, we obtain a set of policies (often a single policy, in the parameter-sharing case) that has been exposed to a range of team sizes and is thus well-prepared to be deployed in a variety of scenarios. This methodological design directly supports our research goal of **transferable heterogeneous-agent teamwork** across different scales.

% \subsection{Evaluation Environments and Protocol} 
% We validate our approach on three multi-agent environments chosen for their diversity and challenge to heterogeneous-agent learning: \textbf{(1) Waterworld}, \textbf{(2) Multiwalker}, and \textbf{(3) Level-Based Foraging (LBF)}. These domains, originally introduced by Gupta et al. (2017) and extended in the PettingZoo MARL benchmark [oai_citation_attribution:24‡openreview.net](https://openreview.net/pdf?id=WoLQsYU8aZ#:~:text=We%20finally%20included%20the%20three,%282002%29%20where%206), feature different observation structures and degrees of heterogeneity, providing a thorough testbed for our transfer learning framework.

% \textbf{Waterworld:} Waterworld is a continuous control environment where $N$ \emph{pursuer} agents navigate a two-dimensional continuous arena to collect food targets while avoiding poisonous targets [oai_citation_attribution:25‡openreview.net](https://openreview.net/pdf?id=WoLQsYU8aZ#:~:text=randomly%20generated%20evaders%20by%20surrounding,while%20trying%20to%20avoid%20poison). The agents are homogeneous and have identical action spaces (e.g., thrust in different directions). Each agent’s observation consists of continuous readings (such as distances and angles) to nearby food pellets, poison pellets, and other agents, often making the environment fully observable in practice (each agent can sense all relevant objects within a certain range). The team receives a dense reward: every food target consumed yields a positive reward (often shared among agents or given to the specific collector, but ultimately contributing to the team’s total), and contact with poison yields a negative reward. Cooperation is required in the sense that agents must spread out to cover the area and herd targets effectively without collisions or wasted effort. Waterworld primarily evaluates scalability in a symmetric setting — since all agents are identical, an effective policy should handle an increase in the number of pursuers seamlessly. We use Waterworld to test whether our learned policy can coordinate larger swarms of agents hunting targets, without sacrificing performance on individual avoidance of poison.

% \textbf{Multiwalker:} Multiwalker is a more complex continuous cooperative task based on the OpenAI Gym BipedalWalker. In this environment, multiple bipedal ``walker'' agents must jointly transport a payload (e.g., a ladder or package balanced on their heads) from one end of a field to the other without dropping it [oai_citation_attribution:26‡openreview.net](https://openreview.net/references/pdf?id=TpkQ7ItXO3#:~:text=Each%20robot%20is%20given%20a,large%20penalty%20for%20dropping). The walkers are physically coupled through the shared object: if any walker moves too fast or falls, the object may drop, penalizing the team. Each agent receives a reward signal that is mostly global – for instance, a small positive reward for each time step that the package moves forward without falling, and a large negative reward if the package is dropped (this penalty is applied to all walkers) [oai_citation_attribution:27‡openreview.net](https://openreview.net/references/pdf?id=TpkQ7ItXO3#:~:text=Each%20robot%20is%20given%20a,large%20penalty%20for%20dropping). The agents are again homogeneous in dynamics and control (each controls its two-legged walker), and each observes its own state (joint angles, velocities) as well as some information about the package (e.g., its relative position/tilt) and possibly the other walkers’ positions. We consider Multiwalker to have \emph{full observability} as implemented (the state can be reconstructed from the agents’ joint observations, and in some implementations each agent may even get the full state). The key challenge here is tight coordination: the policy must ensure synchronous movement among walkers to keep the payload balanced. This environment tests our method’s ability to scale a locomotion-based policy from a smaller team (e.g., 2 walkers) to a larger one (e.g., 3 or 4 walkers). A successful transferable policy would allow additional walkers to be integrated into the team without retraining, maintaining balance and forward progress. We evaluate how our curriculum training helps the walkers learn complementary gait patterns and whether those patterns remain effective as the team grows.

% \textbf{Level-Based Foraging (LBF):} LBF is a discrete grid-world environment that explicitly requires heterogeneous-agent cooperation [oai_citation_attribution:28‡marllib.readthedocs.io](https://marllib.readthedocs.io/en/latest/handbook/env.html#:~:text=Level,with%20other%20agents%20if%20needed). Agents and food items are placed on a grid; each food item has a level (an integer requirement) and each agent has its own skill level. An agent can collect a food item only if one or more agents cooperatively occupy the same cell as that food and the sum of their skill levels meets or exceeds the food’s level requirement. All agents share a team reward for successful food collection, making it a fully cooperative game. However, the environment is partially observable: agents have a limited field of view (they only observe nearby cells, not the entire grid) [oai_citation_attribution:29‡marllib.readthedocs.io](https://marllib.readthedocs.io/en/latest/handbook/env.html#:~:text=). This means agents must explore and possibly coordinate via implicit communication (e.g., meeting at a target) under uncertainty. The heterogeneity arises naturally because agents may have different skill levels; for example, a level-3 agent can collect certain food alone that a level-1 agent cannot, whereas two level-1 agents together might achieve it. Optimal play often involves \emph{specialization}, where higher-level agents take on the role of tackling high-value food or assisting lower-level agents, and lower-level agents focus on targets that match their ability or support others. We treat LBF as a critical test for our approach’s ability to handle agents with \textbf{diverse capabilities and partial observability}. Our training framework includes the agents’ skill level as part of the observation input, so the shared policy can condition decisions on an agent’s own ability. During evaluation, we examine whether the learned policy indeed yields different behaviors for agents of different levels (indicating emergent specialization) and how well it generalizes to changes in team composition, such as adding an extra agent with a new skill level. LBF being a mixed cooperative-competitive domain in general [oai_citation_attribution:30‡marllib.readthedocs.io](https://marllib.readthedocs.io/en/latest/handbook/env.html#:~:text=Level,with%20other%20agents%20if%20needed), we focus on the fully cooperative version here (all agents on one team, no adversaries), aligning with our centralized team reward setting.

% \paragraph{Evaluation protocol.} For each environment, we train our agents using the curriculum strategy up to a maximum team size $N_{\max}$ (which is 5 in Waterworld, 3 in Multiwalker, and 5 in LBF in our experiments, as dictated by environment difficulty). We then evaluate the following: (a) \textit{Within-distribution performance}: the policy’s performance on the team sizes it was trained on (to ensure our method does not sacrifice optimal performance in the training scenarios); and (b) \textit{Transfer performance across team sizes}: we deploy the final learned policy on **unseen team sizes** and measure its zero-shot effectiveness [oai_citation_attribution:31‡arxiv.org](https://arxiv.org/html/2404.03869v2#:~:text=Specifically%2C%20in%20this%20paper%2C%20we,with%20varying%20numbers%20of%20vehicles). For example, a policy trained via curriculum from 2 up to 5 agents is tested on 4 agents (if 4 was never explicitly trained) and on 6 agents (beyond the trained range) to assess generalization. We quantify performance using average episodic return and success metrics specific to each domain (such as number of food targets collected in Waterworld, distance carried in Multiwalker, and completion rate of all food in LBF). To provide context, we compare against a baseline where no curriculum is used (i.e. training directly on the largest team size with random initialization). This allows us to attribute any improvement in sample efficiency or final performance to the proposed curriculum transfer approach. Moreover, by analyzing behaviors (through visualizations or log traces), we verify if agents trained under our method indeed exhibit coordinated strategies and adaptive role assignment consistent with the research goals. All experiments are run for multiple random seeds, and we report mean and variance of the performance to ensure statistical significance. The overall experimental methodology is thus structured to demonstrate that our approach yields policies that are not only effective on the training team sizes but also \emph{transferable} to different team configurations and robust in the face of heterogeneity and scaling challenges. This directly supports our claims of improved transferability, agent specialization, and scalable learning in MARL.






    % \subsection{Experimental Setup}
    % We conducted experiments in the Waterworld environment, a multi-agent domain within the SISL package. Agents navigate a continuous space, collecting resources and avoiding obstacles. Key attributes of the environment include:
    % \begin{itemize}
    %     \item Adjustable agent count with cooperative dynamics.
    %     \item Potential for role differentiation through independent policy learning.
    % \end{itemize}

    % Our training protocol utilizes Proximal Policy Optimization (PPO) \cite{schulman2017}, implemented within the RLlib framework, providing flexibility for future algorithmic comparisons. Training is conducted over multiple iterations, with performance measured based on reward progression and convergence speed.

    % \subsection{Adjusted Time Steps}
    % To compare computational costs across agent counts, we introduce adjusted time steps, accounting for the linear scaling of per-step computation. The empirical relationship for training step duration is given by:
    % \begin{equation}
    %     \text{time(ms)} = 324.2441 + 7004.7673n
    % \end{equation}
    % where $n$ is the number of agents. Adjusted time steps enable fair cost comparisons across training configurations.

    % \subsection{Training Process}
    % Our approach involves:
    % \begin{itemize}
    %     \item Pretraining a smaller number of agents, saving policy checkpoints periodically.
    %     \item Upsampling pretrained agents without replacement to instantiate larger environments.
    %     \item Retraining the scaled-up agent team and comparing performance against a tabula rasa baseline.
    % \end{itemize}

This study evaluates the training efficiency of upsampling policies learned in smaller-team configurations, compared to baseline models trained tabula rasa with full team sizes. We conduct experiments in three multi-agent reinforcement learning (MARL) environments to assess the transferability and cost-effectiveness of this approach under varying heterogeneity conditions.

\subsection{Environments}

We selected three environments with distinct heterogeneity profiles:
\begin{itemize}
    \item \textbf{Waterworld} \cite{gupta2017} (from SISL in PettingZoo) exhibits \emph{emergent behavioral heterogeneity}, where agents begin identical but may develop distinct roles through independent policy updates.
    \item \textbf{Multiwalker} \cite{gupta2017} includes \emph{static intrinsic heterogeneity}, as agents occupy fixed positions within a chain-like system that impacts their observations and coordination roles.
    \item \textbf{Level-Based Foraging (LBF)} \cite{papoudakis2021} supports \emph{dynamic intrinsic heterogeneity}, as agents independently level up and adopt roles of varying effectiveness during training.
\end{itemize}

Among these, only LBF required modifications to its observation structure to decouple team size from input dimensionality. As detailed in Section~\ref{sec:observation}, we applied observation restrictions and fixed-length representations to enable policy reuse across team sizes.

\subsection{Training and Evaluation Procedure}

We adopted Proximal Policy Optimization (PPO) as implemented in RLlib, using default parameters for Waterworld and LBF, and a modified network architecture for Multiwalker. \textbf{[Placeholder: Insert Table with Environment-Specific Hyperparameters]}

For each environment, we identified a minimal viable agent count for pretraining: 2 agents for Waterworld and LBF, and 3 agents for Multiwalker. These agents were trained until convergence, with periodic policy checkpoints saved throughout training.

Upsampling was performed by duplicating pretrained policies to create larger teams of up to 8 agents for Waterworld and LBF, and up to 7 agents for Multiwalker. Duplication was performed without replacement, and when the target agent count exceeded a multiple of the pretrained pool, one additional copy was selected at random. For Waterworld and LBF, all agent IDs were treated as interchangeable. In Multiwalker, however, the leading agent was kept fixed, with the middle and rear agents selected from the checkpoint pool.

Retraining was conducted using these upsampled teams, initializing each agent with one of the pretrained policies. During retraining, all agents updated their policies independently, allowing behavioral differentiation to emerge or continue from the initial state. Multiple checkpoints were tested to represent different pretraining durations.

\subsection{Baseline and Metrics}

To assess efficiency gains, we compared retrained configurations to baseline models trained tabula rasa with the same number of agents. Performance was evaluated using \textit{agent-steps}, defined as the product of the number of agents and training iterations. This metric accounts for the linear scaling of per-step computational cost with team size, as empirically observed in all environments. Agent-steps provides a standardized cost measure, enabling fair comparisons across curricula.

Additional analyses in the Results section will investigate reward progression, convergence speed, and environmental sensitivity to policy transfer. This design allows us to isolate the effects of upsampling under different heterogeneity pressures, and to evaluate when this curriculum strategy may be most beneficial.


\vspace{3em}

% \begin{figure}[h]
%     \centering
%     \begin{tikzpicture}
%         % Small observation array (Pretraining Phase)
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm] (goal) at (0,0) {Goal};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of goal] (self) {Self};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of self, fill=blue!20] (ally1) {Ally 1};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of ally1, fill=blue!20] (ally2) {Ally 2};
        
%         \node at (-2, 0) {Pretraining (Small Team)};
        
%         % Large observation array (Scaled-up Phase)
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm] (goal2) at (0,-2) {Goal};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of goal2] (self2) {Self};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of self2, fill=blue!20] (ally1_2) {Ally 1};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of ally1_2, fill=blue!20] (ally2_2) {Ally 2};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of ally2_2, fill=red!20] (sampled) {Sampled Ally};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of sampled, fill=red!20] (sampled2) {Sampled Ally};
        
%         \node at (-2, -2) {Full Training (Larger Team)};
        
%         % Arrows to indicate sampling process
%         \draw[->, thick] (ally1) -- (ally1_2);
%         \draw[->, thick] (ally2) -- (ally2_2);
%         \draw[->, dashed, thick] (ally1) -- (sampled);
%         \draw[->, dashed, thick] (ally2) -- (sampled2);
%     \end{tikzpicture}
%     \caption{Illustration of observation array handling when transitioning from a small-team configuration to a larger one. Dashed arrows indicate sampled ally data.}
% \end{figure}

\vspace{3em}

% We retain the formal definition of the original observation structure here for reference:
% \[
% \underbrace{[\text{level}_{n_i},\ \text{row}_{n_i},\ \text{col}_{n_i}]}_{\text{Own Features}} ++
% \underbrace{[f \in F]}_{\text{Food Matrix}} ++
% \underbrace{[n \in N_{/i}]}_{\text{Agent Matrix}}
% \]



% Default observation space
% Reduced observation space
% Egocentric observation space

% Not ego-st (carried behavioral implication)
    
\subsection{Environment}

We use the Level-Based Foraging (LBF) environment \cite{papoudakis2021},
a grid-based reinforcement learning benchmark designed to evaluate 
coordination and cooperation in multi-agent settings,
that requires agents to jointly collect food items scattered across a map. 

Each agent and food item is assigned a discrete level, with the level representing 
the agent's foraging capability and the food's consumption requirement, respectively.
A food item can only be foraged when the sum of the levels of the agents 
occupying adjacent cells meets or exceeds the food's level.

The environment emphasizes multi-agent cooperation under spatial and temporal constraints. 
Agents must learn to navigate the grid, identify appropriate foraging opportunities, 
and coordinate with nearby teammates to successfully collect higher-level food items. 
Rewards are sparse and granted only upon successful foraging, 
making exploration and credit assignment particularly challenging. 
The environment can be configured for partial observability by limiting agent vision range, 
further increasing the need for implicit or explicit coordination.


% \section{Results and Observations}
% Our findings indicate that convergence occurs earlier with fewer agents, particularly when agent counts differ significantly. Performance gains may be due to task tessellation, warranting further investigation. Key observations include:
% \begin{itemize}
%     \item Training time per step scales linearly with the number of agents.
%     \item Pretraining smaller groups can lead to faster convergence in certain scenarios.
%     \item Performance benefits diminish when tasks lack natural role division.
% \end{itemize}

% \section{Future Work}
% Building on these findings, Contribution 2 will explore curriculum-based training strategies, progressively increasing agent counts in complex tasks. Additionally, we aim to investigate biological models of cooperation and specialization for further insights into HARL systems.

% \section{Conclusion}
% This study demonstrates the potential benefits of pretraining smaller MARL teams before scaling up. By introducing adjusted time steps and analyzing training efficiency, we contribute to understanding scalable MARL techniques. Future work will refine these methods and expand the applicability to more complex environments.


% \nocite{*}
\printbibliography

\end{document}