\documentclass{article}
\usepackage[english]{babel}
\usepackage{csquotes}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{hyperref}

\usepackage{graphicx}
\graphicspath{{../images/}}

\usepackage{tikz}
\usetikzlibrary{positioning}

% \usepackage{cleveref}
\usepackage[backend=biber, style=ieee]{biblatex}
\usepackage[T1]{fontenc}
\usepackage[final]{microtype}
\addbibresource{../../2025Bibs/Prospectus.bib}

\title{Investigating Training Efficiency of Direct Scaling in Multi-Agent Reinforcement Learning}
\author{Brandon Hosley}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    This paper investigates the feasibility of training multi-agent reinforcement learning (MARL) 
    systems with a reduced number of agents before scaling up to full team sizes. 
    Inspired by prior work, particularly Smit et al. (2023), 
    we analyze whether pretraining smaller agent groups can improve training efficiency 
    without sacrificing final performance. 
    We introduce an agent-steps metric, which provides a standardized measure of 
    total training effort across different agent counts.
    Experiments conducted in the Waterworld, Multiwalker, and Level-based Foraging environments 
    reveal that the effectiveness of this approach appears to be inversely related 
    to the diversity required among agents in the final team. 
    When tasks allow agents to adopt similar roles, 
    pretraining on smaller groups accelerates learning; 
    however, in environments where agents must specialize into distinct roles, 
    the benefits of early training are diminished. 
    These findings inform future work in curriculum learning and scalable 
    heterogeneous-agent reinforcement learning (HARL).
\end{abstract}

\section{Introduction}

Multi-agent reinforcement learning (MARL) has shown significant promise in solving complex 
decision-making tasks across diverse domains, including strategic gameplay, robotics, 
and autonomous systems \cite{silver2016, vinyals2019, berner2019}. 
However, training MARL models at scale remains a computationally expensive process, 
often requiring extensive simulation time and large numbers of training episodes. 
The challenge is further compounded in heterogeneous-agent reinforcement learning (HARL), 
where agents may possess differing roles, observation spaces, or action capabilities, 
leading to an increase in learning complexity \cite{rizk2019, yang2021a}.
Efficient training methodologies that reduce computational costs while maintaining 
performance are crucial for enabling broader adoption of MARL and HARL systems.

\section{Introduction}

Multi-agent reinforcement learning (MARL) has shown significant promise in solving complex 
decision-making tasks across domains such as strategic gameplay, robotics, and 
autonomous systems \cite{silver2016, vinyals2019, berner2019}. 
However, training MARL models at scale remains computationally expensive, 
often requiring vast data and simulation time. These challenges intensify in 
heterogeneous-agent reinforcement learning (HARL), where agents differ in roles, 
observations, or action capabilities \cite{rizk2019, yang2021a}. 
Efficient training methodologies that reduce computational costs while maintaining 
performance are therefore critical for the broader adoption of these systems.

Beyond differences in architecture, HARL also encompasses what we refer to as 
\emph{behavioral heterogeneity}, where agents may be structurally identical 
but train independently, allowing their roles to diverge over time. 
This framing captures a range of realistic deployments—such as drone swarms, 
robotic warehouses \cite{rizk2019}, and cooperative games—where interchangeable agents 
can evolve distinct roles through interaction and policy drift. 
In these settings, the coordination burden increases even when agents are nominally homogeneous.

One proposed strategy to improve training efficiency is to pretrain a smaller subset of agents 
before scaling to the target configuration. By learning policies in reduced-team scenarios, 
it may be possible to accelerate convergence and reduce total training time. 
Smit et al.\ \cite{smit2023} explored this in cooperative football tasks, 
but found inconsistent benefits when scaling up to full-team configurations. 
Their results suggest that task structure—especially the need for specialization—can determine 
whether scaling curricula are effective.

In this study, we investigate the feasibility of this strategy in environments 
that vary in their demands for specialization and coordination. 
We introduce an agent-steps metric that standardizes training cost by accounting for 
both agent count and time, enabling direct comparisons across curricula. 
Our evaluation spans three benchmark environments—Waterworld \cite{gupta2017}, 
Multiwalker \cite{gupta2017}, and Level-Based Foraging (LBF) \cite{papoudakis2021}—selected 
for their varying degrees of behavioral symmetry and cooperative structure.

While many MARL benchmarks, such as MAgent2 \cite{zheng2017}, 
support large agent populations with shared policies, most HARL environments fix the number 
of agents in observation structures. This coupling between team size and observation space 
restricts transferability and impairs methods like curriculum learning and zero-shot generalization. 
The LBF and Multiwalker environments in PettingZoo \cite{terry2021}, 
as well as SMAC \cite{samvelyan2019} and GRF \cite{kurach2020}, all embed team size 
into their input formats, making policy reuse across scales nontrivial.

To enable fair comparisons, we adapted the observation space in LBF 
using two lightweight reduction strategies that decouple agent count from input dimensionality. 
These changes allow us to evaluate transferability and convergence without modifying 
policy architectures mid-experiment. Our findings indicate that pretraining smaller teams 
can improve training efficiency in environments with interchangeable roles, 
but may be less effective when role differentiation emerges during learning. 
These results provide new insights into the conditions under which scaling curricula are 
most beneficial and offer practical tools for improving training in scalable HARL systems.

% We retain the formal definition of the original observation structure here for reference:
% \[
% \underbrace{[\text{level}_{n_i},\ \text{row}_{n_i},\ \text{col}_{n_i}]}_{\text{Own Features}} ++
% \underbrace{[f \in F]}_{\text{Food Matrix}} ++
% \underbrace{[n \in N_{/i}]}_{\text{Agent Matrix}}
% \]



\section{Related Work}
        % Prior research has explored MARL scalability \cite{shoham2007a, busoniu2008, foerster2017} and training efficiency in large-scale environments \cite{ackerman2019, lowe2020}. HARL studies have examined role differentiation and heterogeneous task assignments \cite{wakilpoor2020, koster2020}. Despite these advances, limited work has focused on evaluating pretraining smaller agent groups and scaling up, making our investigation a novel contribution.

        % \includegraphics[width=0.75\linewidth]{2d_football.png}



        % %% sub c1 - hetero rl
        % % argue confusion
        % % define terms
        % % b. het
        % % i. het
        % \section{Methodology}

        % \subsection{Experimental Setup}
        % We conducted experiments in the Waterworld environment, a multi-agent domain within the SISL package. Agents navigate a continuous space, collecting resources and avoiding obstacles. Key attributes of the environment include:
        % \begin{itemize}
        %     \item Adjustable agent count with cooperative dynamics.
        %     \item Potential for role differentiation through independent policy learning.
        % \end{itemize}

        % Our training protocol utilizes Proximal Policy Optimization (PPO) \cite{schulman2017}, implemented within the RLlib framework, providing flexibility for future algorithmic comparisons. Training is conducted over multiple iterations, with performance measured based on reward progression and convergence speed.

        % \subsection{Adjusted Time Steps}
        % To compare computational costs across agent counts, we introduce adjusted time steps, accounting for the linear scaling of per-step computation. The empirical relationship for training step duration is given by:
        % \begin{equation}
        %     \text{time(ms)} = 324.2441 + 7004.7673n
        % \end{equation}
        % where $n$ is the number of agents. Adjusted time steps enable fair cost comparisons across training configurations.

        % \subsection{Training Process}
        % Our approach involves:
        % \begin{itemize}
        %     \item Pretraining a smaller number of agents, saving policy checkpoints periodically.
        %     \item Upsampling pretrained agents without replacement to instantiate larger environments.
        %     \item Retraining the scaled-up agent team and comparing performance against a tabula rasa baseline.
        % \end{itemize}


A key challenge encountered when training in the Level-Based Foraging (LBF) environment 
was handling variations in the observation space due to changes in the number of allied agents. 
By default, LBF provides each agent with a fully observable state representation, 
consisting of the objective location, self-information, and an ordered array of ally information. 
However, when the number of allies changes—such that during the transition from pretraining 
with fewer agents to full-scale training—the size of this observation array also changes, 
making policy transfer between different team sizes nontrivial. 
To ensure consistency, we implemented two approaches: 
(1) restricted observation, where agents were blind to ally locations, 
ensuring that observation size remained static across training phases, and 
(2) structured observation abbreviation, where the observation size was fixed based 
on the pretraining phase, and when the number of allies increased, the additional ally 
information was sampled from the larger representation to maintain structural consistency. 
These modifications allowed us to systematically evaluate the impact of observation 
consistency on policy transfer and learning stability in heterogeneous-agent training scenarios.

\vspace{3em}


% \begin{figure}[h]
%     \centering
%     \begin{tikzpicture}
%         % Small observation array (Pretraining Phase)
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm] (goal) at (0,0) {Goal};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of goal] (self) {Self};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of self, fill=blue!20] (ally1) {Ally 1};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of ally1, fill=blue!20] (ally2) {Ally 2};
        
%         \node at (-2, 0) {Pretraining (Small Team)};
        
%         % Large observation array (Scaled-up Phase)
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm] (goal2) at (0,-2) {Goal};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of goal2] (self2) {Self};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of self2, fill=blue!20] (ally1_2) {Ally 1};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of ally1_2, fill=blue!20] (ally2_2) {Ally 2};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of ally2_2, fill=red!20] (sampled) {Sampled Ally};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of sampled, fill=red!20] (sampled2) {Sampled Ally};
        
%         \node at (-2, -2) {Full Training (Larger Team)};
        
%         % Arrows to indicate sampling process
%         \draw[->, thick] (ally1) -- (ally1_2);
%         \draw[->, thick] (ally2) -- (ally2_2);
%         \draw[->, dashed, thick] (ally1) -- (sampled);
%         \draw[->, dashed, thick] (ally2) -- (sampled2);
%     \end{tikzpicture}
%     \caption{Illustration of observation array handling when transitioning from a small-team configuration to a larger one. Dashed arrows indicate sampled ally data.}
% \end{figure}


\vspace{3em}


        % \section{Results and Observations}
        % Our findings indicate that convergence occurs earlier with fewer agents, particularly when agent counts differ significantly. Performance gains may be due to task tessellation, warranting further investigation. Key observations include:
        % \begin{itemize}
        %     \item Training time per step scales linearly with the number of agents.
        %     \item Pretraining smaller groups can lead to faster convergence in certain scenarios.
        %     \item Performance benefits diminish when tasks lack natural role division.
        % \end{itemize}

        % \section{Future Work}
        % Building on these findings, Contribution 2 will explore curriculum-based training strategies, progressively increasing agent counts in complex tasks. Additionally, we aim to investigate biological models of cooperation and specialization for further insights into HARL systems.

        % \section{Conclusion}
        % This study demonstrates the potential benefits of pretraining smaller MARL teams before scaling up. By introducing adjusted time steps and analyzing training efficiency, we contribute to understanding scalable MARL techniques. Future work will refine these methods and expand the applicability to more complex environments.


        \subsection{Observation Design in Multi-Agent Systems}

        Observation space design plays a central role in the generalization, scalability, 
        and transferability of learned policies in reinforcement learning more broadly, 
        and presents unique challenges in the multi-agent setting that we focus on here. 
        In many environments—such as SMAC \cite{samvelyan2019}, Google Research Football \cite{kurach2020},
        and Level-Based Foraging (LBF) \cite{papoudakis2021}—the observation vector encodes fixed-size 
        segments that correspond to all agents and nearby objects. 
        This leads to observation dimensions that are tightly coupled to the number of agents
        in the environment, which complicates transfer to variable team sizes and undermines architectural
        modularity. 
        Such design choices limit the feasibility of curriculum learning, zero-shot generalization,
        and policy reuse across heterogeneous configurations.
        
        While few works frame this issue directly as a problem of \textit{dimension reduction}, 
        several methods indirectly address the challenge by compressing, pooling, 
        or abstracting over agent-specific information.
        Entity-centric approaches such as REFIL \cite{iqbal2021} and mean embedding strategies
        \cite{huttenrauch2019} represent a collection of agents or objects using permutation-invariant
        transformations, effectively decoupling policy input from team size. Similarly, attention-based methods
        (e.g., top-$k$ pooling) and mean-field approximations \cite{yang2021a} reduce observation complexity
        by focusing only on the most relevant entities. Role-conditioned architectures
        \cite{gupta2017, kouzeghar2023} also compress observations by filtering agent features through
        a low-dimensional role representation.
        
        These approaches, however, often introduce architectural complexity or assume centralized 
        training with shared state information \cite{foerster2017}. Moreover, they typically require 
        retraining or re-encoding when the agent pool or spatial layout changes significantly. 
        Our work takes a complementary approach: we treat the dimensionality of the observation vector 
        itself as a constraint and propose lightweight observation-level reductions that preserve 
        task-relevant structure while enabling flexible agent counts. 
        Related efforts in high-dimensional robotics and policy compression similarly 
        leverage latent representations to improve sample efficiency and structural 
        adaptability \cite{bitzer2010, tangkaratt2016}.
        
        % \todo{Consider adding citation for attention-based message pruning or top-k pooling.}
        
        \subsection{Heterogeneous MARL and Policy Transferability}
        
        Heterogeneous MARL (HARL) introduces additional complexity by requiring agents with differing
        roles, capabilities, or observation modalities to coordinate effectively. This setting reflects
        many real-world domains, including swarm robotics \cite{hoang2023}, traffic control \cite{calvo2018},
        and decentralized logistics \cite{rizk2019}, where adaptability to team composition and role diversity
        is essential.
        
        Prior work has explored architectures that support specialization and flexibility. 
        For example, Gupta et al. \cite{gupta2017a} and Hüttenrauch et al. \cite{huttenrauch2019} 
        both demonstrate techniques for encoding structured input using single-agent policies that 
        can generalize across role configurations, but do not address multi-agent coordination directly. 
        Iqbal et al. \cite{iqbal2021} introduce entity-centric methods in a homogeneous MARL setting, 
        which allow pooling across similar agents but still assume identical agent capabilities. 
        While these methods improve generalization within their respective assumptions, they often 
        maintain a fixed number of roles or homogeneous agent groups, limiting their applicability 
        in dynamic heterogeneous settings.
        
        Our work contributes to this space by focusing on observation-level design, offering strategies
        that allow agent populations to vary without modifying the underlying network architecture.
        This enables more efficient policy reuse and deployment across tasks with differing team sizes,
        a capability not well supported in most existing HARL approaches.
        
        \subsection{Benchmarks and Evaluation in HARL}
        
        Evaluating HARL systems under varying team configurations remains underexplored in most
        benchmark suites. While environments like LBF \cite{papoudakis2021} permit adjustable agent counts,
        their default observation encodings entangle observation length with the number of agents,
        hindering direct comparison across configurations. Other platforms, such as Melting Pot
        \cite{leibo2021} and MAgent2 \cite{zheng2017}, support large-scale simulations but typically focus
        on homogeneous populations with shared policies.
        
        Recent benchmarks such as SMACv2 \cite{ellis2023} introduce generalization challenges via
        procedural variation, and Google Research Football (GRF) \cite{kurach2020} provides rich agent
        coordination scenarios. However, both maintain fixed observation schemas and assume static team
        structures during training and evaluation.
        
        Our work complements these efforts by demonstrating how reduced and agent-count-invariant
        observation encodings can support variable-team evaluation within a consistent architecture.
        This approach enhances the flexibility of HARL evaluation protocols without requiring environment- or
        network-specific modifications.
        
        \section{Methodology}
        % Default observation space
        % Reduced observation space
        % Egocentric observation space
        
        % Not ego-st (carried behavioral implication)
        
        \subsection{Environment}
        
        We use the Level-Based Foraging (LBF) environment \cite{papoudakis2021},
        a grid-based reinforcement learning benchmark designed to evaluate 
        coordination and cooperation in multi-agent settings,
        that requires agents to jointly collect food items scattered across a map. 
        
        Each agent and food item is assigned a discrete level, with the level representing 
        the agent's foraging capability and the food's consumption requirement, respectively.
        A food item can only be foraged when the sum of the levels of the agents 
        occupying adjacent cells meets or exceeds the food's level.
        
        The environment emphasizes multi-agent cooperation under spatial and temporal constraints. 
        Agents must learn to navigate the grid, identify appropriate foraging opportunities, 
        and coordinate with nearby teammates to successfully collect higher-level food items. 
        Rewards are sparse and granted only upon successful foraging, 
        making exploration and credit assignment particularly challenging. 
        The environment can be configured for partial observability by limiting agent vision range, 
        further increasing the need for implicit or explicit coordination.


% \nocite{*}
\printbibliography

\end{document}