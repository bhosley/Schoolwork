\documentclass{article}
\usepackage[english]{babel}
\usepackage{csquotes}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{hyperref}

\usepackage{graphicx}
\graphicspath{{../images/}{../data/}}

\usepackage{tikz}
\usetikzlibrary{positioning}

% \usepackage{cleveref}
\usepackage[backend=biber, style=ieee]{biblatex}
\usepackage[T1]{fontenc}
\usepackage[final]{microtype}
\addbibresource{../../../2025Bibs/Prospectus.bib}

\title{Investigating Training Efficiency of Direct Scaling in Multi-Agent Reinforcement Learning}
\author{Brandon Hosley}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    % TODO: First sentence, should state problem
    % The so-what
    This paper investigates the feasibility of training multi-agent reinforcement learning (MARL) 
    systems with a reduced number of agents before scaling up to full team sizes. 
    Inspired by prior work, particularly Smit et al. (2023), 
    we analyze whether pretraining smaller agent groups can improve training efficiency 
    without sacrificing final performance. 
    We introduce an agent-steps metric, which provides a standardized measure of 
    total training effort across different agent counts.
    Experiments conducted in the Waterworld, Multiwalker, and Level-based Foraging environments 
    reveal that the effectiveness of this approach appears to be inversely related 
    to the diversity required among agents in the final team. 
    When tasks allow agents to adopt similar roles, 
    pretraining on smaller groups accelerates learning; 
    however, in environments where agents must specialize into distinct roles, 
    the benefits of early training are diminished. 
    These findings inform future work in curriculum learning and scalable 
    heterogeneous-agent reinforcement learning (HARL).
\end{abstract}

\section{Introduction}

Multi-agent reinforcement learning (MARL) has shown significant promise in solving complex 
decision-making tasks across diverse domains, including strategic gameplay, robotics, 
and autonomous systems \cite{silver2016, vinyals2019, berner2019}. 
However, training MARL models at scale remains a computationally expensive process, 
often requiring extensive simulation time and large numbers of training episodes. 
The challenge is further compounded in heterogeneous-agent reinforcement learning (HARL), 
where agents may possess differing roles, observation spaces, or action capabilities, 
leading to an increase in learning complexity \cite{rizk2019, yang2021a}.
Efficient training methodologies that reduce computational costs while maintaining 
performance are crucial for enabling broader adoption of MARL and HARL systems.

Beyond differences in architecture, HARL also encompasses what we refer to as 
\emph{behavioral heterogeneity}, % a subtype of emergent heterogeneity
where agents may be structurally identical but their policies are updated independently 
during training, allowing their roles and behaviors to diverge over time.
This framing captures a range of realistic deployments—such as drone swarms, 
robotic warehouses \cite{rizk2019}, and cooperative games—where interchangeable agents 
can evolve distinct roles through interaction and policy drift. 
In these settings, the coordination burden increases even when agents are nominally homogeneous.

One proposed strategy to improve training efficiency involves pretraining a reduced subset of agents 
before scaling to the full target configuration. The intuition is that policies learned in smaller-team 
scenarios may converge more quickly and subsequently transfer to larger-scale tasks with lower 
overall cost. Smit et al.\ \cite{smit2023} evaluated this approach in cooperative football environments, 
but observed mixed results when scaling to full-team configurations. Their findings suggest that 
the effectiveness of such curricula is sensitive to task structure—particularly the degree to which 
role specialization is required.

We investigate the feasibility of a direct scaling strategy in environments with varying 
demands for coordination and role specialization. Our experiments employ an agent-steps 
metric that standardizes training cost by accounting for both agent count and time, 
enabling direct comparisons across curricula. To evaluate the generality of this approach, 
we test it on three benchmark environments—Waterworld \cite{gupta2017}, Multiwalker 
\cite{gupta2017}, and Level-Based Foraging (LBF) \cite{papoudakis2021}—selected for their 
differing degrees of behavioral symmetry and cooperative complexity. Building on prior work, 
we extend the basic strategy with an additional retraining phase: policies trained with 
fewer agents are used to initialize larger teams, which are then further adapted in the 
target configuration. This approach enables evaluation of transferability across 
configurations with varying degrees of agent specialization.

While environments such as MAgent2 \cite{zheng2017} support large agent populations through 
shared policies, most HARL benchmarks fix the number of agents in their observation structures. 
This entanglement of team size with input dimensionality restricts policy transfer across 
configurations and complicates the use of curriculum learning or zero-shot generalization. 
Environments like LBF and Multiwalker from PettingZoo \cite{terry2021}, as well as 
SMAC \cite{samvelyan2019} and Google Research Football (GRF) \cite{kurach2020}, 
encode team size directly into observation formats, making policy reuse across scales nontrivial.

To enable fair comparisons, we adapted the observation structure in the Level-Based Foraging (LBF) 
environment to decouple the number of agents from the observation dimensionality. 
Each agent in LBF receives a fully observable state, which includes a structured array of 
features for all teammates—an input that scales with the size of the team. As a result, changing 
team size alters the observation dimensionality, making policy transfer across configurations 
incompatible with fixed architectures. To address this, we implemented two lightweight 
observation modifications: (1) a restricted mode that omits all ally-specific information, 
maintaining a constant observation size across configurations, 
and (2) an abbreviated format that retains a fixed number of teammate feature slots, 
randomly sampling allies when the team exceeds the pretraining configuration. 
These adaptations preserve architectural compatibility and facilitate systematic 
evaluation of policy transferability across team sizes.


\section{Related Work}

\subsection{Training Efficiency and Curriculum Learning in MARL}

Improving training efficiency in multi-agent reinforcement learning (MARL) has become a research 
priority, especially given the computational costs and sample complexity that scale with team 
size and task difficulty. Prior research has explored MARL scalability and efficiency in 
large-scale environments, investigating methods to reduce overestimation bias, 
support policy decentralization, and optimize communication overheads 
\cite{shoham2007a, busoniu2008, foerster2017, ackermann2019, lowe2020}.
Curriculum learning has emerged as a promising approach, where tasks are presented in a structured 
sequence to facilitate progressive skill acquisition. This strategy can take many forms, 
including environment simplification \cite{shukla2022}, task decomposition \cite{shi2023}, 
or gradually increasing the number of agents during training \cite{smit2023, albrecht2024}.

Transfer learning approaches have also been used to mitigate the cost of retraining agents 
tabula rasa (i.e., without prior initialization) in each new configuration. 
In typical applications, knowledge learned in a source task—often with reduced 
complexity or smaller scale—is used to initialize agents, 
which are then fine-tuned to adapt to a target task \cite{cui2022}. 
These methods have shown success in domains where skills are composable 
or generalize well across task variants. However, their effectiveness often 
depends on architectural compatibility and the degree of agent or task homogeneity.

Smit et al.\ \cite{smit2023} investigated curriculum learning in a simplified version of the 
Google Research Football (GRF) environment, focused on player positioning rather than 
fine-grained actions or physics simulation. Their approach trained agents in 2v2 scenarios 
before scaling to full 11v11 teams, evaluating transfer performance using multiple baselines. 
While conceptually appealing, their curriculum yielded mixed results: agents trained directly 
in 11v11 settings consistently outperformed those pretrained in smaller games, exhibiting 
stronger coordination and role differentiation. These findings suggest that when behavioral 
specialization is critical, small-scale pretraining may not expose agents to sufficiently 
diverse interactions to foster generalizable cooperation.

Our work extends this line of research by evaluating how smaller-team pretraining followed 
by retraining in the full agent configuration can accelerate learning. 
Distinct from earlier efforts, we prioritize structural compatibility between 
training phases through deliberate observation design. 
We also examine the effects of this strategy in environments with varying degrees of 
behavioral heterogeneity, offering insight into how task structure influences 
transferability and efficiency.

\subsection{Heterogeneous MARL and Policy Transferability}

Heterogeneous MARL (HARL) introduces additional complexity by requiring agents with differing
roles, capabilities, or observation modalities to coordinate effectively. This setting reflects
many real-world domains, including swarm robotics \cite{hoang2023}, traffic control \cite{calvo2018},
and decentralized logistics \cite{rizk2019}, where adaptability to team composition and role diversity
is essential.

Prior work has explored architectures that support specialization and flexibility. 
For example, Gupta et al. \cite{gupta2017a} and H{\"u}ttenrauch et al. \cite{huttenrauch2019} 
both demonstrate techniques for encoding structured input using single-agent policies that 
can generalize across role configurations, but do not address multi-agent coordination directly. 
Iqbal et al. \cite{iqbal2021} introduce entity-centric methods in a homogeneous MARL setting, 
which allow pooling across similar agents but still assume identical agent capabilities. 
While these methods improve generalization within their respective assumptions, they often 
maintain a fixed number of roles or homogeneous agent groups, limiting their applicability 
in dynamic heterogeneous settings.

Our work contributes to this space by focusing on observation-level design, offering strategies
that allow agent populations to vary without modifying the underlying network architecture.
This enables more efficient policy reuse and deployment across tasks with differing team sizes,
a capability not well supported in most existing HARL approaches.

\subsection{Observation Design in Multi-Agent Systems}
\label{sec:related_work-observation_design}

Observation space design plays a central role in the generalization, scalability, 
and transferability of learned policies in reinforcement learning, 
particularly in the multi-agent setting. 
Many environments—such as SMAC \cite{samvelyan2019}, 
Google Research Football (GRF) \cite{kurach2020}, 
and Level-Based Foraging (LBF) \cite{papoudakis2021}—encode observations as fixed-size vectors, 
with dedicated segments for each teammate or nearby object. 
As a result, the dimensionality of the observation space scales with the 
number of agents present in the environment. 
This tight coupling between team size and observation format restricts the 
reuse of trained policies across varying team configurations and undermines 
modular architectural design. These limitations pose challenges for curriculum learning, 
zero-shot generalization, and transfer learning—scenarios that require robust generalization 
to variable input structures.

% \includegraphics[width=0.75\linewidth]{2d_football.png}

While few works frame this issue directly as a problem of \textit{dimension reduction}, 
several methods address the challenge indirectly by compressing, pooling, 
or abstracting over agent-specific information. 
Entity-centric approaches such as REFIL \cite{iqbal2021} and mean embedding strategies 
\cite{huttenrauch2019} represent collections of agents or objects using permutation-invariant 
transformations, effectively decoupling policy input from team size. 
Similarly, attention-based strategies (e.g., top-$k$ pooling) and mean-field approximations 
\cite{yang2021a} reduce observation complexity by emphasizing only the most relevant entities.

Some works explore architectural modifications to better accommodate agent specialization 
or variability. Kouzeghar et al.~\cite{kouzeghar2023}, for example, adopt a decentralized 
approach to UAV swarm coordination by training distinct policies for different drone types, 
without employing role embeddings or shared parameterization. 
Gupta et al.~\cite{gupta2017a}, in contrast, employ an embedding-based representation to 
enable knowledge transfer across morphologically distinct robots, though their setting 
involves single-agent learning. These studies reflect broader architectural strategies that 
address variability in agent capabilities or embodiment, even if not in a MARL context.

Despite their promise, these strategies often come with architectural overhead or assume 
centralized training with shared state information \cite{foerster2017}. 
Moreover, they typically require retraining or re-encoding when the agent pool or spatial 
layout changes significantly. Our work takes a complementary approach: 
we treat the dimensionality of the observation vector itself as a constraint and propose 
lightweight observation-level reductions that preserve task-relevant structure 
while enabling flexible agent counts. 
Related efforts in high-dimensional robotics and policy compression similarly leverage 
latent representations to improve sample efficiency and structural adaptability 
\cite{bitzer2010, tangkaratt2016}.

\subsection{Benchmarks and Evaluation}

Prior literature has explored alternative approaches to HARL evaluation. 
Wakilpoor et al.~\cite{wakilpoor2020} introduce an attention-guided multi-agent actor-critic 
that enables efficient coordination between agents with complementary abilities. 
Koster et al.~\cite{koster2020} similarly propose an attention-based method for centralized 
training of heterogeneous agents, enabling shared learning across distinct roles. 
These works demonstrate scalable performance in specialized evaluation domains, 
yet the broader field still lacks general-purpose benchmarks that support systematic 
study of heterogeneity across configurations.

Benchmarking HARL systems under varying team configurations presents persistent
challenges for experimental design and policy generalization. While many benchmarks 
such as LBF \cite{papoudakis2021}, Melting Pot \cite{leibo2021}, and MAgent2 \cite{zheng2017} 
support multi-agent setups, they are primarily designed for homogeneous MARL 
and often rely on shared policies or fixed observation formats. This limits 
their utility in studying systems with intrinsic heterogeneity—where agents possess 
distinct capabilities or behaviors by design. As a result, dedicated HARL benchmarks 
remain sparse, and evaluating emergent or role-specific behaviors is often constrained 
by the assumptions and architectures of existing platforms.

Recent benchmarks such as SMACv2 \cite{ellis2023} and 
Google Research Football (GRF) \cite{kurach2020} offer rich multi-agent coordination scenarios 
and introduce generalization challenges through procedural variation and strategic diversity. 
While both environments often employ fixed team sizes, 
they can be adapted to support varying agent counts.
Our work complements these efforts by demonstrating how a simple, practical approach to observation 
restructuring can enable policy evaluation across varying team sizes. By preserving a fixed 
observation format through lightweight modifications, we support compatibility across configurations 
without requiring architectural changes or complex preprocessing.
This approach contributes toward scalable benchmarking practices and supports 
more flexible evaluation of heterogeneous-agent reinforcement learning (HARL) systems.

% %% sub c1 - hetero rl
% % argue confusion
% % define terms
% % b. het
% % i. het

\section{Methodology}\label{sec:methodology}

\subsection{Training Framework for Heterogeneous MARL}

We formulate each task as a cooperative multi-agent partially observable Markov 
decision process (MA-POMDP). At each timestep, each of the $n\in N$ agents receives 
an observation of the environment $o_n$ and selects an action $a_n$ from its action space. 
Although the agents receive individual rewards $r_n$, these reward functions are aligned 
toward a shared team objective, reflecting the cooperative nature of the task.
The objective function is such that policies \(\pi_n(a_n|o_n)\)
that maximizes the expected cumulative team reward. Thus, within the PPO objective function:
\[
    \mathcal{L}(\theta) = \mathbb{E}_{t,n} \left[ 
    \min \left( 
    \frac{\pi_\theta(a_{n,t} \mid o_{n,t})}{\pi_{\theta_{\text{old}}}(a_{n,t} \mid o_{n,t})} \hat{A}_{n,t},\ 
    \text{clip}\left( 
    \frac{\pi_\theta(a_{n,t} \mid o_{n,t})}{\pi_{\theta_{\text{old}}}(a_{n,t} \mid o_{n,t})}, 
    1 \pm \epsilon
    \right) \hat{A}_{n,t}
    \right) 
    \right]
\]
individual rewards are aggregated within the advantage estimation as
\[
    \hat{A}_{n,t} = \left( \sum_{k=t}^{K} \sum_{n\in N} r_{n,k} \right) - V(o_{n,t})
\]
This is the clipped surrogate objective used in PPO, extended here to a multi-agent setting by 
applying it independently to each agent's trajectory. While agents optimize their policies using
local observations and advantages, their rewards are aligned toward a shared cooperative outcome.
These cooperative settings benefit the multi-agent problem as a single-team optimization, 
encouraging agents to work together. 

\begin{table}[h]
    \centering
    \caption{Notation used in PPO objective and advantage estimation}
    \begin{tabular}{ll}
    \toprule
    \textbf{Symbol} & \textbf{Description} \\
    \midrule
    $\pi_\theta(a_{n,t} \mid o_{n,t})$ & Probability of action $a_{n,t}$ under current policy for agent $n$ \\
    $\pi_{\theta_{\text{old}}}(a_{n,t} \mid o_{n,t})$ & Probability under the previous policy before update \\
    $\hat{A}_{n,t}$ & Estimated advantage for agent $n$ at time $t$ \\
    $\epsilon$ & PPO clipping parameter (e.g., 0.2) \\
    $r_{n,k}$ & Reward received by agent $n$ at timestep $k$ \\
    $V(o_{n,t})$ & Estimated value of observation $o_{n,t}$ \\
    $K$ & Horizon length or final timestep in trajectory \\
    $N$ & Set of all agents in the team \\
    $t$ & Timestep index in a trajectory \\
    $a_{n,t}$ & Action taken by agent $n$ at time $t$ \\
    $o_{n,t}$ & Observation of agent $n$ at time $t$ \\
    \bottomrule
    \end{tabular}
\end{table}

We utilize the centralized training with decentralized execution (CTDE) paradigm, 
a common framework in cooperative MARL settings \cite{guo2024}. 
This approach allows agents to be trained using global information 
(e.g., shared state or joint observations), while each agent independently 
executes its policy based solely on local observations at runtime. 
This balances scalability with coordination, and is particularly well-suited 
to environments where centralized control during training is feasible, 
but decentralized action is required at deployment.

\paragraph{Shared and Specialized Policy Representations.} 
We implemented a heterogeneous-agent PPO (HAPPO) training regimen that supports both 
knowledge sharing and agent specialization. In scenarios where agents are 
homogeneous—meaning they share identical observation and action spaces—we train agents 
independently using distinct policies, without parameter sharing. 
This choice aligns with our emphasis on observing the natural emergence of 
behavioral heterogeneity through independently updated policies. 
Although parameter sharing is often used to accelerate convergence, 
we avoid it in this study to maintain the independence of learning trajectories across agents. 
This allows agents to develop divergent behaviors even when they begin identically, 
supporting emergent specialization under cooperative constraints.

To support heterogeneous agents—those differing in capabilities or observation/action 
structures—we avoid relying on specialized embeddings or architectural modifications. 
Instead, we train distinct policies where required and maintain consistent observation 
structures where possible. In LBF, for example, an agent's skill level is included 
directly in its observation. This enables the policy to adapt its behavior based on 
agent identity or ability, without additional architectural complexity. 
When agents differ only by role or position, as in Multiwalker, 
we preserve architectural consistency by assigning fixed policies based on role, 
allowing behavioral differences to emerge through independent updates.

Through this design, we balance policy independence with structural compatibility. 
Rather than introducing complex role-based encodings or modifying network architecture, 
we maintain fixed observation structures to support agent variation across configurations. 
This allows us to evaluate the efficiency of scaling strategies—such as upsampling policies from 
smaller teams—without confounding effects from parameter sharing or architectural entanglements. 
Our policies are trained using PPO with a clipped surrogate objective and generalized 
advantage estimation, employing minibatch updates after each training epoch.

One observation-related challenge for transferability arises when the dimensionality 
of each agent's observation increases with team size, a situation that complicates 
architectural reuse across different configurations. While this is not a universal issue 
across all environments, it does affect Level-Based Foraging (LBF) in our setup. 
To mitigate this, we explored two minimally invasive solutions. First, we constructed a wrapper 
for the environment in which each agent's observation excludes ally-specific features entirely, 
ensuring a consistent input dimensionality regardless of team size. Second, 
we implemented a truncated observation version: a fixed number of teammate feature slots 
are retained in the observation, and when the team exceeds that limit, a random subset of 
teammates is selected to populate those slots. Both adaptations maintain architectural 
compatibility while allowing for scalable evaluation across variable team configurations.

This setup enables a structured comparison of training efficiency by initializing larger teams 
with independently updating policy duplicates from smaller configurations, allowing us to assess 
the benefits of policy reuse without requiring multi-stage curricula.

\subsection{Evaluation Environments} 

We evaluate our approach in three multi-agent environments selected for their contrasting 
coordination demands and forms of agent heterogeneity: 
Waterworld, Multiwalker, and Level-Based Foraging (LBF). These tasks span continuous 
and discrete control domains and include symmetric as well as role-differentiated team dynamics.

\textbf{Waterworld} features fully cooperative continuous control with homogeneous agents 
navigating a 2D arena to collect food while avoiding poison. The environment supports emergent 
behavioral heterogeneity through independent learning but imposes no intrinsic agent distinctions. 
It tests whether smaller-team coordination can generalize to larger swarms with minimal 
conflict or redundancy.

\textbf{Multiwalker} requires tight temporal coordination among physically coupled bipedal agents 
transporting a payload. Though all walkers share the same embodiment and action space, 
their fixed spatial positions in the chain introduce static observation asymmetries. 
This environment challenges policy reuse in settings where coordination relies on 
role-consistent behavior.

\textbf{Level-Based Foraging (LBF)} is a discrete grid-world where agents with variable skill 
levels must cooperate to collect leveled food targets. Coordination depends on matching agent 
abilities to task demands, with skill levels dynamically affecting each agent's effectiveness. 
This environment introduces a dynamic intrinsic heterogeneity, as agent roles shift throughout 
training.

To support fair evaluation across team scales, we adapted LBF's observation structure using 
lightweight modifications described in Section~\ref{sec:related_work-observation_design}, 
allowing fixed 
architectures to operate across varying team sizes. Each environment was evaluated for both 
raw performance and training efficiency under our curriculum-based transfer framework.

\subsection{Evaluation Protocol and Training Procedure}

We adopted Proximal Policy Optimization (PPO)\cite{schulman2017} using the RLlib 
framework to structure and execute our experiments, with Hyperparameters described in
Table~\ref{tab:ppo-hyperparams}. For Waterworld and LBF, 
we used RLlib's default PPO configuration. For Multiwalker, we modified the default 
network architecture to better accommodate the higher-dimensional continuous control space and 
tighter coordination demands.

In each environment, we first identified a minimal viable agent count for pretraining: 
2 agents for Waterworld and LBF, and 3 agents for Multiwalker. 
These smaller teams were trained until convergence, with periodic policy checkpoints saved 
throughout training. From each checkpoint, we upsampled to create larger teams—up to 8 agents in 
Waterworld and LBF, and up to 7 agents in Multiwalker—by duplicating the available policies. 
When the target agent count exceeded a multiple of the checkpointed policies, 
one additional copy was selected at random. In Waterworld and LBF, agent IDs were treated as 
interchangeable. In Multiwalker, the leading agent was kept fixed while the remaining agents 
were selected from the checkpoint pool. Retraining was then conducted with each agent 
initialized using one of the pretrained policies. Agents updated their policies independently 
throughout retraining, allowing for behavioral differentiation to emerge or continue from the 
initial state.

The primary metric for comparison was agent-steps, defined as the product of the number of 
agents and training iterations. This standardizes computational cost across team configurations 
by accounting for the linear scaling observed in all environments.

Each training configuration was repeated independently multiple times. We report mean and 
variance across these repetitions to assess the consistency of training outcomes. 
This evaluation framework allows us to compare the efficiency of the curriculum-based 
upsampling strategy against full tabula rasa training for the same final team sizes, 
providing insight into the cost-benefit tradeoffs under varying heterogeneity conditions.

Additional analyses in the Results section explore how reward progression, convergence speed, 
and environmental sensitivity vary across configurations and pretraining durations.

\begin{table}[h]
    \centering
    \caption{PPO Hyperparameters used in experiments}
    \begin{tabular}{ll}
    \toprule
    \textbf{Parameter} & \textbf{Value} \\
    \midrule
    Learning rate & $5 \times 10^{-5}$ \\
    Training batch size & 4000 \\
    Number of SGD epochs per update & 30 \\
    Minibatch size & 128 \\
    GAE parameter $\lambda$ & 1.0 \\
    Clipping parameter $\epsilon$ & 0.3 \\
    KL coefficient & 0.2 \\
    Target KL divergence & 0.01 \\
    Value function loss coefficient & 1.0 \\
    Value function clipping range & $\pm 10.0$ \\
    Entropy coefficient & 0.0 \\
    Policy network (Waterworld, LBF) & 2 layers, 256 units, ReLU \\% + Softmax \\
    Policy network (Multiwalker) & 6 layers, 256 units, ReLU \\% + Softmax \\
    \bottomrule
    \end{tabular}
    \label{tab:ppo-hyperparams}
\end{table}

\vspace{3em}

% \begin{figure}[h]
%     \centering
%     \begin{tikzpicture}
%         % Small observation array (Pretraining Phase)
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm] (goal) at (0,0) {Goal};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of goal] (self) {Self};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of self, fill=blue!20] (ally1) {Ally 1};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of ally1, fill=blue!20] (ally2) {Ally 2};
        
%         \node at (-2, 0) {Pretraining (Small Team)};
        
%         % Large observation array (Scaled-up Phase)
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm] (goal2) at (0,-2) {Goal};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of goal2] (self2) {Self};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of self2, fill=blue!20] (ally1_2) {Ally 1};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of ally1_2, fill=blue!20] (ally2_2) {Ally 2};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of ally2_2, fill=red!20] (sampled) {Sampled Ally};
%         \node[draw, rectangle, minimum width=1cm, minimum height=0.8cm, right=of sampled, fill=red!20] (sampled2) {Sampled Ally};
        
%         \node at (-2, -2) {Full Training (Larger Team)};
        
%         % Arrows to indicate sampling process
%         \draw[->, thick] (ally1) -- (ally1_2);
%         \draw[->, thick] (ally2) -- (ally2_2);
%         \draw[->, dashed, thick] (ally1) -- (sampled);
%         \draw[->, dashed, thick] (ally2) -- (sampled2);
%     \end{tikzpicture}
%     \caption{Illustration of observation array handling when transitioning from a small-team configuration to a larger one. Dashed arrows indicate sampled ally data.}
% \end{figure}

\vspace{3em}

% We retain the formal definition of the original observation structure here for reference:
% \[
% \underbrace{[\text{level}_{n_i},\ \text{row}_{n_i},\ \text{col}_{n_i}]}_{\text{Own Features}} ++
% \underbrace{[f \in F]}_{\text{Food Matrix}} ++
% \underbrace{[n \in N_{/i}]}_{\text{Agent Matrix}}
% \]

\section{Results and Observations}

Before evaluating learning outcomes, we validated the consistency of training step costs.
Since all experiments were conducted on a shared computing cluster with limited 
control over resource allocation, we measured the time taken per training
step throughout the course of the experiment. This validation served two purposes. First, 
it allowed us to verify that resource provisioning remained stable throughout the experiment, 
mitigating concerns about variability introduced by background cluster activity or virtualization.
Second, and more critically, it confirmed that the cost of training scaled linearly 
with the number of agents—an anticipated relationship, but simple enough to verify empirically.
This linearity justifies our use of \emph{agent-steps} as the primary cost metric in our analysis.
%These timing results are presented in the appendix for completeness.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{time_costs.png}
    \caption{Mean training step duration across agent counts.}
    \label{fig:agent-steps}
\end{figure}


We next examine the learning process in the three experimental environments: 
Waterworld, Multiwalker, and Level-Based Foraging (LBF). For each environment, 
we visualized a series of training curves corresponding to a fixed target number of agents.
Each plot compares the baseline—tabula rasa training of the full agent team—with 
retraining runs initialized from pretrained policies.
The baseline is visualized as a dotted mean line, with shaded bands 
representing a 95\% prediction interval based on 30 independent training runs.
Mean training trajectories of the retrained runs are grouped by the amount 
of pretraining used to initialize them.

In most settings, retrained configurations ultimately converged to 
performance levels comparable to those of tabula rasa training.
This consistency across end performance suggests that retraining from a smaller 
pretrained team does not appear to limit the achievable policy performance, 
at least within the range of configurations evaluated.

In the Waterworld environment, the primary benefit of pretraining was accelerated learning. 
Increasing the ratio between the pretraining and final team sizes led to greater improvements 
in convergence speed compared to training the full team from scratch. For instance, 
while retraining from 2-agent policies to 3 or 4 agents offered minimal benefit 
(Figure \ref{fig:waterworld-4}), scaling to 8 agents from the same 2-agent base resulted in 
substantial speedups (Figure~\ref{fig:waterworld-8}). These gains diminished rapidly, 
with little additional benefit observed beyond 60 pretraining steps (120 agent-steps).
This suggests that relatively short pretraining can provide sufficient foundational 
coordination to improve sample efficiency at scale, though final tuning may 
be necessary to achieve full performance with shorter pretraining periods.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Waterworld-4-agent.png}
    \caption{}
    \label{fig:waterworld-4}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Waterworld-8-agent.png}
    \caption{}
    \label{fig:waterworld-8}
\end{figure}

In Figure~\ref{fig:waterworld-aucs} we summarize overall efficiency trends across configurations. 
Each cell represents the percentage improvement in training efficiency 
achieved through pretraining, relative to full tabula rasa training.
This percentage was calculated by comparing the area under the reward curve 
(AUC) of each retrained configuration to its corresponding baseline, as: $\text{Improvement (\%)} 
= \frac{\text{AUC}{\text{retrain}} - \text{AUC}{\text{baseline}}}{\text{AUC}_{\text{retrain}}}$
The resulting heatmap supports a general trend that larger target teams tend to 
benefit more from pretraining when initialized from a smaller cohort.
Although we observe peaks at 40 and 180 pretraining steps for the 7- and 8-agent configurations, 
we attribute these to stochastic variation rather than indicative of structural trends.
Overall, these results reinforce the observation that direct scaling is most effective 
when the ratio between pretraining and target team sizes is high.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Waterworld-AUCs.png}
    \caption{}
    \label{fig:waterworld-aucs}
\end{figure}


In Multiwalker, we observed a notable shift in the utility of pretraining. 
For smaller teams (4 or 5 agents), retrained and tabula rasa runs performed similarly, 
with retraining providing only modest acceleration (Figure~\ref{fig:multiwalker-5}). 
In contrast, for larger teams (6 and 7 agents), tabula rasa training frequently failed to 
converge or terminated early, likely due to the environment's sparse rewards and heightened 
sensitivity to miscoordination (Figure~\ref{fig:multiwalker-6}). 
In these scenarios, pretraining acted as a stabilizing mechanism by equipping agents 
with sufficient baseline competence to engage meaningfully in early training episodes. 
Without this foundation, larger teams (more vulnerable to premature episode termination 
due to the failure of any single agent) struggled to generate useful feedback. 
Pretraining thus mitigated an early-stage bottleneck and enabled learning to proceed 
in configurations where tabula rasa attempts often stalled.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Multiwalker-5-agent.png}
    \caption{}
    \label{fig:multiwalker-5}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Multiwalker-6-agent.png}
    \caption{}
    \label{fig:multiwalker-6}
\end{figure}

To further examine these trends, we generated an AUC-based heatmap for Multiwalker as shown 
earlier for Waterworld. The resulting pattern is more polarized than that observed for Waterworld.
For the larger teams the apparent efficiency gains, often exceeding 100\%
reflect the stabilizing role of pretraining.
In contrast, negative gains observed in some smaller-team configurations (e.g., 4 and 5 agents) 
are better understood as a result of the scaling transition requiring additional training. 
When this time exceeded the total convergence time of tabula rasa training, 
the net cost is necessarily higher, despite final performance often eventually met the baseline.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Multiwalker-AUCs.png}
    \caption{}
    \label{fig:multiwalker-aucs}
\end{figure}



We next consider the Level-Based Foraging (LBF) environment. 
In this setting, 
we anticipated potential impact of the observation structures
to 
training performance.
To characterize this impact, 
we obtained baseline performance from tabula rasa
for each of the observation schemas described in 
\ref{sec:related_work-observation_design}

Specifically, we trained full teams 
(ranging from 2 to 8 agents) from scratch using three observation schemas that differ in 
the degree of teammate information provided: (1) \emph{full observability}, which supplies agents with complete information about all allies; (2) \emph{truncated observability}, which offers a fixed number of randomly sampled ally features; and (3) \emph{ally-ignorant} observations, which exclude all teammate information. For each team size and observation schema, 30 independent runs were conducted, and the resulting performance distributions were compared.


Table%~\ref{tab:observation_comparisons} 
summarizes these comparisons. Across all agent counts, the full observability schema produced significantly different training outcomes than the two reduced forms. Statistical tests (two-sample t-tests with Bonferroni correction) confirmed that the full schema distributions were distinct from both the truncated and ally-ignorant variants, with $p < 0.001$ in all cases. However, the truncated and ally-ignorant observation settings were not statistically distinguishable from one another at any agent count, even under uncorrected significance thresholds. This suggests that limiting or excluding teammate features leads to comparable training outcomes in LBF, while full observability induces a distinct learning dynamic.

Interestingly, the implied structure of the LBF heatmap is inverted compared to the earlier environments. Rather than showing greater benefit with larger target teams, we observe diminishing returns from pretraining as the number of agents increases. We attribute this shift to the complexity introduced by LBF’s dynamic intrinsic heterogeneity. As agents level up asynchronously, their roles and capabilities diverge over time, increasing the coordination burden. In this setting, pretraining with smaller teams provides limited benefit, as it fails to expose agents to the richer, role-specific interactions that emerge in larger, heterogeneous teams.

% Heatmap Figure


\section{Conclusion}
% This study demonstrates the potential benefits of pretraining smaller MARL teams before scaling up. By introducing adjusted time steps and analyzing training efficiency, we contribute to understanding scalable MARL techniques. Future work will refine these methods and expand the applicability to more complex environments.


\section{Future Work}
% Building on these findings, Contribution 2 will explore curriculum-based training strategies, progressively increasing agent counts in complex tasks. Additionally, we aim to investigate biological models of cooperation and specialization for further insights into HARL systems.
% Invariant obs space learning

% \nocite{*}
\printbibliography

\end{document}


%%%
%   Appendix?
%%%

% We validate our approach on three multi-agent environments chosen for their 
% diversity and challenge to heterogeneous-agent learning: 
% \textbf{(1) Waterworld}, \textbf{(2) Multiwalker}, and \textbf{(3) Level-Based Foraging (LBF)}. 
% These domains, originally introduced by Gupta et al. (2017) and extended in the 
% PettingZoo MARL benchmark,
% feature different observation structures and degrees of heterogeneity, 
% providing a thorough testbed for our transfer learning framework.

% \textbf{Waterworld:} Waterworld is a continuous control environment where $N$ \emph{pursuer} 
% agents navigate a two-dimensional continuous arena to collect food targets while avoiding 
% poisonous targets. The agents are homogeneous and have identical action spaces 
% (e.g., thrust in different directions). Each agent's observation consists of continuous 
% readings (such as distances and angles) to nearby food pellets, poison pellets, 
% and other agents, often making the environment fully observable in practice 
% (each agent can sense all relevant objects within a certain range). 
% The team receives a dense reward: every food target consumed yields a positive reward 
% (often shared among agents or given to the specific collector, 
% but ultimately contributing to the team’s total), and contact with 
% poison yields a negative reward. Cooperation is required in the sense that 
% agents must spread out to cover the area and herd targets effectively without 
% collisions or wasted effort. Waterworld primarily evaluates scalability in a 
% symmetric setting — since all agents are identical, an effective policy should 
% handle an increase in the number of pursuers seamlessly. We use Waterworld to 
% test whether our learned policy can coordinate larger swarms of agents hunting targets, 
% without sacrificing performance on individual avoidance of poison.

% \textbf{Multiwalker:} Multiwalker is a more complex continuous cooperative task 
% based on the OpenAI Gym BipedalWalker. In this environment, multiple bipedal 
% ``walker'' agents must jointly transport a payload (e.g., a ladder or package balanced on 
% their heads) from one end of a field to the other without dropping it. 
% The walkers are physically coupled through the shared object: 
% if any walker moves too fast or falls, the object may drop, penalizing the team. 
% Each agent receives a reward signal that is mostly global – for instance, 
% a small positive reward for each time step that the package moves forward without falling, 
% and a large negative reward if the package is dropped (this penalty is applied to all walkers). 
% The agents are again homogeneous in dynamics and control (each controls its two-legged walker), 
% and each observes its own state (joint angles, velocities) as well as some information about 
% the package (e.g., its relative position/tilt) and possibly the other walkers' positions. 
% We consider Multiwalker to have \emph{full observability} as implemented 
% (the state can be reconstructed from the agents' joint observations, and in some 
% implementations each agent may even get the full state). The key challenge here is 
% tight coordination: the policy must ensure synchronous movement among walkers to 
% keep the payload balanced. This environment tests our method's ability to scale a 
% locomotion-based policy from a smaller team (e.g., 2 walkers) 
% to a larger one (e.g., 3 or 4 walkers). A successful transferable policy would allow 
% additional walkers to be integrated into the team without retraining, maintaining 
% balance and forward progress. We evaluate how our curriculum training helps the walkers 
% learn complementary gait patterns and whether those patterns remain effective as the team grows.

% \textbf{Level-Based Foraging (LBF):} LBF is a discrete grid-world environment 
% that explicitly requires heterogeneous-agent cooperation. 
% Agents and food items are placed on a grid; each food item has a level 
% (an integer requirement) and each agent has its own skill level. 
% An agent can collect a food item only if one or more agents cooperatively 
% occupy the same cell as that food and the sum of their skill levels meets or 
% exceeds the food's level requirement. All agents share a team reward for 
% successful food collection, making it a fully cooperative game. 
% However, the environment is partially observable: agents have a limited 
% field of view (they only observe nearby cells, not the entire grid). 
% This means agents must explore and possibly coordinate via implicit communication 
% (e.g., meeting at a target) under uncertainty. The heterogeneity arises 
% naturally because agents may have different skill levels; for example, a level-3 
% agent can collect certain food alone that a level-1 agent cannot, whereas 
% two level-1 agents together might achieve it. Optimal play often involves 
% \emph{specialization}, where higher-level agents take on the role of tackling 
% high-value food or assisting lower-level agents, and lower-level agents focus 
% on targets that match their ability or support others. We treat LBF as a 
% critical test for our approach’s ability to handle agents with 
% \textbf{diverse capabilities and partial observability}. 
% Our training framework includes the agents' skill level as part of the observation input, 
% so the shared policy can condition decisions on an agent's own ability. 
% During evaluation, we examine whether the learned policy indeed yields 
% different behaviors for agents of different levels (indicating emergent specialization) 
% and how well it generalizes to changes in team composition, such as adding an extra 
% agent with a new skill level. LBF being a mixed cooperative-competitive domain in general, 
% we focus on the fully cooperative version here (all agents on one team, no adversaries), 
% aligning with our centralized team reward setting.
