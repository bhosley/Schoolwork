\section{Introduction}
Deep reinforcement learning (RL) traditionally relies on fixed-capacity neural networks that are architecturally determined before training. However, many real-world scenarios would benefit from adaptive model complexity: as an agent learns new tasks or faces increasing task difficulty, it may require additional network capacity to represent more complex policies or value functions. A naive approach to increasing capacity is to train a larger network from scratch or fine-tune a pre-trained smaller network, but this often leads to losing previously learned behaviors (catastrophic forgetting) or inefficient use of prior learning. Projection-based network expansion offers a promising alternative. In this paradigm, a policy network is progressively expanded (for example by adding neurons, layers, or modules) through functional or tensor projections that ensure the expanded network initially replicates the behavior of the original. This allows learning to continue with a larger architecture without discarding or overwriting prior knowledge. In other words, the agent’s policy is preserved as a subset of the expanded network’s functionality, providing a warm-start for further training.

Historically, methods such as NeuroEvolution of Augmenting Topologies (NEAT) allowed neural networks to grow in complexity during reinforcement learning by adding neurons and connections [stanley2002]. However, such evolutionary approaches did not guarantee that added components would preserve the agent’s existing functionality – they relied on selection over a population to gradually improve performance. In contrast, the modern approaches surveyed in this review explicitly aim for function-preserving expansions. This literature review examines the development of projection-based neural network expansion techniques with an emphasis on their (potential and actual) use in reinforcement learning. Because dedicated RL-specific research on network expansion remains relatively sparse, we draw on insights from related areas including supervised learning, continual/lifelong learning, curriculum learning, and network morphism theory. We focus on approaches that grow network architectures in a function-preserving manner – for example, using identity-initialized new layers or tensor transformations – so that previously acquired skills are retained. Key themes include: (1) methods for function-preserving expansion such as Net2Net and other function-preserving mappings [chen2015], (2) theoretical frameworks like network morphisms that define conditions under which a “child” network can exactly replicate a “parent” network [wei2016], (3) dynamic architecture growth in continual learning (e.g. progressive neural networks [rusu2016] and dynamically expandable networks [yoon2018]) which prevent forgetting by allocating new capacity, and (4) known or potential applications of these ideas in deep RL contexts (e.g. multi-task adaptation, sample-efficient training via progressive model scaling, or handling non-stationary task complexity). We also highlight how projection-based expansion supports training continuity – enabling an RL agent to incorporate new capabilities or handle greater complexity without resetting training. Finally, we discuss empirical findings on performance and outline common evaluation domains (with placeholders for specific benchmark details to be decided).

\section{Function-Preserving Network Expansion Methods}

\subsection{Net2Net: Expanding Networks with Function Preservation}
One of the seminal works on function-preserving expansion is the Net2Net technique proposed by Chen et al. [chen2015]. Net2Net introduced the idea of transforming a “teacher” network into a larger “student” network that initially realizes the same function, thereby accelerating training of the larger model by warm-starting from the smaller model’s knowledge. Two specific transformations were described: (1) \textbf{Net2WiderNet}, which increases the width (number of hidden units or filters) of a layer, and (2) \textbf{Net2DeeperNet}, which increases the depth (adds new layers). In Net2WiderNet, new neurons are added in a hidden layer and the existing weights are copied (and slightly perturbed or split) in a manner that preserves the layer’s output. For example, if a fully-connected layer’s output is $y = W x$ (with $W$ of shape $m \times n$) and we expand it to $\tilde{W}$ of shape $m \times n’$ with $n’ > n$, one can initialize $\tilde{W}$ such that the extra $n’ - n$ columns are initially copies of the original columns (perhaps scaled and split between new neurons) and adjust the next layer’s weights accordingly to keep the function unchanged [chen2015]. In convolutional layers, new filters can be introduced by copying old filter weights (possibly dividing their values by the number of copies so that the average effect is the same). Net2DeeperNet similarly inserts a new layer that acts as an identity mapping, so that the overall network function remains the same. For ReLU networks, an identity mapping can be implemented by initializing a new layer’s weights to an identity matrix and biases to zero (since $\mathrm{ReLU}(x)$ is idempotent, i.e. $\mathrm{ReLU}(\mathrm{ReLU}(x)) = \mathrm{ReLU}(x)$). By these mechanisms, the larger “student” network starts off performing exactly like the smaller “teacher” network. Empirically, Chen et al. demonstrated that this function-preserving initialization leads the larger network to converge much faster than training the large network from scratch to reach the same accuracy [chen2015]. The Net2Net approach is particularly attractive in reinforcement learning, where a policy or value network trained on some task could be widened or deepened to tackle a more complex variant, without losing the original policy’s performance as a starting point.

\subsection{Network Morphism: Theoretical Framework for Architecture Expansion}
Building on similar concepts, Wei et al. formalized the idea of network expansion as a network morphism problem [wei2016]. A network morphism defines a transformation from a parent network to a child network such that the child network is a strict superset of the parent (e.g., it has additional neurons, filters, or layers) and yet exactly replicates the parent’s functionality (mapping from inputs to outputs) for all inputs. Wei et al. provided a theoretical foundation for various morphing operations: adding a neuron to an existing layer (width morphing), inserting new layers (depth morphing), and even increasing filter sizes or number of channels in convolutional networks. They introduced methods to initialize new parameters for these operations so that the output of the network remains unchanged by the expansion. For instance, to insert a new fully-connected layer with a non-linear activation (say Sigmoid or Tanh, which are not idempotent), one cannot simply drop in an identity initialization without altering the function. Wei et al. addressed this by proposing parametric activation functions that can be tuned such that the new layer initially performs an identity mapping for any input (essentially linearizing the activation at initialization) [wei2016]. They also developed improved initialization schemes (such as a deconvolution-based algorithm for inserting layers) to avoid the pitfalls of naive identity mappings (which can be sparse or not truly identity for certain activations). The network morphism framework demonstrates that a wide range of architectural changes can be made while strictly keeping the network’s original input-output mapping intact. Empirically, child networks created via network morphism often reach higher final performance with dramatically reduced training time. For example, Wei et al. report morphing a well-known 16-layer VGG model to a deeper/wider model that achieves better accuracy in only $\sim!1/15$ of the training time compared to training from scratch [wei2016]. This suggests an internal function-preserving regularization effect: expanding a network via morphism not only retains prior knowledge but can also lead to improved generalization once the network is fine-tuned. In an RL context, although direct applications of network morphism have been limited so far, the theory suggests one could seamlessly morph a learned policy network to a larger architecture when needed (for instance, expanding a policy’s layers or neurons to improve capacity for a more complex environment) without any drop in performance immediately after expansion.

\subsection{Tensor Projections and Composable Expansions}
Function-preserving expansions rely on initialization schemes that essentially project the learned function onto a larger parameter space. In other words, the original network’s parameters are embedded as a subset (or special case) of the new network’s parameters. This can be viewed as a tensor projection where the identity of the old function is maintained within the expanded network. For example, inserting an identity layer or expanding weight matrices by padding with zero blocks and identity sub-blocks are practical ways to achieve this projection. Recently, these ideas have been extended to more complex architectures. For instance, a 2023 study by Google DeepMind researchers introduced composable function-preserving transformations for Transformer networks [gesmundo2023]. They defined a set of six specific expansion operations that can increase various dimensions of a Transformer while preserving its function. These include:

\begin{itemize}
\item Increasing the size of the feed-forward network’s hidden layer (wider MLP layers)
\item Increasing the number of attention heads in the multi-head attention module
\item Increasing the dimensionality of each attention head’s output representation
\item Increasing the dimensionality of the attention input (query/key) representations
\item Increasing the model’s overall embedding size (the per-layer input/output representation dimensionality)
\item Adding more Transformer layers (increasing depth of the network)
\end{itemize}

Each of these transformations is accompanied by a function-preserving parameter initialization. For example, new attention heads can be initialized to behave exactly like an existing head (by copying the existing head’s weight matrices), or new MLP neurons can be initialized so that they initially pass their input through unchanged (e.g. by inserting them as parallel identity pathways). By composing such expansions, one can grow a Transformer from a smaller configuration to a much larger one in stages, each time starting training from a functionally equivalent model [gesmundo2023]. This strategy holds potential for reinforcement learning models as well: an agent’s network could be gradually scaled up as tasks demand, using similar tensor projection tricks to guarantee the agent’s policy or value function remains unchanged immediately after each expansion. The benefit is a smooth training curve – the policy’s performance does not crash when capacity is added; instead, the new parameters start in a state that preserves existing behavior and can then be refined to improve performance further. This composable expansion approach reflects a general principle behind many function-preserving methods: new parameters are introduced in such a way that they lie in the null space of the original function (at insertion time), thus not perturbing the outputs until they learn to contribute positively.

\section{Progressive and Dynamic Network Growth in Continual Learning}

\subsection{Progressive Neural Networks for Transfer}
A prominent approach to avoiding catastrophic forgetting while accumulating new skills is the Progressive Neural Network architecture [rusu2016]. Progressive networks were introduced as a solution for sequential multi-task learning in RL and related domains. The idea is to allocate a new neural network “column” (a set of layers/units) for each new task, while retaining the previously learned columns (for earlier tasks) in an unmodified, frozen state. Lateral connections are added from the frozen older columns to the new column, allowing the new network to reuse representations learned on previous tasks. For example, if an agent has learned to play several Atari games one after another, a progressive network would dedicate a separate neural module to each game; when learning a new game, the agent instantiates a fresh column of weights for that game but can receive input from all prior game columns via fixed lateral connections [rusu2016]. This architecture ensures that previously learned policies are never overwritten (since the parameters for earlier tasks are not changed), thus completely eliminating forgetting. At the same time, transfer learning is enabled: the new task’s module can leverage the rich feature detectors and high-level behaviors encoded in earlier columns. In their experiments, Rusu et al. demonstrated that progressive networks significantly outperformed fine-tuning baselines on sequences of tasks including classic Atari 2600 games and a 3D maze navigation challenge [rusu2016]. The agent with a progressive architecture was able to leverage low-level visual features and even high-level control knowledge from earlier games via the lateral connections, leading to faster learning and higher final performance on the new game. Progressive nets represent an extreme form of network expansion (adding an entire network’s worth of new parameters per task); while memory-intensive, they provide a clear proof-of-concept that expanding architecture can enable continual learning without loss of existing skills. In an RL context where tasks arrive sequentially or increase in complexity, progressive expansion offers a straightforward way to preserve the “policy skeleton” of earlier tasks and build new functionality on top.

\subsection{Dynamically Expandable Networks in Lifelong Learning}
In supervised lifelong learning research, more fine-grained expansion strategies have been explored to balance growth with compactness. Yoon et al. proposed Dynamically Expandable Networks (DEN), which grow a network only as needed for each new task in a sequence [yoon2018]. In DEN, when a new task arrives, the algorithm first attempts to accommodate the task using the current network capacity, encouraging the reuse of existing neurons. It employs sparse regularization (e.g., group LASSO) during training to identify which neurons or filters are important for the new task and which are free or less utilized. If the new task cannot be learned to high performance with the existing capacity (without significantly altering or degrading performance on prior tasks), DEN will then allocate a limited number of new neurons (and corresponding weights) in those layers where extra capacity is required. Crucially, these new units are initialized in a way that minimally interferes with existing functionality – for example, new weights might be initialized to near-zero, so that initially the new neurons produce negligible output and do not alter the network’s predictions on old tasks. After expansion, the network is further trained on the new task (often with a small amount of fine-tuning on old tasks or regularization to retain old task performance). Through this approach, the model expands incrementally, adding just enough capacity per task while preserving previous knowledge. Yoon et al. showed on benchmarks like sequential image classification (e.g. MNIST variations and CIFAR-100 splits) that DEN could achieve high accuracy on all tasks with significantly less network growth than naive progressive stacking [yoon2018]. This indicates that a judicious expansion policy can retain past knowledge and accommodate new information without unbounded growth. Although DEN was demonstrated on supervised tasks, the principles translate to reinforcement learning: an RL agent could similarly detect when its current policy network is inadequate to represent a new task or an increase in task complexity, and then extend the network (with extra neurons or layers) to meet the new demands. The key is to do so in a controlled, function-preserving way so that skills on earlier tasks are retained – which DEN achieves via careful regularization and initialization.

\subsection{Other Strategies and Curriculum-Based Expansion}
Beyond the above methods, several other approaches support dynamic model scaling in a continual or curriculum learning context. An example from evolutionary strategy is PathNet [fernando2017], which allocates and reuses subnetworks within a larger network for different tasks. In PathNet, a large fixed network is initialized, and for each new task a genetic algorithm is used to select a pathway (a subset of neurons and connections) through this network to train, while keeping other weights fixed. After training on a task, those weights can be frozen, and a new task will evolve a new pathway. This method allows knowledge to be preserved in the frozen portions (no forgetting) and reused if advantageous (if the genetic search chooses to re-use some parts of previous paths), effectively achieving a form of structural transfer. PathNet does not grow the network (since the full network is allocated initially), but it illustrates a paradigm of reusing and locking different parts of a network per task to preserve knowledge, which is conceptually related to expansion approaches (allocating new resources for new tasks). Another line of research has explored using learning algorithms to decide when and how to expand networks. For instance, in neural architecture search, some works have treated architecture changes as actions in a meta-controller. Cai et al. [cai2018] and Ashok et al. [ashok2018] both used reinforcement learning (policy gradient controllers) to modify neural network architectures: their controllers could choose to apply operations like adding layers, widening layers, or removing components, with the goal of optimizing accuracy under resource constraints. In those works (which were in supervised domains), Net2Net-style function-preserving transformations were often used as the permissible operations for growing or shrinking the network. This demonstrates that function-preserving expansion is not only useful for continual learning, but also as a tool for efficient architecture search – the network can be morphed and fine-tuned rather than entirely re-trained whenever a change is made.

The concept of curriculum learning – training on tasks of increasing difficulty – can be naturally combined with model expansion. In a curriculum, early stages might involve simpler environments or tasks which a small network can learn adequately. As the task complexity grows (e.g., more difficult levels, additional obstacles, higher-dimensional observations, or more agents to control), the agent’s network can be expanded to provide additional capacity to learn the more complex behavior, while keeping the earlier policy as a starting point (embedded in the larger network). This progressive scaling of model size alongside task difficulty can improve stability and sample efficiency: the agent always has a network appropriate to the current task’s complexity, initialized with a competent policy from the previous stage. Although this specific paradigm (gradually increasing network size during an RL curriculum) has not been extensively formalized in literature, it aligns with practices reported in some large-scale RL systems. For example, OpenAI’s Dota 2 bot underwent several changes in model architecture during its development; rather than retrain from scratch after each change, the engineers applied ad-hoc function-preserving transformations (akin to Net2Net) to initialize the larger network with the smaller network’s learned parameters and policy, allowing training to resume without performance regression [agarwal2022]. Similarly, DeepMind’s AlphaStar in StarCraft II employed population-based training (PBT), wherein many agents with varying hyper-parameters (including network sizes) were trained in parallel and could inherit weights from one another; over time, this process effectively increased the effective capacity of the best agents while reusing knowledge from earlier agents [agarwal2022]. These cases underscore that dynamically increasing model capacity is not only a theoretical idea but has been applied in practice to handle escalating complexity in extended training runs. We anticipate future research will formalize such adaptive capacity curricula, defining when to trigger expansions and how to integrate them seamlessly into RL training loops.

\section{Benchmarks and Empirical Evaluation}
A variety of benchmarks can be used to assess projection-based network expansion in RL. Prior works have demonstrated these methods on supervised benchmarks (e.g. CIFAR-10 for Net2Net [chen2015], ImageNet for network morphism [wei2016], Permuted MNIST for DEN [yoon2018]) and on RL tasks (Atari games for progressive nets [rusu2016], etc.). For the purposes of this review, we outline a prospective evaluation setup for RL (to be refined once specific environments are chosen):
	•	Discrete control (classic or Atari games): For example, an agent could first be trained with a small network on a simpler task or an easier version of an Atari game. The network is then expanded and training continues on a more difficult game or the full-scale version of the task. We would measure learning curves (to see if the expanded network learns faster or achieves higher reward than a fixed network) and check that performance on the initial task is preserved after expansion (if the task is revisited or as a measure of backward compatibility).
	•	Continuous control (robotics or MuJoCo): As a case, an agent might learn a basic locomotion task (like walking) with a modest-sized policy network. Then the network could be expanded (e.g., adding neurons in hidden layers) and the agent trained on a harder control task, such as running or navigating uneven terrain, which demands a richer policy. Evaluation would focus on sample efficiency in the new task and the agent’s ability to retain competency on the simpler task (if tested).
	•	Multi-task or curriculum scenarios: We can consider a sequence of tasks of increasing complexity (for instance, navigation in progressively larger mazes, or a robotic manipulation task with increasing number of objects to handle). The agent’s network is grown at predetermined milestones or when learning saturates on the current task. We will evaluate how the incremental expansion impacts overall performance: ideally, the agent should solve later tasks more efficiently than if it had started from scratch, and maintain performance on earlier tasks. This would be compared against baselines like training a fixed large network on all tasks, or sequential fine-tuning without expansion.

Benchmark details are to be determined. In designing the evaluation, key metrics will include sample efficiency (does expanding the network yield faster convergence on new tasks, compared to training a large network from scratch or other baselines?) and training continuity (does the agent avoid performance drops when the network is expanded, indicating successful function preservation?). We will also track the final performance on each task and the overall parameter growth. This will illuminate the practical benefits and costs of projection-based expansion in RL settings.

\section{Conclusion}
Projection-based network expansion is a powerful concept that addresses a fundamental challenge in reinforcement learning: how to continue learning and adapting without forgetting or having to restart from scratch when task demands increase or change. We have reviewed a spectrum of approaches, from the original Net2Net transformations [chen2015] that introduce new neurons or layers without changing the network’s function, to advanced network morphism techniques [wei2016] that provide a formal framework for such expansions, and to progressive and dynamic expansion strategies in continual learning [rusu2016][yoon2018] that demonstrate how agents can accumulate skills over time by growing their networks. A common thread in these methods is the emphasis on preserving functionality during expansion – this theoretical guarantee means that adding capacity does not induce an immediate drop in performance. Empirically, these approaches have shown substantial benefits: faster convergence to high performance in larger models (thanks to reusing prior knowledge), the ability to solve sequences of tasks without catastrophic forgetting, and improved final performance on complex problems owing to increased model expressiveness.

In the context of reinforcement learning specifically, research on deliberate architectural expansion is still emerging, but the potential benefits are significant. By maintaining training continuity – never losing the agent’s proficiency when the network is expanded – an RL agent can be deployed in a persistent learning scenario and progressively improve itself. This stands in contrast to the conventional approach of training a fixed-capacity model until it plateaus and then having to start over with a larger model. Projection-based expansion allows an agent’s knowledge to be cumulative: the network’s capacity grows in tandem with the task complexity, rather than being a one-shot design decision made beforehand.

There remain open questions and practical considerations. One challenge is deciding when and how much to expand a network during training. Expanding too late might bottleneck the agent’s learning (if the network saturates and cannot represent a policy for a harder task), whereas expanding too early or too often could introduce unnecessary complexity and slow down training. Approaches like DEN offer heuristic criteria by gauging performance plateaus and weight utilization to trigger expansions [yoon2018], but applying such triggers in a fully autonomous RL training regime is an area for further research. Another consideration is the computational cost: although function-preserving initialization makes subsequent training efficient, each expansion increases model size and inference cost. In long-running or deployment scenarios, one might need to balance expansion with compression; for example, the “Progress & Compress” scheme has been suggested to alternate between expanding for new tasks and compressing knowledge into a smaller model via distillation [schwarz2018]. Developing such combined strategies could ensure an agent grows when needed but also remains efficient.

In summary, projection-based network expansion provides a compelling framework for continual and curriculum learning in RL. It bridges insights from transfer learning, continual learning, and network design to enable agents that can grow their neural representations as they learn, without forgetting the past. The theoretical foundations ensure that expansions are safe (function-preserving), and empirical evidence across supervised and reinforcement learning domains shows improved learning dynamics and outcomes. As these techniques are further refined and adopted, we expect them to play a crucial role in building lifelong learning RL agents that can handle increasingly complex tasks by progressively expanding their knowledge and capacity.

References: (Placeholder for full reference list corresponding to citations [chen2015], [wei2016], [rusu2016], [fernando2017], [yoon2018], [ashok2018], [cai2018], [gesmundo2023], [agarwal2022], [stanley2002], [schwarz2018], etc.)