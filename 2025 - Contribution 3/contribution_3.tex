\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{booktabs}
\graphicspath{{Data}}

\usepackage[nomain,symbols,abbreviations]{glossaries-extra}
\makeatletter
\glsdisablehyper
\makeatother
\newabbreviation{rl}{RL}{reinforcement learning}
\newabbreviation{harl}{HARL}{heterogeneous-agent reinforcement learning}
\newabbreviation{marl}{MARL}{multi-agent reinforcement learning}
\newabbreviation{ppo}{PPO}{proximal policy optimization}
\newabbreviation{mappo}{MAPPO}{multi-agent \gls{ppo}}
\newabbreviation{happo}{HAPPO}{heterogeneous-agent \gls{ppo}}
\newabbreviation{ctde}{CTDE}{centralized training with decentralized execution}
\newabbreviation{posg}{POSG}{partially observable stochastic game}
\newabbreviation{dec-pomdp}{Dec-POMDP}{decentralised partially observable Markov decision process}
\newabbreviation{ippo}{IPPO}{independent \gls{ppo}}
\newabbreviation{hpn}{HPN}{hyper policy network}
\newabbreviation{mlp}{MLP}{multi-layer perceptron}
\newabbreviation{pic}{PIC}{permutation-invariant critic}
\newabbreviation{gnn}{GNN}{graph neural network}

\begin{document}

\title{Representational vs. Architectural Invariance: \\ Comparing Implicit Indication and Permutation Invariant Critics in Heterogeneous MARL}

\author{\IEEEauthorblockN{Brandon Hosley, Bruce Cox, Nicholas Yielding, Matthew Robbins}
\IEEEauthorblockA{\textit{Air Force Institute of Technology} \\
Wright-Patterson AFB, USA \\
\{brandon.hosley.1, bruce.cox, nicholas.yielding, matthew.robbins\}@afit.edu}
}

\maketitle

\begin{abstract}
Multi-agent reinforcement learning (MARL) with heterogeneous agents presents 
significant challenges for parameter sharing. 
Two distinct paradigms have emerged to address this: 
representational homogenization and architectural invariance. 
This work compares \textit{Implicit Indication}, 
a framework that constructs a homogenized input space 
via the union of agent capabilities, against the \textit{Permutation Invariant Critic} (PIC), 
which employs graph neural networks to achieve invariance. 
We evaluate both methods on the HyperGrid environment, 
analyzing their robustness to sensor degradation and scalability. 
While PIC achieves invariance through commutative aggregation in the critic, 
we demonstrate that Implicit Indication offers a lightweight alternative 
that shifts the burden of heterogeneity from the network architecture to the input representation.
\end{abstract}

\begin{IEEEkeywords}
Multi-Agent Reinforcement Learning, Heterogeneous Agents, Parameter Sharing, Implicit Indication
\end{IEEEkeywords}

\section{Introduction}

\Gls{marl} has achieved remarkable success in cooperative 
domains ranging from multiplayer video games to robotic coordination tasks~\cite{albrecht2024}. 
A key enabler of this success has been parameter sharing, where multiple agents utilize the 
same policy network, dramatically improving sample efficiency and enabling scalable 
training~\cite{gupta2017, terry2020}. However, standard parameter sharing assumes 
agent interchangeability: all agents possess identical observation and action spaces 
and can be treated as functionally equivalent. This assumption breaks down in 
heterogeneous multi-agent systems, where agents differ in their sensing capabilities, 
actuation constraints, or operational roles due to hardware limitations, 
mission specialization, or environmental constraints~\cite{calvo2018, cao2012}.

When agents are structurally heterogeneous, possessing different observation or 
action spaces, standard parameter sharing becomes problematic. 
Na\"ive approaches such as padding observations to a common dimensionality or 
concatenating inputs in a fixed order either obscure meaningful differences 
between agents or create spurious dependencies on arbitrary ordering decisions~\cite{terry2020}. 
This has motivated two distinct paradigms for enabling shared learning across heterogeneous 
agents: architectural solutions that enforce invariance through network design, and 
representational solutions that construct unified input spaces that preserve 
structural distinctions.


The architectural paradigm, exemplified by the \gls{pic}~\cite{liu2020b}, 
addresses heterogeneity by employing \glspl{gnn} to aggregate agent 
information in a permutation-invariant manner. \Gls{pic}'s graph-based critic treats each 
agent as a node and uses graph convolutional layers with symmetric pooling operations 
to produce a team-level value estimate that remains unchanged under agent permutations. 
To handle heterogeneity, \gls{pic} augments node representations with explicit attribute 
vectors encoding agent types or capabilities, allowing the \gls{gnn} to distinguish between 
agents while maintaining permutation invariance. 
This architectural approach has demonstrated effectiveness in scaling to large teams 
and handling varying numbers of agents~\cite{liu2020b}.

The representational paradigm, exemplified by Implicit Indication,%~\cite{hosley2025}, 
takes a fundamentally different approach. Rather than enforcing invariance architecturally, 
it constructs a homogenized observation space that spans all observation elements available 
to any agent in the team. Each agent's observation occupies a subset of this space, 
with unused elements masked or left empty. The policy network receives these homogenized 
observations and implicitly infers agent capabilities from the pattern of populated 
observation elements—hence ``implicit indication" of agent identity through observable 
structure rather than explicit identifiers. 
This representational approach enables parameter sharing without specialized 
network architectures, using standard feed-forward or convolutional networks.

While both paradigms address the same fundamental challenge, enabling shared policies 
across heterogeneous agents, they embody different philosophical commitments about 
where complexity should reside. 
The architectural approach places the burden on the network structure, 
requiring specialized layers (graph convolutions, attention mechanisms) 
to handle agent heterogeneity. 
The representational approach shifts this burden to the input encoding, 
relying on explicit masking to communicate observation availability 
while using standard network architectures. 
This raises a fundamental question: 
\textit{which paradigm is more effective for heterogeneous multi-agent learning?}

This work provides a direct empirical comparison between these two paradigms 
through controlled experiments in a multi-agent gridworld environment designed 
to isolate observation heterogeneity. 
We evaluate \gls{pic} against Implicit Indication across four sensor configurations 
spanning complete visibility, intersecting observation coverage, disjoint observation coverage, 
and incomplete coverage. 
Both methods are trained with identical hyperparameters and network capacities, 
and evaluated under eight conditions designed to assess robustness to structural perturbations: 
baseline performance, agent loss, sensor degradation, sensor improvement, coverage reduction, 
coverage expansion, shuffled agent assignments, 
and zero-shot generalization to novel observation patterns.

Our empirical findings reveal a consistent and substantial performance advantage for 
the representational approach. 
Notably, Implicit Indication also outperforms a heterogeneous baseline employing 
separate policies per agent, suggesting that the homogenized representation not 
only enables parameter sharing but actively facilitates learning. 
While both methods exhibit similar training dynamics, 
their evaluation performance diverges significantly, 
with Implicit Indication demonstrating superior robustness to sensor changes, 
team composition modifications, and novel agent configurations.

These results challenge the intuition that architectural invariance 
through graph neural networks is necessary for effective heterogeneous MARL. 
In domains where heterogeneity stems primarily from differing observation 
capabilities that can be decomposed into semantically aligned elements, 
explicit representational structure through masking appears to provide 
more effective conditioning for policy learning than learned graph-based aggregation. 
The simplicity of the representational approach—requiring no specialized architectural 
components beyond standard policy networks—makes it readily accessible for 
practitioners while delivering strong empirical performance.

The contributions of this work are threefold. 
First, we provide the first direct comparison between architectural and representational 
approaches to heterogeneity in MARL, using controlled experiments with matched network 
capacities and hyperparameters. 
Second, we demonstrate through comprehensive evaluation 
that representational homogenization can substantially outperform graph-based architectural 
solutions in domains characterized by observation heterogeneity. 
Third, we offer insights into when and why representational approaches may be preferable, 
identifying domain characteristics and design principles that favor explicit 
observation masking over learned invariant architectures. 
These findings have practical implications for multi-agent system design, 
suggesting that careful representation design can be as important as architectural 
innovation for enabling effective learning in heterogeneous teams.

\section{Related Work}

\subsection{Parameter Sharing in Multi-Agent Reinforcement Learning}

Parameter sharing has been a cornerstone of scalable \gls{marl}, 
enabling efficient learning in cooperative scenarios by allowing 
multiple agents to utilize the same policy network~\cite{gupta2017, foerster2018}. 
In homogeneous settings where agents possess identical observation and action spaces, 
parameter sharing improves sample efficiency by aggregating experience across agents 
and promotes coordinated behavior through shared representations~\cite{terry2020}. 
\Gls{mappo}~\cite{yu2022} has established parameter sharing as a 
strong baseline for cooperative tasks, demonstrating that simple weight-sharing 
schemes can achieve state-of-the-art performance when agents are structurally identical.

However, this paradigm breaks down when agents are heterogeneous. 
Terry et al.~\cite{terry2020} systematically evaluated various agent indication schema, 
methods for distinguishing agents under shared parameters; including one-hot identity vectors, 
learned embeddings, and geometric encodings. 
While these approaches enable differentiation among agents, 
they do not resolve underlying structural heterogeneity when 
observation or action spaces differ in dimensionality or semantics. 
Christianos et al.~\cite{christianos2021} proposed selective parameter sharing 
mechanisms that allow agents to share some network components while 
maintaining specialized modules, though this approach requires careful 
design of which components to share.

\subsection{Invariant and Equivariant Architectures}

A parallel line of research has explored architectures that explicitly 
enforce invariance or equivariance to input permutations. 
Deep Sets~\cite{zaheer2017} established a theoretical foundation for 
permutation-invariant functions over sets, proving that any such function 
can be decomposed into a transformation of aggregated element-wise representations. 
This framework has been widely adopted in domains where input ordering is arbitrary, 
providing both theoretical guarantees and practical architectures for set-based learning.

In \gls{marl}, these principles naturally extend to agent sets when agents are exchangeable,
when permuting agent identities does not change the underlying decision problem. 
Mean field approaches~\cite{yang2018, li2021b} leverage this by aggregating agent 
interactions through symmetric operations, achieving invariance to team size and agent ordering. 
However, these methods sacrifice relational information through commutative pooling, 
motivating richer architectures that preserve both invariance and expressiveness.

\Glspl{gnn} provide one solution by representing agents as nodes and their 
interactions as edges~\cite{yang2021a}. Graph convolutional layers process agent 
information through neighborhood aggregation, producing representations that are 
invariant to node ordering while capturing relational structure. 
Yang et al.~\cite{yang2021a} demonstrated that graph-based policies can adapt to 
dynamic interaction topologies in cooperative tasks, outperforming non-relational baselines. 
Attention mechanisms offer an alternative, using learned weights to aggregate 
information without fixed graph structure~\cite{zambaldi2018, tang2021}. 
These architectures have shown improved credit assignment and scalability in 
multi-agent domains~\cite{hao2023}.

\subsection{Permutation Invariant Critic}

The \gls{pic}~\cite{liu2020b} represents a 
prominent application of graph-based architectures to \gls{ctde}. 
\Gls{pic} addresses a fundamental challenge in centralized critics: na\"ive 
concatenation of agent observations in a fixed order creates arbitrary 
dependencies on agent indexing, breaking when agents are permuted or 
when team size varies. \Gls{pic} resolves this by employing a \gls{gnn} as the critic, 
where each agent corresponds to a node and graph 
convolutional layers aggregate information symmetrically.

Critically for heterogeneous teams, \gls{pic} augments node representations 
with explicit attribute vectors encoding agent types or capabilities~\cite{liu2020b}. 
This allows the GNN to distinguish between agents with different roles or capabilities 
while maintaining permutation invariance among agents of the same type. 
Liu et al. demonstrated that \gls{pic} scales effectively to teams of hundreds of 
agents and handles varying team compositions without architectural modifications, 
substantially outperforming fixed-order critics in environments 
requiring flexible coordination~\cite{liu2020b}.

The success of \gls{pic} has motivated extensions incorporating attention 
mechanisms and more sophisticated graph structures~\cite{hao2023, hao2022}. 
These variants demonstrate that architectural invariance through graph-based processing 
represents a viable paradigm for handling both permutation symmetry and agent heterogeneity. 
However, graph-based approaches introduce architectural complexity and may incur 
computational overhead compared to standard feed-forward networks, 
motivating investigation of alternative solutions.

\subsection{Handling Heterogeneity in Multi-Agent Systems}

Several frameworks specifically target heterogeneous multi-agent learning. 
\Gls{happo}~\cite{zhong2024} extends \gls{ppo} to heterogeneous 
teams by maintaining separate policy networks for agents with different observation 
or action spaces, while using a shared optimization framework to ensure joint training. 
\Gls{happo} achieves strong performance by allowing specialized policies per agent type, 
though this approach does not enable parameter sharing across structurally distinct 
agents and scales linearly with the number of agent types.

Hypernetwork-based approaches offer an alternative by using auxiliary 
networks to generate agent-specific parameters~\cite{hao2023}. 
Hao et al. proposed \gls{hpn} that use a separate network to produce 
the parameters of input processing modules conditioned on agent features, 
effectively treating the team as a single structured entity. 
This enables flexible parameter generation while maintaining a unified architecture, 
though at the cost of additional meta-network complexity.

Implicit Indication%~\cite{hosley2025} 
takes a different approach entirely, 
addressing heterogeneity through representational rather than architectural means. 
By constructing a homogenized observation space that spans all 
observation elements available to any agent, and using masking to 
indicate which elements each agent can observe, 
Implicit Indication enables standard network architectures to handle heterogeneous agents. 
The method demonstrated comparable or superior performance to HAPPO in evaluation scenarios 
while using a single shared policy, suggesting that representational solutions can be 
competitive with architectural and algorithmic approaches.

\subsection{Positioning This Work}

While prior work has developed both architectural solutions (\gls{pic}, graph-based methods) 
and representational solutions (Implicit Indication) for \gls{harl}, 
no previous study has directly compared these paradigms under controlled conditions. 
Most comparisons in the literature focus on variants within a single paradigm 
(e.g., comparing different graph architectures or different indication schemes) 
rather than evaluating fundamentally different approaches to the same problem.

This work fills that gap by providing a systematic empirical comparison between 
\gls{pic} (architectural invariance) and Implicit Indication (representational invariance) 
using matched experimental conditions. 
We isolate the impact of the invariance mechanism itself (architectural versus 
representational) by controlling for network capacity, hyperparameters, training procedures, 
and evaluation protocols. 
This enables us to directly assess which paradigm is more effective for 
heterogeneous multi-agent learning and to identify the conditions under which each approach excels.

Our findings have implications beyond the specific methods compared. 
By demonstrating that representational homogenization can substantially 
outperform architectural invariance in certain domains, we challenge 
assumptions about the necessity of specialized architectures for heterogeneous MARL. 
These results suggest that the choice between architectural and representational 
solutions should be guided by domain characteristics, particularly the nature of 
heterogeneity (observation-based versus behavioral) 
and the semantic structure of observation spaces. 
This work thus contributes both specific empirical findings and broader insights 
into design principles for heterogeneous multi-agent learning systems.

\section{Methodology}

This section describes the experimental methodology used to compare architectural and 
representational approaches to heterogeneity in \gls{marl}.
We provide a detailed account of both methods under evaluation, the experimental environment, 
training procedures, and evaluation protocols to ensure reproducibility and facilitate 
interpretation of results.

\subsection{Methods Under Comparison}

\subsubsection{Implicit Indication (Representational Approach)}

Implicit Indication addresses heterogeneity through representational homogenization.
%~\cite{hosley2025}.
The method constructs a unified observation space $\widetilde{O}$ 
defined as the union of all observation elements $O_c$ available to any agent in the team. 
Each agent $i$ has access to a subset of these elements determined by its sensor 
configuration $C_i \subseteq C$, where $C$ is the set of all possible observation element indices.

For agent $i$ with observation capability $C_i$, the homogenized observation 
$\widetilde{o}_i$ is constructed by:
\begin{equation*}
\widetilde{o}_i[c] = \begin{cases}
o_i^c & \text{if } c \in C_i \\
\mathbf{0} & \text{if } c \notin C_i
\end{cases}
\end{equation*}
where $o_i^c$ is the observation from element $c$ and $\mathbf{0}$ 
represents a null or masked value. 
The policy network $\pi_\theta(\widetilde{o}_i)$ receives this 
homogenized observation and implicitly infers agent capabilities 
from the pattern of populated versus masked elements. 
No explicit agent identifiers are provided; agent differentiation emerges 
naturally from the observation structure.

The implementation uses \gls{mappo}~\cite{yu2022} with a centralized value 
function and decentralized execution. The policy network processes the 
homogenized observation through convolutional layers (for spatial observation elements) 
and fully-connected layers, with masking implemented via zero-padding of 
unavailable observation channels. The critic receives the concatenation of all agents' 
homogenized observations, enabling centralized training while the policy 
operates only on individual observations during execution.


\subsubsection{Permutation Invariant Critic (Architectural Approach)}

The \gls{pic}~\cite{liu2020b} addresses 
heterogeneity through architectural invariance. \Gls{pic} employs a \gls{gnn}
as the centralized critic, treating each agent as a node in a graph. 
Node features are constructed by concatenating each agent's observation with an 
explicit attribute vector encoding its type or capabilities.

For a team of $N$ agents, the critic constructs a fully-connected graph where 
each node $i$ has feature vector $\mathbf{h}_i^{(0)} = [\mathbf{o}_i; \mathbf{a}_i]$, 
where $\mathbf{o}_i$ is the agent's observation and $\mathbf{a}_i$ is its one-hot 
encoded agent type. Graph convolutional layers update node representations through 
neighborhood aggregation:
\begin{equation*}
\mathbf{h}_i^{(l+1)} = \sigma\left(\mathbf{W}^{(l)}\mathbf{h}_i^{(l)} + \sum_{j \in \mathcal{N}(i)} \frac{1}{|\mathcal{N}(i)|}\mathbf{W}^{(l)}\mathbf{h}_j^{(l)}\right)
\end{equation*}
where $\mathcal{N}(i)$ denotes the neighbors of node $i$ and $\sigma$ is a nonlinear activation. 
After $L$ graph convolutional layers, node representations are aggregated 
through max-pooling to produce a team-level value estimate that is invariant to node ordering.

The decentralized policies in \gls{pic} use standard actor networks that process 
individual observations augmented with agent type attributes. 
The architectural invariance resides entirely in the critic,
which enables learning a shared value function that generalizes across 
agent permutations while the explicit attributes enable differentiation 
based on agent capabilities.

\subsubsection{Heterogeneous Baseline}

As an additional baseline, we include \gls{happo}~\cite{zhong2024}, 
which maintains separate policy and value networks for each agent type. 
\Gls{happo} employs a sequential update scheme that ensures monotonic policy improvement 
while accommodating different policy parameterizations across agents. 
This represents the standard approach to heterogeneous MARL without parameter sharing, 
providing a reference point for evaluating the benefits of shared representations.

\subsection{Experimental Environment}

Experiments are conducted in the HyperGrid environment, 
a multi-agent gridworld designed to isolate observation 
heterogeneity from other confounding factors.%~\cite{hosley2025}. 
The environment features discrete spatial navigation with cooperative 
objectives and supports flexible specification of per-agent observation capabilities.

\subsubsection{Environment Structure}

The environment consists of an $n$-dimensional discrete grid populated by multiple agents, 
collectible objectives, static obstacles, and hazards. 
All agents share the same action space (movement, orientation adjustment, and object interaction) 
and pursue a fully cooperative objective: collecting target objects while avoiding hazards. 
Episodes terminate when all objectives are collected or a fixed time horizon is reached. 
Rewards are shared across agents, ensuring alignment toward the common goal.

The key feature enabling our comparison is the environment's support for heterogeneous 
observation configurations. Each entity type in the environment 
(objectives, hazards, obstacles, agents) has an associated 
visibility profile across observation channels. 
Agents are assigned subsets of these channels corresponding to different sensor modalities. 
For example, one agent might observe through "forward camera" and "LIDAR" channels, 
while another uses "forward camera" and "thermal" channels. 
The environment respects these assignments when constructing agent observations, 
masking entities that are not visible in an agent's available channels.

\subsubsection{Sensor Configurations}

We evaluate performance across four sensor configurations that 
span a range of heterogeneity structures:

\begin{itemize}
    \item \textbf{Complete Visibility}: 
        All agents have access to all observation elements ($C_i = C$ for all $i$). 
        This configuration serves as a homogeneous baseline where both methods 
        should perform identically in principle.
        \item \textbf{Intersecting Span}: 
        Agents have partially overlapping observation capabilities such that 
        $\cup_{i} C_i = C$ and $C_i \cap C_j \neq \emptyset$ for all pairs $i,j$. 
        This represents moderate heterogeneity where agents have some 
        shared observational capacity.
        \item \textbf{Disjoint Span}: 
        Agents have non-overlapping observation elements such that 
        $\cup_{i} C_i = C$ and $C_i \cap C_j = \emptyset$ for $i \neq j$. 
        This maximal heterogeneity ensures agents observe completely different 
        aspects of the environment state.
        \item \textbf{Incomplete Coverage}: 
        Agents' combined observation capabilities do not span the full state space: 
        $\exists c \in C$ such that $c \notin \cup_{i} C_i$. 
        This configuration tests robustness when information is 
        fundamentally missing from the team.
\end{itemize}

These configurations enable systematic evaluation of how observation structure 
affects the relative performance of architectural versus representational approaches.

\subsection{Training Setup}

\subsubsection{Network Architectures}

To ensure fair comparison, both methods use equivalent network capacities. 
For observation processing, convolutional encoders with architecture 
$[16, 32, 64]$ filters (kernel size $2 \times 2$) process spatial observation channels. 
Direction information is encoded through a small MLP with 16 hidden units. 
These encoded representations are concatenated and passed through a shared trunk with 
hidden layers $[128, 64]$ using tanh activations. 
he policy head outputs action probabilities over the discrete action space.

For \gls{pic}, the graph convolutional network uses 2 layers with hidden dimension 128, 
followed by max-pooling aggregation. Node features are constructed by concatenating
encoded observations with 4-dimensional one-hot agent type vectors. 
For Implicit Indication, the critic processes the concatenation of all agents' 
homogenized observations through an \gls{mlp} with architecture matching the total 
parameter count of \gls{pic}'s graph network.

\subsubsection{Training Hyperparameters}

All methods are trained using \gls{ppo} with the following hyperparameters: 
learning rate $\alpha = 0.001$, discount factor $\gamma = 0.99$, 
GAE parameter $\lambda = 0.95$, \gls{ppo} clip ratio $\epsilon = 0.2$, 
value function clip parameter $5.0$, batch size of 4096 samples, and entropy coefficient $0.0$. 
Training proceeds for up to 500M environment steps across 8 parallel rollout workers. 
These hyperparameters were selected through preliminary tuning to ensure strong 
baseline performance for both methods.

Each sensor configuration is trained with 30 independent runs for Implicit Indication 
and the heterogeneous baseline, and 5 independent runs for \gls{pic}
(reflecting resource constraints while ensuring statistical validity). 
All runs use different random seeds affecting environment initialization, 
network initialization, and sampling procedures.

\subsection{Evaluation Protocol}

Following training, we evaluate learned policies under eight distinct 
conditions designed to assess robustness, generalization, and adaptability:

\begin{enumerate}
    \item \textbf{Baseline}: 
    Standard evaluation in the training configuration with no modifications.
    \item \textbf{Agent Loss}: 
    One agent is removed from the team, testing robustness to team size reduction.
    \item \textbf{Sensor Degradation}: 
    A randomly selected agent loses access to one observation element 
    $c \in C_i$, simulating sensor failure.
    \item \textbf{Sensor Improvement}: 
    An agent gains access to a new observation element $c \notin C_i$ (where possible), 
    testing adaptability to enhanced sensing.
    \item \textbf{Degrade Coverage}: 
    All agents simultaneously lose access to a specific observation element, 
    reducing total team observability.
    \item \textbf{Improve Coverage}: 
    All agents simultaneously gain access to a 
    new observation element (where possible), enhancing team observability.
    \item \textbf{Shuffled Set}: 
    Agent-to-policy assignments are randomly permuted, testing sensitivity 
    to agent ordering assumptions.
    \item \textbf{Novel Span}: 
    The team is reconfigured with observation-element combinations not 
    encountered during training, evaluating zero-shot generalization.
\end{enumerate}

Each evaluation condition is assessed over multiple episodes, 
and we report mean episode return as the primary performance metric. 
Evaluations use the top-performing 15 policies (out of 30 training runs) 
selected based on final training performance, 
to focus analysis on well-learned policies while maintaining sufficient 
sample size for statistical analysis. 

This evaluation framework enables comprehensive assessment of learned policies 
beyond in-distribution performance, probing the fundamental robustness and g
eneralization properties that distinguish architectural and 
representational approaches to heterogeneity.

\subsection{Implementation Details}

All experiments are implemented using RLlib~\cite{yu2022} 
with custom environment and policy modifications. 
The HyperGrid environment is implemented as a multi-agent extension of MiniGrid, 
with modifications to support flexible observation masking and heterogeneous 
sensor configurations. Training is conducted on a compute cluster using 
Intel Xeon E5-2660 v3 processors with 32 CPU cores and 128 GB RAM per job. 
No GPU is used.

% Code, environment specifications, and trained models are available at the 
% repository referenced in the acknowledgments to support reproducibility 
% and enable extension of this work.
% #TODO : Add Github link

\section{Experimental Evaluation}

\subsection{Training Performance}

Training dynamics for both methods were evaluated across all four sensor configurations: 
complete visibility, intersecting span, disjoint span, and incomplete coverage.
Each configuration was trained using identical hyperparameters and network architectures 
to ensure controlled comparison. Each were trained for 30 independent 
runs per configuration.

\Cref{fig:training_curves} presents the learning curves for Implicit Indication and 
\gls{pic} across all configurations. Both methods exhibited similar convergence behavior, 
with comparable asymptotic performance and variance during training. This similarity 
suggests that the choice between architectural invariance (\gls{pic}'s graph-based critic) 
and representational invariance (Implicit Indication's homogenized input space) does not 
significantly impact training efficiency or the ability to learn from experience in this domain.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{training_curves.png}
    \caption{
        Training performance comparison between Implicit Indication and \gls{pic} across 
        sensor configurations. Shaded regions indicate standard error across training runs.}
    \label{fig:training_curves}
\end{figure*}

\subsection{Evaluation Performance}

Following training, policies were evaluated under eight distinct conditions designed 
to assess robustness and generalization: baseline performance, agent loss, 
sensor degradation, sensor improvement, coverage reduction, coverage expansion, 
shuffled agent assignments, and novel spanning sets. For comparison, 
we also evaluated the heterogeneous baseline (\gls{happo} with separate policies per agent).

\subsubsection{Overall Performance Comparison}

\Cref{fig:eval_comparison} presents the evaluation 
performance across all conditions and sensor configurations. 
The results reveal a clear and consistent pattern: Implicit Indication substantially 
outperforms both \gls{pic} and the heterogeneous baseline across virtually all evaluation scenarios.

% # TODO: Replace glossary items
\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{eval_scale.png}
    \caption{
        Evaluation performance across all conditions and sensor configurations. 
        Error bars represent standard error across evaluation trials. 
        Higher values indicate better performance. Implicit Indication (red) 
        consistently outperforms both the heterogeneous baseline (blue) and 
        \gls{pic} (green) across all tested scenarios.}
    \label{fig:eval_comparison}
\end{figure*}

\Cref{tab:overall_performance} summarizes the aggregate performance across all evaluation conditions. In the complete-visibility configuration, Implicit Indication achieves a mean return of $9.10$, compared to $5.22$ for the heterogeneous baseline and $3.89$ for PIC. This performance advantage persists across all sensor configurations: in intersecting-span ($5.04$ vs. $1.61$), disjoint-span ($5.80$ vs. $1.84$), and incomplete ($4.63$ vs. $1.05$).

\begin{table}[ht]
    \centering
    \caption{
        Overall evaluation performance across sensor configurations. 
        Values represent mean episode return averaged across all evaluation conditions.}
    \label{tab:overall_performance}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Sensor Configuration} & \textbf{Heterogeneous} & 
            \textbf{Implicit Indication} & \textbf{PIC} \\
        \midrule
        Complete             & $5.22$ & $9.10$ & $3.89$ \\
        Intersecting Span    & $3.43$ & $5.04$ & $1.61$ \\
        Disjoint Span        & $3.96$ & $5.80$ & $1.84$ \\
        Incomplete           & $3.58$ & $4.63$ & $1.05$ \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Performance Across Evaluation Types}

\Cref{fig:eval_heatmaps} provides a detailed breakdown of performance across 
evaluation types and sensor configurations. The heatmaps demonstrate that 
Implicit Indication's advantage is not limited to specific evaluation conditions 
but extends consistently across all tested scenarios.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{eval_heatmaps_cleaned.png}
    \caption{
        Evaluation performance heatmaps showing mean returns for Implicit Indication (left),
        PIC (center), and their difference (right). The difference heatmap shows PIC - 
        Implicit Indication, where negative values (blue) indicate Implicit Indication outperforms.
        Implicit Indication demonstrates superior performance across all evaluation types and 
        sensor configurations.}
    \label{fig:eval_heatmaps}
\end{figure*}

In the baseline evaluation, Implicit Indication achieves mean returns ranging from $4.63$ 
(incomplete) to $9.10$ (complete), while PIC achieves $1.05$ to $3.89$ in the same configurations. 
This pattern persists under structured perturbations: in sensor degradation evaluations, 
Implicit Indication maintains performance ($4.37$ to $8.69$) while PIC shows reduced performance 
($1.13$ to $5.07$). Similarly, under sensor improvement conditions, 
Implicit Indication achieves $4.79$ to $5.22$, compared to PIC's $1.14$ to $1.86$.

The novel-span evaluation, which tests zero-shot generalization to unseen 
observation-element combinations, reveals the same pattern. 
Implicit Indication achieves mean returns of $4.23$ to $7.79$ across configurations, 
while PIC achieves $1.33$ to $3.39$. This suggests that Implicit Indication's representational 
approach provides more robust transfer to novel agent compositions.

\subsubsection{Robustness Analysis}

The consistency of Implicit Indication's performance advantage across all evaluation 
types—including agent loss, coverage changes, and shuffled assignments—demonstrates 
robust handling of team composition changes and observation perturbations. 
In the agent-loss condition, where one agent is removed from the team, 
Implicit Indication maintains strong performance ($5.03$ to $8.87$) 
compared to PIC ($1.49$ to $3.61$). Similarly, in the shuffled-set condition, 
which randomizes agent-to-policy assignments, Implicit Indication achieves $4.10$ to $9.00$ 
while PIC achieves $1.18$ to $3.99$.

These results indicate that the explicit masking mechanism in Implicit Indication 
provides stable and interpretable encoding of observation availability, 
enabling the policy to adapt effectively to structural changes in the observation space. 
The performance gap between methods is particularly pronounced in configurations with 
inherent observation heterogeneity (intersecting-span and incomplete).

\subsubsection{Comparison with Heterogeneous Baseline}

Notably, Implicit Indication also outperforms the heterogeneous baseline 
(HAPPO with separate policies per agent) across all configurations, 
despite using a single shared policy. In the complete-visibility configuration, 
Implicit Indication achieves $9.10$ compared to the heterogeneous baseline's $5.22$. 
This advantage suggests that the homogenized representation not only enables parameter 
sharing but may also facilitate more effective learning by providing a unified 
representation space that captures commonalities across agent types.

\subsection{Discussion of Results}

The experimental results demonstrate that Implicit Indication's representational approach 
to handling heterogeneity provides substantial performance advantages over both 
architectural solutions (PIC) and agent-specific policies (heterogeneous baseline). 
While training dynamics were similar between Implicit Indication and PIC, 
evaluation performance diverged significantly, with Implicit Indication consistently 
achieving higher returns across configurations.

The observed performance gap between methods warrants careful consideration. 
While PIC's graph-based architecture has demonstrated effectiveness in other 
domains~\cite{liu2020b}, several factors may contribute to its relative underperformance 
in this environment. 
These may include the discrete nature of the gridworld domain, 
the specific observation structure used, hyperparameter sensitivities, 
or fundamental differences in how the two approaches handle partial observability. 
A comprehensive analysis of these factors remains a direction for future investigation.

Regardless of the underlying causes, the empirical findings provide clear evidence 
that representational homogenization through Implicit Indication offers a practical 
and effective approach for enabling shared policies in heterogeneous multi-agent teams. 
The method's consistent performance across diverse evaluation conditions, 
including structural perturbations, team composition changes, 
and zero-shot generalization scenarios—suggests broad applicability to 
heterogeneous multi-agent domains where observation heterogeneity is a primary challenge.

\section{Discussion}

\subsection{Representational vs. Architectural Approaches to Heterogeneity}

The experimental results demonstrate a clear performance advantage for the representational 
approach (Implicit Indication) over the architectural approach (PIC) in this domain. 
While both methods address heterogeneity, PIC through graph-based invariant architectures 
and Implicit Indication through homogenized observation spaces, 
the empirical evidence indicates that representational homogenization 
provides superior performance across all tested configurations and evaluation conditions.

This finding challenges the intuition that architectural solutions leveraging 
graph neural networks would necessarily outperform simpler representational approaches. 
PIC's graph-based critic was designed to aggregate agent information in a permutation-invariant 
manner while explicitly encoding agent types through node attributes~\cite{liu2020b}. 
However, in environments where heterogeneity stems primarily from differing observation 
capabilities rather than complex relational structures, the explicit masking mechanism 
of Implicit Indication appears to provide more effective conditioning for policy learning.

The performance advantage of Implicit Indication suggests that when observation 
heterogeneity is structural and can be decomposed into semantically aligned elements, 
direct representation of this structure through masking may be preferable to 
learning implicit representations through graph convolutions. 
The homogenized observation space provides the policy network with explicit information 
about which observation elements are available, potentially enabling more 
direct learning of observation-conditioned behaviors.

\subsection{Implications for Multi-Agent System Design}

Beyond the comparison with PIC, the results reveal that Implicit Indication 
also outperforms the heterogeneous baseline using separate policies per agent. 
This finding is particularly noteworthy as it demonstrates that parameter 
sharing under the Implicit Indication framework not only enables computational 
efficiency but may actually facilitate learning by providing a unified 
representational space that captures commonalities across agent types.

This has practical implications for multi-agent system design. 
In scenarios where agents differ primarily in their observation capabilities, such as 
heterogeneous robot teams with varying sensor suites or mixed-capability surveillance networks.
Implicit Indication offers a straightforward implementation path that requires minimal 
architectural specialization while delivering strong empirical performance. 
The method's consistent advantage across diverse evaluation conditions, 
including sensor loss, sensor gain, and novel agent compositions, 
further suggests robust generalization properties that are valuable 
for deployment in dynamic operational environments.

\subsection{Understanding PIC's Performance}

The observed underperformance of PIC relative to expectations warrants discussion. 
While PIC has demonstrated effectiveness in other multi-agent domains~\cite{liu2020b}, 
several factors may contribute to the results observed in this study. 
The discrete gridworld environment used for evaluation differs substantially 
from the continuous control tasks where PIC was originally validated. 
Additionally, the specific structure of observation heterogeneity in 
our experiments, characterized by varying observation element availability 
rather than complex relational patterns, may not align optimally with the 
relational inductive biases of graph neural networks.

Hyperparameter sensitivity could also play a role, as graph-based architectures 
often require careful tuning of graph construction parameters, aggregation functions, 
and attention mechanisms. The relatively small team size (four agents) used in our 
experiments may not fully leverage the scalability advantages that graph-based 
approaches can provide in larger populations. 
Furthermore, the homogenized observation space used by Implicit Indication provides 
very direct gradient signals about observation availability, whereas PIC must learn 
these relationships through the graph structure.

These considerations suggest that the comparison between methods may be domain-dependent 
and that PIC's architectural approach may prove more effective in environments with richer 
relational structure or larger team sizes. 
A comprehensive analysis isolating the impact of domain characteristics, team size, 
and hyperparameter choices on relative performance remains an important direction for future work.

\subsection{Limitations and Assumptions}

The effectiveness of Implicit Indication relies on semantic decomposability, 
the assumption that observation spaces can be factorized into elements 
with consistent meaning across agents. In domains where heterogeneity is purely behavioral 
(agents have identical observations but require different behaviors based on role),
Implicit Indication provides no mechanism for differentiation without reintroducing 
explicit agent identifiers. In such cases, architectural approaches that explicitly 
model agent types may be necessary.

The evaluation environment, while designed to isolate observation heterogeneity, 
represents a simplified testbed. Real-world multi-agent systems may involve additional 
complexities such as communication constraints, asynchronous execution, 
or continuous observation and action spaces. 
The extent to which the observed performance advantages transfer to 
such settings requires empirical validation in more complex domains.

Additionally, the computational comparison focused primarily on final 
performance rather than training efficiency or inference cost. 
While Implicit Indication avoids the architectural complexity of graph neural networks, 
a comprehensive evaluation of computational trade, 
offs—including wall-clock training time, memory usage, and inference latency, 
would provide a more complete picture of the practical advantages of each approach.

\section{Conclusion}

This work presented a comparative evaluation of representational and architectural 
approaches to handling heterogeneity in multi-agent reinforcement learning. 
Through controlled experiments in a gridworld environment with varying observation structures, 
we demonstrated that Implicit Indication's representational approach, based on homogenized 
observation spaces with explicit masking, provides substantial performance advantages 
over both PIC's graph-based architectural approach and heterogeneous baselines 
using separate per-agent policies.

The consistent performance of Implicit Indication across diverse evaluation conditions, 
including structural perturbations, team composition changes, 
and zero-shot generalization scenarios, demonstrates the practical viability of 
representational homogenization for heterogeneous teams. The method's simplicity, 
requiring no specialized architectural components beyond standard policy networks,
makes it readily accessible for practitioners working with heterogeneous multi-agent systems.

While the reasons for PIC's relative underperformance in this domain remain an open 
question requiring further investigation, the empirical evidence clearly indicates 
that representational solutions can be highly effective when observation heterogeneity 
is structural and semantically decomposable. 
The success of Implicit Indication suggests that explicitly encoding observation 
availability through masking may provide more direct learning signals than 
requiring the network to infer such structure through learned graph representations.

Future work should extend this comparison to more complex domains, including continuous 
control tasks, larger team sizes, and environments with richer relational structure. 
Investigating the interaction between domain characteristics and method performance 
would help establish clearer guidelines for when representational versus architectural 
approaches are most appropriate. Additionally, 
exploring hybrid approaches that combine explicit observation masking 
with relational architectures could potentially capture the benefits of both paradigms.

The broader implication of this work is that addressing heterogeneity through 
careful representation design—rather than solely through architectural innovation, 
can yield significant practical benefits. As multi-agent systems continue to 
grow in complexity and diversity, understanding when and how to leverage 
representational structure will be essential for building effective learning 
algorithms for heterogeneous teams.

\bibliographystyle{IEEEtran}
% #FINAL: Localize Bib File
\bibliography{../2025Bibs/Prospectus.bib} % Uses bibtex

\end{document}


% 
% 
% 
% 
% 













\section{Introduction}
Deep reinforcement learning (RL) traditionally relies on fixed-capacity neural networks that are architecturally determined before training. However, many real-world scenarios would benefit from adaptive model complexity: as an agent learns new tasks or faces increasing task difficulty, it may require additional network capacity to represent more complex policies or value functions. A naive approach to increasing capacity is to train a larger network from scratch or fine-tune a pre-trained smaller network, but this often leads to losing previously learned behaviors (catastrophic forgetting) or inefficient use of prior learning. Projection-based network expansion offers a promising alternative. In this paradigm, a policy network is progressively expanded (for example by adding neurons, layers, or modules) through functional or tensor projections that ensure the expanded network initially replicates the behavior of the original. This allows learning to continue with a larger architecture without discarding or overwriting prior knowledge. In other words, the agent’s policy is preserved as a subset of the expanded network’s functionality, providing a warm-start for further training.

Historically, methods such as NeuroEvolution of Augmenting Topologies (NEAT) allowed neural networks to grow in complexity during reinforcement learning by adding neurons and connections [stanley2002]. However, such evolutionary approaches did not guarantee that added components would preserve the agent’s existing functionality – they relied on selection over a population to gradually improve performance. In contrast, the modern approaches surveyed in this review explicitly aim for function-preserving expansions. This literature review examines the development of projection-based neural network expansion techniques with an emphasis on their (potential and actual) use in reinforcement learning. Because dedicated RL-specific research on network expansion remains relatively sparse, we draw on insights from related areas including supervised learning, continual/lifelong learning, curriculum learning, and network morphism theory. We focus on approaches that grow network architectures in a function-preserving manner – for example, using identity-initialized new layers or tensor transformations – so that previously acquired skills are retained. Key themes include: (1) methods for function-preserving expansion such as Net2Net and other function-preserving mappings [chen2015], (2) theoretical frameworks like network morphisms that define conditions under which a “child” network can exactly replicate a “parent” network [wei2016], (3) dynamic architecture growth in continual learning (e.g. progressive neural networks [rusu2016] and dynamically expandable networks [yoon2018]) which prevent forgetting by allocating new capacity, and (4) known or potential applications of these ideas in deep RL contexts (e.g. multi-task adaptation, sample-efficient training via progressive model scaling, or handling non-stationary task complexity). We also highlight how projection-based expansion supports training continuity – enabling an RL agent to incorporate new capabilities or handle greater complexity without resetting training. Finally, we discuss empirical findings on performance and outline common evaluation domains (with placeholders for specific benchmark details to be decided).

\section{Function-Preserving Network Expansion Methods}

\subsection{Net2Net: Expanding Networks with Function Preservation}
One of the seminal works on function-preserving expansion is the Net2Net technique proposed by Chen et al. [chen2015]. Net2Net introduced the idea of transforming a “teacher” network into a larger “student” network that initially realizes the same function, thereby accelerating training of the larger model by warm-starting from the smaller model’s knowledge. Two specific transformations were described: (1) \textbf{Net2WiderNet}, which increases the width (number of hidden units or filters) of a layer, and (2) \textbf{Net2DeeperNet}, which increases the depth (adds new layers). In Net2WiderNet, new neurons are added in a hidden layer and the existing weights are copied (and slightly perturbed or split) in a manner that preserves the layer’s output. For example, if a fully-connected layer’s output is $y = W x$ (with $W$ of shape $m \times n$) and we expand it to $\tilde{W}$ of shape $m \times n’$ with $n’ > n$, one can initialize $\tilde{W}$ such that the extra $n’ - n$ columns are initially copies of the original columns (perhaps scaled and split between new neurons) and adjust the next layer’s weights accordingly to keep the function unchanged [chen2015]. In convolutional layers, new filters can be introduced by copying old filter weights (possibly dividing their values by the number of copies so that the average effect is the same). Net2DeeperNet similarly inserts a new layer that acts as an identity mapping, so that the overall network function remains the same. For ReLU networks, an identity mapping can be implemented by initializing a new layer’s weights to an identity matrix and biases to zero (since $\mathrm{ReLU}(x)$ is idempotent, i.e. $\mathrm{ReLU}(\mathrm{ReLU}(x)) = \mathrm{ReLU}(x)$). By these mechanisms, the larger “student” network starts off performing exactly like the smaller “teacher” network. Empirically, Chen et al. demonstrated that this function-preserving initialization leads the larger network to converge much faster than training the large network from scratch to reach the same accuracy [chen2015]. The Net2Net approach is particularly attractive in reinforcement learning, where a policy or value network trained on some task could be widened or deepened to tackle a more complex variant, without losing the original policy’s performance as a starting point.

\subsection{Network Morphism: Theoretical Framework for Architecture Expansion}
Building on similar concepts, Wei et al. formalized the idea of network expansion as a network morphism problem [wei2016]. A network morphism defines a transformation from a parent network to a child network such that the child network is a strict superset of the parent (e.g., it has additional neurons, filters, or layers) and yet exactly replicates the parent’s functionality (mapping from inputs to outputs) for all inputs. Wei et al. provided a theoretical foundation for various morphing operations: adding a neuron to an existing layer (width morphing), inserting new layers (depth morphing), and even increasing filter sizes or number of channels in convolutional networks. They introduced methods to initialize new parameters for these operations so that the output of the network remains unchanged by the expansion. For instance, to insert a new fully-connected layer with a non-linear activation (say Sigmoid or Tanh, which are not idempotent), one cannot simply drop in an identity initialization without altering the function. Wei et al. addressed this by proposing parametric activation functions that can be tuned such that the new layer initially performs an identity mapping for any input (essentially linearizing the activation at initialization) [wei2016]. They also developed improved initialization schemes (such as a deconvolution-based algorithm for inserting layers) to avoid the pitfalls of naive identity mappings (which can be sparse or not truly identity for certain activations). The network morphism framework demonstrates that a wide range of architectural changes can be made while strictly keeping the network’s original input-output mapping intact. Empirically, child networks created via network morphism often reach higher final performance with dramatically reduced training time. For example, Wei et al. report morphing a well-known 16-layer VGG model to a deeper/wider model that achieves better accuracy in only $\sim!1/15$ of the training time compared to training from scratch [wei2016]. This suggests an internal function-preserving regularization effect: expanding a network via morphism not only retains prior knowledge but can also lead to improved generalization once the network is fine-tuned. In an RL context, although direct applications of network morphism have been limited so far, the theory suggests one could seamlessly morph a learned policy network to a larger architecture when needed (for instance, expanding a policy’s layers or neurons to improve capacity for a more complex environment) without any drop in performance immediately after expansion.

\subsection{Tensor Projections and Composable Expansions}
Function-preserving expansions rely on initialization schemes that essentially project the learned function onto a larger parameter space. In other words, the original network’s parameters are embedded as a subset (or special case) of the new network’s parameters. This can be viewed as a tensor projection where the identity of the old function is maintained within the expanded network. For example, inserting an identity layer or expanding weight matrices by padding with zero blocks and identity sub-blocks are practical ways to achieve this projection. Recently, these ideas have been extended to more complex architectures. For instance, a 2023 study by Google DeepMind researchers introduced composable function-preserving transformations for Transformer networks [gesmundo2023]. They defined a set of six specific expansion operations that can increase various dimensions of a Transformer while preserving its function. These include:

\begin{itemize}
\item Increasing the size of the feed-forward network’s hidden layer (wider MLP layers)
\item Increasing the number of attention heads in the multi-head attention module
\item Increasing the dimensionality of each attention head’s output representation
\item Increasing the dimensionality of the attention input (query/key) representations
\item Increasing the model’s overall embedding size (the per-layer input/output representation dimensionality)
\item Adding more Transformer layers (increasing depth of the network)
\end{itemize}

Each of these transformations is accompanied by a function-preserving parameter initialization. For example, new attention heads can be initialized to behave exactly like an existing head (by copying the existing head’s weight matrices), or new MLP neurons can be initialized so that they initially pass their input through unchanged (e.g. by inserting them as parallel identity pathways). By composing such expansions, one can grow a Transformer from a smaller configuration to a much larger one in stages, each time starting training from a functionally equivalent model [gesmundo2023]. This strategy holds potential for reinforcement learning models as well: an agent’s network could be gradually scaled up as tasks demand, using similar tensor projection tricks to guarantee the agent’s policy or value function remains unchanged immediately after each expansion. The benefit is a smooth training curve – the policy’s performance does not crash when capacity is added; instead, the new parameters start in a state that preserves existing behavior and can then be refined to improve performance further. This composable expansion approach reflects a general principle behind many function-preserving methods: new parameters are introduced in such a way that they lie in the null space of the original function (at insertion time), thus not perturbing the outputs until they learn to contribute positively.

\section{Progressive and Dynamic Network Growth in Continual Learning}

\subsection{Progressive Neural Networks for Transfer}
A prominent approach to avoiding catastrophic forgetting while accumulating new skills is the Progressive Neural Network architecture [rusu2016]. Progressive networks were introduced as a solution for sequential multi-task learning in RL and related domains. The idea is to allocate a new neural network “column” (a set of layers/units) for each new task, while retaining the previously learned columns (for earlier tasks) in an unmodified, frozen state. Lateral connections are added from the frozen older columns to the new column, allowing the new network to reuse representations learned on previous tasks. For example, if an agent has learned to play several Atari games one after another, a progressive network would dedicate a separate neural module to each game; when learning a new game, the agent instantiates a fresh column of weights for that game but can receive input from all prior game columns via fixed lateral connections [rusu2016]. This architecture ensures that previously learned policies are never overwritten (since the parameters for earlier tasks are not changed), thus completely eliminating forgetting. At the same time, transfer learning is enabled: the new task’s module can leverage the rich feature detectors and high-level behaviors encoded in earlier columns. In their experiments, Rusu et al. demonstrated that progressive networks significantly outperformed fine-tuning baselines on sequences of tasks including classic Atari 2600 games and a 3D maze navigation challenge [rusu2016]. The agent with a progressive architecture was able to leverage low-level visual features and even high-level control knowledge from earlier games via the lateral connections, leading to faster learning and higher final performance on the new game. Progressive nets represent an extreme form of network expansion (adding an entire network’s worth of new parameters per task); while memory-intensive, they provide a clear proof-of-concept that expanding architecture can enable continual learning without loss of existing skills. In an RL context where tasks arrive sequentially or increase in complexity, progressive expansion offers a straightforward way to preserve the “policy skeleton” of earlier tasks and build new functionality on top.

\subsection{Dynamically Expandable Networks in Lifelong Learning}
In supervised lifelong learning research, more fine-grained expansion strategies have been explored to balance growth with compactness. Yoon et al. proposed Dynamically Expandable Networks (DEN), which grow a network only as needed for each new task in a sequence [yoon2018]. In DEN, when a new task arrives, the algorithm first attempts to accommodate the task using the current network capacity, encouraging the reuse of existing neurons. It employs sparse regularization (e.g., group LASSO) during training to identify which neurons or filters are important for the new task and which are free or less utilized. If the new task cannot be learned to high performance with the existing capacity (without significantly altering or degrading performance on prior tasks), DEN will then allocate a limited number of new neurons (and corresponding weights) in those layers where extra capacity is required. Crucially, these new units are initialized in a way that minimally interferes with existing functionality – for example, new weights might be initialized to near-zero, so that initially the new neurons produce negligible output and do not alter the network’s predictions on old tasks. After expansion, the network is further trained on the new task (often with a small amount of fine-tuning on old tasks or regularization to retain old task performance). Through this approach, the model expands incrementally, adding just enough capacity per task while preserving previous knowledge. Yoon et al. showed on benchmarks like sequential image classification (e.g. MNIST variations and CIFAR-100 splits) that DEN could achieve high accuracy on all tasks with significantly less network growth than naive progressive stacking [yoon2018]. This indicates that a judicious expansion policy can retain past knowledge and accommodate new information without unbounded growth. Although DEN was demonstrated on supervised tasks, the principles translate to reinforcement learning: an RL agent could similarly detect when its current policy network is inadequate to represent a new task or an increase in task complexity, and then extend the network (with extra neurons or layers) to meet the new demands. The key is to do so in a controlled, function-preserving way so that skills on earlier tasks are retained – which DEN achieves via careful regularization and initialization.

\subsection{Other Strategies and Curriculum-Based Expansion}
Beyond the above methods, several other approaches support dynamic model scaling in a continual or curriculum learning context. An example from evolutionary strategy is PathNet [fernando2017], which allocates and reuses subnetworks within a larger network for different tasks. In PathNet, a large fixed network is initialized, and for each new task a genetic algorithm is used to select a pathway (a subset of neurons and connections) through this network to train, while keeping other weights fixed. After training on a task, those weights can be frozen, and a new task will evolve a new pathway. This method allows knowledge to be preserved in the frozen portions (no forgetting) and reused if advantageous (if the genetic search chooses to re-use some parts of previous paths), effectively achieving a form of structural transfer. PathNet does not grow the network (since the full network is allocated initially), but it illustrates a paradigm of reusing and locking different parts of a network per task to preserve knowledge, which is conceptually related to expansion approaches (allocating new resources for new tasks). Another line of research has explored using learning algorithms to decide when and how to expand networks. For instance, in neural architecture search, some works have treated architecture changes as actions in a meta-controller. Cai et al. [cai2018] and Ashok et al. [ashok2018] both used reinforcement learning (policy gradient controllers) to modify neural network architectures: their controllers could choose to apply operations like adding layers, widening layers, or removing components, with the goal of optimizing accuracy under resource constraints. In those works (which were in supervised domains), Net2Net-style function-preserving transformations were often used as the permissible operations for growing or shrinking the network. This demonstrates that function-preserving expansion is not only useful for continual learning, but also as a tool for efficient architecture search – the network can be morphed and fine-tuned rather than entirely re-trained whenever a change is made.

The concept of curriculum learning – training on tasks of increasing difficulty – can be naturally combined with model expansion. In a curriculum, early stages might involve simpler environments or tasks which a small network can learn adequately. As the task complexity grows (e.g., more difficult levels, additional obstacles, higher-dimensional observations, or more agents to control), the agent’s network can be expanded to provide additional capacity to learn the more complex behavior, while keeping the earlier policy as a starting point (embedded in the larger network). This progressive scaling of model size alongside task difficulty can improve stability and sample efficiency: the agent always has a network appropriate to the current task’s complexity, initialized with a competent policy from the previous stage. Although this specific paradigm (gradually increasing network size during an RL curriculum) has not been extensively formalized in literature, it aligns with practices reported in some large-scale RL systems. For example, OpenAI’s Dota 2 bot underwent several changes in model architecture during its development; rather than retrain from scratch after each change, the engineers applied ad-hoc function-preserving transformations (akin to Net2Net) to initialize the larger network with the smaller network’s learned parameters and policy, allowing training to resume without performance regression [agarwal2022]. Similarly, DeepMind’s AlphaStar in StarCraft II employed population-based training (PBT), wherein many agents with varying hyper-parameters (including network sizes) were trained in parallel and could inherit weights from one another; over time, this process effectively increased the effective capacity of the best agents while reusing knowledge from earlier agents [agarwal2022]. These cases underscore that dynamically increasing model capacity is not only a theoretical idea but has been applied in practice to handle escalating complexity in extended training runs. We anticipate future research will formalize such adaptive capacity curricula, defining when to trigger expansions and how to integrate them seamlessly into RL training loops.

\section{Benchmarks and Empirical Evaluation}
A variety of benchmarks can be used to assess projection-based network expansion in RL. Prior works have demonstrated these methods on supervised benchmarks (e.g. CIFAR-10 for Net2Net [chen2015], ImageNet for network morphism [wei2016], Permuted MNIST for DEN [yoon2018]) and on RL tasks (Atari games for progressive nets [rusu2016], etc.). For the purposes of this review, we outline a prospective evaluation setup for RL (to be refined once specific environments are chosen):
	•	Discrete control (classic or Atari games): For example, an agent could first be trained with a small network on a simpler task or an easier version of an Atari game. The network is then expanded and training continues on a more difficult game or the full-scale version of the task. We would measure learning curves (to see if the expanded network learns faster or achieves higher reward than a fixed network) and check that performance on the initial task is preserved after expansion (if the task is revisited or as a measure of backward compatibility).
	•	Continuous control (robotics or MuJoCo): As a case, an agent might learn a basic locomotion task (like walking) with a modest-sized policy network. Then the network could be expanded (e.g., adding neurons in hidden layers) and the agent trained on a harder control task, such as running or navigating uneven terrain, which demands a richer policy. Evaluation would focus on sample efficiency in the new task and the agent’s ability to retain competency on the simpler task (if tested).
	•	Multi-task or curriculum scenarios: We can consider a sequence of tasks of increasing complexity (for instance, navigation in progressively larger mazes, or a robotic manipulation task with increasing number of objects to handle). The agent’s network is grown at predetermined milestones or when learning saturates on the current task. We will evaluate how the incremental expansion impacts overall performance: ideally, the agent should solve later tasks more efficiently than if it had started from scratch, and maintain performance on earlier tasks. This would be compared against baselines like training a fixed large network on all tasks, or sequential fine-tuning without expansion.

Benchmark details are to be determined. In designing the evaluation, key metrics will include sample efficiency (does expanding the network yield faster convergence on new tasks, compared to training a large network from scratch or other baselines?) and training continuity (does the agent avoid performance drops when the network is expanded, indicating successful function preservation?). We will also track the final performance on each task and the overall parameter growth. This will illuminate the practical benefits and costs of projection-based expansion in RL settings.

\section{Conclusion}
Projection-based network expansion is a powerful concept that addresses a fundamental challenge in reinforcement learning: how to continue learning and adapting without forgetting or having to restart from scratch when task demands increase or change. We have reviewed a spectrum of approaches, from the original Net2Net transformations [chen2015] that introduce new neurons or layers without changing the network’s function, to advanced network morphism techniques [wei2016] that provide a formal framework for such expansions, and to progressive and dynamic expansion strategies in continual learning [rusu2016][yoon2018] that demonstrate how agents can accumulate skills over time by growing their networks. A common thread in these methods is the emphasis on preserving functionality during expansion – this theoretical guarantee means that adding capacity does not induce an immediate drop in performance. Empirically, these approaches have shown substantial benefits: faster convergence to high performance in larger models (thanks to reusing prior knowledge), the ability to solve sequences of tasks without catastrophic forgetting, and improved final performance on complex problems owing to increased model expressiveness.

In the context of reinforcement learning specifically, research on deliberate architectural expansion is still emerging, but the potential benefits are significant. By maintaining training continuity – never losing the agent’s proficiency when the network is expanded – an RL agent can be deployed in a persistent learning scenario and progressively improve itself. This stands in contrast to the conventional approach of training a fixed-capacity model until it plateaus and then having to start over with a larger model. Projection-based expansion allows an agent’s knowledge to be cumulative: the network’s capacity grows in tandem with the task complexity, rather than being a one-shot design decision made beforehand.

There remain open questions and practical considerations. One challenge is deciding when and how much to expand a network during training. Expanding too late might bottleneck the agent’s learning (if the network saturates and cannot represent a policy for a harder task), whereas expanding too early or too often could introduce unnecessary complexity and slow down training. Approaches like DEN offer heuristic criteria by gauging performance plateaus and weight utilization to trigger expansions [yoon2018], but applying such triggers in a fully autonomous RL training regime is an area for further research. Another consideration is the computational cost: although function-preserving initialization makes subsequent training efficient, each expansion increases model size and inference cost. In long-running or deployment scenarios, one might need to balance expansion with compression; for example, the “Progress & Compress” scheme has been suggested to alternate between expanding for new tasks and compressing knowledge into a smaller model via distillation [schwarz2018]. Developing such combined strategies could ensure an agent grows when needed but also remains efficient.

In summary, projection-based network expansion provides a compelling framework for continual and curriculum learning in RL. It bridges insights from transfer learning, continual learning, and network design to enable agents that can grow their neural representations as they learn, without forgetting the past. The theoretical foundations ensure that expansions are safe (function-preserving), and empirical evidence across supervised and reinforcement learning domains shows improved learning dynamics and outcomes. As these techniques are further refined and adopted, we expect them to play a crucial role in building lifelong learning RL agents that can handle increasingly complex tasks by progressively expanding their knowledge and capacity.

References: (Placeholder for full reference list corresponding to citations [chen2015], [wei2016], [rusu2016], [fernando2017], [yoon2018], [ashok2018], [cai2018], [gesmundo2023], [agarwal2022], [stanley2002], [schwarz2018], etc.)








% Moved from old C2:

\subsection{Graph-Based Architectures}

\Glspl{gnn} naturally represent a set of entities (nodes) along with 
their relations (edges) in a way that is invariant to node ordering. 
\Glspl{gnn} are thus well-suited for \gls{marl}: each agent may be a node in a graph, 
with edges representing interactions (such as physical proximity, 
communication links, or joint team membership)~\cite{liu2020b}. 
By design, graph convolutions or message-passing layers treat permutations of 
node indices equivalently as they operate on the graph structure itself. 
Yang et al.~\cite{yang2021a} introduced Inductive Heterogeneous Graph Multi-agent 
Actor-critic (IHG-MA), where a multi-agent environment is modeled as a dynamic graph. 
Each agent shares a policy network that includes graph convolution layers, 
allowing it to adapt to changing neighbor relationships in highly dynamic environments. 
IHG-MA shows substantially improved cooperation in tasks where agents move and form time-varying 
interaction topologies, outperforming non-relational baselines~\cite{yang2021a}. 

\subsubsection{Centralized Critics with GNNs}
A prominent example of a graph-based architecture is the Permutation 
Invariant Critic (PIC) proposed by Liu et al.~\cite{liu2020b}. 
PIC uses a graph network as the critic in a \gls{ctde} framework. 
Instead of a monolithic \gls{mlp} that takes the concatenation of 
all agents' observations/actions, which would produce entirely different 
outputs if agents were relabeled, PIC's critic treats each agent as a node 
and applies graph convolutional layers to propagate information among agents. 
The final critic value is read out by pooling over node embeddings, 
yielding a single joint value that is invariant to agent permutations. 

Liu et al.~\cite{liu2020b} demonstrated that PIC scales effectively 
to environments with up to 200 agents, learning optimal policies where 
a standard \gls{mlp} critic failed. The graph-based critic flexibly adapts to 
different team sizes without redesign, as the \gls{gnn} simply expands to more nodes. 
In contrast, an \gls{mlp} critic struggles with larger input dimensions 
and inconsistent agent orderings. PIC also handles heterogeneity by 
assigning each node an attribute vector encoding its type or capabilities, 
allowing the shared \gls{gnn} to condition on agent-specific features while p
reserving symmetry among identical agents.

Noppakun and Akkarajitsakul~\cite{noppakun2022} explored a similar
approach by implementing a \gls{coma}~\cite{foerster2018}
replacing the standard \gls{mlp} critic with a \gls{gnn} one.
In this work they demonstrated the feasibility of adapting
existing actor-critic architectures to use \glspl{gnn}
for problems that might benefit from invariance.

Graph Attention Mean Field (GAT-MF)~\cite{hao2023} approach combines mean-field 
theory with a graph attention network to handle very large swarms of agents. 
By converting dense agent-agent interactions into an agent-“virtual neighbor” 
interaction via attention weights, GAT-MF remains invariant to agent permutations 
while focusing each agent's critic on the most influential neighbors, 
and has been applied to extremely large-scale scenarios (hundreds of agents)~\cite{hao2022}.

% #TODO Potentially redundant
\subsection{Transformers and Attention for Sets}

The remarkable success of attention mechanisms in sequence modeling 
(notably Transformers~\cite{vaswani2017} applied to large language models) 
has carried over to set-based inputs by removing positional encodings. 
An attention layer, by default, is permutation-invariant to its inputs 
(when no order information is added); treating each query-key-value triplet 
agnostically to ordering and learning to weigh interactions based purely on content. 

Lee et al.~\cite{lee2019} 
formalized this in the Set Transformer, an architecture that uses 
self-attention to model interactions among elements of an input set. 
The Set Transformer can capture higher-order interactions
yielding far greater representational power than naive pooling. 

\subsubsection{Attention in Multi-Agent Critics}
Iqbal and Sha~\cite{iqbal2019} introduced a [Multi] Actor-Attention-Critic (MAAC), 
where the central critic uses an attention mechanism to dynamically 
favor agent interactions most impactful for a given agent's \Gls{q}-value. 
In MAAC, each agent's contribution to another's value is weighted by attention scores, 
which in effect, allows the critic to disregard some agents and emphasize others. 
This both handles varying numbers of agents and improves credit assignment by 
favoring joint partner agents that significantly affect outcomes. 

Hazra et al.~\cite{hazra2024} further demonstrated the effectiveness 
of attention mechanisms by applying a self-attention-based architecture 
to the StarCraft Multi-Agent Challenge (SMAC)~\cite{samvelyan2019}, 
achieving strong performance and scalability,
in a challenge more complex than those approached by the previous 
literature discussed in this section.


% #TODO: Evaluate text brought from C2

% We begin by grounding our discussion of input-invariant architectures 
% in a key probabilistic result: de Finetti's theorem (\cref{eq:deFinnetti}). 
% This theorem formalizes the idea that, under suitable conditions, 
% the elements of an exchangeable sequence can be treated as if 
% they were drawn independently from a shared latent distribution.

% \begin{theorem}[de Finetti's Theorem]
%     Let \(X = \{x_1, \ldots, x_M\}\) be a sequence of exchangeable random variables.
%     Then there exists a latent variable \(\theta\) such that:
%     \begin{equation}
%         p(X \mid \alpha, M_0) = \int p(\theta \mid \alpha, M_0) 
%         \prod_{m=1}^{M} p(x_m \mid \theta)\, d\theta
%         \label{eq:deFinnetti}
%     \end{equation}
%     That is, the joint distribution over \(X\) can be represented as a mixture of 
%     i.i.d. variables conditional on \(\theta\).
% \end{theorem}



% In their paper introducing Deep Sets, Zaheer et al.~\cite{zaheer2017} 
% present a parameter-sharing scheme inspired by \cref{eq:deFinnetti}, 
% which allows networks to handle unordered inputs by design.

% They propose two architectures to achieve this.
% For the invariant approach, they demonstrate that any 
% permutation-invariant function over a set can 
% be decomposed into a transformation (\(\rho\)) of a sum over 
% transformed (\(\phi\)) elements,
% \begin{equation*}
%     \rho\left(\sum_i \phi(x_i)\right)
% \end{equation*}
% providing a universal approximator for such functions. 

% Zaheer et al.~\cite{zaheer2017} also proposed an equivariant approach,
% providing a neural network layer of the form:
% \begin{equation*}
%     \mathbf{f}(\mathbf{x}) \doteq \mathbf\sigma (\lambda\mathbf{Ix} 
%     + \gamma \text{maxpool}(\mathbf{x})\mathbf{1}) 
% \end{equation*}
% where the weight matrix is constrained to the form 
% \(\lambda\mathbf{I} + \gamma\mathbf{11}^\top\), ensuring all 
% diagonal elements are equal and all off-diagonal elements are equal. 
% This construction guarantees permutation equivariance.
% The \(\sigma\) operation represents a non-linearity function (such as a sigmoid).
% The authors argue that this equivariant form is functionally equivalent to the 
% invariant form during inference; their distinction primarily emerges during backpropagation, 
% where the structure of gradient updates differs between the two.

% These structures dramatically reduce the effective symmetry 
% group of the input space, enabling better generalization and 
% sample efficiency in problems with intrinsic symmetries. 
% In \gls{marl}, these ideas naturally extend to agent sets: 
% policies or critics can process agents' states as elements 
% of an unordered set, enforcing invariance to permutations in agent ordering. 
% This supports scalable learning across varying team compositions 
% and aligns with the theoretical underpinnings of exchangeability laid out earlier.


