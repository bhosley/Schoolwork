\documentclass[]{jair}
% \documentclass[manuscript, screen, review]{jair}

%% For JAIR Checklist:
\usepackage{soul}                   % Strikeout with \st{}
\renewcommand{\bf}[1]{\textbf{#1}}  % Bold with \bf{}

\setcopyright{cc}
\copyrightyear{2025}
\acmDOI{10.1613/jair.1.xxxxx}

%%
\JAIRAE{JAIR AE Name}
\JAIRTrack{} % Insert JAIR Track Name only if part of a special track
\acmVolume{1}
\acmArticle{1}
\acmMonth{1}
\acmYear{2030}

\RequirePackage[
  datamodel=acmdatamodel,
  style=acmauthoryear,
  backend=biber,
  giveninits=true,
  uniquename=init,
  ]{biblatex}

\renewcommand*{\bibopenbracket}{(}
\renewcommand*{\bibclosebracket}{)}

% #FINAL: Bibliography link
\addbibresource{../2025Bibs/Prospectus.bib}

\author{Brandon Hosley}
\orcid{0000-0002-2152-8192}
\email{brandon.hosley.1@af.au.edu}
\affiliation{%
  \institution{Air Force Instute of Technology}
  \city{Wright-Patterson AFB}
  \state{Ohio}
  \country{USA}
}
\author{Bruce Cox}
\orcid{0000-0003-0149-1836}
\email{bruce.cox@af.au.edu}
\affiliation{%
  \institution{Air Force Instute of Technology}
  \city{Wright-Patterson AFB}
  \state{Ohio}
  \country{USA}
}
% #TODO: Add other authors

\renewcommand{\shortauthors}{Hosley \& Cox}

\usepackage[nomain,symbols,abbreviations]{glossaries-extra}
\makeatletter
\glsdisablehyper
\makeatother
\newabbreviation{rl}{RL}{reinforcement learning}
\newabbreviation{harl}{HARL}{heterogeneous-agent reinforcement learning}
\newabbreviation{marl}{MARL}{multi-agent reinforcement learning}
\newabbreviation{ppo}{PPO}{proximal policy optimization}
\newabbreviation{mappo}{MAPPO}{multi-agent \gls{ppo}}
\newabbreviation{happo}{HAPPO}{heterogeneous-agent \gls{ppo}}
\newabbreviation{ctde}{CTDE}{centralized training with decentralized execution}
\newabbreviation{posg}{POSG}{partially observable stochastic game}
% \newabbreviation{lbf}{LBF}{Level-based Foraging}
% \newabbreviation{mpe}{MPE}{Multi Particle Environments}
% \newabbreviation{gnn}{GNN}{graph neural network}
% \newabbreviation{mlp}{MLP}{mulit-layer perceptron}
% \newabbreviation{maddpg}{MADDPG}{multi-agent deep-deterministic policy gradient}
% \newabbreviation{sisl}{SISL}{Stanford Intelligent Systems Laboratory}
% \newabbreviation{coma}{COMA}{counterfactual multi-agent policy gradients}
% \newglossaryentry{q}{text=q,name=Q,sort=q,type=symbols,category=symbol,
%     description={Set of state-values or state-value functions.}}


% #TODO: Change math to glossary symbols
\newglossaryentry{i}{text=i,name=I,sort=i,type=symbols,category=symbol,
    description={Set of agents \(i\).}}


% \title[]{Homogenization for Input-Invariant Solutions for Heterogeneous Agent Reinforcement Learning}
\title{Homogenization for Input-Invariant Solutions for Heterogeneous Agent Reinforcement Learning}
% TODO: For implicit agent embedding

\begin{document}

% Structured Abstract
\begin{abstract}
    {\bf Background:} MARL with heterogeneous agents; limits of standard parameter sharing and ad-hoc agent indication.\\
    {\bf Objectives:} Introduce and analyze a homogenized observation-action construction that enables input-invariant policies across heterogeneous agents.\\
    {\bf Methods:} Formal definition of a covering (homogenized) observation space with validity masks and implicit agent indication; theoretical results on representability and invariance; empirical evaluation on n-D gridworld variants with MAPPO control.\\
    {\bf Results:} Comparable or improved robustness under sensor degradation and team-size variation; small memory/runtime advantages in some regimes; limitations when coherence assumptions fail.\\
    {\bf Conclusions:} Homogenization is a practical alternative to parameter sharing when a coherent spanning space exists; we outline conditions, trade-offs, and open questions.
\end{abstract}

% #FINAL: Submission date
\received{DD Month YYYY}
% \received[accepted]{DD Month YYYY}
\maketitle

\section{Introduction}

\Gls{marl} provides a framework for coordinating multiple decision makers in partially
observable environments. While parameter sharing and \gls{ctde} have enabled impressive progress,
these methods typically assume that agents are interchangeable.
% #TODO: Citation needed
In practice, heterogeneity
may be unavoidable or even desirable.
% #TODO: Citation needed
Real-world systems often consist of agents with different sensors, action capabilities, or roles, 
reflecting hardware constraints or mission assignments. 
% #TODO: Citation needed
These differences complicate generalization, transfer, and scalability.

A solution is to retain shared network parameters but append explicit agent identifiers
or type embeddings to the input \cite{terry2020, christianos2021}. 
This strategy allows a network to distinguish among agents, 
but it does not resolve structural heterogeneity. 
When agents differ in the dimensionality or semantics of their observations or actions, 
explicit identifiers must be coupled with padding, duplication, or handcrafted encodings. 
Such fixes increase representational overhead and can obscure the information that truly 
distinguishes agents.

We propose an alternative approach based on implicit indication. 
Rather than encoding identity explicitly, we construct a comprehensive observation 
and action space that spans all agent subspaces. 
Each agent's specific capabilities are represented implicitly through a 
binary validity mask that marks which channels are active. 
Policies are then trained over this comprehensive domain,
with masks conditioning both perception and action. 
Differences among agents thus emerge naturally from the subset of valid subspaces, 
eliminating the need for separate identifiers.

This construction yields an input-invariant policy class: 
a single policy that is agnostic to team composition, 
robust to missing or degraded channels, and transferable across heterogeneous configurations. 
The central assumption is that the implicit-identity space remains coherent; 
that spanning the agents' observation and action subspaces preserves task-relevant 
semantics without aliasing. 
While this assumption may not hold for every use-case,
it provides a flexible foundation that practitioners can evaluate for their domains.

The contributions of this paper are threefold:
\begin{enumerate}
    \item Methodological: We formalize an implicit-identity scheme as a 
    spanning construction for observation and action spaces, 
    paired with mask-based conditioning that enables invariant 
    parameter sharing across heterogeneous agents.
    \item Theoretical: We prove conditions under which the implicit-indication 
    approach preserves representability and discuss the coherence assumption, 
    providing examples of when it holds and when it fails.
    \item Empirical: We compare implicit encoding against a \gls{happo} baseline, 
    in a custom environment with configurable heterogeneity. 
    Robustness is evaluated under sensor dropout, team-size variation, and team composition changes.
\end{enumerate}

Together, these contributions position implicit encoding as a general and lightweight 
approach to parameter sharing for heterogeneous-agent reinforcement learning.




\section{Related Work}
\label{con2:sec:related_work}
2. Background and Related Work

2.1 MARL preliminaries (POSGs, CTDE; brief)
2.2 Parameter sharing \& agent indication (Foerster; Gupta; Terry et al.—positioned as closely related but distinct in aim)
	•	Limitations with heterogeneous observation/action spaces; padding/overloading strategies.
2.3 Permutation-invariant/equivariant architectures
	•	Deep Sets, mean-field RL, GNN/attention critics; invariance vs. scalability trade-offs.
2.4 Heterogeneity handling (IPPO, MAPPO/HAPPO, selective sharing, hypernetworks)
	•	Where your approach fits: alternative representation-level unification rather than per-agent specialization.
(Use your bib keys; keep Terry et al. in this section.)

---

- Input invariance
    - Motivation
        - Provides exchangeability
        - Which allows sharing of information at some level
        - Such as parameter sharing
- Parameter sharing
    - Improves efficiency of training
    - Limitations wrt heterogeneity
- Heterogeneous training
    - Cited approaches
        - IPPO
        - MAPPO
        - Agent indication
- Lead in to next section



% #TODO: Citation needed
% A core insight from recent literature is that when agents are symmetric or 
% play exchangeable roles, 
% enforcing invariance to permutations in agent inputs 
% and identities can significantly improve learning efficiency and generalization. 
% Input-invariant architectures, such as those based on pooling, graph networks, 
% or attention mechanisms, can leverage these symmetries directly in the 
% policy or critic network. 

% #TODO: IPPO
% #TODO: MAPPO

% \cite{witt2020}
% % propose IPPO, as an extension to PPO

% \cite{yu2022}
% % Compares IPPO and MAPPO
% % resolves the previous weakness with central/shared value function


% \cite{terry2020}
% % Directly address hetero
% % Provide 4 proposed solutions which all draw from transformed obs
% % either a geometric mask, binary indicator, inversion, or inversion with replacement


% #TODO: Citation needed




% This section surveys prior work relevant to input-invariant architectures for \gls{harl}. 
% The topics below are parallel threads that converge to inform our architectural design.
% We begin with theoretical underpinnings of exchangeability and symmetry, 
% followed by architectural strategies for permutation invariance and equivariance
% via pooling, mean-field methods, graph networks, and attention. 
% These are then examined through applications in \gls{marl} 
% before concluding with \gls{harl} considerations.

% We begin by grounding our discussion of input-invariant architectures 
% in a key probabilistic result: de Finetti's theorem (\cref{eq:deFinnetti}). 
% This theorem formalizes the idea that, under suitable conditions, 
% the elements of an exchangeable sequence can be treated as if 
% they were drawn independently from a shared latent distribution.

% \begin{theorem}[de Finetti's Theorem]
%     Let \(X = \{x_1, \ldots, x_M\}\) be a sequence of exchangeable random variables.
%     Then there exists a latent variable \(\theta\) such that:
%     \begin{equation}
%         p(X \mid \alpha, M_0) = \int p(\theta \mid \alpha, M_0) 
%         \prod_{m=1}^{M} p(x_m \mid \theta)\, d\theta
%         \label{eq:deFinnetti}
%     \end{equation}
%     That is, the joint distribution over \(X\) can be represented as a mixture of 
%     i.i.d. variables conditional on \(\theta\).
% \end{theorem}

% This result justifies the treatment of inputs that are unordered but statistically similar,
% such as agent observations in a cooperative \gls{marl} task, 
% as conditionally i.i.d. elements drawn from a shared distribution. 
% In practice, this means that the ordering of inputs does not convey meaningful information, 
% and architectures may (and perhaps should) be designed to respect this symmetry.

% As an example, in environments like \gls{lbf}~\cite{papoudakis2021}, 
% each agent's observation is constructed by concatenating their own state, 
% the state of objectives, and the states of other agents in a fixed but arbitrary order. 
% While the ordering is deterministic, it typically does not encode meaningful 
% semantic distinctions. In such settings, treating the input as an exchangeable set,
% rather than a structured sequence, aligns with the assumptions of de Finetti's theorem 
% and motivates the permutation-invariant design choices in the rest of this section.

% Permutation-invariant functions and networks (e.g., pooling operations or 
% attention mechanisms without positional encoding) align with this insight, %%
% embedding the inductive bias that order should not affect the outcome. 
% This forms the basis for the architectural designs reviewed in the following sections.


% \subsection{Permutation Invariance and Equivariance}

% In their paper introducing Deep Sets, Zaheer et al.~\cite{zaheer2017} 
% present a parameter-sharing scheme inspired by \cref{eq:deFinnetti}, 
% which allows networks to handle unordered inputs by design.

% They propose two architectures to achieve this.
% For the invariant approach, they demonstrate that any 
% permutation-invariant function over a set can 
% be decomposed into a transformation (\(\rho\)) of a sum over 
% transformed (\(\phi\)) elements,
% \begin{equation*}
%     \rho\left(\sum_i \phi(x_i)\right)
% \end{equation*}
% providing a universal approximator for such functions. 

% Zaheer et al.~\cite{zaheer2017} also proposed an equivariant approach,
% providing a neural network layer of the form:
% \begin{equation*}
%     \mathbf{f}(\mathbf{x}) \doteq \mathbf\sigma (\lambda\mathbf{Ix} 
%     + \gamma \text{maxpool}(\mathbf{x})\mathbf{1}) 
% \end{equation*}
% where the weight matrix is constrained to the form 
% \(\lambda\mathbf{I} + \gamma\mathbf{11}^\top\), ensuring all 
% diagonal elements are equal and all off-diagonal elements are equal. 
% This construction guarantees permutation equivariance.
% The \(\sigma\) operation represents a non-linearity function (such as a sigmoid).
% The authors argue that this equivariant form is functionally equivalent to the 
% invariant form during inference; their distinction primarily emerges during backpropagation, 
% where the structure of gradient updates differs between the two.

% These structures dramatically reduce the effective symmetry 
% group of the input space, enabling better generalization and 
% sample efficiency in problems with intrinsic symmetries. 
% In \gls{marl}, these ideas naturally extend to agent sets: 
% policies or critics can process agents' states as elements 
% of an unordered set, enforcing invariance to permutations in agent ordering. 
% This supports scalable learning across varying team compositions 
% and aligns with the theoretical underpinnings of exchangeability laid out earlier.


% \subsection{Mean-Field and Exchangeable Approximations}

% In scenarios where agents are approximately indistinguishable, 
% a natural extension of Zaheer et al.'s~\cite{zaheer2017} 
% permutation-invariant formulation is to normalize the summation of elements. 
% Rather than summing over agent embeddings, one may average them,
% yielding permutation invariance \emph{and} insensitivity to the number of agents. 
% This refinement aligns closely with mean-field theory, which replaces complex 
% many-body interactions with an average effect.

% Yang et al.~\cite{yang2018} apply this idea in the context of \glsadd{marl}, 
% introducing mean-field Q-learning and actor-critic algorithms. 
% Their approach approximates the influence of neighboring agents with 
% an average embedding, enabling each agent to learn a local policy that 
% scales gracefully with team size. This is particularly 
% useful in settings where each agent interacts with many others, 
% but where the precise identity of those agents is irrelevant.

% Building on this, Li et al.~\cite{li2021b} approach mean-field \gls{marl} 
% problems formally as permutation-invariant Markov decision processes. 
% They propose Mean-Field Proximal Policy Optimization (MF-PPO), 
% a variant of PPO that incorporates mean-field assumptions into 
% both actor and critic networks. The resulting architecture maintains a 
% shared policy across agents, with inputs aggregated through mean pooling.

% Theoretically, MF-PPO achieves convergence to a global optimum at 
% a sublinear rate, and notably, its sample complexity is proven to be 
% independent of the number of agents. This highlights the power of 
% encoding exchangeability as an inductive bias: by exploiting the 
% ergodic structure of agent interactions, MF-PPO updates a shared policy 
% more efficiently than na\"ive multi-agent baselines.

% Empirically, MF-PPO outperforms non-invariant baselines in the 
% \gls{mpe}~\cite{li2021b}, demonstrating faster convergence, 
% higher average returns, and significantly reduced parameter counts. 
% This is largely attributed to weight sharing across agents and the 
% use of input-aggregation strategies that preserve permutation symmetry. 

% One significant downside to the examined mean-field approaches is that 
% information about subset interactions is lost in the process of 
% applying their respective commutative functions.
% % #TODO: Appendix proof
% % We argue that this is not a necessary limitation of mean-field methods,
% This limitation motivated more sophisticated invariant architectures like 
% graph-based networks and attention-based pooling, which we discuss next.

% % #TODO - Redundant from C3


% % Change this section as reflecting back to single agent applications
% \subsubsection{Relational Reasoning in Single-Agent RL}
% While much of the prior work focuses on multi-agent settings, 
% the benefits of invariant relations extend to single-agent tasks as well. 
% These architectures demonstrate that modeling inputs as a set of 
% interacting entities can improve both performance and generalization.

% Zambaldi et al.~\cite{zambaldi2018} showed that a single-agent \gls{rl} agent could 
% benefit from interpreting its observations as a graph of entities and relations. 
% Using a relation network with iterative message passing, their agent achieved 
% superhuman performance on several StarCraft II~\cite{vinyals2019} mini-games, 
% outperforming non-relational baselines by reasoning over unit interactions 
% rather than raw input features.

% In their single-agent formulation, Tang and Ha~\cite{tang2021} demonstrated 
% that treating sensory channels as interchangeable input elements enabled 
% the agent to maintain performance despite significant input corruption. 
% By passing each sensory input through a shared encoder and aggregating 
% with self-attention, the network learned to interpret observations based 
% on contextual relationships rather than fixed positions. 
% This design conferred resilience to occluded or missing inputs, 
% allowing the agent to function even when sections of the sensory 
% data were absent at test time. Their results suggest that invariance 
% to input structure can provide strong robustness to
% severe perceptual perturbations.

% % # TODO: May consider ending on Tang and Ha for similarity purposes.
% \begin{comment}
%     They propose a very similar question:
%         "Sensory substitution refers to the brain’s ability to use one sensory modality 
%         (e.g., touch) to supply environmental information normally gathered by 
%         another sense (e.g., vision)."
%     - Extremely similar to my interest in sensory degradation.
%     - However, in their experiments they instead demonstrate an invariance in a singular
%     observational domain. (They take a 2d image, chunk it and shuffle.)
% \end{comment}

% \subsection{Heterogeneous Agents}
% In many \gls{marl} scenarios, agents are not identical, a setting formally studied 
% as \gls{harl}. Agents may differ in abilities, observation spaces, or roles. 
% Many input-invariant architectures assume that all inputs are drawn from the 
% same distribution and processed by a shared encoder, an assumption that breaks 
% down when agents have structurally different observations. In such cases, 
% invariant frameworks must be adapted to handle this heterogeneity, 
% either by mapping observations into a shared embedding space or by 
% augmenting the input with agent-specific identifiers.

% One practical strategy for supporting heterogeneity in input-invariant architectures 
% is to include an agent-specific feature encoding that captures type or 
% role~\cite{liu2020b,hao2022,hao2023}. As noted earlier, PIC~\cite{liu2020b}
% incorporates this by embedding attribute vectors into each node's representation, 
% preserving symmetry among similar agents while allowing differentiation where needed.

% Similarly, HPN~\cite{hao2023} uses a hypernetwork conditioned on 
% identity to generate specialized agent modules, enabling tailored policies 
% while maintaining permutation invariance at the architectural level. 
% This structure supports diversity among agents while preserving the 
% benefits of shared computation and symmetry where applicable.

% Such flexibility is especially valuable in mixed-agent teams and 
% competitive settings, where an agent may need to reason differently 
% about teammates and opponents. The architecture should remain invariant 
% when swapping two equivalent opponents but avoid conflating agents 
% with distinct roles.

% Despite the demonstrated advantages of invariant and equivariant architectures in handling 
% agent symmetry, scalability, and heterogeneity, their practical cost remains underexplored. 
% The reviewed methods, ranging from mean-field approximations and deep sets, 
% to graph-based and attention-driven critics share a common goal: to encode 
% structural priors that simplify learning in multi-agent settings. However, 
% these methods often increase architectural complexity, requiring additional computation, 
% memory, and tuning. Importantly, few studies offer direct comparisons of their 
% training or inference costs relative to simpler baselines such as \glspl{mlp}. 
% This gap complicates our understanding of the tradeoffs involved in deploying 
% more expressive models in real-world systems.

% In this work, we contribute to closing that gap by empirically evaluating the efficiency 
% and robustness of input-invariant policy architectures in heterogeneous-agent settings, 
% quantifying both their benefits and computational costs. In parallel, we 
% propose a novel exchangeable approximation framework that extends the principles 
% of input invariance to settings with observation and action heterogeneity. 
% By constructing a shared observation-action spanning space and integrating dynamic 
% masking within an equivariant network, we offer a lightweight and scalable 
% approach to handling agent diversity. Together, these contributions aim to 
% clarify not only when these architectures are helpful, 
% but how they might be practically and broadly applied.








\section{Problem Formulation}
\label{con2:sec:problem_formulation}
% #TODO: Formulation is a bit redundant with the section afterward

% #TODO: Add explicit def of components
We formalize the setting as a \gls{posg}. 
Let \(\Gls{i}\) denote the set of agents. 
Each agent \(\gls{i} \in \Gls{i}\) is defined by %% #TODO: Not defined
an observation space \(O_i\) 
and an action space \(A_i\), which may differ across agents.

\subsection{Decomposition of Observations}

Each observation space \({O}_i\) is assumed to decompose into 
subspaces associated with sensor channels: %% #TODO: Sensor is an intuitive nickname
\[{O}_i = \bigcup_{c \in {C}_i} {O}_c,\]
where \({C}_i \subseteq {C}\) 
indexes the subset of channels available to agent \({i}\), 
and each \({O}_c\) is linearly independent. 
For instance, one agent might have access to RGB vision and GPS, 
while another relies solely on lidar. 
The global set of all possible channels is \({C}\). %% #TODO: All possible or all acknowledged

The homogenized observation space is then defined as:
\[O^\prime := \bigcup_{c \in {C}_i, i \in {I}} O_c,\]
which spans the union of all agent subspaces. 
Each agent's actual observation is embedded into \(O\) with a binary 
validity mask \(m_i \in \{0,1\}^{|{C}|}\), 
where \(m_{ic} = 1\) if channel \(c\) is active for agent \(i\). The pair \((o, m_i)\) 
thus conveys both the observation and the structure of agent \(i\)'s capabilities, 
without the need for explicit identifiers.

\subsection{Homogenized Actions}

Analogously, the homogenized action space is defined as the union of all agent action spaces:
\[A^\prime := \bigcup_{i \in I} A_i.\]
This superset allows a shared representation of action outputs. 
Each agent's feasible set is again specified by a mask, 
ensuring that invalid actions are never selected.

\subsection{Objective}

% #TODO: Add appendix reference to full environment description
% \cref{con2:app:env_model}
% #TODO: This appears to be experiment objective, not env objective
The objective is to train a shared policy
\[\pi_\theta : O \times \{0,1\}^{|\mathcal{C}|} \to \Delta(A^\mathrm{span}),\]
that maximizes expected return under the \gls{posg} dynamics. 
By construction, \(\pi_\theta\) is input-invariant: 
it operates over the same universal domain for all agents, 
with heterogeneity resolved via masks. Robustness is assessed along three axes:
%     1. Channel robustness - resilience to missing or degraded observation channels.
%     2. Team robustness - stability under changes in the number of agents.
%     3. Composition robustness - adaptability to shifts in the distribution of agent types.

This problem formulation provides the foundation for the homogenization 
method and subsequent theoretical analysis.








\section{Homogenization and Implicit Indication Method}
\label{con2:sec:method_homogenization}
% #TODO: Perhaps change "channel" to "Observation Element"

The homogenization approach unifies heterogeneous agent interfaces 
by constructing a shared spanning space for observations and actions. %% #TODO: Ambiguous on shared
This construction enables parameter sharing without requiring explicit agent identifiers, 
while still preserving the structural distinctions that arise from heterogeneity. 
In what follows, we define the homogenized spaces, describe the role of validity masks, 
and outline the resulting invariant policy class.

\subsection{Homogenized Observation Space}

As defined in Section~\ref{con2:sec:problem_formulation}, 
each agent \(i \in I\) observes a subset of channels \({C}_i \subseteq {C}\).
The homogenized observation space is the Cartesian product
\[
    O := \bigcup_{c \in {C}} O_c,
\]
where each \(O_c\) is the subspace associated with channel \(c\).
For each agent, a binary validity mask \(m_i \in \{0,1\}^{|{C}|}\)
specifies which channels are active. 
The embedded observation for agent \(i\) is therefore represented as
\[\tilde{o}_i = (o, m_i), \quad o \in O.\]
This encoding ensures that every agent's observation is drawn from the same universal space, 
with the mask preserving the structural differences among agents. 
Crucially, the mask conveys all information needed to distinguish agents' capabilities, 
eliminating the need for an explicit identity vector.

\subsection{Homogenized Action Space}

The homogenized action interface is constructed in parallel.
Let each agent have an action space \(A_i\). The homogenized action space is the spanning set
\[
    A^\prime := \bigcup_{i \in I}A_i^{unit},
\]
where each \(A_i\) corresponds to a 
primitive action 
dimension included in at least one agent's space. 
Agents interact with only a masked subset of this span.
For a discrete action dimension, infeasible logits are suppressed by the mask;
for continuous dimensions, invalid components are clamped or ignored.
This ensures that an agent never selects an action outside its feasible repertoire,
while still enabling parameter sharing across the full space.

\subsection{Input-Invariant Policy Class}

The homogenized policy is defined as
\[
    \pi_\theta: O \times \{0,1\}^{|\mathcal{C}|} \to \Delta(A^\mathrm{span}),
\]
mapping embedded observations into distributions over the spanning action set.
In practice, the network comprises three parts:
% #TODO: Revise the network description
% 	1.	Channel encoders: Each O_c is passed through a channel-specific encoder shared across all agents.
% 	2.	Aggregation: Encoded channels are pooled by a commutative operation (e.g., sum, mean, attention), ensuring invariance to the ordering of active channels.
% 	3.	Action head with masking: The pooled embedding is mapped to action logits over A^\mathrm{span}, with validity masks applied to exclude infeasible actions.

This design enforces permutation invariance over channels and provides a 
consistent mapping from heterogeneous inputs to a shared action domain. 
The use of masks ensures that structural heterogeneity is preserved without 
requiring per-agent specialization.

\subsection{Training Protocol}

The training follows the \gls{ctde} paradigm. 
All agents share a common policy network parameterized by \(\theta\).
% #TODO: Include parameter table
A centralized critic, such as that used in \gls{mappo}, is employed for variance reduction, 
though the method is agnostic to the choice of critic. %% # What?
Training loss functions incorporate the validity masks to ensure that gradients 
are computed only with respect to feasible observations and actions.
% #TODO: Revise the loss function
\[
    \mathcal{L}(\theta) = \mathbb{E}{(o,m,a)} \big[ - \log \pi\theta(a \mid o, m) \, \hat{A}(o, a) \big],
\]
where infeasible action logits are excluded from the softmax and 
masked observations are zeroed before encoding. 
This ensures that the optimization is unaffected by invalid 
components of the homogenized space.

\subsection{Practical Considerations}

% #TODO: Revise these considerations. They are close, but warrant more precise descriptions
The homogenization framework introduces several implementation details:
\begin{itemize}
    \item Initialization: Care must be taken to prevent masked channels from destabilizing training. 
    Channel encoders should be initialized consistently across agents.
    \item Partial observability: Sensor degradation or dropout can be 
    modeled directly by modifying the mask \(m_i\) at inference time, without retraining.
    \item Mixed action types: Hybrid discrete/continuous spaces can be accommodated by 
    treating each type as a separate unit dimension in \(A^\mathrm{span}\).
    \item Scalability: The dimensionality of \(O\) and \(A^\mathrm{span}\) grows with 
    the number of distinct channels and action primitives. In practice, 
    this growth is modest in structured domains, but it highlights the importance 
    of evaluating the coherence of the spanning construction.
\end{itemize}







\section{Theoretical Results}
\label{con2:sec:theory}

This section establishes conditions under which the homogenization 
approach preserves representability, ensures invariance properties, 
and clarifies the role of the coherence assumption. 
Proof sketches are provided for intuition; full derivations are deferred to the appendix.
% #TODO: Provide full derivation?



\subsection{Representability under Disjoint Union}

A key concern is whether a single policy over the homogenized space can 
represent the optimal per-agent policies of the heterogeneous system. 
We first consider the case where observation spaces are disjoint.

% #TODO: Revisit lemma 1

% Borrowing lemma 1 from \cite{terry2020}

% Lemma 1 (Disjoint Union Policy).
If \(\{{O}_i\}_{i \in I}\) are disjoint, then there exists a policy \(\pi^*\)
over the disjoint union \(\biguplus{i \in I} \Omega_i\) such that for each agent \(i\),
the restriction of \(\pi\) to \(\Omega_i\) is equivalent to the agent's optimal policy 
\(\pi_i\).

% Proof sketch. Construct a union space \Omega^\uplus with an indicator for the originating subspace. Define \pi^(o) = \pi_i^(o) whenever o \in \Omega_i. Since the subspaces are disjoint, \pi^* reproduces the per-agent policies exactly. \square

% This lemma underpins the homogenization strategy: if embeddings are injective, the same representability result applies in the homogenized space.

% #TODO: Implicit indication forgoes the possibility of divergent behavior for like agents
% ⸻

% 5.2 Representability with Masked Homogenization

% We extend the argument to the case where agent observations are embedded into a shared spanning space O with masks.

% Theorem 1 (Masked Representability).
% Let each embedding E_i: \Omega_i \to O map agent i’s observation into the homogenized space, paired with a mask m_i \in \{0,1\}^{|\mathcal{C}|}. If each (o, m_i) uniquely identifies o \in \Omega_i, then there exists a policy \pi^ over O \times \{0,1\}^{|\mathcal{C}|} whose restriction to (o, m_i) is equivalent to the agent’s optimal policy \pi_i^.

% Proof sketch. The embedding induces a bijection between \Omega_i and the masked subset of O. Construct \pi^(o, m_i) = \pi_i^(E_i^{-1}(o)). Since masks uniquely identify feasible subspaces, the homogenized policy recovers each optimal per-agent policy. \square

% This result shows that homogenization does not sacrifice representability, provided embeddings are coherent.

% ⸻

\subsection{Permutation Invariance}

% Because homogenization introduces channel-wise encoders and aggregation, we can establish invariance guarantees.

% Proposition 1 (Permutation Invariance).
% If encoded channel representations are aggregated by a commutative operation (e.g., sum, mean, or attention with symmetric keys), then \pi_\theta is invariant to permutations of active channels within an agent’s mask.

% Proof sketch. Let \{z_c\} be encoded features. A commutative aggregation f(\{z_c\}) satisfies f(\{z_{c_1}, z_{c_2}, \ldots\}) = f(\{z_{c_{\pi(1)}}, z_{c_{\pi(2)}}, \ldots\}) for any permutation \pi. Since the downstream policy depends only on f(\{z_c\}), outputs are permutation-invariant. \square

% This property ensures that policies are robust to channel ordering and supports scalability across varying team compositions.

% ⸻

% 5.4 Robustness to Dropout

% The homogenized representation also supports resilience to missing inputs.

% Corollary 1 (Dropout Robustness).
% Suppose aggregation is mean pooling. If a channel c is removed (i.e., m_{ic} is set to 0), the resulting embedding differs from the full-channel embedding by at most \frac{1}{|\mathcal{C}_i|}\|z_c\|.

% Thus, performance degrades smoothly with the loss of channels, rather than catastrophically.

% ⸻

% 5.5 The Coherence Assumption

% The preceding results rely on an implicit assumption: that embeddings into the homogenized space preserve the semantics of the task.

% Definition 1 (Coherence).
% A homogenized embedding is coherent if, for each agent i, the mapping E_i:\Omega_i \to O together with mask m_i is a sufficient statistic for decision making in the POSG.

% Intuitively, coherence requires that the spanning space separates distinct information without aliasing. Failure of coherence arises when two channels overlap in semantics (e.g., redundant encodings) or when masks do not adequately distinguish agent types. In such cases, the homogenized policy may not faithfully reproduce optimal behavior.

% ⸻

% 5.6 Summary

% These results establish that homogenization preserves representability (Theorem 1), provides invariance guarantees (Proposition 1), and introduces robustness to missing inputs (Corollary 1). The primary limitation is the coherence assumption, which determines whether homogenization is valid in a given domain. This assumption must therefore be evaluated by practitioners when applying the method.






















\section{Methodology}
\label{con2:sec:methodology}

---

6. Experimental Setup

6.1 Environments
	•	n-D gridworld family with configurable sensor channels (heterogeneous \(\mathcal{C}_i\)) and action heterogeneity; brief task description; pointers to appendix for details.
6.2 Baselines
	•	MAPPO (shared critic), IPPO (optional), ablation: parameter sharing + explicit ID (Terry-style) on matched encoders.
6.3 Architectures
	•	Shared encoder with channel-wise modules + pooling; masked action heads; critic variants.
6.4 Protocols
	•	Training budgets; seeds; eval regimes: (i) in-distribution, (ii) sensor dropout, (iii) team-size perturbation (± agents), (iv) composition shift (new \(\mathcal{C}_i\) mixes).
6.5 Metrics
	•	Reward vs agent-steps (your preferred cost metric), convergence sample-efficiency, robustness deltas, RAM/throughput snapshots.
6.6 Implementation details
	•	Framework (RLlib or equivalent), hyperparam ranges, masking specifics; reproducibility hooks (seeds, versions).

---

7. Results

7.1 Learning efficiency
	•	Convergence curves (mean ± CI) vs agent-steps; comparisons to MAPPO/IPPO; any RAM/runtime notes.
7.2 Robustness
	•	Sensor-dropout performance; ablations on which channels are removed; stability variance.
7.3 Team-size/Composition transfer
	•	Add/remove agents at test time; policy degradation and recovery behaviors.
7.4 Ablations
	•	Remove masks → performance drop; replace mask with explicit ID only → effect; different pooling ops; equivariant vs plain MLP.
7.5 Summary of trade-offs
	•	When homogenization wins/loses; interaction with coherence.



% All configurations are evaluated across multiple independent 
% training runs to account for stochastic variability.
% Learning efficiency is defined in terms of convergence rate 
% relative to total agent-training steps and computational cost.


- Approach
    - Implicit agent indication
        - Allows for complete enumeration of agent types
- Control
    - MAPPO (with shared critic)
        - Explain why using shared critic
- Model/Environment
    - nD Grid world
    - Differ to appendix for further description for now, pending fully separate paper

% % #TODO Make more terse (in favor of exporting)
% \subsection{Environment Model}


% % #TODO Update Environment and agent design
% \subsection{Environment and Agent Design}

% To evaluate agents with overlapping but non-identical observation capabilities, 
% we design a custom environment derived from the \gls{mpe}. 
% Each agent is equipped with a subset of sensor types, 
% and each sensor type defines a linearly independent subspace of the 
% full observation space. An agent's full observation vector is constructed by 
% concatenating the outputs of its sensors, and overlapping sensor types across 
% agents induce structured correlations.

% The agents are heterogeneous in both their observation and action spaces. 
% The union of all agent sensor types forms a global spanning observation space. 
% Likewise, the union of all agent action spaces defines a spanning action set, 
% from which agent-specific valid actions are masked during inference.


% % #TODO Update architectures
% \subsection{Architectures Compared}
% Three network architectures are evaluated:

% \paragraph{0. HAPPO Baseline}
% To provide a baseline comparison, we include a standard implementation 
% of Heterogeneous-Agent Proximal Policy Optimization (HAPPO)~\cite{zhong2024}. 
% With HAPPO each agent is trained with its own policy using PPO updates in a centralized 
% training setting, but without shared parameters or input-invariant structure. 
% HAPPO represents a strong non-invariant architecture designed specifically 
% for heterogeneous teams, serving as a strong reference point for evaluating the 
% efficiency and robustness benefits of the more structured approaches.

% % \paragraph{1. COMA with GNN and Transformer}
% % This model extends the counterfactual multi-agent policy gradient (COMA) 
% % algorithm by incorporating a graph neural network (GNN) to encode agent relations, 
% % and a transformer-based critic for contextual value estimation. 
% % This structure follows prior work (e.g., PIC and MAAC) and 
% % serves as a high-capacity input-invariant baseline. 
% % The actor and critic operate under a centralized training with 
% % decentralized execution (CTDE) paradigm, with the GNN providing relational inductive bias.


% \subsection{Perturbation Evaluation}

% To assess runtime robustness, trained agents are tested under two types of perturbations:

% \begin{itemize}
%     \item \textbf{Sensor degradation}: Agents experience random dropout 
%         of one or more observation channels during execution. 
%         The masking vector is updated accordingly to simulate partial observability.
%     \item \textbf{Team-size changes}: Agents are removed or added at test 
%         time without retraining. Policies are evaluated for stability and 
%         reward degradation in the new configuration.
% \end{itemize}

% Each condition is evaluated across multiple trials. 
% Policy stability is measured by the variance and degradation in episode rewards, 
% and recovery is assessed where applicable. 
% Final comparisons are made between the two architectures across all perturbation conditions.


Training \& Evaluation. 
We compare a heterogeneous baseline (\gls{happo}), 
a parameter-sharing baseline with explicit agent indication (MAPPO-share), 
and our homogenization method (shared actor over a masked spanning space with a centralized critic). 
Experiments vary team size \(N=4\) 
and four heterogeneity regimes
complete observation space,
overlap, intersecting, and disjoint-heavy—across 10 random seeds per condition. 
Training proceeds for a fixed budget of agent-steps (up to 500M, 5e8), 
and evaluation occurs every 100k agent-steps on held-out scenarios: 
in-distribution, sensor-dropout (removing 20-40\% of active channels), 
team-size shifts \((N\rightarrow N\pm)\), and composition shifts 
(resampled channel sets) including addition of cold-start agent types 
formed from unions of seen channels. We report learning efficiency 
via area-under-curve of return vs agent-steps, final return at budget, 
and robustness deltas relative to in-distribution performance. 
Statistical comparisons use Welch's t-tests with FDR control and Hedges' g effect sizes.



\section{Results}
\label{con2:sec:results}

- Training
    - did not have significant benefit on way or another
    - slight improvement in RAM usage
- Evaluation
    - (Some combos of training v eval are redundant or unnecessary)
    - Overall Trends:
    - Complete Conf:
        - Greater robustness
            - From greater use of information? vs mappo
    - Intersecting:
    - Disjoint:
        - 


---

8. Discussion
•	Practical guidance: when the coherence assumption is plausible; how to design channels; pitfalls (aliasing, mis-specified spans).
•	Relation to parameter sharing: complementary tools; when to prefer which.
•	Limitations: emergent behavioral heterogeneity not directly modeled; partial support for heterogeneous action types; potential masking overhead.

---

9. Conclusion
•	One-paragraph recap; actionable takeaways; future work (learned channel dictionaries, adaptive spans, continuous action masking, coupling with hypernetworks).

---

\section{Conclusion}
\label{con2:sec:conclusion}

- Benefits
    - Allowing better than naive novel agents
    - Improved stability at deployment
- Limitations
    - Ours does not currently support emergent (behavioral) heterogeneity
    - Did not have broad testing of hetero-action space
- Future work
    - (While the current focus is on observation-space heterogeneity,  additional forms such as action-space heterogeneity may be considered in future extensions, pending environment support.)

    
% \section{}
% \label{con2:sec:}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%           Other JAIR Items
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% \section{Acknowledgments}
% \nocite{*}
\printbibliography

% % --- End Dissertation Inclusion ---
\clearpage
\appendix
% % \addcontentsline{toc}{section}{Appendix}

\section{Hyperparameterization}
% Table of used parameters
% Search Range


\section{Environment Model}
\label{con2:app:env_model}

% We model the environment following the structure outlined by Powell~\cite{powell2022}, 
% consisting of the state, decision/action, exogenous information, transition function, and 
% objective function.

% \paragraph{State.} 
% Let \(T \subset \mathbb{N}\) be the set of decision epochs, 
% and define entities \(E := (I, J, K, L)\), where 
% \(i \in I\) are agents, 
% \(j \in J\) are objectives, 
% \(k \in K\) are hazards, and 
% \(l \in L\) are obstacles. 
% At each time \(t \in T\), the joint state \(s\in S\) is:
% \[
%     s_t := \{s_{te} \mid e \in E\}, \quad s_t \in S
% \]
% Where the marginal state for agents \(i \in I\), 
% includes both location and orientation:
% \[
%     s_{ti} := (d_{ti}, \theta_{ti})
% \]
% and the non-agent entities \(e \in (J, K, L)\) are represented by location alone:
% \[
%     s_{te} := (d_{te})
% \]
% For spatial dimensions \(N \in \mathbb{N}\), 
% let \(D^{(n)} \in \mathbb{F}\) be the domain of dimension \(n\).
% Then, we define each entity \(e\)'s location as a tuple:
% \[
%     d_{te} := \left(d_{te}^{(1)},\ldots,d_{te}^{(N)}\right) \in \prod_{n=1}^N D_e^{(n)},
% \]
% and orientation is represented by the tuple:
% \[
%     \theta_{ti} := \left(\theta_{ti}^{(1,2)},\ldots,\theta_{ti}^{(N-1,N)}\right)
%     \in \prod_{n=1}^{N-1} \Theta_i^{(n,n+1)}
% \]

% \paragraph{Observation Model.}
% Let \({C} := \{c_1, \ldots, c_m\}\) denote a set of abstract sensor channels.
% Each sensor channel \(c \in {C}\) corresponds to a linearly 
% independent observation subspace.

% For each entity visibility in each channel must be defined,
% as a trait this may be referred to as \(e^{(c)} \in \{\text{True, False}\}\).
% It may be encoded in the state as:
% \[
% s_{tec} = 
% \begin{cases}
%     1& \text{if \(e\) is visible in \(c\)} \\ 
%     0& \text{if \(e\) is not visible in \(c\)}
% \end{cases} 
% \quad\forall t\in T, e\in E, c\in C.
% \]
% Thus the marginal states becomes
% \[s_{ti} :(c_{ti},d_{ti},\theta_{ti})\]
% for agents, and
% \[s_{te} :(c_{te},d_{te})\]
% for all other entities.

% Each agent \(i \in I\) is assigned a subset of sensor channels 
% \({C}_i \subseteq {C}\), 
% with their observation defined as the concatenation of the outputs 
% from the corresponding subspaces:
% Note that an agent's ability to perceive an entity within a given 
% subspace may be limited by spatial range, meaning each agent may 
% only observe a portion of the environment within each active channel.
% \[
%     o_{ti} := \bigoplus_{c \in {C}_i} o_{tic}
% \]
% where \(o_{tic}\) is agent \(i\)'s observation of channel \(c\) at time \(t\), 
% and \(\oplus\) denotes concatenation over the ordered tuple of subspaces.

% The global observation space \(O\) is the Cartesian product of all channel subspaces:
% \[
%     O := \prod_{c \in {C}} O_c
% \]
% and each agent's observation is a masked subset of this space.
% During policy training and inference, a binary mask vector \(m_i \in \{0,1\}^{|\mathcal{C}|}\) 
% is applied to indicate which subspaces are active for agent \(i\), 
% allowing a shared network to condition on heterogeneous observations without assuming symmetry.


% \paragraph{Action Space.} 
% The joint action \(a \in A\) consists of marginal agent actions:
% \[
%     a := (a_1, \ldots, a_{|I|}) \in A := \prod_{i \in I} A_i
% \]
% Each agent's action at time \(t\) is a tuple:
% \[
%     a_{ti} := (a_{ti}^\text{interact}, a_{ti}^\text{move}, \Delta\theta_{ti})
% \]
% where:
% \begin{itemize}
%     \item \(a_{ti}^\text{interact} \in \{0,1\}\) toggles interaction,
%     \item \(a_{ti}^\text{move} \in M_i \subseteq \mathbb{F}\) denotes movement magnitude,
%     \item \(\Delta\theta_{ti} \in \prod_{n=1}^{N-1} \Delta\Theta_i^{(n,n+1)}\) encodes orientation adjustments.
% \end{itemize}

% % \paragraph{Exogenous Information.} 
% % Agents observe a partially observable state with complete reward knowledge. 
% % Probabilistic channel observability is future work.

% \paragraph{Transition Function.} 
% Deterministic and synchronous:
% \[
%     s_{t+1} = \mathcal{T}(s_t, a_t) \quad \text{or} \quad \mathcal{T}(a_t \mid s_t) = s_{t+1}.
% \]
% Provided that the actions chosen by the agents are valid, they are executed.

% \paragraph{Objective Function.} 
% The cumulative reward is defined as:
% \[
%     \max \sum_{t \in T} \left[
%         \sum_{i \in I, j \in J} r_j \cdot \mathbb{I}[\Xi]
%         - \sum_{i \in I} r_i \cdot \mathbb{I}[a_{ti}^\text{interact} = 1]
%         - \sum_{i \in I, k \in K} r_k \cdot \mathbb{I}[\|s_{ti} - s_{tk}\|_\infty \leq k^\text{range}]
%     \right]
% \]
% where:
% \begin{itemize}
%     \item \(r_j \in \mathbb{R}_{>0}\) is the reward for collecting objective \(j \in J\),
%     \item \(\mathbb{I}[\Xi]\) encodes conditions under which \(j\) is valid for collection,
%     \item \(r_i \in \mathbb{R}_{<0}\) is the cost for agent \(i \in I\) to interact,
%     \item \(r_k \in \mathbb{R}_{<0}\) is a penalty for agent \(i\) being near hazard \(k \in K\).
% \end{itemize}


% #TODO: Reproducibility Checklist
\clearpage
% [\st{yes} / \st{no}]
% [\st{yes} / \st{partially} / \st{no} / \st{NA}]
\section{Reproducibility Checklist for JAIR}

Select the answers that apply to your research -- one per item. 

\subsection*{All articles:}

%\hh{revised for stylistic consistency:}
\begin{enumerate}
    \item All claims investigated in this work are clearly stated. \newline
    % #TODO: Explicit summary of claims
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item Clear explanations are given how the work reported substantiates the claims. \newline
    % #TODO: Connect work to claims - intro or methods
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item Limitations or technical assumptions are stated clearly and explicitly. \newline
    % #TODO: Recheck limits and assumptions
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item Conceptual outlines and/or pseudo-code descriptions of the AI methods introduced 
    in this work are provided, and important implementation details are discussed. \newline
    % #TODO: Figure out what this is to meant to be
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item 
    Motivation is provided for all design choices, including algorithms, implementation choices, 
    parameters, data sets and experimental protocols beyond metrics. \newline
    % #TODO: This is an entire-paper concern. Ongoing need.
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
\end{enumerate}

\subsection*{Articles containing theoretical contributions:}
Does this paper make theoretical contributions?
% [yes/no] 
[\bf{yes} / \st{no}]

If yes, please complete the list below.

\begin{enumerate}
    \item All assumptions and restrictions are stated clearly and formally. \newline
    % #TODO: Recheck theoretical results section
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item All novel claims are stated formally (e.g., in theorem statements). \newline
    % #TODO: Add theorems
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item Proofs of all non-trivial claims are provided in sufficient detail
    to permit verification by readers with a reasonable degree of expertise
    (e.g., that expected from a PhD candidate in the same area of AI). \newline
    % #TODO: Recheck theoretical results section
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item Complex formalism, such as definitions or proofs,
    is motivated and explained clearly. \newline
    %hh: was:
    %Proof sketches or intuitions are given for complex and/or novel results.
    % #TODO: Recheck theoretical results section
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item The use of mathematical notation and formalism serves the purpose of 
    enhancing clarity and precision; gratuitous use of mathematical formalism 
    (i.e., use that does not enhance clarity or precision) is avoided. \newline
    % #TODO: Recheck theoretical results section
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item Appropriate citations are given for all non-trivial
    theoretical tools and techniques. \newline
    % #TODO: Recheck theoretical results section - Cite lemma 1
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
\end{enumerate}

\subsection*{Articles reporting on computational experiments:}
Does this paper include computational experiments? %[yes/no]
[\bf{yes} / \st{no}]

If yes, please complete the list below.
\begin{enumerate}
    \item 
    All source code required for conducting experiments is included in an online appendix 
    or will be made publicly available upon publication of the paper.
    The online appendix follows best practices for source code readability 
    and documentation as well as for long-term accessibility. \newline
    % #TODO: Add link to Hypergrid git https://github.com/bhosley/Hypergrid
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item The source code comes with a license that 
    allows free usage for reproducibility purposes. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item The source code comes with a license that
    allows free usage for research purposes in general. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item 
    Raw, unaggregated data from all experiments is included in an online appendix 
    or will be made publicly available upon publication of the paper.
    The online appendix follows best practices for long-term accessibility. \newline
    % #TODO: Add CSV to the git repo?
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item The unaggregated data comes with a license that
    allows free usage for reproducibility purposes. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item The unaggregated data comes with a license that
    allows free usage for research purposes in general. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item If an algorithm depends on randomness, 
    then the method used for generating random numbers and for setting seeds is 
    described in a way sufficient to allow replication of results. \newline
    % [yes/partially/no/NA]
    [\st{yes} / \st{partially} / \st{no} / \bf{NA}]
    \item The execution environment for experiments, the computing infrastructure 
    (hardware and software) used for running them, is described, 
    including GPU/CPU makes and models; amount of memory (cache and RAM); 
    make and version of operating system; names and versions of relevant software 
    libraries and frameworks. 
    % #TODO: Add Hardware data
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item 
    The evaluation metrics used in experiments are clearly explained and 
    their choice is explicitly motivated. \newline
    % #TODO: Explain evaluation metrics
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item 
    The number of algorithm runs used to compute each result is reported. \newline
    % #TODO: Report run numbers
    [yes/no]
    % [\bf{yes} / \st{no}]
    \item 
    Reported results have not been ``cherry-picked'' by silently ignoring unsuccessful 
    or unsatisfactory experiments. \newline
    % #TODO: Explain superlatives chosen for analysis
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item 
    Analysis of results goes beyond single-dimensional summaries of performance 
    (e.g., average, median) to include measures of variation, confidence, or 
    other distributional information. \newline
    % [yes/no]
    [\bf{yes} / \st{no}]
    \item 
    All (hyper-) parameter settings for the algorithms/methods used in experiments 
    have been reported, along with the rationale or method for determining them. \newline
    % #TODO: Hyperparameter appendix
    [yes/partially/no/NA]
    % [\bf{yes} / \st{partially} / \st{no} / \st{NA}]
    \item 
    The number and range of (hyper-) parameter settings explored prior to 
    conducting final experiments have been indicated, along with the effort spent on 
    (hyper-) parameter optimization. \newline
    % #TODO: Hyperparameter appendix
    [yes/partially/no/NA]
    % [\bf{yes} / \st{partially} / \st{no} / \st{NA}]
    \item 
    Appropriately chosen statistical hypothesis tests are used to establish 
    statistical significance in the presence of noise effects. \newline
    % #TODO: Results section stats
    [yes/partially/no/NA]
    % [\bf{yes} / \st{partially} / \st{no} / \st{NA}]
\end{enumerate}


\subsection*{Articles using data sets:}
Does this work rely on one or more data sets 
(possibly obtained from a benchmark generator or similar software artifact)? 
% [yes/no]
[\st{yes} / \bf{no}]

% If yes, please complete the list below.
% \begin{enumerate}
%     \item 
%     All newly introduced data sets 
%     are included in an online appendix 
%     or will be made publicly available upon publication of the paper.
%     The online appendix follows best practices for long-term accessibility with a license
%     that allows free usage for research purposes.
%     [yes/partially/no/NA]
%     \item The newly introduced data set comes with a license that
%     allows free usage for reproducibility purposes.
%     [yes/partially/no]
%     \item The newly introduced data set comes with a license that
%     allows free usage for research purposes in general.
%     [yes/partially/no]
%     \item All data sets drawn from the literature or other public sources (potentially including authors' own previously published work) are accompanied by appropriate citations.
%     [yes/no/NA]
%     \item All data sets drawn from the existing literature (potentially including authors’ own previously published work) are publicly available. [yes/partially/no/NA]
%     %\item All data sets that are not publicly available are described in detail.
%     %[yes/partially/no/NA]
%     \item All new data sets and data sets that are not publicly available are described in detail, including relevant statistics, the data collection process and annotation process if relevant.
%     [yes/partially/no/NA]
%     \item 
%     All methods used for preprocessing, augmenting, batching or splitting data sets (e.g., in the context of hold-out or cross-validation)
%     are described in detail. [yes/partially/no/NA]
% \end{enumerate}

\subsection*{Explanations on any of the answers above (optional):}

% [Text here; please keep this brief.]


\end{document}
\endinput