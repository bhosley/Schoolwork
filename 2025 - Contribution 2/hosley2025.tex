\documentclass[]{jair}
% \documentclass[manuscript, screen, review]{jair}
% --- Paths ---
\makeatletter
\def\input@path{{Figures/}}
\makeatother
\graphicspath{{Figures}}

\usepackage[capitalize]{cleveref}
\AddToHook{cmd/appendix/before}{\crefalias{section}{appendix}} % Fixes cref appendix error
\usepackage{enumitem}
\usepackage{svg}

%% For JAIR Checklist:
\usepackage{soul}                   % Strikeout with \st{}
\renewcommand{\bf}[1]{\textbf{#1}}  % Bold with \bf{}

\setcopyright{cc}
\copyrightyear{2025}
\acmDOI{10.1613/jair.1.xxxxx}

%%
\JAIRAE{JAIR AE Name}
\JAIRTrack{} % Insert JAIR Track Name only if part of a special track
\acmVolume{1}
\acmArticle{1}
\acmMonth{1}
\acmYear{2030}

\RequirePackage[
  datamodel=acmdatamodel,
  style=acmauthoryear,
  backend=biber,
  giveninits=true,
  uniquename=init,
  ]{biblatex}

\renewcommand*{\bibopenbracket}{(}
\renewcommand*{\bibclosebracket}{)}

% #FINAL: Bibliography link
\addbibresource{../2025Bibs/Prospectus.bib}

\author{Brandon Hosley}
\orcid{0000-0002-2152-8192}
\email{brandon.hosley.1@af.au.edu}
\affiliation{%
  \institution{Air Force Instute of Technology}
  \city{Wright-Patterson AFB}
  \state{Ohio}
  \country{USA}
}
\author{Bruce Cox}
\orcid{0000-0003-0149-1836}
\email{bruce.cox@af.au.edu}
\affiliation{%
  \institution{Air Force Instute of Technology}
  \city{Wright-Patterson AFB}
  \state{Ohio}
  \country{USA}
}
\author{Nicholas Yielding}
% \orcid{0009-0002-1978-6018} %Not enough information to confirm
\email{Nicholas.Yielding@afit.edu}
\affiliation{%
  \institution{Air Force Instute of Technology}
  \city{Wright-Patterson AFB}
  \state{Ohio}
  \country{USA}
}
\author{Matthew Robbins}
\orcid{0000-0002-1718-6839}
\email{Matthew.Robbins@afit.edu}
\affiliation{%
  \institution{Air Force Instute of Technology}
  \city{Wright-Patterson AFB}
  \state{Ohio}
  \country{USA}
}
\renewcommand{\shortauthors}{Hosley, Cox, Yielding, \& Robbins}

\usepackage[nomain,symbols,abbreviations]{glossaries-extra}
\makeatletter
\glsdisablehyper
\makeatother
\newabbreviation{rl}{RL}{reinforcement learning}
\newabbreviation{harl}{HARL}{heterogeneous-agent reinforcement learning}
\newabbreviation{marl}{MARL}{multi-agent reinforcement learning}
\newabbreviation{ppo}{PPO}{proximal policy optimization}
\newabbreviation{mappo}{MAPPO}{multi-agent \gls{ppo}}
\newabbreviation{happo}{HAPPO}{heterogeneous-agent \gls{ppo}}
\newabbreviation{ctde}{CTDE}{centralized training with decentralized execution}
\newabbreviation{posg}{POSG}{partially observable stochastic game}
\newabbreviation{dec-pomdp}{Dec-POMDP}{decentralised partially observable Markov decision process}
\newabbreviation{ippo}{IPPO}{independent \gls{ppo}}
\newabbreviation{hpn}{HPN}{hyper policy network}
\newabbreviation{mlp}{MLP}{multi-layer perceptron}
\newabbreviation{pic}{PIC}{permutation-invariant critic}
% \newabbreviation{lbf}{LBF}{Level-based Foraging}
% \newabbreviation{mpe}{MPE}{Multi Particle Environments}
% \newabbreviation{gnn}{GNN}{graph neural network}
% \newabbreviation{maddpg}{MADDPG}{multi-agent deep-deterministic policy gradient}
% \newabbreviation{sisl}{SISL}{Stanford Intelligent Systems Laboratory}
% \newabbreviation{coma}{COMA}{counterfactual multi-agent policy gradients}

% \newglossaryentry{q}{text=q,name=Q,sort=q,type=symbols,category=symbol,
%     description={Set of state-values or state-value functions.}}
% #TODO: Change math to glossary symbols
\newglossaryentry{s}{text=s,name=S,sort=s,type=symbols,category=symbol,
    description={State.}}
\newglossaryentry{o}{text=o,name=O,sort=o,type=symbols,category=symbol,
    description={Observation.}}
\newglossaryentry{a}{text=a,name=A,sort=a,type=symbols,category=symbol,
    description={Action.}}
\newglossaryentry{r}{text=r,name=R,sort=r,type=symbols,category=symbol,
    description={Reward.}}
\newglossaryentry{p}{text=p,name=P,sort=p,type=symbols,category=symbol,
    description={Transition probability.}}
\newglossaryentry{discount}{text=\ensuremath{\gamma},name=\ensuremath{\gamma},sort=\ensuremath{\gamma},type=symbols,category=symbol,
    description={Discount factor.}}
\newglossaryentry{pi}{text=\ensuremath{\pi},name=\ensuremath{\pi},sort=\ensuremath{\pi},type=symbols,category=symbol,
    description={Policy.}}
\newglossaryentry{i}{text=i,name=I,sort=i,type=symbols,category=symbol,
    description={Set of agents \(i\).}}


% \title[]{Homogenization for Input-Invariant Solutions for Heterogeneous Agent Reinforcement Learning}
\title{Homogenization and Implicit Indication: an Input-Invariant Solution for Heterogeneous Agent Reinforcement Learning}
% TODO: For implicit agent embedding

\begin{document}

% Structured Abstract
\begin{abstract}
    {\bf Background:} \Gls{marl} with heterogeneous agents; 
    limits of standard parameter sharing and ad-hoc agent indication.\\
    {\bf Objectives:} Introduce and analyze homogenized observation spaces and
    homogenized action spaces that enable input-invariant policies across heterogeneous agents.\\
    {\bf Methods:} Formal definition of a covering (homogenized) observation space with validity
    masks and implicit agent indication; theoretical results on representability and invariance;
    empirical evaluation on $n$-D gridworld variants with MAPPO control.\\
    {\bf Results:} Comparable or improved robustness under sensor degradation and
    team-size variation; memory/runtime advantages in some regimes;
    limitations when coherence assumptions fail.\\
    {\bf Conclusions:} Homogenization is a practical alternative to parameter sharing when a
    coherent spanning space exists; we outline conditions, trade-offs, and open questions.
\end{abstract}

% #FINAL: Submission date
\received{DD Month YYYY}
% \received[accepted]{DD Month YYYY}
\maketitle
\glsresetall

\section{Introduction}

\Gls{marl} addresses coordination among multiple decision 
makers operating under partial observability.
Standard approaches, such as parameter sharing and \gls{ctde},
have enabled strong performance in many cooperative 
domains~\cite{albrecht2024, christianos2021}.
However, these methods typically assume that agents are interchangeable: 
they possess the same observation structure, action capabilities, and behavioral roles. 
This interchangeability assumption simplifies learning, 
but may become restrictive in settings where agents differ due to hardware constraints, 
sensing modalities, or mission specialization. 
In such cases, na\"{i}ve parameter sharing obscures the 
structural differences that matter for decision making, 
while fully separating policies prevents useful knowledge sharing across the 
group of agents~\cite{albrecht2024, christianos2021, terry2020, canese2021}.

Heterogeneity is common in real-world multi-agent systems, where agents may differ 
in sensing, actuation, or operational roles due to hardware constraints 
or task specialization \cite{calvo2018, cao2012}. 
These structural differences complicate parameter sharing, generalization, and transfer, 
since policies must account for observation and action spaces that are not merely 
variations of a shared template but may differ in dimensionality and semantics \cite{canese2021}. 
As a result, methods that assume agent interchangeability often degrade when applied to 
heterogeneous teams, motivating representations that preserve agent-specific structure 
while still enabling shared learning \cite{christianos2021, albrecht2024}.

An effective approach is to retain shared network parameters but append explicit agent 
identifiers or type embeddings to the input \cite{terry2020, christianos2021}. 
This strategy allows a network to distinguish among agents, 
but it does not resolve structural heterogeneity. 
When agents differ in the dimensionality or interpretation of their observations or actions,
explicit identifiers must be coupled with padding, duplication, or handcrafted encodings.
Here, differences in interpretation include cases where observation vectors have the same
dimensionality but encode different physical quantities, coordinate frames, or sensing
modalities (e.g., forward-facing versus downward-facing visual inputs).
Such fixes increase representational overhead and can obscure the information that
distinguishes agents.

% 
To illustrate, consider a team of drones that all carry a forward-facing camera but 
differ in their additional sensing capabilities.
One drone may also carry a downward-facing camera for terrain mapping, 
another a thermal camera for target detection, and a third only the forward-facing sensor.
A parameter-sharing policy with explicit agent 
identifiers must still pad missing channels or duplicate dimensions 
to force these observations into a common tensor shape, despite the 
fact that the additional channels encode distinct viewpoints or sensing modalities.
In contrast, a homogenized representation defines a shared set of observation 
elements (e.g., forward-facing imagery, downward-facing imagery, and thermal imagery) 
and allows each drone to populate only the elements it observes.
The policy can then infer sensor capabilities implicitly from the pattern of 
populated elements, without requiring explicit identity features.

We propose an alternative approach based on \emph{implicit indication}. 
Rather than appending explicit identity features, we construct homogenized 
observation and action spaces that jointly span all agent-specific subspaces. 
Each agent's individual capabilities correspond to a distinct subset within 
these shared domains, realized by leaving unused elements empty or null. 
Policies are trained over the complete domain, 
learning to condition implicitly on which elements are populated. 
Differences among agents therefore arise naturally from the structure of 
their accessible subsets, eliminating the need for separate identifiers. 
This construction yields an input-invariant policy class;
a single network that generalizes across heterogeneous teams, 
remains robust to degraded or partial observations, 
and can be transferred to new configurations without architectural change.

Practically, this relies on \emph{semantic decomposability}: 
the assumption that an agent's observation and action spaces can be factorized
into elements that play comparable functional roles across agents.
For example, two agents may each observe an \(n\)-dimensional vector, 
where corresponding components encode the same physical quantity 
(e.g., distance to an object), even if other components differ or are absent.
Under this assumption, homogenization requires \emph{consistent element correspondence},
meaning that corresponding elements across agents refer to the same underlying quantity
and are expressed on a compatible scale (e.g., the distance measurements represent
the same physical distance in the same units).

Applicability hinges on semantically aligned elements across agents; when present, 
homogenization provides a lightweight basis for shared learning, and when 
absent it may function only as a representational scaffold without reliable transfer.

The paper contributes three complementary advances. 
First, it formalizes \emph{implicit indication} as a homogenization-based construction that 
enables shared policy parameters across heterogeneous agents without explicit identifiers. 
Second, it establishes theoretical conditions under which such constructions preserve 
representability and analyzes the role of semantic decomposability in maintaining policy validity. 
Finally, it provides empirical evidence using a configurable heterogeneous-agent 
environment\footnote{
    Code availability---The custom multi-agent environment (HyperGrid) 
    and all training/evaluation scripts used in this study are available at 
    \href{https://github.com/bhosley/Hypergrid}{\texttt{https://github.com/bhosley/Hypergrid}}.
}, 
comparing the proposed approach against \gls{happo}~\cite{zhong2024} and 
evaluating robustness to sensor dropout, team-size variation, and composition changes. 
Together, these results position homogenization as a lightweight mechanism 
for parameter sharing in \gls{harl}.

We next situate this formulation relative to prior work on parameter sharing, 
agent indication, and invariant architectures.


\section{Related Work}
\label{con2:sec:related_work}

Enabling shared learning across heterogeneous agents 
presents a continuing challenge in \gls{marl}.
Existing research addresses this challenge through three main paradigms: 
parameter-sharing mechanisms, invariant and equivariant policy architectures, 
and policy-optimization algorithms designed for heterogeneous settings. 
Homogenization offers a complementary framework that integrates these 
paradigms by providing a shared representational domain in which 
heterogeneous agents can be trained under a unified policy structure.

\subsection{Parameter Sharing and Agent Indication}

Parameter sharing has been highly effective in homogeneous \gls{marl},
where agents have identical observation and action spaces and are intended
to learn interchangeable behaviors~\cite{gupta2017, foerster2018}.
In these settings, a single shared policy improves sample efficiency and
can promote coordinated behavior without additional architectural machinery.
However, this benefit relies critically on structural symmetry:
once agents differ in sensing, actuation, or role, parameter sharing
no longer applies straightforwardly, and consequently most heterogeneous-agent
systems instead train separate policies for each agent or agent type.

To recover some of the efficiency benefits of sharing, 
prior work has introduced \emph{explicit agent indication};
supplying the policy with features that distinguish among agents.
One approach is to use learned type or role embeddings~\cite{christianos2021, albrecht2024}, 
while Terry et al.~\cite{terry2020} evaluated encodings; 
such as geometric masks, binary indicators, and inversion-based transforms; 
that are all functionally equivalent to providing a one-hot identity signal.
These approaches allow divergent behavior under shared parameters, 
but they still require manually injecting identity information and 
do not resolve underlying structural mismatches among agents.

\subsection{Invariant and Equivariant Architectures}

While explicit agent indication removes symmetry assumptions by conditioning on agent identity,
a complementary line of research instead exploits symmetry directly by designing policies that are
invariant or equivariant to permutations of their inputs~\cite{zaheer2017, yang2018}.
When the ordering or indexing of agents and entities carries no semantic meaning,
such architectures enforce permutation symmetry through shared encoders combined with
symmetric aggregation operators.
This inductive bias has been shown to improve generalization, stability, and sample efficiency
in domains where agents or observed entities are structurally uniform.

Invariant and equivariant models operationalize this principle by applying a common encoder
to each entity and aggregating the resulting representations using commutative operations
such as summation or averaging~\cite{yang2018, li2021b}, or through attention mechanisms
without positional encodings~\cite{tang2021, zambaldi2018}.
These designs rely on the assumption that agents are exchangeable, meaning that permuting
their order does not alter the underlying decision problem.
Exchangeability results, such as de Finetti's theorem~\cite{diaconis1980},
provide a formal justification for this assumption and clarify when permutation-invariant
modeling is appropriate~\cite{alvarez-melis2015, zaheer2017}.
Deep Sets~\cite{zaheer2017} and graph-based \gls{marl} architectures~\cite{yang2021a} instantiate 
this approach by ensuring that reordering agents or entities leaves the policy unchanged.
While these methods assume a uniform input structure across agents,
homogenization through implicit-indication differs by aligning structurally 
heterogeneous observation spaces within a shared spanning domain, enabling invariant 
processing even when agents do not share identical feature sets.

A related class of invariant approximations arises in mean-field 
\gls{marl} solutions, where interactions among many agents are 
aggregated through an averaging operator~\cite{yang2018, li2021b}, 
yielding permutation invariance and insensitivity to team size. 
These methods demonstrate that invariant aggregation can 
significantly reduce sample complexity in large populations, 
but the commutative pooling they employ inevitably discards 
relational structure among interacting agents.

To recover relational information while retaining permutation invariance, 
the \gls{pic}~\cite{liu2020b} introduces a graph-based critic that aggregates 
per-agent information through message passing. \Gls{pic} improves information 
utilization in centralized training and remains insensitive to agent ordering; 
moreover, by including type or role labels as node attributes, \gls{pic} can 
accommodate heterogeneous agents without departing from its invariant design. 
These developments motivate more expressive invariant architectures; notably, 
attention-based pooling~\cite{zambaldi2018} and graph-based attention~\cite{hao2023};
which preserve relational structure while maintaining order invariance, 
though at the cost of losing population-size invariance due to pairwise 
message passing or full attention matrices. Such findings parallel observations 
in single-agent settings, where relational encoders have shown robustness 
to missing or occluded inputs~\cite{tang2021}, reinforcing the 
importance of architectural invariance in domains where observation 
availability varies across agents.

While invariant and relational architectures can support heterogeneous agents
through node- or type-specific features in graph and attention-based models,
they still assume a fixed architectural template and incur costs that 
grow with the number of entities. In many practical multi-agent systems, 
agents differ more fundamentally in their observation and action 
structures~\cite{calvo2018}, and handling these differences purely 
architecturally can become cumbersome~\cite{cao2012, canese2021, hao2023}. 
This motivates other lines of work that address heterogeneity at 
the level of policy parameterization and training algorithms, 
for example by decoupling per-type policy heads or generating 
agent-specific parameters via hypernetworks.

\subsection{Handling Heterogeneity in MARL}

Several recent frameworks have explicitly targeted the challenges of heterogeneity 
in cooperative MARL. Independent PPO (IPPO) and Multi-Agent PPO (MAPPO)\cite{yu2022} 
provide stable baselines for homogeneous teams but require architectural duplication 
to handle differing agent spaces. Heterogeneous-Agent PPO (HAPPO)\cite{zhong2024} 
extends these ideas by enabling joint training across agent types while maintaining 
distinct policy heads, improving sample efficiency and role specialization.
Other works introduce selective sharing mechanisms or hypernetworks to generate 
per-agent parameters~\cite{hao2023, witt2020}.These approaches primarily 
address heterogeneity through network architecture or parameterization, 
whereas the method introduced here addresses it through representation: 
homogenization creates a shared input-output space in which a single 
policy can operate across agents with structurally distinct capabilities.

Several frameworks directly target the challenge of handling heterogeneity in \gls{marl}.
\Gls{ippo} and \gls{mappo}~\cite{witt2020, yu2022} provide stable baselines for 
homogeneous teams, but accommodating agents with different observation or action 
spaces typically requires duplicating architectures or maintaining separate policies. 
\Gls{happo}~\cite{zhong2024} provides a strong solution for behaviorally 
heterogeneous teams by enabling agents to learn distinct, emergent behaviors 
under a shared optimization framework while maintaining separate policy networks 
for agents with structurally different observation or action spaces. 
This improves sample efficiency and role specialization, but because differently 
structured agents cannot share parameters directly, \gls{happo} does not by itself 
support unified policies across structurally heterogeneous agents without additional 
architectural mechanisms.

Other methods address heterogeneity through conditional parameter sharing, 
combining a shared backbone with modules whose parameters are generated 
by an auxiliary network. Hypernetwork-based approaches, such as 
\gls{hpn}~\cite{hao2023}, use a separate \gls{mlp} to produce the 
parameters of an input module that processes aggregated per-agent inputs 
before they are passed through a single shared backbone that outputs actions 
for all agents, effectively treating the team as a single structured entity. 
Selective-sharing mechanisms~\cite{christianos2021} allow agents to 
access shared components when beneficial and retain specialized modules where needed. 

These strategies offer flexible ways to capture cross-agent commonalities without 
enforcing strict parameter tying. The approach introduced in this work is complementary: 
rather than conditioning parameterization on agent identity or type, 
homogenization aligns heterogeneous observation and action spaces within a 
shared representational domain, enabling a single policy to operate across 
structurally distinct agents without explicit identifiers or per-type modules.

Together, these methods illustrate the range of existing strategies for 
handling heterogeneity; through explicit indication, invariant architectures, 
selective parameter sharing, or type-specific policy modules. 
Yet all of these approaches treat heterogeneity as an architectural or 
algorithmic constraint on how policies are parameterized. 
In contrast, the approach introduced in this work treats heterogeneity as a 
representational problem: rather than specializing networks per agent type, 
it constructs shared input and action domains, homogenized across agents,
that allow a single policy to operate over structurally distinct observation 
and action spaces without requiring architectural specialization.
The next section develops this representational perspective in detail and 
formalizes the homogenization framework used in this work.


\section{Homogenization and Implicit Indication Method}
\label{con2:sec:method_homogenization}

\subsection{Preliminaries}

The homogenization approach constructs a unified space defined by the union set 
of the elements that comprise the heterogeneous spaces of the constituent agents.
This can be done for action spaces, observation spaces, or both independently.

This construction enables parameter sharing without requiring explicit agent identifiers, 
while still preserving the structural distinctions that arise from heterogeneity. 
In what follows, we define the homogenized spaces, describe the role of validity masks, 
and outline the resulting invariant policy class.

To formally represent the cooperative multi-agent setting,
we model the environment as a \gls{dec-pomdp}
defined by the tuple \((I,S,O,A,P,R,\gamma)\) where:
%
\begin{description}[labelindent=2em, labelwidth=5em, itemsep=3pt, topsep=8pt]
    \item[\(\Gls{i}\)] : Set of all agents
    \item[\(\Gls{s} = \{S_i\}_{i\in I}\)] : Joint state space
    \item[\(\Gls{o} = \{O_i\}_{i\in I}\)] : Joint observation space
    \item[\(\Gls{a} = \{A_i\}_{i\in I}\)] : Joint action space
    \item[\(\Gls{p}(s^\prime|s,a)\)] : Transition probability function
    \item[\(\Gls{r} = \{R_i\}_{i\in I}\)] : Joint rewards
    \item[\gls{discount}] : Reward discount factor.
\end{description}
%
When combined with a joint policy \(\gls{pi} = \{\pi_i\}_{i\in I}\) we have a \gls{posg}.

\subsection{Decomposition of Observations}

For some agent \(i\), consider its observation space \({O}_i\).
The proposed homogenization method assumes that these spaces are 
decomposable into salient subspaces, referred to as \emph{observation elements}.
Intuitively, an observation element may represent a distinct sensor on an agent,
thus the combination of all such elements form the agent's observation.

Let \(c\) be an observation element index and \(C\) be the global index set.
Then, \(C_i \subseteq C\) represents the set of indices corresponding to 
agent \(i\)'s available observation elements.
In the absence of a predefined global index set, one may define
\(C := \bigcup_{i \in I} C_i\).  If a global index set already exists,
it must at minimum form a superset of this union to serve as a valid spanning set.
% 
Thus, the observation space of agent \(i\) is the collection of its active
observation elements:
\[
    O_i := \{\, O_c \mid c \in C_i \,\}.
\]
and the homogenized observation space,
\[
    O^\prime := \{\, O_c \mid c \in C \,\},
\]
which collects every observation element available to any agent.

To map observations into the homogenized space, let \(C' \subseteq C\) 
denote a set of observation element indices with corresponding 
observation space \(O_{C'} := \{\, O_c \mid c \in C' \,\}\).
Define \(E_{C'} : O_{C'} \to O^\prime\) by populating the coordinates
\(c \in C'\) with their element values and assigning a distinguished null 
value (e.g., \(0\) or \(\emptyset\)) to all coordinates in \(C \setminus C'\). 

Then, the mapping
\[
    \iota : O^\prime \to \mathcal{P}(C), \qquad
    \iota(o) := \{\, c \in C \mid o_c \notin \{0,\emptyset\} \,\},
\]
recovers the set of active observation elements from an observation in \(O^\prime\).

Although the observation elements \(\{O_c\}_{c \in C}\) are 
defined as linearly independent subspaces of the representation, 
they are not necessarily statistically independent. 
Two elements may encode distinct representational dimensions while 
exhibiting correlated outputs when they observe the same underlying 
physical process, e.g., thermal and visible-spectrum cameras. 
Conversely, elements such as GPS and vision may be both 
representationally independent and largely uncorrelated in practice. 

\subsection{Homogenized Actions}

The action spaces of agents can be unified using the same 
construction and assumptions described for observations. 

Formally, the homogenized action space is defined as the union 
of all individual agent action spaces:
\[
    A' := \bigcup_{i \in I} A_i.
\]
This spanning set provides a shared representational basis for 
policy outputs across heterogeneous agents.

A policy over the spanning action space \(A^\prime\) 
must be mapped to an agent-feasible action in \(A_i\).
For single-dimensional action domains, clipping continuous actions
or masking invalid logits allow for selecting feasible actions.
% 
For composite or multi-discrete actions, a simple element-wise 
mask is often insufficient: dependencies between action components 
mean that modifying one dimension (e.g., clipping a steering command) 
can change which joint action is optimal. 
In such cases, mapping the homogenized action to an agent-feasible 
action amounts to projecting onto a constrained joint action set, 
which may only approximate the action that would have been 
chosen had the policy been trained directly in \(A_i\).

\subsection{Implicit Indication}

Implicit indication provides the link between 
heterogeneous agents and a shared policy function.
Instead of embedding learned identity vectors or categorical indices, 
each agent is characterized implicitly through its observable structure.
The policy network therefore remains agnostic to agent identity and infers 
relevant distinctions directly from the pattern of active channels in the input.
% 
This construction yields a simple invariance property: agents that share the 
same pattern of active channels are functionally equivalent under the policy, 
while agents with different masks are distinguished automatically through 
their observable structure. The policy therefore conditions only on which 
subspaces are populated, not on any arbitrary agent identifiers.

Because agent identity is implicit by the set of populated observation elements, 
the framework naturally supports unseen agent compositions: 
an agent defined by a previously unobserved subset of elements 
\(\iota(O_j) = C_j \subseteq C,\) \(j \notin I\)
can be introduced without changing the policy architecture, 
provided its observations embed into the same spanning space. 
This same mechanism yields inherent robustness to sensor dropout, 
since the temporary loss of a sensor simply produces a different subset \(C_j\), 
allowing the shared policy to operate without retraining.
The same principle also yields team-size robustness: 
adding or removing agents simply changes the set of evaluated inputs 
without altering the policy architecture, since the policy depends 
only on the populated observation elements of each individual agent.

A natural limitation of this approach is that it does not introduce behavioral 
diversity among agents that share the same set of observation elements: 
for agents \(i,j \in I\), \(i\neq j\), if \(C_i = C_j\), then \(\pi_i = \pi_j\).
Implicit indication therefore does not model emergent specialization 
within homogeneous subgroups; rather, it provides a unified representational 
basis that enables parameter sharing across structurally distinct agents.


\section{Theoretical Results}
\label{con2:sec:theory}

This section formalizes the conditions under which homogenization 
supports valid shared policies across heterogeneous agents.
% We show that representability is preserved when agent observations are embedded into a common 
% spanning space with consistent element correspondence, derive the invariance properties that arise 
% from channel-wise encoding and commutative aggregation, and examine the assumptions required for 
% the homogenized embedding to remain decision-sufficient for control. These results clarify when an 
% input-invariant policy can faithfully reproduce or approximate the optimal behaviors of structurally 
% distinct agents.


\subsection{Representability}

A unified policy over the homogenized domain is most useful if it remains expressive 
enough to reproduce the optimal policies associated with each heterogeneous agent. 
Under implicit indication, each agent type is characterized by its subset of 
observation elements \(C_i \subseteq C\), and its observations occupy the 
embedded region \(E_{C_i}(O_{C_i}) \subseteq O^\prime\). 
Because the active-element pattern is always recoverable via \(\iota(E_{C_i}(o)) = C_i\), 
the homogenized space preserves the structural distinctions among agents: 
each embedded observation carries both its value and the information 
needed to determine which observation elements generated it. 
These properties ensure that agents with different \(C_i\) remain 
distinguishable in the shared domain, enabling a unified policy to 
condition correctly on heterogeneous observation structures.

With this structure in place, we can now show that this method of 
homogenization preserves representability at the level of agent types: 
for any collection of optimal per-agent-type policies, there exists 
a single policy over \(O^\prime\) whose restriction to each region 
\(E_{C^\prime}(O_{C^\prime})\) matches the corresponding optimal 
policy over \(O_{C^\prime}\).

\begin{lemma}[Representability under injective, disjoint embeddings]
Let \(O'\) be a domain, and let \(\{O_j\}_{j \in J}\) be a family of sets with
embeddings \(E_j : O_j \to O'\) such that
\begin{enumerate}[label=(\roman*), nosep]
    \item each \(E_j\) is injective; and
    \item the images are pairwise disjoint:
    \[
        E_j(O_j) \cap E_k(O_k) = \emptyset
        \quad\text{whenever } j \neq k.
    \]
\end{enumerate}
Then for any collection of policies
\(\{\pi^*_j : O_j \to \Delta(A_j)\}_{j\in J}\),
there exists a single policy
\(\pi^* : O' \to \Delta(A')\)
whose restriction to each image \(E_j(O_j)\) matches \(\pi^\star_j\):
\[
    \pi^*(E_j(o)) = \pi^*_j(o)
    \quad\text{for all } j\in J,\; o \in O_j.
\]
\end{lemma}

% #TODO : add a proof
% You have a lemma, but no Proof.  Therefore no theoretical results.  Can you rephrase your discussion as a proof of the lemma?

In our homogenization setting, this lemma applies directly. 
Each agent type corresponds to a subset of observation elements \(C_i\), 
and the embedding \(E_{C_i}\) places its observations into the homogenized space 
\(O'\) by activating only the coordinates associated with those elements. 
Because no two distinct subsets \(C_i\) produce overlapping embeddings, 
the images \(E_{C_i}(O_{C_i})\) form a disjoint family within \(O'\). 
Injectivity follows from the fact that each observation element is 
mapped to a unique coordinate block. 
Consequently, any collection of per-type optimal policies can be 
represented by a single unified policy defined over the homogenized space, 
whose behavior restricted to each embedded region matches the corresponding 
per-type policy. This establishes that implicit indication preserves representability 
across heterogeneous agents and provides the foundation for using a 
single policy network without explicit agent identifiers.

% Consider theorem as "Any partitioned representation is representative?"


\section{Experiments}
\label{con2:sec:experiments}

\subsection{Experimental Objectives}

Our empirical evaluation examines whether implicit indication 
realizes the capabilities predicted by our theoretical analysis 
and overcomes the limitations of existing \gls{harl} approaches. 
In particular, we evaluate whether a single parameter-sharing policy can:
\begin{enumerate}
    \item Learn effectively across agents with distinct observation-element subsets.
    \item Generalize to novel combinations of observation structures not encountered during training.
    \item Remain robust to structured perturbations such as sensor loss, sensor gain,
          and changes in team composition.
\end{enumerate}

\subsection{Environment}
% #TODO: Add Visuals
% #TODO: Expand section (reposition app items)
% Add some of the visuals you used in your dissertation to this section.  It seems like you just ran out of steam, the material above this is well developed from here down it is very sparse.
% This probably needs about a page not a paragraph.
To support controlled manipulation of observation elements, we use a custom
extension of the MiniGrid environment that enables multiple agents, 
$n$-dimensional spatial layouts, and selectively visible entities whose
properties determine which observation elements they appear on.  
The task is a variant of level-based foraging in which agents 
must coordinate to collect objects while navigating obstacles, but 
objects and obstacles may be visible only on specific observation channels.
This design provides fine-grained control over heterogeneous observability 
while keeping the underlying task simple.  
Full environment details are provided in \cref{con2:app:env_model}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\linewidth]{Figures/marl-minigrid.png}
    \caption{Mini-grid style rendering of multi-agent environment.}
    \label{con2:fig:marl-minigrid}
\end{figure}
\cref{con2:fig:marl-minigrid} shows the two-dimensional rendering
fo the environment retained from minigrid.



\subsection{Training Setup}

Because prior explicit-indication methods cannot operate directly on
heterogeneous input structures, we compare implicit indication against a 
heterogeneous-agent baseline, \gls{happo}, in which each agent type is trained
with its own dedicated policy. This baseline provides strong representational
flexibility without enforcing shared parameters beyond a shared critic and
therefore serves as an appropriate reference point for evaluating
representability and robustness.

To ensure a controlled comparison, both implicit-indication and \gls{happo}
policies use identical neural-network architectures and matched hyperparameters, 
with all settings listed in \cref{con2:app:hyperparameters}. 
The two implementations used networks of identical size.
This choice ensures that any performance differences arise from 
the representational method rather than model capacity.
Each model is trained with a team of four agents for up to 500M steps; 
during hyperparameter exploration, 
longer training horizons did not meaningfully improve performance on the task.
All reported results aggregate performance across 30 independent training runs.  

All experiments were conducted on a compute cluster using a Windows Subsystem 
for Linux (WSL) virtual machine. Each training job was allocated 32 CPU cores on an 
Intel(R) Xeon(R) E5-2660 v3 @ 2.60GHz processor and 128 GB of RAM. No GPUs were used.

For each method, we train four compositions of teams that differ in the
structure of their observation-element coverage:
\begin{enumerate}
    \item \emph{Full visibility}: \(C_i = C\) for all \(i \in I\).
    \item \emph{Intersecting span}: \(\cup_{i\in I} C_i = C\) and 
        \(C_i \cap C_j \neq \emptyset\) for all \(i,j\in I\).
    \item \emph{Disjoint span}: \(\cup_{i\in I} C_i = C\) and 
        \(C_i \cap C_j = \emptyset\) for all  \(i,j\in I\) where \(i\neq j\).
    \item \emph{Incomplete coverage}: \(\exists c \in C\) such that 
        \(c\notin \cup_{i\in I} C_i\)
\end{enumerate}

\begin{figure}
    \input{team_compositions.pdf_tex}
    \caption{Team compositions and observation element coverage.}
\end{figure}

\subsection{Evaluation}

To assess robustness and generalization, we evaluate the trained policies
under eight test conditions:
\begin{enumerate}
    \item Unmodified Team.
    \item Loss of an agent.
    \item Sensor degradation, in which an agent loses access to an element \(c \in C_i\).
    \item Sensor improvement, in which an agent gains a new observation element.
    \item Reduced coverage, in which the entire team loses access to a specific element.
    \item Increased coverage, in which every agent gains \(c\in C\) if possible.
    \item Shuffled assignments, in which agents are randomly permuted across policy outputs.
    \item \emph{Novel spanning set}, in which the team is instantiated with
        observation-element combinations not seen during training.
\end{enumerate}
These conditions probe representability, stability, and zero-shot compositional
generalization in a unified manner.

For all conditions, the primary evaluation metric is mean episodic return, 
averaged over rollouts, which reflects overall task performance.
All environment modifications and training code used in these experiments are included 
in the public repository referenced in the introduction to support full reproducibility.


\section{Results}
\label{con2:sec:results}
% #TODO: Expand, choose examplar
% Similar to section 5 this is all FAR FAR too sparse.
% Table or figure or anything?

The first comparison examines whether implicit indication imposes any additional
computational burden relative to the heterogeneous-agent baseline. At every
training step, we recorded CPU utilization, RAM utilization, and wall-clock time
per environment step for both methods. Because these measurements were not
expected to follow a normal distribution, we evaluated distributional
differences using a Mann-Whitney U test, with the null hypothesis that both
samples were drawn from the same distribution. We found no statistically
significant differences in any of the three metrics: RAM utilization (p = 0.161),
CPU utilization (p = 0.687), and time per training step (p > 0.05 across all
conditions). Accordingly, we do not reject the null hypothesis and conclude that,
under our experimental conditions, implicit indication does not introduce any
measurable increase or decrease in computational resource usage relative to the
baseline.

\subsection{Training}
% #TODO: Add example
% provide one here as an exemplar and then relegate the remainder to the appendix.  Use that exemplar as a discussion point.

The training curves for all conditions are provided in Appendix \ref{con2:app:training_curve}. 
Across all four team-composition settings, the two approaches exhibited remarkably similar 
learning dynamics: convergence rates, variance across seeds, and asymptotic returns were 
nearly indistinguishable. The most notable difference lies in model size; 
as the implicit method has a storage footprint \(1/|I|\) that of this baseline, 
which maintains a separate policy per agent. 

\subsection{Evaluation}
\label{con2:app:evaluations}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{Data/performance_delta.png}
    \caption{Comparison between methods trained with different observation-element coverage.}
    \label{con2:fig:eval_delta}
\end{figure}

To compare generalization performance across evaluation conditions, 
we selected the top 15 policies out of 30 for each condition. 
This reflects the common practice of training multiple models and evaluating 
the superlative subset, balancing removal of underperforming runs with avoidance 
of excessive variance.
A summary of performance over evaluations is presented in 
Appendix~\ref{con2:app:evaluations}. \Cref{con2:fig:eval_delta} 
presents a heatmap summarizing the performance 
differences between implicit indication and the \gls{happo} baseline. 
For these evaluations, evaluation episodes were longer than individual training 
episodes.

Across most evaluation conditions, implicit indication showed modest but 
consistent improvements relative to the baseline, 
particularly in regimes involving altered observability or changes in team composition. 
The most pronounced gains appeared in the \emph{complete-visibility} training case; 
agents trained in this condition share identical observation-element coverage,
resulting in implicit-indication's equivalence to parameter sharing over a 
homogeneous team. 

The resulting performance advantage therefore reflects more efficient 
use of shared experience rather than differences in representational capacity. 
A difference that becomes most pronounced in the case of agent loss,
where \gls{happo} allowed for emergent specialization, and 
implicit indication did not.

\section{Conclusion}
\label{con2:sec:conclusion}

This paper introduced homogenization with implicit indication as a
representation-based approach for enabling shared policies across
heterogeneous agents. By embedding agent observations into a unified
spanning space and allowing the policy to condition implicitly on active
observation elements, the method avoids explicit identity encodings and
removes the need for per-type policy networks. We formalized conditions
under which this embedding preserves representability and clarified the
role of semantic alignment among observation elements.

Empirically, implicit indication matched the performance of a
\gls{happo} baseline during training, with no detectable
differences in computational resource usage. 
Despite its smaller model footprint, it showed comparable 
or improved robustness under sensor loss, sensor gain,
and loss of member agents. These results suggest that a
single homogenized policy can generalize effectively across 
structured heterogeneity and provide greater deployment 
stability than per-configuration training.

Future extensions include handling richer action-space heterogeneity,
improvements in information sharing and communication between agents 
reflect differences in composition and information access,
and exploring how behavioral heterogeneity might be facilitated
through some method of accounting for the total capabilities of 
a group of agents. 
Taken together, our
results indicate that homogenization offers a simple and practical tool
for enabling shared learning across structurally diverse agents.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%           Other JAIR Items
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% \section{Acknowledgments}
% \nocite{*}
\printbibliography

% % --- End Dissertation Inclusion ---
\clearpage
\appendix

\section{Hyperparameters}
\label{con2:app:hyperparameters}

For transparency and reproducibility, we report both 
(i) the final hyperparameters selected for our experiments, and 
(ii) the ranges explored during tuning. 
All tuning was conducted using \texttt{Ray Tune}~\cite{liaw2018}, 
which automatically schedules and evaluates candidate configurations under 
fixed computational budgets.

\subsection{Final Hyperparameters Used}
Table~\ref{tab:final-hparams} lists the hyperparameters used for the 
experiments reported in the main text. These were selected based on performance 
aggregated over tuning runs, with consistency checks across random seeds.

\begin{table}[H]
    \centering
    \caption{
        Final hyperparameters used for homogenization and baseline methods. 
        Values are reported as applied across all main experiments.
    }
    \label{tab:final-hparams}
    \begin{tabular}{l l l}
        \toprule
        \textbf{Parameter} & \textbf{Value} & \textbf{Notes} \\
        \midrule
        Learning rate & \(0.001\) & Selected via Ray Tune search \\
        Batch size & \(4096\) & Number of samples per PPO update \\
        PPO clip ratio & \(0.2\) & Standard PPO clipping parameter \\
        Value Function Clip & \(5\) & Clipping parameter for the value function \\
        GAE parameter $\lambda$ & \(0.95\) & Used in all methods \\
        Discount factor $\gamma$ & \(0.99\) & Applied across all methods \\
        Entropy coefficient & \(0.0\) & Exploration regularization \\
        Value loss coefficient & \(1.0\) & PPO value function weight \\
        Number of rollout workers & \(8\) & Vectorized env instances \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[H]
    \centering
    \caption{
        Final architecture specifications for the policy networks used in all experiments.
        Layer dimensions, activation functions, and convolutional parameters correspond to the 
        configuration selected after hyperparameter tuning and applied uniformly across methods.
    }
    \label{tab:policy_network}
    \begin{tabular}{l l}
        \toprule
        \textbf{Layer} & \textbf{Value} \\
        \midrule
        Image Conv. Layers & \([16, 32, 64]\) \\
        Image Conv. Kernel size & \([2,2]\) \\
        Img Enc. Activation Function & \(\text{relu}\) \\
        Dir Enc. Activation Function & \(\text{relu}\) \\
        Encoder Dimensions (Image, Direction) & \([256, 16]\) \\
        Trunk Layers & \([128, 64]\) \\
        Trunk Activation Function & \(\text{tanh}\) \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Hyperparameter Search Space}
Table~\ref{tab:hparam-search} summarizes the ranges considered during tuning.
We used the Asynchronous Successive Halving Algorithm (ASHA) scheduler \cite{li2018b}
with early stopping under a fixed budget of \(1e6\) agent-steps per trial. 
The hyperparameter search space was explored with
Optuna-search \cite{akiba2019}.

\begin{table}[H]
    \centering
    \caption{Hyperparameter ranges explored during tuning.}
    \label{tab:hparam-search}
    \begin{tabular}{l l l}
        \toprule
        \textbf{Parameter} & \textbf{Range / Options} \\
        \midrule
        Learning rate & \([5\mathrm{e}{-5}, 1\mathrm{e}{-3}]\) Log-uniform\\
        Batch size & \(\{4096, 8192, 16384\}\) \\
        PPO Clipping & \(\{0.1, 0.2, 0.3\}\) \\
        Value Function Clipping & \(\{5.0, 10.0\}\) \\
        GAE parameter $\lambda$ & \([0.90, 0.99]\) Uniform \\
        Discount factor $\gamma$ & \(\{0.95, 0.99\}\) \\
        Entropy coefficient & \(\{0.0, 0.005, 0.01, 0.02\}\) \\
        Number of epochs per update & \(\{2, 4, 6, 8\}\) \\
        \midrule
        \textbf{Network Parameters} & \\
        \midrule
        Image Conv. Channels & \(\{16, 32, 64\}\) \\
        Image Conv. Layers & \(\{[32], [32, 32], [32, 64], [64, 64], [16, 32, 64], [32, 64, 64]\}\) \\
        Image Conv. Kernel size & \(\{[2,2],[3,3]\}\) \\
        Encoder Dimensions (Image, Direction) & \(\{[128, 16], [128, 32], [256, 16], [256, 32]\}\) \\
        Trunk Layers & \(\{[64], [128], [128, 64], [256, 128, 64]\}\) \\
        Activation Functions & \(\{\text{relu, tanh}\}\) \\
        \bottomrule
    \end{tabular}
\end{table}
\clearpage

\section{Training Curves}
\label{con2:app:training_curve}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{Data/training_curves.png}
    \caption{Training curves representing mean \(\pm\) standard deviation for each 
        policy configuration and visibility regime.}
\end{figure}
\clearpage

\section{Evaluation Comparison}
\label{con2:app:evaluations}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{Data/eval_scale.png}
    \caption{Comparison between methods trained with different observation-element coverage.}
\end{figure}
\clearpage

\section{Environment Model}
\label{con2:app:env_model}

We model the environment using a structure consisting of the state, 
decision/action, exogenous information, transition function, and 
objective function~\cite{powell2022}.

\paragraph{State.} 
Let \(T \subset \mathbb{N}\) be the set of decision epochs, 
and define entities \(E := I\cup J\cup K\cup L\), where 
\(i \in I\) are agents, 
\(j \in J\) are objectives, 
\(k \in K\) are hazards, and 
\(l \in L\) are obstacles. 
At each time \(t \in T\), the joint state \(s\in S\) is:
\[
    s_t := \{s_{te} \mid e \in E\}, \quad s_t \in S
\]
Where the marginal state for agents \(i \in I\), 
includes both location and orientation:
\[
    s_{ti} := (d_{ti}, \theta_{ti})
\]
and the non-agent entities \(e \in (J, K, L)\) are represented by location alone:
\[
    s_{te} := (d_{te})
\]
For spatial dimensions \(N \in \mathbb{N}\), 
let \(D^{(n)} \in \mathbb{F}\) be the domain of dimension \(n\).
Then, we define each entity \(e\)'s location as a tuple:
\[
    d_{te} := \left(d_{te}^{(1)},\ldots,d_{te}^{(N)}\right) \in \prod_{n=1}^N D_e^{(n)},
\]
and orientation is represented by the tuple:
\[
    \theta_{ti} := \left(\theta_{ti}^{(1,2)},\ldots,\theta_{ti}^{(N-1,N)}\right)
    \in \prod_{n=1}^{N-1} \Theta_i^{(n,n+1)}
\]

\paragraph{Observation Model.}
Let \({C} := \{c_1, \ldots, c_m\}\) denote a set of abstract sensor channels.
Each sensor channel \(c \in {C}\) corresponds to a linearly 
independent observation subspace.

For each entity visibility in each channel must be defined,
as a trait this may be referred to as \(e^{(c)} \in \{\text{True, False}\}\).
It may be encoded in the state as:
\[
s_{tec} = 
\begin{cases}
    1& \text{if \(e\) is visible in \(c\)} \\ 
    0& \text{if \(e\) is not visible in \(c\)}
\end{cases} 
\quad\forall t\in T, e\in E, c\in C.
\]
Thus the marginal states becomes
\[s_{ti} :(c_{ti},d_{ti},\theta_{ti})\]
for agents, and
\[s_{te} :(c_{te},d_{te})\]
for all other entities.

Each agent \(i \in I\) is assigned a subset of sensor channels 
\({C}_i \subseteq {C}\), 
with their observation defined as the concatenation of the outputs 
from the corresponding subspaces:
Note that an agent's ability to perceive an entity within a given 
subspace may be limited by spatial range, meaning each agent may 
only observe a portion of the environment within each active channel.
\[
    o_{ti} := \bigoplus_{c \in {C}_i} o_{tic}
\]
where \(o_{tic}\) is agent \(i\)'s observation of channel \(c\) at time \(t\), 
and \(\oplus\) denotes concatenation over the ordered tuple of subspaces.

The global observation space \(O\) is the Cartesian product of all channel subspaces:
\[
    O := \prod_{c \in {C}} O_c
\]
and each agent's observation is a masked subset of this space.
During policy training and inference, a binary mask vector \(m_i \in \{0,1\}^{|\mathcal{C}|}\) 
is applied to indicate which subspaces are active for agent \(i\), 
allowing a shared network to condition on heterogeneous observations without assuming symmetry.


\paragraph{Action Space.} 
The joint action \(a \in A\) consists of marginal agent actions:
\[
    a := (a_1, \ldots, a_{|I|}) \in A := \prod_{i \in I} A_i
\]
Each agent's action at time \(t\) is a tuple:
\[
    a_{ti} := (a_{ti}^\text{interact}, a_{ti}^\text{move}, \Delta\theta_{ti})
\]
where:
\begin{itemize}
    \item \(a_{ti}^\text{interact} \in \{0,1\}\) toggles interaction,
    \item \(a_{ti}^\text{move} \in M_i \subseteq \mathbb{F}\) denotes movement magnitude,
    \item \(\Delta\theta_{ti} \in \prod_{n=1}^{N-1} \Delta\Theta_i^{(n,n+1)}\) encodes orientation adjustments.
\end{itemize}

% \paragraph{Exogenous Information.} 
% Agents observe a partially observable state with complete reward knowledge. 
% Probabilistic channel observability is future work.

\paragraph{Transition Function.} 
Deterministic and synchronous:
\[
    s_{t+1} = \mathcal{T}(s_t, a_t) \quad \text{or} \quad \mathcal{T}(a_t \mid s_t) = s_{t+1}.
\]
Provided that the actions chosen by the agents are valid, they are executed.

\paragraph{Objective Function.} 
The cumulative reward is defined as:
\[
    \max \sum_{t \in T} \left[
        \sum_{i \in I, j \in J} r_j \cdot \mathbb{I}[\Xi]
        - \sum_{i \in I} r_i \cdot \mathbb{I}[a_{ti}^\text{interact} = 1]
        - \sum_{i \in I, k \in K} r_k \cdot \mathbb{I}[\|s_{ti} - s_{tk}\|_\infty \leq k^\text{range}]
    \right]
\]
where:
\begin{itemize}
    \item \(r_j \in \mathbb{R}_{>0}\) is the reward for collecting objective \(j \in J\),
    \item \(\mathbb{I}[\Xi]\) encodes conditions under which \(j\) is valid for collection e.g.
    distance threshold, orientation requirement, or agent interaction flag,
    \item \(r_i \in \mathbb{R}_{<0}\) is the cost for agent \(i \in I\) to interact,
    \item \(r_k \in \mathbb{R}_{<0}\) is a penalty for agent \(i\) being near hazard \(k \in K\).
\end{itemize}

\subsection{Implementation}
The environment model described here reflects the full generality of 
the underlying framework, which was developed as a configurable 
multi-agent environment independent of the present study. 
For the experiments in this paper, we use a restricted instantiation 
\(M_i \subseteq \mathbb{Z}\) 
Several components of the formal modelsuch as general N-dimensional orientation, 
continuous movement magnitudes, and broad state definitionsare therefore 
supersets of the configuration actually used in our evaluations.


\clearpage
% [\st{yes} / \st{no}]
% [\st{yes} / \st{partially} / \st{no} / \st{NA}]
\section{Reproducibility Checklist for JAIR}

Select the answers that apply to your research -- one per item. 

\subsection*{All articles:}

%\hh{revised for stylistic consistency:}
\begin{enumerate}
    \item All claims investigated in this work are clearly stated. \newline
    % #OPT: Explicit summary of claims like
    [\bf{yes} / \st{partially} / \st{no}]
    \item Clear explanations are given how the work reported substantiates the claims. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item Limitations or technical assumptions are stated clearly and explicitly. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item Conceptual outlines and/or pseudo-code descriptions of the AI methods introduced 
    in this work are provided, and important implementation details are discussed. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item 
    Motivation is provided for all design choices, including algorithms, implementation choices, 
    parameters, data sets and experimental protocols beyond metrics. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
\end{enumerate}

\subsection*{Articles containing theoretical contributions:}
Does this paper make theoretical contributions? % [yes/no]
[\bf{yes} / \st{no}]

If yes, please complete the list below.

\begin{enumerate}
    \item All assumptions and restrictions are stated clearly and formally. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item All novel claims are stated formally (e.g., in theorem statements). \newline
    % #OPT: Add theorem
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item Proofs of all non-trivial claims are provided in sufficient detail
    to permit verification by readers with a reasonable degree of expertise
    (e.g., that expected from a PhD candidate in the same area of AI). \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item Complex formalism, such as definitions or proofs,
    is motivated and explained clearly. \newline
    %Proof sketches or intuitions are given for complex and/or novel results.
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item The use of mathematical notation and formalism serves the purpose of 
    enhancing clarity and precision; gratuitous use of mathematical formalism 
    (i.e., use that does not enhance clarity or precision) is avoided. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item Appropriate citations are given for all non-trivial
    theoretical tools and techniques. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
\end{enumerate}

\subsection*{Articles reporting on computational experiments:}
Does this paper include computational experiments? %[yes/no]
[\bf{yes} / \st{no}]

If yes, please complete the list below.
\begin{enumerate}
    \item 
    All source code required for conducting experiments is included in an online appendix 
    or will be made publicly available upon publication of the paper.
    The online appendix follows best practices for source code readability 
    and documentation as well as for long-term accessibility. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item The source code comes with a license that 
    allows free usage for reproducibility purposes. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item The source code comes with a license that
    allows free usage for research purposes in general. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item 
    Raw, unaggregated data from all experiments is included in an online appendix 
    or will be made publicly available upon publication of the paper.
    The online appendix follows best practices for long-term accessibility. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item The unaggregated data comes with a license that
    allows free usage for reproducibility purposes. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item The unaggregated data comes with a license that
    allows free usage for research purposes in general. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item If an algorithm depends on randomness, 
    then the method used for generating random numbers and for setting seeds is 
    described in a way sufficient to allow replication of results. \newline
    % [yes/partially/no/NA]
    [\st{yes} / \st{partially} / \st{no} / \bf{NA}]
    \item The execution environment for experiments, the computing infrastructure 
    (hardware and software) used for running them, is described, 
    including GPU/CPU makes and models; amount of memory (cache and RAM); 
    make and version of operating system; names and versions of relevant software 
    libraries and frameworks. 
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item 
    The evaluation metrics used in experiments are clearly explained and 
    their choice is explicitly motivated. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item 
    The number of algorithm runs used to compute each result is reported. \newline
    % [yes/no]
    [\bf{yes} / \st{no}]
    \item 
    Reported results have not been ``cherry-picked'' by silently ignoring unsuccessful 
    or unsatisfactory experiments. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item 
    Analysis of results goes beyond single-dimensional summaries of performance 
    (e.g., average, median) to include measures of variation, confidence, or 
    other distributional information. \newline
    % [yes/no]
    [\bf{yes} / \st{no}]
    \item 
    All (hyper-) parameter settings for the algorithms/methods used in experiments 
    have been reported, along with the rationale or method for determining them. \newline
    % [yes/partially/no/NA]
    [\bf{yes} / \st{partially} / \st{no} / \st{NA}]
    \item 
    The number and range of (hyper-) parameter settings explored prior to 
    conducting final experiments have been indicated, along with the effort spent on 
    (hyper-) parameter optimization. \newline
    % [yes/partially/no/NA]
    [\bf{yes} / \st{partially} / \st{no} / \st{NA}]
    \item 
    Appropriately chosen statistical hypothesis tests are used to establish 
    statistical significance in the presence of noise effects. \newline
    % [yes/partially/no/NA]
    [\bf{yes} / \st{partially} / \st{no} / \st{NA}]
\end{enumerate}


\subsection*{Articles using data sets:}
Does this work rely on one or more data sets 
(possibly obtained from a benchmark generator or similar software artifact)? 
% [yes/no]
[\st{yes} / \bf{no}]

% If yes, please complete the list below.
% \begin{enumerate}
%     \item 
%     All newly introduced data sets 
%     are included in an online appendix 
%     or will be made publicly available upon publication of the paper.
%     The online appendix follows best practices for long-term accessibility with a license
%     that allows free usage for research purposes.
%     [yes/partially/no/NA]
%     \item The newly introduced data set comes with a license that
%     allows free usage for reproducibility purposes.
%     [yes/partially/no]
%     \item The newly introduced data set comes with a license that
%     allows free usage for research purposes in general.
%     [yes/partially/no]
%     \item All data sets drawn from the literature or other public sources (potentially including authors' own previously published work) are accompanied by appropriate citations.
%     [yes/no/NA]
%     \item All data sets drawn from the existing literature (potentially including authors own previously published work) are publicly available. [yes/partially/no/NA]
%     %\item All data sets that are not publicly available are described in detail.
%     %[yes/partially/no/NA]
%     \item All new data sets and data sets that are not publicly available are described in detail, including relevant statistics, the data collection process and annotation process if relevant.
%     [yes/partially/no/NA]
%     \item 
%     All methods used for preprocessing, augmenting, batching or splitting data sets (e.g., in the context of hold-out or cross-validation)
%     are described in detail. [yes/partially/no/NA]
% \end{enumerate}

\subsection*{Explanations on any of the answers above (optional):}

% [Text here; please keep this brief.]


\end{document}
\endinput