\documentclass[]{jair}
% \documentclass[manuscript, screen, review]{jair}
\usepackage[capitalize]{cleveref}
% Fix cref appendix error
\AddToHook{cmd/appendix/before}{\crefalias{section}{appendix}}
\usepackage{enumitem} 


%% For JAIR Checklist:
\usepackage{soul}                   % Strikeout with \st{}
\renewcommand{\bf}[1]{\textbf{#1}}  % Bold with \bf{}

\setcopyright{cc}
\copyrightyear{2025}
\acmDOI{10.1613/jair.1.xxxxx}

%%
\JAIRAE{JAIR AE Name}
\JAIRTrack{} % Insert JAIR Track Name only if part of a special track
\acmVolume{1}
\acmArticle{1}
\acmMonth{1}
\acmYear{2030}

\RequirePackage[
  datamodel=acmdatamodel,
  style=acmauthoryear,
  backend=biber,
  giveninits=true,
  uniquename=init,
  ]{biblatex}

\renewcommand*{\bibopenbracket}{(}
\renewcommand*{\bibclosebracket}{)}

% #FINAL: Bibliography link
\addbibresource{../2025Bibs/Prospectus.bib}

\author{Brandon Hosley}
\orcid{0000-0002-2152-8192}
\email{brandon.hosley.1@af.au.edu}
\affiliation{%
  \institution{Air Force Instute of Technology}
  \city{Wright-Patterson AFB}
  \state{Ohio}
  \country{USA}
}
\author{Bruce Cox}
\orcid{0000-0003-0149-1836}
\email{bruce.cox@af.au.edu}
\affiliation{%
  \institution{Air Force Instute of Technology}
  \city{Wright-Patterson AFB}
  \state{Ohio}
  \country{USA}
}
\author{Nicholas Yielding}
% \orcid{0009-0002-1978-6018} %Not enough information to confirm
\email{Nicholas.Yielding@afit.edu}
\affiliation{%
  \institution{Air Force Instute of Technology}
  \city{Wright-Patterson AFB}
  \state{Ohio}
  \country{USA}
}
\author{Matthew Robbins}
\orcid{0000-0002-1718-6839}
\email{Matthew.Robbins@afit.edu}
\affiliation{%
  \institution{Air Force Instute of Technology}
  \city{Wright-Patterson AFB}
  \state{Ohio}
  \country{USA}
}
\renewcommand{\shortauthors}{Hosley, Cox, Yielding, \& Robbins}

\usepackage[nomain,symbols,abbreviations]{glossaries-extra}
\makeatletter
\glsdisablehyper
\makeatother
\newabbreviation{rl}{RL}{reinforcement learning}
\newabbreviation{harl}{HARL}{heterogeneous-agent reinforcement learning}
\newabbreviation{marl}{MARL}{multi-agent reinforcement learning}
\newabbreviation{ppo}{PPO}{proximal policy optimization}
\newabbreviation{mappo}{MAPPO}{multi-agent \gls{ppo}}
\newabbreviation{happo}{HAPPO}{heterogeneous-agent \gls{ppo}}
\newabbreviation{ctde}{CTDE}{centralized training with decentralized execution}
\newabbreviation{posg}{POSG}{partially observable stochastic game}
\newabbreviation{dec-pomdp}{Dec-POMDP}{decentralised partially observable Markov decision process}
% \newabbreviation{lbf}{LBF}{Level-based Foraging}
% \newabbreviation{mpe}{MPE}{Multi Particle Environments}
% \newabbreviation{gnn}{GNN}{graph neural network}
% \newabbreviation{mlp}{MLP}{mulit-layer perceptron}
% \newabbreviation{maddpg}{MADDPG}{multi-agent deep-deterministic policy gradient}
% \newabbreviation{sisl}{SISL}{Stanford Intelligent Systems Laboratory}
% \newabbreviation{coma}{COMA}{counterfactual multi-agent policy gradients}
% \newglossaryentry{q}{text=q,name=Q,sort=q,type=symbols,category=symbol,
%     description={Set of state-values or state-value functions.}}
% #TODO: Change math to glossary symbols
\newglossaryentry{s}{text=s,name=S,sort=s,type=symbols,category=symbol,
    description={State.}}
\newglossaryentry{o}{text=o,name=O,sort=o,type=symbols,category=symbol,
    description={Observation.}}
\newglossaryentry{a}{text=a,name=A,sort=a,type=symbols,category=symbol,
    description={Action.}}
\newglossaryentry{r}{text=r,name=R,sort=r,type=symbols,category=symbol,
    description={Reward.}}
\newglossaryentry{p}{text=p,name=P,sort=p,type=symbols,category=symbol,
    description={Transition probability.}}
\newglossaryentry{discount}{text=\ensuremath{\gamma},name=\ensuremath{\gamma},sort=\ensuremath{\gamma},type=symbols,category=symbol,
    description={Discount factor.}}
\newglossaryentry{pi}{text=\ensuremath{\pi},name=\ensuremath{\pi},sort=\ensuremath{\pi},type=symbols,category=symbol,
    description={Policy.}}
\newglossaryentry{i}{text=i,name=I,sort=i,type=symbols,category=symbol,
    description={Set of agents \(i\).}}


% \title[]{Homogenization for Input-Invariant Solutions for Heterogeneous Agent Reinforcement Learning}
\title{Homogenization for Input-Invariant Solutions for Heterogeneous Agent Reinforcement Learning}
% TODO: For implicit agent embedding

\begin{document}

% Structured Abstract
\begin{abstract}
    {\bf Background:} MARL with heterogeneous agents; limits of standard parameter sharing and ad-hoc agent indication.\\
    {\bf Objectives:} Introduce and analyze homogenized observation spaces and homogenized action spaces that enable input-invariant policies across heterogeneous agents.\\
    {\bf Methods:} Formal definition of a covering (homogenized) observation space with validity masks and implicit agent indication; theoretical results on representability and invariance; empirical evaluation on $n$-D gridworld variants with MAPPO control.\\
    {\bf Results:} Comparable or improved robustness under sensor degradation and team-size variation; small memory/runtime advantages in some regimes; limitations when coherence assumptions fail.\\
    {\bf Conclusions:} Homogenization is a practical alternative to parameter sharing when a coherent spanning space exists; we outline conditions, trade-offs, and open questions.
\end{abstract}

% #FINAL: Submission date
\received{DD Month YYYY}
% \received[accepted]{DD Month YYYY}
\maketitle

\section{Introduction}

\Gls{marl} addresses coordination among multiple decision 
makers operating under partial observability. 
Standard approaches such as parameter sharing and
\gls{ctde} have enabled strong performance in many cooperative domains
\cite{albrecht2024, christianos2021}, 
but these methods typically assume that agents are interchangeable: 
they possess the same observation structure, action capabilities, and behavioral roles. 
This interchangeability assumption simplifies learning, 
but may become restrictive in settings where agents differ due to hardware constraints, 
sensing modalities, or mission specialization. 
In such cases, na\:{i}ve parameter sharing obscures the 
structural differences that matter for decision making, 
while fully separating policies prevents useful knowledge sharing across the team
\cite{albrecht2024, christianos2021, terry2020, canese2021}.

Heterogeneity is common in real-world multi-agent systems, where agents may differ 
in sensing, actuation, or operational roles due to hardware constraints 
or task specialization \cite{calvo2018, cao2012}. 
These structural differences complicate parameter sharing, generalization, and transfer, 
since policies must account for observation and action spaces that are not merely 
variations of a shared template but may differ in dimensionality and semantics \cite{canese2021}. 
As a result, methods that assume agent interchangeability often degrade when applied to 
heterogeneous teams, motivating representations that preserve agent-specific structure 
while still enabling shared learning \cite{christianos2021, albrecht2024}.

An effective approach is to retain shared network parameters but append explicit agent identifiers
or type embeddings to the input \cite{terry2020, christianos2021}. 
This strategy allows a network to distinguish among agents, 
but it does not resolve structural heterogeneity. 
When agents differ in the dimensionality or semantics of their observations or actions, 
explicit identifiers must be coupled with padding, duplication, or handcrafted encodings. 
Such fixes increase representational overhead and can obscure the information that truly 
distinguishes agents.

We propose an alternative approach based on \emph{implicit indication}. 
Rather than appending explicit identity features, we construct homogenized 
observation and action spaces that jointly span all agent-specific subspaces. 
Each agent's individual capabilities correspond to a distinct subset within 
these shared domains, realized by leaving unused elements empty or null. 
Policies are trained over the complete domain, 
learning to condition implicitly on which elements are populated. 
Differences among agents therefore arise naturally from the structure of 
their accessible subsets, eliminating the need for separate identifiers. 
This construction yields an input-invariant policy class;
a single network that generalizes across heterogeneous teams, 
remains robust to degraded or partial observations, 
and can be transferred to new configurations without architectural change.

Practically, this relies on \emph{semantic decomposability}: 
that the observation and action spaces of different agents can be factorized 
into elements that correspond meaningfully across agents, 
such that spanning these elements preserves task-relevant semantics without aliasing. 
Equivalently, it requires \emph{consistent element correspondence}: 
the observation and action elements can be defined and matched across agents in a consistent, 
semantically meaningful way.

Applicability hinges on semantically aligned elements across agents; when present, 
homogenization provides a lightweight basis for shared learning, and when 
absent it may function only as a representational scaffold without reliable transfer.

The paper contributes three complementary advances. 
First, it formalizes \emph{implicit indication} as a homogenization-based construction that 
enables shared policy parameters across heterogeneous agents without explicit identifiers. 
Second, it establishes theoretical conditions under which such constructions preserve 
representability and analyzes the role of semantic decomposability in maintaining policy validity. 
Finally, it provides empirical evidence using a configurable heterogeneous-agent 
environment\footnote{
    Code availability---The custom multi-agent environment (HyperGrid) 
    and all training/evaluation scripts used in this study are available at 
    \href{https://github.com/bhosley/Hypergrid}{\texttt{https://github.com/bhosley/Hypergrid}}.
}, 
comparing the proposed approach against \gls{happo}~\cite{zhong2024} and 
evaluating robustness to sensor dropout, team-size variation, and composition changes. 
Together, these results position homogenization as a lightweight mechanism 
for parameter sharing in \gls{harl}.

We next situate this formulation relative to prior work on parameter sharing, 
agent indication, and invariant architectures.



\section{Related Work}
\label{con2:sec:related_work}

Enabling shared learning across heterogeneous agents 
presents a continuing challenge in \gls{marl}.
Existing research addresses this challenge through three main paradigms: 
parameter-sharing mechanisms, invariant and equivariant policy architectures, 
and policy-optimization algorithms designed for heterogeneous settings. 
Homogenization offers a complementary framework that integrates these 
paradigms by providing a shared representational domain in which 
heterogeneous agents can be trained under a unified policy structure.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Parameter Sharing and Agent Indication}

Parameter sharing has been highly effective in homogeneous \gls{marl},
where agents have identical observation and action spaces and are intended
to learn interchangeable behaviors~\cite{foerster2016, gupta2017}.
In these settings, a single shared policy improves sample efficiency and
can promote coordinated behavior without additional architectural machinery.
However, this benefit relies critically on structural symmetry:
once agents differ in sensing, actuation, or role, parameter sharing
no longer applies straightforwardly, and consequently most heterogeneous-agent
systems instead train separate policies for each agent or agent type.

To recover some of the efficiency benefits of sharing, 
prior work has introduced \emph{explicit agent indication};
supplying the policy with features that distinguish among agents.
One approach is to use learned type or role embeddings~\cite{christianos2021, albrecht2024}, 
while Terry et al.~\cite{terry2020} evaluated encodings; 
such as geometric masks, binary indicators, and inversion-based transforms; 
that are all functionally equivalent to providing a one-hot identity signal.
These approaches allow divergent behavior under shared parameters, 
but they still require manually injecting identity information and 
do not resolve underlying structural mismatches among agents.

% #TODO: Citation needed
% A core insight from recent literature is that when agents are symmetric or 
% play exchangeable roles, 
% enforcing invariance to permutations in agent inputs 
% and identities can significantly improve learning efficiency and generalization. 
% Input-invariant architectures, such as those based on pooling, graph networks, 
% or attention mechanisms, can leverage these symmetries directly in the 
% policy or critic network. 

% This result justifies the treatment of inputs that are unordered but statistically similar,
% such as agent observations in a cooperative \gls{marl} task, 
% as conditionally i.i.d. elements drawn from a shared distribution. 
% In practice, this means that the ordering of inputs does not convey meaningful information, 
% and architectures may (and perhaps should) be designed to respect this symmetry.

% As an example, in environments like \gls{lbf}~\cite{papoudakis2021}, 
% each agent's observation is constructed by concatenating their own state, 
% the state of objectives, and the states of other agents in a fixed but arbitrary order. 
% While the ordering is deterministic, it typically does not encode meaningful 
% semantic distinctions. In such settings, treating the input as an exchangeable set,
% rather than a structured sequence, aligns with the assumptions of de Finetti's theorem 
% and motivates the permutation-invariant design choices in the rest of this section.

% Permutation-invariant functions and networks (e.g., pooling operations or 
% attention mechanisms without positional encoding) align with this insight, %%
% embedding the inductive bias that order should not affect the outcome. 
% This forms the basis for the architectural designs reviewed in the following sections.

While explicit indication provides a mechanism for breaking the symmetry among agents, 
a complementary line of research seeks to exploit symmetry by designing policies that are invariant or equivariant to permutations in their inputs.
These approaches are motivated by exchangeability results in probability theory, which formalize conditions under which the ordering of agents or observed entities carries no semantic meaning.
When such symmetries are present, architectures that enforce permutation invariance can improve generalization, stabilize learning, and reduce the reliance on explicit identifiers.
We next review these invariant and equivariant architectures and contrast their inductive biases with those of explicit indication.


\subsection{Invariant and Equivariant Architectures}


A complementary direction arises from efforts to design policies that respect symmetries in their inputs or environments.  Works on permutation-invariant and equivariant representations, such as Deep Sets~\cite{zaheer2017} and graph neural network–based MARL architectures~\cite{yang2018, tang2021, li2021b, zambaldi2018}, seek to make learned policies invariant to reordering of agents or entities.
These architectures trace their theoretical motivation to exchangeability results in probability theory (e.g., de Finetti and mean-field approximations), which formalize conditions for consistent behavior under permutation.
Such approaches achieve invariance within symmetric teams but still rely on uniform input structure—each agent processes an equivalent set of features.
Homogenization, by contrast, achieves a form of subset-based input invariance that spans structurally diverse observation spaces, aligning the representational domain rather than the ordering of its elements.


Although not required for our formulation, exchangeability results such as 
de Finetti's theorem~\cite{diaconis1980} 
help clarify why permutation-invariant architectures benefit symmetric teams: 
they formalize conditions under which agents can be treated as statistically 
interchangeable~\cite{alvarez-melis2015, zaheer2017}.


% In their paper introducing Deep Sets, Zaheer et al.~\cite{zaheer2017} 
% present a parameter-sharing scheme inspired by \cref{eq:deFinnetti}, 
% which allows networks to handle unordered inputs by design.

% They propose two architectures to achieve this.
% For the invariant approach, they demonstrate that any 
% permutation-invariant function over a set can 
% be decomposed into a transformation (\(\rho\)) of a sum over 
% transformed (\(\phi\)) elements,
% \begin{equation*}
%     \rho\left(\sum_i \phi(x_i)\right)
% \end{equation*}
% providing a universal approximator for such functions. 

% Zaheer et al.~\cite{zaheer2017} also proposed an equivariant approach,
% providing a neural network layer of the form:
% \begin{equation*}
%     \mathbf{f}(\mathbf{x}) \doteq \mathbf\sigma (\lambda\mathbf{Ix} 
%     + \gamma \text{maxpool}(\mathbf{x})\mathbf{1}) 
% \end{equation*}
% where the weight matrix is constrained to the form 
% \(\lambda\mathbf{I} + \gamma\mathbf{11}^\top\), ensuring all 
% diagonal elements are equal and all off-diagonal elements are equal. 
% This construction guarantees permutation equivariance.
% The \(\sigma\) operation represents a non-linearity function (such as a sigmoid).
% The authors argue that this equivariant form is functionally equivalent to the 
% invariant form during inference; their distinction primarily emerges during backpropagation, 
% where the structure of gradient updates differs between the two.

% These structures dramatically reduce the effective symmetry 
% group of the input space, enabling better generalization and 
% sample efficiency in problems with intrinsic symmetries. 
% In \gls{marl}, these ideas naturally extend to agent sets: 
% policies or critics can process agents' states as elements 
% of an unordered set, enforcing invariance to permutations in agent ordering. 
% This supports scalable learning across varying team compositions 
% and aligns with the theoretical underpinnings of exchangeability laid out earlier.


% \subsection{Mean-Field and Exchangeable Approximations}

% In scenarios where agents are approximately indistinguishable, 
% a natural extension of Zaheer et al.'s~\cite{zaheer2017} 
% permutation-invariant formulation is to normalize the summation of elements. 
% Rather than summing over agent embeddings, one may average them,
% yielding permutation invariance \emph{and} insensitivity to the number of agents. 
% This refinement aligns closely with mean-field theory, which replaces complex 
% many-body interactions with an average effect.

% Yang et al.~\cite{yang2018} apply this idea in the context of \glsadd{marl}, 
% introducing mean-field Q-learning and actor-critic algorithms. 
% Their approach approximates the influence of neighboring agents with 
% an average embedding, enabling each agent to learn a local policy that 
% scales gracefully with team size. This is particularly 
% useful in settings where each agent interacts with many others, 
% but where the precise identity of those agents is irrelevant.

% Building on this, Li et al.~\cite{li2021b} approach mean-field \gls{marl} 
% problems formally as permutation-invariant Markov decision processes. 
% They propose Mean-Field Proximal Policy Optimization (MF-PPO), 
% a variant of PPO that incorporates mean-field assumptions into 
% both actor and critic networks. The resulting architecture maintains a 
% shared policy across agents, with inputs aggregated through mean pooling.

% Theoretically, MF-PPO achieves convergence to a global optimum at 
% a sublinear rate, and notably, its sample complexity is proven to be 
% independent of the number of agents. This highlights the power of 
% encoding exchangeability as an inductive bias: by exploiting the 
% ergodic structure of agent interactions, MF-PPO updates a shared policy 
% more efficiently than na\"ive multi-agent baselines.

% Empirically, MF-PPO outperforms non-invariant baselines in the 
% \gls{mpe}~\cite{li2021b}, demonstrating faster convergence, 
% higher average returns, and significantly reduced parameter counts. 
% This is largely attributed to weight sharing across agents and the 
% use of input-aggregation strategies that preserve permutation symmetry. 

% One significant downside to the examined mean-field approaches is that 
% information about subset interactions is lost in the process of 
% applying their respective commutative functions.
% % #TODO: Appendix proof
% % We argue that this is not a necessary limitation of mean-field methods,
% This limitation motivated more sophisticated invariant architectures like 
% graph-based networks and attention-based pooling, which we discuss next.

% % #TODO : Redundant from C3


% % Change this section as reflecting back to single agent applications
% \subsubsection{Relational Reasoning in Single-Agent RL}
% While much of the prior work focuses on multi-agent settings, 
% the benefits of invariant relations extend to single-agent tasks as well. 
% These architectures demonstrate that modeling inputs as a set of 
% interacting entities can improve both performance and generalization.

% Zambaldi et al.~\cite{zambaldi2018} showed that a single-agent \gls{rl} agent could 
% benefit from interpreting its observations as a graph of entities and relations. 
% Using a relation network with iterative message passing, their agent achieved 
% superhuman performance on several StarCraft II~\cite{vinyals2019} mini-games, 
% outperforming non-relational baselines by reasoning over unit interactions 
% rather than raw input features.

% In their single-agent formulation, Tang and Ha~\cite{tang2021} demonstrated 
% that treating sensory channels as interchangeable input elements enabled 
% the agent to maintain performance despite significant input corruption. 
% By passing each sensory input through a shared encoder and aggregating 
% with self-attention, the network learned to interpret observations based 
% on contextual relationships rather than fixed positions. 
% This design conferred resilience to occluded or missing inputs, 
% allowing the agent to function even when sections of the sensory 
% data were absent at test time. Their results suggest that invariance 
% to input structure can provide strong robustness to
% severe perceptual perturbations.

% % # TODO: May consider ending on Tang and Ha for similarity purposes.
% \begin{comment}
%     They propose a very similar question:
%         "Sensory substitution refers to the brain’s ability to use one sensory modality 
%         (e.g., touch) to supply environmental information normally gathered by 
%         another sense (e.g., vision)."
%     - Extremely similar to my interest in sensory degradation.
%     - However, in their experiments they instead demonstrate an invariance in a singular
%     observational domain. (They take a 2d image, chunk it and shuffle.)
% \end{comment}

\subsection{Handling Heterogeneity in MARL}

Several recent frameworks have explicitly targeted the challenges of heterogeneity in cooperative MARL.
Independent PPO (IPPO) and Multi-Agent PPO (MAPPO)\cite{yu2022} provide stable baselines for homogeneous teams but require architectural duplication to handle differing agent spaces.
Heterogeneous-Agent PPO (HAPPO)\cite{zhong2024} extends these ideas by enabling joint training across agent types while maintaining distinct policy heads, improving sample efficiency and role specialization.
Other works introduce selective sharing mechanisms or hypernetworks to generate per-agent parameters~\cite{hao2023, witt2020}.
These approaches primarily address heterogeneity through network architecture or parameterization, whereas the method introduced here addresses it through representation: homogenization creates a shared input–output space in which a single policy can operate across agents with structurally distinct capabilities.

% #TODO: IPPO
% #TODO: MAPPO

% \cite{witt2020}
% % propose IPPO, as an extension to PPO

% \cite{yu2022}
% % Compares IPPO and MAPPO
% % resolves the previous weakness with central/shared value function


% \cite{terry2020}
% % Directly address hetero
% % Provide 4 proposed solutions which all draw from transformed obs
% % either a geometric mask, binary indicator, inversion, or inversion with replacement

% \subsection{Heterogeneous Agents}
% In many \gls{marl} scenarios, agents are not identical, a setting formally studied 
% as \gls{harl}. Agents may differ in abilities, observation spaces, or roles. 
% Many input-invariant architectures assume that all inputs are drawn from the 
% same distribution and processed by a shared encoder, an assumption that breaks 
% down when agents have structurally different observations. In such cases, 
% invariant frameworks must be adapted to handle this heterogeneity, 
% either by mapping observations into a shared embedding space or by 
% augmenting the input with agent-specific identifiers.

% One practical strategy for supporting heterogeneity in input-invariant architectures 
% is to include an agent-specific feature encoding that captures type or 
% role~\cite{liu2020b,hao2022,hao2023}. As noted earlier, PIC~\cite{liu2020b}
% incorporates this by embedding attribute vectors into each node's representation, 
% preserving symmetry among similar agents while allowing differentiation where needed.

% Similarly, HPN~\cite{hao2023} uses a hypernetwork conditioned on 
% identity to generate specialized agent modules, enabling tailored policies 
% while maintaining permutation invariance at the architectural level. 
% This structure supports diversity among agents while preserving the 
% benefits of shared computation and symmetry where applicable.

% Such flexibility is especially valuable in mixed-agent teams and 
% competitive settings, where an agent may need to reason differently 
% about teammates and opponents. The architecture should remain invariant 
% when swapping two equivalent opponents but avoid conflating agents 
% with distinct roles.

% Despite the demonstrated advantages of invariant and equivariant architectures in handling 
% agent symmetry, scalability, and heterogeneity, their practical cost remains underexplored. 
% The reviewed methods, ranging from mean-field approximations and deep sets, 
% to graph-based and attention-driven critics share a common goal: to encode 
% structural priors that simplify learning in multi-agent settings. However, 
% these methods often increase architectural complexity, requiring additional computation, 
% memory, and tuning. Importantly, few studies offer direct comparisons of their 
% training or inference costs relative to simpler baselines such as \glspl{mlp}. 
% This gap complicates our understanding of the tradeoffs involved in deploying 
% more expressive models in real-world systems.

\subsection{Positioning of This Work}

This work bridges these strands by integrating invariant-design principles with heterogeneous-agent learning.
It formalizes implicit indication as a representation-level generalization of parameter sharing, grounded in the assumption of semantic decomposability: that observation and action spaces can be partitioned into elements that correspond meaningfully across agents.
In doing so, it extends the reach of invariant architectures beyond symmetric teams, enabling a single shared policy to generalize across heterogeneous agents without explicit identity encoding.

% In this work, we contribute to closing that gap by empirically evaluating the efficiency 
% and robustness of input-invariant policy architectures in heterogeneous-agent settings, 
% quantifying both their benefits and computational costs. In parallel, we 
% propose a novel exchangeable approximation framework that extends the principles 
% of input invariance to settings with observation and action heterogeneity. 
% By constructing a shared observation-action spanning space and integrating dynamic 
% masking within an equivariant network, we offer a lightweight and scalable 
% approach to handling agent diversity. Together, these contributions aim to 
% clarify not only when these architectures are helpful, 
% but how they might be practically and broadly applied.

% #TODO: Make sure paper about indicator variables is mentioned.

\section{Homogenization and Implicit Indication Method}
\label{con2:sec:method_homogenization}

\subsection{Preliminaries}
The homogenization approach constructs a unified space defined by the union set 
of the elements that comprise the heterogeneous spaces of the constituent agents.
This can be done for action spaces, observation spaces, or both independently.

This construction enables parameter sharing without requiring explicit agent identifiers, 
while still preserving the structural distinctions that arise from heterogeneity. 
In what follows, we define the homogenized spaces, describe the role of validity masks, 
and outline the resulting invariant policy class.

To formally represent the cooperative multi-agent setting,
we model the environment as a \gls{dec-pomdp}
defined by the tuple \((I,S,O,A,P,R,\gamma)\) where:
%
\begin{description}[labelindent=2em, labelwidth=5em, itemsep=3pt, topsep=8pt]
    \item[\(\Gls{i}\)] : Set of all agents
    \item[\(\Gls{s} = \{S_i\}_{i\in I}\)] : Joint state space
    \item[\(\Gls{o} = \{O_i\}_{i\in I}\)] : Joint observation space
    \item[\(\Gls{a} = \{A_i\}_{i\in I}\)] : Joint action space
    \item[\(\Gls{p}(s^\prime|s,a)\)] : Transition probability function
    \item[\(\Gls{r} = \{R_i\}_{i\in I}\)] : Joint rewards
    \item[\gls{discount}] : Reward discount factor.
\end{description}
%
When combined with a joint policy \(\gls{pi} = \{\pi_i\}_{i\in I}\) we have a \gls{posg}.

\subsection{Decomposition of Observations}
For some agent \(i\), consider its observation space \({O}_i\).
The proposed homogenization method assumes that these spaces are 
decomposable into salient subspaces, referred to as \emph{observation elements}.
Intuitively, each distinct sensor corresponds to an observation element, 
and the combination of such elements forms the agent's complete observation.

Formally, we define this relationship as
\[{O}_i = \bigcup_{c \in C_i} {O}_c,\]
where \(C_i\) denotes the set of observation elements available to agent \(i\).
That is,
\(c\in[0,1] \forall c\in C_i\)
and
\(C := \bigcup_{i\in I} C_i\).

Although the observation elements \(\{O_c\}_{c \in C}\) are linearly independent
in representation, they are not necessarily independent in their values.
For example, a thermal camera and a visual-spectrum camera each define distinct representational
subspaces, yet their outputs are correlated when observing the same physical region.
Conversely, elements such as GPS and a vision-based sensors are not only independent
in representation but also largely uncorrelated in the values they produce.
The set \({C}\) therefore enumerates all observation elements that the implementation
explicitly acknowledges, encompassing both correlated and uncorrelated channels.

Thus the homogenized observation space is defined as:
\[O^\prime := \bigcup_{c \in {C}} O_c,\]
which spans the union of all agent observation subspaces.

\subsection{Homogenized Actions}
The action spaces of agents can be unified using the same 
construction and assumptions described for observations. 

Formally, the homogenized action space is defined as the union 
of all individual agent action spaces:
\[
    A' := \bigcup_{i \in I} A_i.
\]
This spanning set provides a shared representational basis for 
policy outputs across heterogeneous agents.

For single-dimensional action domains such as 
continuous, discrete, binary, or multi-binary spaces;
the shared output can be specialized for each agent through bounding or masking operations. 
In the case of composite or multi-discrete action spaces, 
similar constraints may be applied independently to each action element. 
However, when dependencies exist among these elements, 
such non-independent constraints may further increase the suboptimality of the resulting policy.


\subsection{Implicit Indication}
The implicit-indication mechanism serves as the bridge between
heterogeneous agents and a shared policy function.
Rather than embedding a learned identity vector or categorical index within the policy input,
each agent is characterized implicitly by its observable structure.
In this framework, the policy network remains agnostic to agent identity and
instead infers relevant distinctions through the pattern of active channels in the input.

Formally, the homogeneity is implicitly represented by a \((C_i,\mathcal{A}_i)\)
where \(\mathcal{A}_i\) represent's agent \(i\)'s observation and action elements,
without explicit or arbitrary identifiers.

This construction ensures that differences among agents arise naturally 
from their accessible subspaces. Two agents with identical active 
channels are functionally equivalent under the policy, while agents that differ 
in their available channels are automatically distinguished by their mask pattern. 
The policy therefore learns mappings conditioned on observable structure, 
rather than on arbitrary agent labels.


By decoupling the agent's structural identity from the policy representation, 
implicit indication preserves input invariance and supports generalization 
to unseen compositions. New agents can be introduced by specifying 
novel mask configurations within the same spanning space, 
enabling flexible team reconfiguration and reuse of shared parameters without retraining.

In this sense, homogenization induces an invariant input domain across agents: each policy operates on the same spanning space, with heterogeneity expressed only through validity masks that delimit each agent’s accessible subset.

A natural limitation of this approach is that it does not by itself facilitate
emergent or behavioral heterogeneity; rather, it provides a
unifying representational basis for maximizing shared information across agents.























% #TODO: Provide full derivation?
% Proof sketches are provided for intuition; full derivations are deferred to the appendix.

\section{Theoretical Results}
\label{con2:sec:theory}


This section establishes conditions under which the homogenization 
approach preserves representability, ensures invariance properties, 
and clarifies the role of the coherence assumption. 


\subsection{Representability under Disjoint Union}

A key concern is whether a single policy over the homogenized space can
represent the optimal per-agent policies of the heterogeneous system.








By Lemma 1 \cite{terry2020},
we have that 
optimal policies
for heterogeneous agents remain learnable
even with shared parameters
when provided with 
disjoint observation spaces.

Terry et al. accomplish the disjoint
observation space through the inclusion
of an \emph{agent indicator} variable.




% Corollary

\(c \in [0,1] \forall c \in C\)






% #TODO: Revisit lemma 1

% Lemma 1 (Disjoint Union Policy).
If \(\{{O}_i\}_{i \in I}\) are disjoint, then there exists a policy \(\pi^*\)
over the disjoint union \(\biguplus{i \in I} \Omega_i\) such that for each agent \(i\),
the restriction of \(\pi\) to \(\Omega_i\) is equivalent to the agent's optimal policy 
\(\pi_i\).

% Proof sketch. Construct a union space \Omega^\uplus with an 
% indicator for the originating subspace. Define \pi^(o) = \pi_i^(o) whenever o \in \Omega_i.
% Since the subspaces are disjoint, \pi^* reproduces the per-agent policies exactly. \square

% This lemma underpins the homogenization strategy: 
% if embeddings are injective, the same representability result applies in the homogenized space.

% #TODO: Implicit indication forgoes the possibility of divergent behavior for like agents
% ⸻

% 5.2 Representability with Masked Homogenization

% We extend the argument to the case where agent observations are embedded into a shared spanning space O with masks.

% Theorem 1 (Masked Representability).
% Let each embedding E_i: \Omega_i \to O map agent i’s observation into the homogenized space, paired with a mask m_i \in \{0,1\}^{|\mathcal{C}|}. If each (o, m_i) uniquely identifies o \in \Omega_i, then there exists a policy \pi^ over O \times \{0,1\}^{|\mathcal{C}|} whose restriction to (o, m_i) is equivalent to the agent’s optimal policy \pi_i^.

% Proof sketch. The embedding induces a bijection between \Omega_i and the masked subset of O. 
% Construct \pi^(o, m_i) = \pi_i^(E_i^{-1}(o)). 
% Since masks uniquely identify feasible subspaces, the homogenized policy recovers each optimal per-agent policy. \square

% This result shows that homogenization does not sacrifice representability, provided embeddings are coherent.

% ⸻

\subsection{Permutation Invariance}

% Because homogenization introduces channel-wise encoders and aggregation, 
% we can establish invariance guarantees.

% Proposition 1 (Permutation Invariance).
% If encoded channel representations are aggregated by a commutative operation 
% (e.g., sum, mean, or attention with symmetric keys), then \pi_\theta is invariant to permutations of active channels within an agent’s mask.

% Proof sketch. Let \{z_c\} be encoded features. A commutative aggregation f(\{z_c\}) satisfies f(\{z_{c_1}, z_{c_2}, \ldots\}) = f(\{z_{c_{\pi(1)}}, z_{c_{\pi(2)}}, \ldots\}) for any permutation \pi. Since the downstream policy depends only on f(\{z_c\}), outputs are permutation-invariant. \square

% This property ensures that policies are robust to channel ordering and supports scalability across varying team compositions.

% ⸻

% 5.4 Robustness to Dropout

% The homogenized representation also supports resilience to missing inputs.

% Corollary 1 (Dropout Robustness).
% Suppose aggregation is mean pooling. If a channel c is removed (i.e., m_{ic} is set to 0), the resulting embedding differs from the full-channel embedding by at most \frac{1}{|\mathcal{C}_i|}\|z_c\|.

% Thus, performance degrades smoothly with the loss of channels, rather than catastrophically.

% ⸻

% 5.5 The Coherence Assumption

% The preceding results rely on an implicit assumption: that embeddings into the homogenized space preserve the semantics of the task.

% Definition 1 (Coherence).
% A homogenized embedding is coherent if, for each agent i, the mapping E_i:\Omega_i \to O together with mask m_i is a sufficient statistic for decision making in the POSG.

% Intuitively, coherence requires that the spanning space separates distinct information without aliasing. Failure of coherence arises when two channels overlap in semantics (e.g., redundant encodings) or when masks do not adequately distinguish agent types. In such cases, the homogenized policy may not faithfully reproduce optimal behavior.

% ⸻

% 5.6 Summary

% These results establish that homogenization preserves representability (Theorem 1), provides invariance guarantees (Proposition 1), and introduces robustness to missing inputs (Corollary 1). The primary limitation is the coherence assumption, which determines whether homogenization is valid in a given domain. This assumption must therefore be evaluated by practitioners when applying the method.






















\section{Methodology}
\label{con2:sec:methodology}



\subsection{Objective}

% #TODO: Add appendix reference to full environment description
% \cref{con2:app:env_model}
% #TODO: This appears to be experiment objective, not env objective
The objective is to train a shared policy
\[\pi_\theta : O \times \{0,1\}^{|\mathcal{C}|} \to \Delta(A^\mathrm{span}),\]
that maximizes expected return under the \gls{posg} dynamics. 
By construction, \(\pi_\theta\) is input-invariant: 
it operates over the same universal domain for all agents, 
with heterogeneity resolved via masks. Robustness is assessed along three axes:
%     1. Channel robustness - resilience to missing or degraded observation channels.
%     2. Team robustness - stability under changes in the number of agents.
%     3. Composition robustness - adaptability to shifts in the distribution of agent types.

This problem formulation provides the foundation for the homogenization 
method and subsequent theoretical analysis.

\subsection{Input-Invariant Policy Class}

The homogenized policy is defined as
\[
    \pi_\theta: O \times \{0,1\}^{|\mathcal{C}|} \to \Delta(A^\mathrm{span}),
\]
mapping embedded observations into distributions over the spanning action set.
In practice, the network comprises three parts:
% #TODO: Revise the network description
% 	1.	Channel encoders: Each O_c is passed through a channel-specific encoder shared across all agents.
% 	2.	Aggregation: Encoded channels are pooled by a commutative operation (e.g., sum, mean, attention), ensuring invariance to the ordering of active channels.
% 	3.	Action head with masking: The pooled embedding is mapped to action logits over A^\mathrm{span}, with validity masks applied to exclude infeasible actions.

This design enforces permutation invariance over channels and provides a 
consistent mapping from heterogeneous inputs to a shared action domain. 
The use of masks ensures that structural heterogeneity is preserved without 
requiring per-agent specialization.

\subsection{Training}

The training follows the \gls{ctde} paradigm. 
All agents share a common policy network parameterized by \(\theta\).
The specific parameter values are provided in~\cref{app:hyperparameters}.

A centralized critic, such as that used in \gls{mappo}, is employed for variance reduction, 
though the method is agnostic to the choice of critic. %% # What?
Training loss functions incorporate the validity masks to ensure that gradients 
are computed only with respect to feasible observations and actions.


\subsection{Practical Considerations}

% #TODO: Revise these considerations. They are close, but warrant more precise descriptions
The homogenization framework introduces several implementation details:
\begin{itemize}
    \item Initialization: Care must be taken to prevent masked channels from destabilizing
        training. Channel encoders should be initialized consistently across agents.
    \item Partial observability: Sensor degradation or dropout can be 
        modeled directly by modifying the mask \(m_i\) at inference time, without retraining.
    \item Composite actions: Hybrid discrete/continuous spaces can be accommodated by 
        treating each type as a separate unit dimension in \(A^\prime\).
    \item Scalability: The dimensionality of \(O^\prime\) and \(A^\prime\) grows with 
        the number of distinct observation and action elements.
        In practice, this growth is modest in structured domains,
        but it highlights the importance of evaluating the coherence of 
        the spanning construction.
\end{itemize}


---

6. Experimental Setup

6.1 Environments
	•	n-D gridworld family with configurable sensor channels (heterogeneous \(\mathcal{C}_i\)) and action heterogeneity; brief task description; pointers to appendix for details.
6.2 Baselines
	•	MAPPO (shared critic), IPPO (optional), ablation: parameter sharing + explicit ID (Terry-style) on matched encoders.
6.3 Architectures
	•	Shared encoder with channel-wise modules + pooling; masked action heads; critic variants.
6.4 Protocols
	•	Training budgets; seeds; eval regimes: (i) in-distribution, (ii) sensor dropout, (iii) team-size perturbation (± agents), (iv) composition shift (new \(\mathcal{C}_i\) mixes).
6.5 Metrics
	•	Reward vs agent-steps (your preferred cost metric), convergence sample-efficiency, robustness deltas, RAM/throughput snapshots.
6.6 Implementation details
	•	Framework (RLlib or equivalent), hyperparam ranges, masking specifics; reproducibility hooks (seeds, versions).




% All configurations are evaluated across multiple independent 
% training runs to account for stochastic variability.
% Learning efficiency is defined in terms of convergence rate 
% relative to total agent-training steps and computational cost.


- Approach
    - Implicit agent indication
        - Allows for complete enumeration of agent types
- Control
    - MAPPO (with shared critic)
        - Explain why using shared critic
- Model/Environment
    - nD Grid world
    - Differ to appendix for further description for now, pending fully separate paper

% % #TODO : Make more terse (in favor of exporting)
% \subsection{Environment Model}


% % #TODO : Update Environment and agent design
% \subsection{Environment and Agent Design}

% To evaluate agents with overlapping but non-identical observation capabilities, 
% we design a custom environment derived from the \gls{mpe}. 
% Each agent is equipped with a subset of sensor types, 
% and each sensor type defines a linearly independent subspace of the 
% full observation space. An agent's full observation vector is constructed by 
% concatenating the outputs of its sensors, and overlapping sensor types across 
% agents induce structured correlations.

% The agents are heterogeneous in both their observation and action spaces. 
% The union of all agent sensor types forms a global spanning observation space. 
% Likewise, the union of all agent action spaces defines a spanning action set, 
% from which agent-specific valid actions are masked during inference.


% \subsection{Perturbation Evaluation}

% To assess runtime robustness, trained agents are tested under two types of perturbations:

% \begin{itemize}
%     \item \textbf{Sensor degradation}: Agents experience random dropout 
%         of one or more observation channels during execution. 
%         The masking vector is updated accordingly to simulate partial observability.
%     \item \textbf{Team-size changes}: Agents are removed or added at test 
%         time without retraining. Policies are evaluated for stability and 
%         reward degradation in the new configuration.
% \end{itemize}

% Each condition is evaluated across multiple trials. 
% Policy stability is measured by the variance and degradation in episode rewards, 
% and recovery is assessed where applicable. 
% Final comparisons are made between the two architectures across all perturbation conditions.


Training \& Evaluation. 
We compare a heterogeneous baseline (\gls{happo}), 
a parameter-sharing baseline with explicit agent indication (MAPPO-share), 
and our homogenization method (shared actor over a masked spanning space with a centralized critic). 
Experiments vary team size \(N=4\) 
and four heterogeneity regimes
complete observation space,
overlap, intersecting, and disjoint-heavy—across 10 random seeds per condition. 
Training proceeds for a fixed budget of agent-steps (up to 500M, 5e8), 
and evaluation occurs every 100k agent-steps on held-out scenarios: 
in-distribution, sensor-dropout (removing 20-40\% of active channels), 
team-size shifts \((N\rightarrow N\pm)\), and composition shifts 
(resampled channel sets) including addition of cold-start agent types 
formed from unions of seen channels. We report learning efficiency 
via area-under-curve of return vs agent-steps, final return at budget, 
and robustness deltas relative to in-distribution performance. 
Statistical comparisons use Welch's t-tests with FDR control and Hedges' g effect sizes.



---

7. Results

7.1 Learning efficiency
	•	Convergence curves (mean ± CI) vs agent-steps; comparisons to MAPPO/IPPO; any RAM/runtime notes.
7.2 Robustness
	•	Sensor-dropout performance; ablations on which channels are removed; stability variance.
7.3 Team-size/Composition transfer
	•	Add/remove agents at test time; policy degradation and recovery behaviors.
7.4 Ablations
	•	Remove masks → performance drop; replace mask with explicit ID only → effect; different pooling ops; equivariant vs plain MLP.
7.5 Summary of trade-offs
	•	When homogenization wins/loses; interaction with coherence.


\section{Results}
\label{con2:sec:results}




- Training
    - did not have significant benefit on way or another
    - slight improvement in RAM usage
        - Insert table of diffs

- Evaluation
    - (Some combos of training v eval are redundant or unnecessary)
    - Overall Trends:
    - Complete Conf:
        - Greater robustness
            - From greater use of information? vs mappo
    - Intersecting:
    - Disjoint:
        - 


---

8. Discussion
•	Practical guidance: when the coherence assumption is plausible; how to design channels; pitfalls (aliasing, mis-specified spans).
•	Relation to parameter sharing: complementary tools; when to prefer which.
•	Limitations: emergent behavioral heterogeneity not directly modeled; partial support for heterogeneous action types; potential masking overhead.



---

\section{Conclusion}
\label{con2:sec:conclusion}

- Benefits
    - Allowing better than naive novel agents
    - Improved stability at deployment
- Limitations
    - Ours does not currently support emergent (behavioral) heterogeneity
    - This can be introduced as needed using a similar (arbitrary distinguishing variable)
- Future work
    - (While the current focus is on observation-space heterogeneity, 
    additional forms such as action-space heterogeneity may be considered in future extensions, 
    pending environment support.)

    
% \section{}
% \label{con2:sec:}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%           Other JAIR Items
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% \section{Acknowledgments}
% \nocite{*}
\printbibliography

% % --- End Dissertation Inclusion ---
\clearpage
\appendix

\section{Hyperparameters}
\label{app:hyperparameters}

For transparency and reproducibility, we report both 
(i) the final hyperparameters selected for our experiments, and 
(ii) the ranges explored during tuning. 
All tuning was conducted using \texttt{Ray Tune}~\cite{liaw2018}, 
which automatically schedules and evaluates candidate configurations under 
fixed computational budgets.

\subsection{Final Hyperparameters Used}
Table~\ref{tab:final-hparams} lists the hyperparameters used for the 
experiments reported in the main text. These were selected based on performance 
aggregated over tuning runs, with consistency checks across random seeds.

\begin{table}[H]
    \centering
    \caption{
        Final hyperparameters used for homogenization and baseline methods. 
        Values are reported as applied across all main experiments.
    }
    \label{tab:final-hparams}
    \begin{tabular}{l l l}
        \toprule
        \textbf{Parameter} & \textbf{Value} & \textbf{Notes} \\
        \midrule
        Learning rate & \(0.001\) & Selected via Ray Tune search \\
        Batch size & \(4096\) & Number of samples per PPO update \\
        PPO clip ratio & \(0.2\) & Standard PPO clipping parameter \\
        Value Function Clip & \(5\) & Clipping parameter for the value function \\
        GAE parameter $\lambda$ & \(0.95\) & Used in all methods \\
        Discount factor $\gamma$ & \(0.99\) & Applied across all methods \\
        Entropy coefficient & \(0.0\) & Exploration regularization \\
        Value loss coefficient & \(1.0\) & PPO value function weight \\
        Number of rollout workers & \(8\) & Vectorized env instances \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[H]
    \centering
    \caption{
        Final architecture specifications for the policy networks used in all experiments.
        Layer dimensions, activation functions, and convolutional parameters correspond to the 
        configuration selected after hyperparameter tuning and applied uniformly across methods.
    }
    \label{tab:policy_network}
    \begin{tabular}{l l}
        \toprule
        \textbf{Layer} & \textbf{Value} \\
        \midrule
        Image Conv. Layers & \([16, 32, 64]\) \\
        Image Conv. Kernel size & \([2,2]\) \\
        Img Enc. Activation Function & \(\text{relu}\) \\
        Dir Enc. Activation Function & \(\text{relu}\) \\
        Encoder Dimensions (Image, Direction) & \([256, 16]\) \\
        Trunk Layers & \([128, 64]\) \\
        Trunk Activation Function & \(\text{tanh}\) \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Hyperparameter Search Space}
Table~\ref{tab:hparam-search} summarizes the ranges considered during tuning.
We used the Asynchronous Successive Halving Algorithm (ASHA) scheduler \cite{li2018b}
with early stopping under a fixed budget of \(1e6\) agent-steps per trial. 
The hyperparameter search space was explored with
Optuna-search \cite{akiba2019}.

\begin{table}[H]
    \centering
    \caption{Hyperparameter ranges explored during tuning.}
    \label{tab:hparam-search}
    \begin{tabular}{l l l}
        \toprule
        \textbf{Parameter} & \textbf{Range / Options} \\
        \midrule
        Learning rate & \([5\mathrm{e}{-5}, 1\mathrm{e}{-3}]\) Log-uniform\\
        Batch size & \(\{4096, 8192, 16384\}\) \\
        PPO Clipping & \(\{0.1, 0.2, 0.3\}\) \\
        Value Function Clipping & \(\{5.0, 10.0\}\) \\
        GAE parameter $\lambda$ & \([0.90, 0.99]\) Uniform \\
        Discount factor $\gamma$ & \(\{0.95, 0.99\}\) \\
        Entropy coefficient & \(\{0.0, 0.005, 0.01, 0.02\}\) \\
        Number of epochs per update & \(\{2, 4, 6, 8\}\) \\
        \midrule
        \textbf{Network Parameters} & \\
        \midrule
        Image Conv. Channels & \(\{16, 32, 64\}\) \\
        Image Conv. Layers & \(\{[32], [32, 32], [32, 64], [64, 64], [16, 32, 64], [32, 64, 64]\}\) \\
        Image Conv. Kernel size & \(\{[2,2],[3,3]\}\) \\
        Encoder Dimensions (Image, Direction) & \(\{[128, 16], [128, 32], [256, 16], [256, 32]\}\) \\
        Trunk Layers & \(\{[64], [128], [128, 64], [256, 128, 64]\}\) \\
        Activation Functions & \(\{\text{relu, tanh}\}\) \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Environment Model}
\label{con2:app:env_model}

% We model the environment following the structure outlined by Powell~\cite{powell2022}, 
% consisting of the state, decision/action, exogenous information, transition function, and 
% objective function.

% \paragraph{State.} 
% Let \(T \subset \mathbb{N}\) be the set of decision epochs, 
% and define entities \(E := (I, J, K, L)\), where 
% \(i \in I\) are agents, 
% \(j \in J\) are objectives, 
% \(k \in K\) are hazards, and 
% \(l \in L\) are obstacles. 
% At each time \(t \in T\), the joint state \(s\in S\) is:
% \[
%     s_t := \{s_{te} \mid e \in E\}, \quad s_t \in S
% \]
% Where the marginal state for agents \(i \in I\), 
% includes both location and orientation:
% \[
%     s_{ti} := (d_{ti}, \theta_{ti})
% \]
% and the non-agent entities \(e \in (J, K, L)\) are represented by location alone:
% \[
%     s_{te} := (d_{te})
% \]
% For spatial dimensions \(N \in \mathbb{N}\), 
% let \(D^{(n)} \in \mathbb{F}\) be the domain of dimension \(n\).
% Then, we define each entity \(e\)'s location as a tuple:
% \[
%     d_{te} := \left(d_{te}^{(1)},\ldots,d_{te}^{(N)}\right) \in \prod_{n=1}^N D_e^{(n)},
% \]
% and orientation is represented by the tuple:
% \[
%     \theta_{ti} := \left(\theta_{ti}^{(1,2)},\ldots,\theta_{ti}^{(N-1,N)}\right)
%     \in \prod_{n=1}^{N-1} \Theta_i^{(n,n+1)}
% \]

% \paragraph{Observation Model.}
% Let \({C} := \{c_1, \ldots, c_m\}\) denote a set of abstract sensor channels.
% Each sensor channel \(c \in {C}\) corresponds to a linearly 
% independent observation subspace.

% For each entity visibility in each channel must be defined,
% as a trait this may be referred to as \(e^{(c)} \in \{\text{True, False}\}\).
% It may be encoded in the state as:
% \[
% s_{tec} = 
% \begin{cases}
%     1& \text{if \(e\) is visible in \(c\)} \\ 
%     0& \text{if \(e\) is not visible in \(c\)}
% \end{cases} 
% \quad\forall t\in T, e\in E, c\in C.
% \]
% Thus the marginal states becomes
% \[s_{ti} :(c_{ti},d_{ti},\theta_{ti})\]
% for agents, and
% \[s_{te} :(c_{te},d_{te})\]
% for all other entities.

% Each agent \(i \in I\) is assigned a subset of sensor channels 
% \({C}_i \subseteq {C}\), 
% with their observation defined as the concatenation of the outputs 
% from the corresponding subspaces:
% Note that an agent's ability to perceive an entity within a given 
% subspace may be limited by spatial range, meaning each agent may 
% only observe a portion of the environment within each active channel.
% \[
%     o_{ti} := \bigoplus_{c \in {C}_i} o_{tic}
% \]
% where \(o_{tic}\) is agent \(i\)'s observation of channel \(c\) at time \(t\), 
% and \(\oplus\) denotes concatenation over the ordered tuple of subspaces.

% The global observation space \(O\) is the Cartesian product of all channel subspaces:
% \[
%     O := \prod_{c \in {C}} O_c
% \]
% and each agent's observation is a masked subset of this space.
% During policy training and inference, a binary mask vector \(m_i \in \{0,1\}^{|\mathcal{C}|}\) 
% is applied to indicate which subspaces are active for agent \(i\), 
% allowing a shared network to condition on heterogeneous observations without assuming symmetry.


% \paragraph{Action Space.} 
% The joint action \(a \in A\) consists of marginal agent actions:
% \[
%     a := (a_1, \ldots, a_{|I|}) \in A := \prod_{i \in I} A_i
% \]
% Each agent's action at time \(t\) is a tuple:
% \[
%     a_{ti} := (a_{ti}^\text{interact}, a_{ti}^\text{move}, \Delta\theta_{ti})
% \]
% where:
% \begin{itemize}
%     \item \(a_{ti}^\text{interact} \in \{0,1\}\) toggles interaction,
%     \item \(a_{ti}^\text{move} \in M_i \subseteq \mathbb{F}\) denotes movement magnitude,
%     \item \(\Delta\theta_{ti} \in \prod_{n=1}^{N-1} \Delta\Theta_i^{(n,n+1)}\) encodes orientation adjustments.
% \end{itemize}

% % \paragraph{Exogenous Information.} 
% % Agents observe a partially observable state with complete reward knowledge. 
% % Probabilistic channel observability is future work.

% \paragraph{Transition Function.} 
% Deterministic and synchronous:
% \[
%     s_{t+1} = \mathcal{T}(s_t, a_t) \quad \text{or} \quad \mathcal{T}(a_t \mid s_t) = s_{t+1}.
% \]
% Provided that the actions chosen by the agents are valid, they are executed.

% \paragraph{Objective Function.} 
% The cumulative reward is defined as:
% \[
%     \max \sum_{t \in T} \left[
%         \sum_{i \in I, j \in J} r_j \cdot \mathbb{I}[\Xi]
%         - \sum_{i \in I} r_i \cdot \mathbb{I}[a_{ti}^\text{interact} = 1]
%         - \sum_{i \in I, k \in K} r_k \cdot \mathbb{I}[\|s_{ti} - s_{tk}\|_\infty \leq k^\text{range}]
%     \right]
% \]
% where:
% \begin{itemize}
%     \item \(r_j \in \mathbb{R}_{>0}\) is the reward for collecting objective \(j \in J\),
%     \item \(\mathbb{I}[\Xi]\) encodes conditions under which \(j\) is valid for collection,
%     \item \(r_i \in \mathbb{R}_{<0}\) is the cost for agent \(i \in I\) to interact,
%     \item \(r_k \in \mathbb{R}_{<0}\) is a penalty for agent \(i\) being near hazard \(k \in K\).
% \end{itemize}


% #TODO: Reproducibility Checklist
\clearpage
% [\st{yes} / \st{no}]
% [\st{yes} / \st{partially} / \st{no} / \st{NA}]
\section{Reproducibility Checklist for JAIR}

Select the answers that apply to your research -- one per item. 

\subsection*{All articles:}

%\hh{revised for stylistic consistency:}
\begin{enumerate}
    \item All claims investigated in this work are clearly stated. \newline
    % #TODO: Explicit summary of claims
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item Clear explanations are given how the work reported substantiates the claims. \newline
    % #TODO: Connect work to claims - intro or methods
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item Limitations or technical assumptions are stated clearly and explicitly. \newline
    % #TODO: Recheck limits and assumptions
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item Conceptual outlines and/or pseudo-code descriptions of the AI methods introduced 
    in this work are provided, and important implementation details are discussed. \newline
    % #TODO: Figure out what this is to meant to be
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item 
    Motivation is provided for all design choices, including algorithms, implementation choices, 
    parameters, data sets and experimental protocols beyond metrics. \newline
    % #TODO: This is an entire-paper concern. Ongoing need.
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
\end{enumerate}

\subsection*{Articles containing theoretical contributions:}
Does this paper make theoretical contributions? % [yes/no]
[\bf{yes} / \st{no}]

If yes, please complete the list below.

\begin{enumerate}
    \item All assumptions and restrictions are stated clearly and formally. \newline
    % #TODO: Recheck theoretical results section
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item All novel claims are stated formally (e.g., in theorem statements). \newline
    % #TODO: Add theorems
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item Proofs of all non-trivial claims are provided in sufficient detail
    to permit verification by readers with a reasonable degree of expertise
    (e.g., that expected from a PhD candidate in the same area of AI). \newline
    % #TODO: Recheck theoretical results section
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item Complex formalism, such as definitions or proofs,
    is motivated and explained clearly. \newline
    %Proof sketches or intuitions are given for complex and/or novel results.
    % #TODO: Recheck theoretical results section
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item The use of mathematical notation and formalism serves the purpose of 
    enhancing clarity and precision; gratuitous use of mathematical formalism 
    (i.e., use that does not enhance clarity or precision) is avoided. \newline
    % #TODO: Recheck theoretical results section
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item Appropriate citations are given for all non-trivial
    theoretical tools and techniques. \newline
    % #TODO: Recheck theoretical results section - Cite lemma 1
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
\end{enumerate}

\subsection*{Articles reporting on computational experiments:}
Does this paper include computational experiments? %[yes/no]
[\bf{yes} / \st{no}]

If yes, please complete the list below.
\begin{enumerate}
    \item 
    All source code required for conducting experiments is included in an online appendix 
    or will be made publicly available upon publication of the paper.
    The online appendix follows best practices for source code readability 
    and documentation as well as for long-term accessibility. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item The source code comes with a license that 
    allows free usage for reproducibility purposes. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item The source code comes with a license that
    allows free usage for research purposes in general. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item 
    Raw, unaggregated data from all experiments is included in an online appendix 
    or will be made publicly available upon publication of the paper.
    The online appendix follows best practices for long-term accessibility. \newline
    % #TODO: Add CSV to the git repo?
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item The unaggregated data comes with a license that
    allows free usage for reproducibility purposes. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item The unaggregated data comes with a license that
    allows free usage for research purposes in general. \newline
    % [yes/partially/no]
    [\bf{yes} / \st{partially} / \st{no}]
    \item If an algorithm depends on randomness, 
    then the method used for generating random numbers and for setting seeds is 
    described in a way sufficient to allow replication of results. \newline
    % [yes/partially/no/NA]
    [\st{yes} / \st{partially} / \st{no} / \bf{NA}]
    \item The execution environment for experiments, the computing infrastructure 
    (hardware and software) used for running them, is described, 
    including GPU/CPU makes and models; amount of memory (cache and RAM); 
    make and version of operating system; names and versions of relevant software 
    libraries and frameworks. 
    % #TODO: Add Hardware data
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item 
    The evaluation metrics used in experiments are clearly explained and 
    their choice is explicitly motivated. \newline
    % #TODO: Explain evaluation metrics
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item 
    The number of algorithm runs used to compute each result is reported. \newline
    % #TODO: Report run numbers
    [yes/no]
    % [\bf{yes} / \st{no}]
    \item 
    Reported results have not been ``cherry-picked'' by silently ignoring unsuccessful 
    or unsatisfactory experiments. \newline
    % #TODO: Explain superlatives chosen for analysis
    [yes/partially/no]
    % [\bf{yes} / \st{partially} / \st{no}]
    \item 
    Analysis of results goes beyond single-dimensional summaries of performance 
    (e.g., average, median) to include measures of variation, confidence, or 
    other distributional information. \newline
    % [yes/no]
    [\bf{yes} / \st{no}]
    \item 
    All (hyper-) parameter settings for the algorithms/methods used in experiments 
    have been reported, along with the rationale or method for determining them. \newline
    % [yes/partially/no/NA]
    [\bf{yes} / \st{partially} / \st{no} / \st{NA}]
    \item 
    The number and range of (hyper-) parameter settings explored prior to 
    conducting final experiments have been indicated, along with the effort spent on 
    (hyper-) parameter optimization. \newline
    % [yes/partially/no/NA]
    [\bf{yes} / \st{partially} / \st{no} / \st{NA}]
    \item 
    Appropriately chosen statistical hypothesis tests are used to establish 
    statistical significance in the presence of noise effects. \newline
    % #TODO: Results section stats
    [yes/partially/no/NA]
    % [\bf{yes} / \st{partially} / \st{no} / \st{NA}]
\end{enumerate}


\subsection*{Articles using data sets:}
Does this work rely on one or more data sets 
(possibly obtained from a benchmark generator or similar software artifact)? 
% [yes/no]
[\st{yes} / \bf{no}]

% If yes, please complete the list below.
% \begin{enumerate}
%     \item 
%     All newly introduced data sets 
%     are included in an online appendix 
%     or will be made publicly available upon publication of the paper.
%     The online appendix follows best practices for long-term accessibility with a license
%     that allows free usage for research purposes.
%     [yes/partially/no/NA]
%     \item The newly introduced data set comes with a license that
%     allows free usage for reproducibility purposes.
%     [yes/partially/no]
%     \item The newly introduced data set comes with a license that
%     allows free usage for research purposes in general.
%     [yes/partially/no]
%     \item All data sets drawn from the literature or other public sources (potentially including authors' own previously published work) are accompanied by appropriate citations.
%     [yes/no/NA]
%     \item All data sets drawn from the existing literature (potentially including authors’ own previously published work) are publicly available. [yes/partially/no/NA]
%     %\item All data sets that are not publicly available are described in detail.
%     %[yes/partially/no/NA]
%     \item All new data sets and data sets that are not publicly available are described in detail, including relevant statistics, the data collection process and annotation process if relevant.
%     [yes/partially/no/NA]
%     \item 
%     All methods used for preprocessing, augmenting, batching or splitting data sets (e.g., in the context of hold-out or cross-validation)
%     are described in detail. [yes/partially/no/NA]
% \end{enumerate}

\subsection*{Explanations on any of the answers above (optional):}

% [Text here; please keep this brief.]


\end{document}
\endinput