\documentclass{article}
\usepackage[english]{babel}
\usepackage{csquotes}       % Used by babel

\usepackage{amsmath}        % Math Typesetting
\usepackage{amssymb}        % Math Typesetting
% \usepackage{booktabs}
% \usepackage{hyperref}
% \usepackage{multirow}
\newtheorem{theorem}{Theorem}

\usepackage{graphicx}
\graphicspath{{Figures}{Data}}

\usepackage{tikz}
\usetikzlibrary{positioning,arrows}

\usepackage{cleveref}
\usepackage[backend=biber, style=ieee]{biblatex}
\addbibresource{../2025Bibs/Prospectus.bib}
% \usepackage[T1]{fontenc}
% \usepackage[final]{microtype}
\emergencystretch=1em % Allow more whitespace to avoid overfull h-boxes

\usepackage[nomain,symbols,abbreviations]{glossaries-extra}
% \makeglossaries
\newabbreviation{harl}{HARL}{heterogeneous-agent reinforcement learning}
\newabbreviation{marl}{MARL}{multi-agent reinforcement learning}
\newabbreviation{lbf}{LBF}{Level-based Foraging}
\newabbreviation{mpe}{MPE}{Multi Particle Environments}
\newabbreviation{gnn}{GNN}{graph neural network}
\newabbreviation{ctde}{CTDE}{centralized training with decentralized execution}
\newabbreviation{mlp}{MLP}{mulit-layer perceptron}
\newabbreviation{maddpg}{MADDPG}{multi-agent deep-deterministic policy gradient}
\newabbreviation{rl}{RL}{reinforcement learning}
% \newabbreviation{sisl}{SISL}{Stanford Intelligent Systems Laboratory}
\newglossaryentry{q}{text=q,name=Q,sort=q,type=symbols,category=symbol,
    description={Set of state-values or state-value functions.}}
\newabbreviation{coma}{COMA}{counterfactual multi-agent policy gradients}
\newabbreviation{ppo}{PPO}{proximal policy optimization}
\newabbreviation{happo}{HAPPO}{heterogeneous-agent \gls{ppo}}

\usepackage{comment}

\title{Working Title: \\
Input-Invariant Architectures in Multi-Agent Reinforcement Learning (MARL)}
\author{Brandon Hosley}
\date{\today}

\begin{document}

\maketitle

% \begin{abstract}
% \end{abstract}
\begin{comment}
    Structured Abstract:
    - Background (intro/related work)
    - Objectives (intro/methodology)
    - Methods (methodology)
    - Results
    - Conclusions
\end{comment}

\section{Introduction}

\Gls{marl} has emerged as a powerful framework for modeling and 
training distributed systems of interacting agents. 
In cooperative \gls{marl} settings, agents often share a common goal and 
must coordinate their behaviors through learning. While many 
successful approaches rely on the assumption that agents are homogeneous and exchangeable,
real-world applications increasingly require support for heterogeneous agents, 
each with distinct observation capabilities, action spaces, or roles. 
This shift introduces challenges for policy architecture design, 
particularly when aiming for efficient training and robust 
execution under dynamic team configurations.

A core insight from recent literature is that when agents are symmetric or 
play exchangeable roles, enforcing invariance to permutations in agent inputs 
and identities can significantly improve learning efficiency and generalization. 
Input-invariant architectures, such as those based on pooling, graph networks, 
or attention mechanisms, can leverage these symmetries directly in the 
policy or critic network. These methods reduce the complexity of the 
input space and support scalable learning across team sizes. 
However, extending these benefits to \gls{harl} remains an open challenge.

\begin{comment} -----------------------------------------------------------------------------------
% REWRITE for JAIR
% #TODO: Reduction in scope:
% - Remove references to new C3 scope: graphs
% - Export environmental detailing


\section{Related Work}
\section{Methodology}
\section{Results}
\section{Conclusions}
\section{Future Work}
\end{comment} % -----------------------------------------------------------------------------------







% #TODO rewrite to just this version - induced homogeneous


% In this work, we propose and evaluate a method for incorporating 
% input-invariance principles into \gls{harl} settings. 
% We focus on a specific form of heterogeneity where agents 
% have partially overlapping observation and action spaces, 
% corresponding to distinct but sometimes shared sensor and effector modalities.

% We introduce a novel architectural approach that constructs 
% global observation and action spaces that span 
% the corresponding spaces of each of member agents.
% This artificial space allows training a 
% shared policy using equivariant neural layers and dynamic masking. 
% This approach supports flexible agent instantiation, input degradation, 
% and team-size perturbation without retraining.







% To evaluate this approach, we use a custom multi-agent 
% environment based on the minigrid environment

% and we compare a \gls{happo} baseline



% and we compare three architectures: 
% a HAPPO baseline, a COMA model augmented with GNN and Transformer components, 
% and our proposed equivariant spanning policy network. 
% We assess both learning efficiency and runtime stability 
% under controlled perturbations, including sensor dropout 
% and dynamic changes in team composition.

% Our results aim to contribute a deeper understanding of how 
% input invariance can be adapted for heterogeneous agents, 
% and what trade-offs arise in terms of performance, 
% generalization, and computational cost.


% \section{Related Work}

% This section surveys prior work relevant to input-invariant architectures for \gls{harl}. 
% The topics below are parallel threads that converge to inform our architectural design.
% We begin with theoretical underpinnings of exchangeability and symmetry, 
% followed by architectural strategies for permutation invariance and equivariance
% via pooling, mean-field methods, graph networks, and attention. 
% These are then examined through applications in \gls{marl} 
% before concluding with \gls{harl} considerations.

% We begin by grounding our discussion of input-invariant architectures 
% in a key probabilistic result: de Finetti's theorem (\cref{eq:deFinnetti}). 
% This theorem formalizes the idea that, under suitable conditions, 
% the elements of an exchangeable sequence can be treated as if 
% they were drawn independently from a shared latent distribution.

% \begin{theorem}[de Finetti's Theorem]
%     Let \(X = \{x_1, \ldots, x_M\}\) be a sequence of exchangeable random variables.
%     Then there exists a latent variable \(\theta\) such that:
%     \begin{equation}
%         p(X \mid \alpha, M_0) = \int p(\theta \mid \alpha, M_0) 
%         \prod_{m=1}^{M} p(x_m \mid \theta)\, d\theta
%         \label{eq:deFinnetti}
%     \end{equation}
%     That is, the joint distribution over \(X\) can be represented as a mixture of 
%     i.i.d. variables conditional on \(\theta\).
% \end{theorem}

% This result justifies the treatment of inputs that are unordered but statistically similar,
% such as agent observations in a cooperative \gls{marl} task, 
% as conditionally i.i.d. elements drawn from a shared distribution. 
% In practice, this means that the ordering of inputs does not convey meaningful information, 
% and architectures may (and perhaps should) be designed to respect this symmetry.

% As an example, in environments like \gls{lbf}~\cite{papoudakis2021}, 
% each agent's observation is constructed by concatenating their own state, 
% the state of objectives, and the states of other agents in a fixed but arbitrary order. 
% While the ordering is deterministic, it typically does not encode meaningful 
% semantic distinctions. In such settings, treating the input as an exchangeable set,
% rather than a structured sequence, aligns with the assumptions of de Finetti's theorem 
% and motivates the permutation-invariant design choices in the rest of this section.

% Permutation-invariant functions and networks (e.g., pooling operations or 
% attention mechanisms without positional encoding) align with this insight, %%
% embedding the inductive bias that order should not affect the outcome. 
% This forms the basis for the architectural designs reviewed in the following sections.


% \subsection{Permutation Invariance and Equivariance}

% In their paper introducing Deep Sets, Zaheer et al.~\cite{zaheer2017} 
% present a parameter-sharing scheme inspired by \cref{eq:deFinnetti}, 
% which allows networks to handle unordered inputs by design.

% They propose two architectures to achieve this.
% For the invariant approach, they demonstrate that any 
% permutation-invariant function over a set can 
% be decomposed into a transformation (\(\rho\)) of a sum over 
% transformed (\(\phi\)) elements,
% \begin{equation*}
%     \rho\left(\sum_i \phi(x_i)\right)
% \end{equation*}
% providing a universal approximator for such functions. 

% Zaheer et al.~\cite{zaheer2017} also proposed an equivariant approach,
% providing a neural network layer of the form:
% \begin{equation*}
%     \mathbf{f}(\mathbf{x}) \doteq \mathbf\sigma (\lambda\mathbf{Ix} 
%     + \gamma \text{maxpool}(\mathbf{x})\mathbf{1}) 
% \end{equation*}
% where the weight matrix is constrained to the form 
% \(\lambda\mathbf{I} + \gamma\mathbf{11}^\top\), ensuring all 
% diagonal elements are equal and all off-diagonal elements are equal. 
% This construction guarantees permutation equivariance.
% The \(\sigma\) operation represents a non-linearity function (such as a sigmoid).
% The authors argue that this equivariant form is functionally equivalent to the 
% invariant form during inference; their distinction primarily emerges during backpropagation, 
% where the structure of gradient updates differs between the two.

% These structures dramatically reduce the effective symmetry 
% group of the input space, enabling better generalization and 
% sample efficiency in problems with intrinsic symmetries. 
% In \gls{marl}, these ideas naturally extend to agent sets: 
% policies or critics can process agents' states as elements 
% of an unordered set, enforcing invariance to permutations in agent ordering. 
% This supports scalable learning across varying team compositions 
% and aligns with the theoretical underpinnings of exchangeability laid out earlier.


% \subsection{Mean-Field and Exchangeable Approximations}

% In scenarios where agents are approximately indistinguishable, 
% a natural extension of Zaheer et al.'s~\cite{zaheer2017} 
% permutation-invariant formulation is to normalize the summation of elements. 
% Rather than summing over agent embeddings, one may average them,
% yielding permutation invariance \emph{and} insensitivity to the number of agents. 
% This refinement aligns closely with mean-field theory, which replaces complex 
% many-body interactions with an average effect.

% Yang et al.~\cite{yang2018} apply this idea in the context of \glsadd{marl}, 
% introducing mean-field Q-learning and actor-critic algorithms. 
% Their approach approximates the influence of neighboring agents with 
% an average embedding, enabling each agent to learn a local policy that 
% scales gracefully with team size. This is particularly 
% useful in settings where each agent interacts with many others, 
% but where the precise identity of those agents is irrelevant.

% Building on this, Li et al.~\cite{li2021b} approach mean-field \gls{marl} 
% problems formally as permutation-invariant Markov decision processes. 
% They propose Mean-Field Proximal Policy Optimization (MF-PPO), 
% a variant of PPO that incorporates mean-field assumptions into 
% both actor and critic networks. The resulting architecture maintains a 
% shared policy across agents, with inputs aggregated through mean pooling.

% Theoretically, MF-PPO achieves convergence to a global optimum at 
% a sublinear rate, and notably, its sample complexity is proven to be 
% independent of the number of agents. This highlights the power of 
% encoding exchangeability as an inductive bias: by exploiting the 
% ergodic structure of agent interactions, MF-PPO updates a shared policy 
% more efficiently than na\"ive multi-agent baselines.

% Empirically, MF-PPO outperforms non-invariant baselines in the 
% \gls{mpe}~\cite{li2021b}, demonstrating faster convergence, 
% higher average returns, and significantly reduced parameter counts. 
% This is largely attributed to weight sharing across agents and the 
% use of input-aggregation strategies that preserve permutation symmetry. 

% One significant downside to the examined mean-field approaches is that 
% information about subset interactions is lost in the process of 
% applying their respective commutative functions.
% % #TODO: Appendix proof
% % We argue that this is not a necessary limitation of mean-field methods,
% This limitation motivated more sophisticated invariant architectures like 
% graph-based networks and attention-based pooling, which we discuss next.

% % #TODO - Redundant from C3


% % Change this section as reflecting back to single agent applications
% \subsubsection{Relational Reasoning in Single-Agent RL}
% While much of the prior work focuses on multi-agent settings, 
% the benefits of invariant relations extend to single-agent tasks as well. 
% These architectures demonstrate that modeling inputs as a set of 
% interacting entities can improve both performance and generalization.

% Zambaldi et al.~\cite{zambaldi2018} showed that a single-agent \gls{rl} agent could 
% benefit from interpreting its observations as a graph of entities and relations. 
% Using a relation network with iterative message passing, their agent achieved 
% superhuman performance on several StarCraft II~\cite{vinyals2019} mini-games, 
% outperforming non-relational baselines by reasoning over unit interactions 
% rather than raw input features.

% In their single-agent formulation, Tang and Ha~\cite{tang2021} demonstrated 
% that treating sensory channels as interchangeable input elements enabled 
% the agent to maintain performance despite significant input corruption. 
% By passing each sensory input through a shared encoder and aggregating 
% with self-attention, the network learned to interpret observations based 
% on contextual relationships rather than fixed positions. 
% This design conferred resilience to occluded or missing inputs, 
% allowing the agent to function even when sections of the sensory 
% data were absent at test time. Their results suggest that invariance 
% to input structure can provide strong robustness to
% severe perceptual perturbations.

% % # TODO: May consider ending on Tang and Ha for similarity purposes.
% \begin{comment}
%     They propose a very similar question:
%         "Sensory substitution refers to the brainâ€™s ability to use one sensory modality 
%         (e.g., touch) to supply environmental information normally gathered by 
%         another sense (e.g., vision)."
%     - Extremely similar to my interest in sensory degradation.
%     - However, in their experiments they instead demonstrate an invariance in a singular
%     observational domain. (They take a 2d image, chunk it and shuffle.)
% \end{comment}

% \subsection{Heterogeneous Agents}
% In many \gls{marl} scenarios, agents are not identical, a setting formally studied 
% as \gls{harl}. Agents may differ in abilities, observation spaces, or roles. 
% Many input-invariant architectures assume that all inputs are drawn from the 
% same distribution and processed by a shared encoder, an assumption that breaks 
% down when agents have structurally different observations. In such cases, 
% invariant frameworks must be adapted to handle this heterogeneity, 
% either by mapping observations into a shared embedding space or by 
% augmenting the input with agent-specific identifiers.

% One practical strategy for supporting heterogeneity in input-invariant architectures 
% is to include an agent-specific feature encoding that captures type or 
% role~\cite{liu2020b,hao2022,hao2023}. As noted earlier, PIC~\cite{liu2020b}
% incorporates this by embedding attribute vectors into each node's representation, 
% preserving symmetry among similar agents while allowing differentiation where needed.

% Similarly, HPN~\cite{hao2023} uses a hypernetwork conditioned on 
% identity to generate specialized agent modules, enabling tailored policies 
% while maintaining permutation invariance at the architectural level. 
% This structure supports diversity among agents while preserving the 
% benefits of shared computation and symmetry where applicable.

% Such flexibility is especially valuable in mixed-agent teams and 
% competitive settings, where an agent may need to reason differently 
% about teammates and opponents. The architecture should remain invariant 
% when swapping two equivalent opponents but avoid conflating agents 
% with distinct roles.

% Despite the demonstrated advantages of invariant and equivariant architectures in handling 
% agent symmetry, scalability, and heterogeneity, their practical cost remains underexplored. 
% The reviewed methods, ranging from mean-field approximations and deep sets, 
% to graph-based and attention-driven critics share a common goal: to encode 
% structural priors that simplify learning in multi-agent settings. However, 
% these methods often increase architectural complexity, requiring additional computation, 
% memory, and tuning. Importantly, few studies offer direct comparisons of their 
% training or inference costs relative to simpler baselines such as \glspl{mlp}. 
% This gap complicates our understanding of the tradeoffs involved in deploying 
% more expressive models in real-world systems.

% In this work, we contribute to closing that gap by empirically evaluating the efficiency 
% and robustness of input-invariant policy architectures in heterogeneous-agent settings, 
% quantifying both their benefits and computational costs. In parallel, we 
% propose a novel exchangeable approximation framework that extends the principles 
% of input invariance to settings with observation and action heterogeneity. 
% By constructing a shared observation-action spanning space and integrating dynamic 
% masking within an equivariant network, we offer a lightweight and scalable 
% approach to handling agent diversity. Together, these contributions aim to 
% clarify not only when these architectures are helpful, 
% but how they might be practically and broadly applied.


% \section{Methodology}
% \label{con2:sec:methodology}


% % #TODO Make more terse (in favor of exporting)
% \subsection{Environment Model}


% % #TODO Update Environment and agent design
% \subsection{Environment and Agent Design}

% To evaluate agents with overlapping but non-identical observation capabilities, 
% we design a custom environment derived from the \gls{mpe}. 
% Each agent is equipped with a subset of sensor types, 
% and each sensor type defines a linearly independent subspace of the 
% full observation space. An agent's full observation vector is constructed by 
% concatenating the outputs of its sensors, and overlapping sensor types across 
% agents induce structured correlations.

% The agents are heterogeneous in both their observation and action spaces. 
% The union of all agent sensor types forms a global spanning observation space. 
% Likewise, the union of all agent action spaces defines a spanning action set, 
% from which agent-specific valid actions are masked during inference.


% % #TODO Update architectures
% \subsection{Architectures Compared}
% Three network architectures are evaluated:

% \paragraph{0. HAPPO Baseline}
% To provide a baseline comparison, we include a standard implementation 
% of Heterogeneous-Agent Proximal Policy Optimization (HAPPO)~\cite{zhong2024}. 
% With HAPPO each agent is trained with its own policy using PPO updates in a centralized 
% training setting, but without shared parameters or input-invariant structure. 
% HAPPO represents a strong non-invariant architecture designed specifically 
% for heterogeneous teams, serving as a strong reference point for evaluating the 
% efficiency and robustness benefits of the more structured approaches.

% % \paragraph{1. COMA with GNN and Transformer}
% % This model extends the counterfactual multi-agent policy gradient (COMA) 
% % algorithm by incorporating a graph neural network (GNN) to encode agent relations, 
% % and a transformer-based critic for contextual value estimation. 
% % This structure follows prior work (e.g., PIC and MAAC) and 
% % serves as a high-capacity input-invariant baseline. 
% % The actor and critic operate under a centralized training with 
% % decentralized execution (CTDE) paradigm, with the GNN providing relational inductive bias.

% % #TODO Rename this structure
% \paragraph{1. Equivariant Spanning Policy}
% This architecture introduces a novel mechanism for enabling 
% exchangeability in heterogeneous settings. A shared network 
% is trained using inputs projected into the global observation 
% spanning space, with a binary masking vector indicating which 
% features are valid for a given agent. The network incorporates 
% equivariant layers of the form $\lambda I + \gamma \mathbf{11}^\top$ 
% to maintain structured permutation equivariance. The output layer 
% is similarly masked to restrict the agent's available actions 
% within the global action set. Policy updates are obtained using PPO.
% % #TODO Add diagram to demonstrate how this works in practice

% % TODO: Continuous action masking strategies are not yet finalized. 
% % Placeholder design assumes discrete masking via argmax over valid action subset.

% \subsection{Training Regime}

% To evaluate learning efficiency and generalization,
% we train each architecture under varying team configurations.
% % #TODO Change the description of compared variables
% Two primary variables are considered: the number of agents in the team, 
% and the composition of agents based on their combinations of observation subspaces. 
% Agent composition affects the overlap in observation space and thus 
% the input complexity presented to the policy network. 
% % (While the current focus is on observation-space heterogeneity, 
% % additional forms such as action-space heterogeneity may be considered in future extensions, 
% % pending environment support.)

% % For each architecture, training runs are conducted across multiple 
% % fixed team sizes (e.g., 2, 4, and 8 agents), and within each team size, 
% % several agent compositions are sampled to reflect varying degrees of sensor overlap. 
% % This allows us to evaluate the architecture's sensitivity to 
% % both agent count and observation diversity.

% All configurations are evaluated across multiple independent 
% training runs to account for stochastic variability.
% Learning efficiency is defined in terms of convergence rate 
% relative to total agent-training steps and computational cost.

% \subsection{Perturbation Evaluation}

% To assess runtime robustness, trained agents are tested under two types of perturbations:

% \begin{itemize}
%     \item \textbf{Sensor degradation}: Agents experience random dropout 
%         of one or more observation channels during execution. 
%         The masking vector is updated accordingly to simulate partial observability.
%     \item \textbf{Team-size changes}: Agents are removed or added at test 
%         time without retraining. Policies are evaluated for stability and 
%         reward degradation in the new configuration.
% \end{itemize}

% Each condition is evaluated across multiple trials. 
% Policy stability is measured by the variance and degradation in episode rewards, 
% and recovery is assessed where applicable. 
% Final comparisons are made between the two architectures across all perturbation conditions.

% \section{Results}

% \printbibliography

% % --- End Dissertation Inclusion ---
% \clearpage
% \appendix
% \section*{Appendix}
% % \addcontentsline{toc}{section}{Appendix}


% % ENVIRONMENT Information:
% \subsection{Environment Model}

% We model the environment following the structure outlined by Powell~\cite{powell2022}, 
% consisting of the state, decision/action, exogenous information, transition function, and 
% objective function.

% \paragraph{State.} 
% Let \(T \subset \mathbb{N}\) be the set of decision epochs, 
% and define entities \(E := (I, J, K, L)\), where 
% \(i \in I\) are agents, 
% \(j \in J\) are objectives, 
% \(k \in K\) are hazards, and 
% \(l \in L\) are obstacles. 
% At each time \(t \in T\), the joint state \(s\in S\) is:
% \[
%     s_t := \{s_{te} \mid e \in E\}, \quad s_t \in S
% \]
% Where the marginal state for agents \(i \in I\), 
% includes both location and orientation:
% \[
%     s_{ti} := (d_{ti}, \theta_{ti})
% \]
% and the non-agent entities \(e \in (J, K, L)\) are represented by location alone:
% \[
%     s_{te} := (d_{te})
% \]
% For spatial dimensions \(N \in \mathbb{N}\), 
% let \(D^{(n)} \in \mathbb{F}\) be the domain of dimension \(n\).
% Then, we define each entity \(e\)'s location as a tuple:
% \[
%     d_{te} := \left(d_{te}^{(1)},\ldots,d_{te}^{(N)}\right) \in \prod_{n=1}^N D_e^{(n)},
% \]
% and orientation is represented by the tuple:
% \[
%     \theta_{ti} := \left(\theta_{ti}^{(1,2)},\ldots,\theta_{ti}^{(N-1,N)}\right)
%     \in \prod_{n=1}^{N-1} \Theta_i^{(n,n+1)}
% \]

% \paragraph{Observation Model.}
% Let \({C} := \{c_1, \ldots, c_m\}\) denote a set of abstract sensor channels.
% Each sensor channel \(c \in {C}\) corresponds to a linearly 
% independent observation subspace.

% For each entity visibility in each channel must be defined,
% as a trait this may be referred to as \(e^{(c)} \in \{\text{True, False}\}\).
% It may be encoded in the state as:
% \[
% s_{tec} = 
% \begin{cases}
%     1& \text{if \(e\) is visible in \(c\)} \\ 
%     0& \text{if \(e\) is not visible in \(c\)}
% \end{cases} 
% \quad\forall t\in T, e\in E, c\in C.
% \]
% Thus the marginal states becomes
% \[s_{ti} :(c_{ti},d_{ti},\theta_{ti})\]
% for agents, and
% \[s_{te} :(c_{te},d_{te})\]
% for all other entities.

% Each agent \(i \in I\) is assigned a subset of sensor channels 
% \({C}_i \subseteq {C}\), 
% with their observation defined as the concatenation of the outputs 
% from the corresponding subspaces:
% Note that an agent's ability to perceive an entity within a given 
% subspace may be limited by spatial range, meaning each agent may 
% only observe a portion of the environment within each active channel.
% \[
%     o_{ti} := \bigoplus_{c \in {C}_i} o_{tic}
% \]
% where \(o_{tic}\) is agent \(i\)'s observation of channel \(c\) at time \(t\), 
% and \(\oplus\) denotes concatenation over the ordered tuple of subspaces.

% The global observation space \(O\) is the Cartesian product of all channel subspaces:
% \[
%     O := \prod_{c \in {C}} O_c
% \]
% and each agent's observation is a masked subset of this space.
% During policy training and inference, a binary mask vector \(m_i \in \{0,1\}^{|\mathcal{C}|}\) 
% is applied to indicate which subspaces are active for agent \(i\), 
% allowing a shared network to condition on heterogeneous observations without assuming symmetry.


% \paragraph{Action Space.} 
% The joint action \(a \in A\) consists of marginal agent actions:
% \[
%     a := (a_1, \ldots, a_{|I|}) \in A := \prod_{i \in I} A_i
% \]
% Each agent's action at time \(t\) is a tuple:
% \[
%     a_{ti} := (a_{ti}^\text{interact}, a_{ti}^\text{move}, \Delta\theta_{ti})
% \]
% where:
% \begin{itemize}
%     \item \(a_{ti}^\text{interact} \in \{0,1\}\) toggles interaction,
%     \item \(a_{ti}^\text{move} \in M_i \subseteq \mathbb{F}\) denotes movement magnitude,
%     \item \(\Delta\theta_{ti} \in \prod_{n=1}^{N-1} \Delta\Theta_i^{(n,n+1)}\) encodes orientation adjustments.
% \end{itemize}

% % \paragraph{Exogenous Information.} 
% % Agents observe a partially observable state with complete reward knowledge. 
% % Probabilistic channel observability is future work.

% \paragraph{Transition Function.} 
% Deterministic and synchronous:
% \[
%     s_{t+1} = \mathcal{T}(s_t, a_t) \quad \text{or} \quad \mathcal{T}(a_t \mid s_t) = s_{t+1}.
% \]
% Provided that the actions chosen by the agents are valid, they are executed.

% \paragraph{Objective Function.} 
% The cumulative reward is defined as:
% \[
%     \max \sum_{t \in T} \left[
%         \sum_{i \in I, j \in J} r_j \cdot \mathbb{I}[\Xi]
%         - \sum_{i \in I} r_i \cdot \mathbb{I}[a_{ti}^\text{interact} = 1]
%         - \sum_{i \in I, k \in K} r_k \cdot \mathbb{I}[\|s_{ti} - s_{tk}\|_\infty \leq k^\text{range}]
%     \right]
% \]
% where:
% \begin{itemize}
%     \item \(r_j \in \mathbb{R}_{>0}\) is the reward for collecting objective \(j \in J\),
%     \item \(\mathbb{I}[\Xi]\) encodes conditions under which \(j\) is valid for collection,
%     \item \(r_i \in \mathbb{R}_{<0}\) is the cost for agent \(i \in I\) to interact,
%     \item \(r_k \in \mathbb{R}_{<0}\) is a penalty for agent \(i\) being near hazard \(k \in K\).
% \end{itemize}



\end{document}