\documentclass[xcolor={dvipsnames}]{beamer}
\usetheme{CambridgeUS}
\usepackage{graphicx}

%\setbeamercolor*{structure}{bg=Red!20,fg=Red}
%
%\setbeamercolor*{palette primary}{use=structure,fg=white,bg=structure.fg}
%\setbeamercolor*{palette secondary}{use=structure,fg=white,bg=structure.fg!75}
%\setbeamercolor*{palette tertiary}{use=structure,fg=white,bg=structure.fg!50!black}
%\setbeamercolor*{palette quaternary}{fg=white,bg=black}
%
%\setbeamercolor{section in toc}{fg=black,bg=white}
%\setbeamercolor{alerted text}{use=structure,fg=structure.fg!50!black!80!black}
%
%\setbeamercolor{titlelike}{parent=palette primary,fg=structure.fg!50!black}
%\setbeamercolor{frametitle}{bg=gray!10!white,fg=PineGreen}
%
%\setbeamercolor*{titlelike}{parent=palette primary}

\AtBeginSection[]
{
	\begin{frame}<beamer>
		%\frametitle{Outline for section \thesection}
		\tableofcontents[currentsection]
	\end{frame}
}

\usepackage[firstpage]{draftwatermark}
\SetWatermarkText{Draft}



\title{Survey Class Imbalance in Machine Learning}
\author{1st Lt Brandon Hosley}
\begin{document}
\begin{frame}
    \maketitle
\end{frame}
\begin{frame}{Outline}
	\tableofcontents
\end{frame}

\begin{frame}{Definitions}
	\begin{description}
		\item[Intrinsic Imbalance] the result of naturally occurring frequencies in the data source
		\item[Extrinsic Imbalance] introduced through collection, measurement, or storage factors
		\item[Imbalance Ratio] $$\rho = \frac{max_i\{|C_i|\}}{min_i\{|C_i|\}}, \quad \forall i \text{ classes}$$
	\end{description}
\end{frame}


\section{Data-level Methods}

\begin{frame}{Oversampling}
	\begin{figure}
		\centering
		\includegraphics[width=0.7\linewidth]{images/Oversampling}
		% source: https://www.kaggle.com/code/rafjaa/resampling-strategies-for-imbalanced-datasets/notebook
	\end{figure}
	\begin{itemize}
		\item Replicating members of a minority class to increase its representation
		\item[Pros:] No loss in majority feature information
		\item[Cons:] Potential exaggeration of minority sample deviation from population
	\end{itemize}
\end{frame}
\begin{frame}{Undersampling}
	\begin{figure}
		\centering
		\includegraphics[width=0.7\linewidth]{images/Undersampling}
		% source: https://www.kaggle.com/code/rafjaa/resampling-strategies-for-imbalanced-datasets/notebook
	\end{figure}
	\begin{itemize}
		\item Removing members of a majority class to decrease its representation
		\item[Pros:] No exaggeration of minority sample deviation from population, opportunity to drop anomalies and reduce noise
		\item[Cons:] Potential loss in majority feature information
	\end{itemize}
\end{frame}
\begin{frame}{Implementation of *-sampling methods}
	\begin{itemize}
		\item Random selection (ROS, RUS)
		\item Intelligent Undersampling
		\begin{itemize}
			\item (Zhang and Mani) Dropping majority class members based on distance from minority members
			\item (Kubat and Matwin) Removed redundant majority members
			\item (Barandela et. al) Remove majority members that would be misclassified within a K-NN classification.
		\end{itemize}
	\end{itemize}
\end{frame}
\begin{frame}{SMOTE - Synthetic Minority Over-sampling Technique}
	\begin{itemize}
		\item Introduces interpolated minority sample members to increase the size of the minority sample
		\item \textbf{Borderline-SMOTE} limits synthetic members to be near decision boundaries
		\item \textbf{Safe-level-SMOTE} defines safe regions that prevent oversampling in regions of overlap or noise
	\end{itemize}
	\begin{figure}
		\centering
		\includegraphics[width=0.7\linewidth]{images/SMOTE}
	\end{figure}
\end{frame}
\section{Algorithm-level Methods}
	
\begin{frame}{Cost-sensitive Learning}
		\begin{itemize}
			\item Introducing a penalty matrix which assigns costs to misclassifications based on the sample's class
			\begin{itemize}
				\item Increasing the cost associated with minority samples increases its associated training penalty
			\end{itemize}
		\end{itemize}
		\begin{figure}
			\centering
			\includegraphics[width=0.5\linewidth]{example-image}
		\end{figure}
\end{frame}


\section{Hybrid Methods}
\begin{frame}{Psuedo-balanced Training}
	\begin{itemize}
		\item An ensemble technique in which classifiers are trained on subsets of the majority combined with the minority sample
		\item Examples: \textbf{EasyEnsemble}, \textbf{BalanceCascade}
	\end{itemize}
	\begin{figure}
		\centering
		\includegraphics[width=0.5\linewidth]{example-image}
	\end{figure}
\end{frame}
\begin{frame}{Boosting}
	\begin{itemize}
		\item SMOTEBoost and DataBoost-IM
		\begin{itemize}
			\item Performs a boosting procedure but uses synthetic upsampling on the misclassified subset
		\end{itemize}
		\item JOUS-Boost
		\begin{itemize}
			\item Jittering training data with Over/Under-Sampling
			\item Improves probability estimates from AdaBoost by implementing class-weights based on costs of false-positives and false-negatives
		\end{itemize}
		\item AdaCost (AdaC1,2,3)
		\begin{itemize}
			\item AdaBoost with an underlying Cost-sensitivity addition
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Deep Learning}
	
	\begin{itemize}
		\item Under normal back-propagation minority gradients have a lower magnitude
		\item Data-level 
		\begin{itemize}
			\item (Hensman and Masko) ROS and RUS can be effective up to about $\rho=2.3$
			\item Two-Phase
			\item Dynamic Sampling
		\end{itemize}
		\item Algorithm-level
		\item Hybrid
	\end{itemize}
\end{frame}

\begin{frame}{Outline}
	\tableofcontents
\end{frame}
\begin{frame}{}
	\begin{center}
		\Large Questions?
	\end{center}
\end{frame}
\end{document}
