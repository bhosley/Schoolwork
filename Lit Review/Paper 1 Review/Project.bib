@online{balduzzi2019,
  title = {Open-Ended {{Learning}} in {{Symmetric Zero-sum Games}}},
  author = {Balduzzi, David and Garnelo, Marta and Bachrach, Yoram and Czarnecki, Wojciech M. and Perolat, Julien and Jaderberg, Max and Graepel, Thore},
  date = {2019-05-13},
  eprint = {1901.08106},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1901.08106},
  url = {http://arxiv.org/abs/1901.08106},
  urldate = {2024-03-15},
  abstract = {Zero-sum games such as chess and poker are, abstractly, functions that evaluate pairs of agents, for example labeling them `winner' and `loser'. If the game is approximately transitive, then self-play generates sequences of agents of increasing strength. However, nontransitive games, such as rock-paper-scissors, can exhibit strategic cycles, and there is no longer a clear objective -- we want agents to increase in strength, but against whom is unclear. In this paper, we introduce a geometric framework for formulating agent objectives in zero-sum games, in order to construct adaptive sequences of objectives that yield open-ended learning. The framework allows us to reason about population performance in nontransitive games, and enables the development of a new algorithm (rectified Nash response, PSRO\_rN) that uses game-theoretic niching to construct diverse populations of effective agents, producing a stronger set of agents than existing algorithms. We apply PSRO\_rN to two highly nontransitive resource allocation games and find that PSRO\_rN consistently outperforms the existing alternatives.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning},
  file = {/Users/brandonhosley/Zotero/storage/JIHZMJL2/Balduzzi et al. - 2019 - Open-ended Learning in Symmetric Zero-sum Games.pdf;/Users/brandonhosley/Zotero/storage/NWLIW2GL/1901.html}
}

@online{berner2019,
  title = {Dota 2 with {{Large Scale Deep Reinforcement Learning}}},
  author = {Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Dębiak, Przemysław and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and Józefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pinto, Henrique P. d O. and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
  date = {2019-12-13},
  eprint = {1912.06680},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1912.06680},
  url = {http://arxiv.org/abs/1912.06680},
  urldate = {2024-03-15},
  abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/brandonhosley/Zotero/storage/GIAX6G7H/OpenAI et al. - 2019 - Dota 2 with Large Scale Deep Reinforcement Learnin.pdf;/Users/brandonhosley/Zotero/storage/HM7FXNK8/1912.html}
}

@inproceedings{bertram2022,
  title = {Supervised and {{Reinforcement Learning}} from {{Observations}} in {{Reconnaissance Blind Chess}}},
  booktitle = {2022 {{IEEE Conference}} on {{Games}} ({{CoG}})},
  author = {Bertram, Timo and Fürnkranz, Johannes and Müller, Martin},
  date = {2022-08},
  pages = {608--611},
  issn = {2325-4289},
  doi = {10.1109/CoG51982.2022.9893588},
  url = {https://ieeexplore.ieee.org/abstract/document/9893588},
  urldate = {2024-04-01},
  abstract = {In this work, we adapt a training approach inspired by the original AlphaGo system to play the imperfect information game of Reconnaissance Blind Chess. Using only the observations instead of a full description of the game state, we first train a supervised agent on publicly available game records. Next, we increase the performance of the agent through self-play with the on-policy reinforcement learning algorithm Proximal Policy optimization. We do not use any search to avoid problems caused by the partial observability of game states and only use the policy network to generate moves when playing. With this approach, we achieve an ELO of 1330 on the RBC leaderboard, which places our agent at position 27 at the time of this writing. We see that self-play significantly improves performance and that the agent plays acceptably well without search and without making assumptions about the true game state.},
  eventtitle = {2022 {{IEEE Conference}} on {{Games}} ({{CoG}})},
  keywords = {Games,Probability distribution,Reconnaissance,Reinforcement learning,Search problems,Training,Writing},
  file = {/Users/brandonhosley/Zotero/storage/WES6J5ZX/Bertram et al_2022_Supervised and Reinforcement Learning from Observations in Reconnaissance Blind.pdf;/Users/brandonhosley/Zotero/storage/QD3EZJN9/9893588.html}
}

@article{campbell2002,
  title = {Deep {{Blue}}},
  author = {Campbell, Murray and Hoane, A. Joseph and Hsu, Feng-hsiung},
  date = {2002-01-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {134},
  number = {1},
  pages = {57--83},
  issn = {0004-3702},
  doi = {10.1016/S0004-3702(01)00129-1},
  url = {https://www.sciencedirect.com/science/article/pii/S0004370201001291},
  urldate = {2024-04-01},
  abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: •a single-chip chess search engine,•a massively parallel system with multiple levels of parallelism,•a strong emphasis on search extensions,•a complex evaluation function, and•effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.},
  keywords = {Computer chess,Evaluation function,Game tree search,Parallel search,Search extensions,Selective search},
  file = {/Users/brandonhosley/Zotero/storage/4HB7XQYF/S0004370201001291.html}
}

@article{fawzi2022,
  title = {Discovering Faster Matrix Multiplication Algorithms with Reinforcement Learning},
  author = {Fawzi, Alhussein and Balog, Matej and Huang, Aja and Hubert, Thomas and Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and R. Ruiz, Francisco J. and Schrittwieser, Julian and Swirszcz, Grzegorz and Silver, David and Hassabis, Demis and Kohli, Pushmeet},
  date = {2022-10},
  journaltitle = {Nature},
  volume = {610},
  number = {7930},
  pages = {47--53},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-022-05172-4},
  url = {https://www.nature.com/articles/s41586-022-05172-4},
  urldate = {2024-03-15},
  abstract = {Improving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems—from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero1 for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the state-of-the-art complexity for many matrix sizes. Particularly relevant is the case of 4\,×\,4 matrices in a finite field, where AlphaTensor’s algorithm improves on Strassen’s two-level algorithm for the first time, to our knowledge, since its discovery 50 years ago2. We further showcase the flexibility of AlphaTensor through different use-cases: algorithms with state-of-the-art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor’s ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria.},
  langid = {english},
  keywords = {Applied mathematics,Computer science},
  file = {/Users/brandonhosley/Zotero/storage/3E6LV4SX/Fawzi et al. - 2022 - Discovering faster matrix multiplication algorithm.pdf}
}

@online{gandhi2020,
  title = {{{XRL}}: {{eXplainable Reinforcement Learning}}},
  shorttitle = {{{XRL}}},
  author = {Gandhi, Meet},
  date = {2020-12-18T04:07:14},
  url = {https://towardsdatascience.com/xrl-explainable-reinforcement-learning-4cd065cdec9a},
  urldate = {2023-10-21},
  abstract = {Detailed overview of promising XRL methodologies},
  langid = {english},
  organization = {Medium},
  file = {/Users/brandonhosley/Zotero/storage/PVRTTCSL/Gandhi_2020_XRL.pdf;/Users/brandonhosley/Zotero/storage/2QQS6Z5Q/xrl-explainable-reinforcement-learning-4cd065cdec9a.html}
}

@article{hammersborg2023,
  title = {Reinforcement {{Learning}} in an {{Adaptable Chess Environment}} for {{Detecting Human-understandable Concepts}}},
  author = {Hammersborg, Patrik and Strümke, Inga},
  date = {2023-01-01},
  journaltitle = {IFAC-PapersOnLine},
  shortjournal = {IFAC-PapersOnLine},
  series = {22nd {{IFAC World Congress}}},
  volume = {56},
  number = {2},
  pages = {9050--9055},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2023.10.135},
  url = {https://www.sciencedirect.com/science/article/pii/S2405896323004810},
  urldate = {2024-04-01},
  abstract = {Self-trained autonomous agents developed using machine learning are showing great promise in a variety of control settings, perhaps most remarkably in applications involving autonomous vehicles. The main challenge associated with self-learned agents in the form of deep neural networks, is their black-box nature: it is impossible for humans to interpret deep neural networks. Therefore, humans cannot directly interpret the actions of deep neural network based agents, or foresee their robustness in different scenarios. In this work, we demonstrate a method for probing which concepts self-learning agents internalise in the course of their training. For demonstration, we use a chess playing agent in a fast and light environment developed specifically to be suitable for research groups without access to enormous computational resources or machine learning models. We provide code for the environment and for self-play learning chess agents, as well as for the state-of-the-art explainable AI (XAI) method of concept detection. We present results for different chess board sizes, discussing the concepts learned by our chess agents in light of human domain knowledge.},
  keywords = {Discrete event modelling and simulation,Explainable artificial intelligence,Knowledge-based control,Machine learning,Reinforcement learning and deep learning in control,Supervision and testing},
  file = {/Users/brandonhosley/Zotero/storage/Q7KAHE54/Hammersborg_Strümke_2023_Reinforcement Learning in an Adaptable Chess Environment for Detecting.pdf;/Users/brandonhosley/Zotero/storage/F2QSP7S5/S2405896323004810.html}
}

@online{hoffman2019,
  title = {Metrics for {{Explainable AI}}: {{Challenges}} and {{Prospects}}},
  shorttitle = {Metrics for {{Explainable AI}}},
  author = {Hoffman, Robert R. and Mueller, Shane T. and Klein, Gary and Litman, Jordan},
  date = {2019-02-01},
  eprint = {1812.04608},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1812.04608},
  url = {http://arxiv.org/abs/1812.04608},
  urldate = {2023-11-02},
  abstract = {The question addressed in this paper is: If we present to a user an AI system that explains how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? In other words, how do we know that an explanainable AI system (XAI) is any good? Our focus is on the key concepts of measurement. We discuss specific methods for evaluating: (1) the goodness of explanations, (2) whether users are satisfied by explanations, (3) how well users understand the AI systems, (4) how curiosity motivates the search for explanations, (5) whether the user's trust and reliance on the AI are appropriate, and finally, (6) how the human-XAI work system performs. The recommendations we present derive from our integration of extensive research literatures and our own psychometric evaluations.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/brandonhosley/Zotero/storage/B7UHVWAR/Hoffman et al_2019_Metrics for Explainable AI.pdf;/Users/brandonhosley/Zotero/storage/RP97VPSU/1812.html}
}

@online{lai2015,
  title = {Giraffe: {{Using Deep Reinforcement Learning}} to {{Play Chess}}},
  shorttitle = {Giraffe},
  author = {Lai, Matthew},
  date = {2015-09-14},
  eprint = {1509.01549},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1509.01549},
  url = {http://arxiv.org/abs/1509.01549},
  urldate = {2024-04-01},
  abstract = {This report presents Giraffe, a chess engine that uses self-play to discover all its domain-specific knowledge, with minimal hand-crafted knowledge given by the programmer. Unlike previous attempts using machine learning only to perform parameter-tuning on hand-crafted evaluation functions, Giraffe's learning system also performs automatic feature extraction and pattern recognition. The trained evaluation function performs comparably to the evaluation functions of state-of-the-art chess engines - all of which containing thousands of lines of carefully hand-crafted pattern recognizers, tuned over many years by both computer chess experts and human chess masters. Giraffe is the most successful attempt thus far at using end-to-end machine learning to play chess.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/brandonhosley/Zotero/storage/SHQN6STL/Lai_2015_Giraffe.pdf;/Users/brandonhosley/Zotero/storage/MYXFL7YN/1509.html}
}

@article{lee2018,
  title = {Modular {{Architecture}} for {{StarCraft II}} with {{Deep Reinforcement Learning}}},
  author = {Lee, Dennis and Tang, Haoran and Zhang, Jeffrey and Xu, Huazhe and Darrell, Trevor and Abbeel, Pieter},
  date = {2018-09-25},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  volume = {14},
  number = {1},
  pages = {187--193},
  issn = {2334-0924},
  doi = {10.1609/aiide.v14i1.13033},
  url = {https://ojs.aaai.org/index.php/AIIDE/article/view/13033},
  urldate = {2023-12-06},
  abstract = {We present a novel modular architecture for StarCraft II AI. The architecture splits responsibilities between multiple modules that each control one aspect of the game, such as buildorder selection or tactics. A centralized scheduler reviews macros suggested by all modules and decides their order of execution. An updater keeps track of environment changes and instantiates macros into series of executable actions. Modules in this framework can be optimized independently or jointly via human design, planning, or reinforcement learning. We present the first result of applying deep reinforcement learning techniques to training a modular agent with selfplay, achieving 92\% or 86\% win rates against the ”Harder” (level 5) built-in Blizzard bot in Zerg vs. Zerg matches, with or without fog-of-war.},
  issue = {1},
  langid = {english},
  keywords = {starcraft},
  file = {/Users/brandonhosley/Zotero/storage/WIC7M9QL/Lee et al_2018_Modular Architecture for StarCraft II with Deep Reinforcement Learning.pdf}
}

@article{li2023b,
  title = {{{3D}} Reconstruction Based on Hierarchical Reinforcement Learning with Transferability},
  author = {Li, Lan and He, Fazhi and Fan, Rubin and Fan, Bo and Yan, Xiaohu},
  date = {2023-01-01},
  journaltitle = {Integrated Computer-Aided Engineering},
  volume = {30},
  number = {4},
  pages = {327--339},
  publisher = {IOS Press},
  issn = {1069-2509},
  doi = {10.3233/ICA-230710},
  url = {https://content.iospress.com/articles/integrated-computer-aided-engineering/ica230710},
  urldate = {2023-11-16},
  abstract = {3D reconstruction is extremely important in CAD (computer-aided design)/CAE (computer-aided Engineering)/CAM (computer-aided manufacturing). For interpretability, reinforcement learning (RL) is used to reconstruct 3D shapes from images by a series of},
  langid = {english},
  file = {/Users/brandonhosley/Zotero/storage/RGAK8FE5/Li et al_2023_3D reconstruction based on hierarchical reinforcement learning with.pdf}
}

@article{madumal2020,
  title = {Explainable {{Reinforcement Learning}} through a {{Causal Lens}}},
  author = {Madumal, Prashan and Miller, Tim and Sonenberg, Liz and Vetere, Frank},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {03},
  pages = {2493--2500},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i03.5631},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/5631},
  urldate = {2023-10-26},
  abstract = {Prominent theories in cognitive science propose that humans understand and represent the knowledge of the world through causal relationships. In making sense of the world, we build causal models in our mind to encode cause-effect relations of events and use these to explain why new events happen by referring to counterfactuals — things that did not happen. In this paper, we use causal models to derive causal explanations of the behaviour of model-free reinforcement learning agents. We present an approach that learns a structural causal model during reinforcement learning and encodes causal relationships between variables of interest. This model is then used to generate explanations of behaviour based on counterfactual analysis of the causal model. We computationally evaluate the model in 6 domains and measure performance and task prediction accuracy. We report on a study with 120 participants who observe agents playing a real-time strategy game (Starcraft II) and then receive explanations of the agents' behaviour. We investigate: 1) participants' understanding gained by explanations through task prediction; 2) explanation satisfaction and 3) trust. Our results show that causal model explanations perform better on these measures compared to two other baseline explanation models.},
  issue = {03},
  langid = {english},
  file = {/Users/brandonhosley/Zotero/storage/E7GCAKIE/Madumal et al_2020_Explainable Reinforcement Learning through a Causal Lens.pdf}
}

@online{mathieu2023,
  title = {{{AlphaStar Unplugged}}: {{Large-Scale Offline Reinforcement Learning}}},
  shorttitle = {{{AlphaStar Unplugged}}},
  author = {Mathieu, Michaël and Ozair, Sherjil and Srinivasan, Srivatsan and Gulcehre, Caglar and Zhang, Shangtong and Jiang, Ray and Paine, Tom Le and Powell, Richard and Żołna, Konrad and Schrittwieser, Julian and Choi, David and Georgiev, Petko and Toyama, Daniel and Huang, Aja and Ring, Roman and Babuschkin, Igor and Ewalds, Timo and Bordbar, Mahyar and Henderson, Sarah and Colmenarejo, Sergio Gómez and van den Oord, Aäron and Czarnecki, Wojciech Marian and {Nando de Freitas} and Vinyals, Oriol},
  date = {2023-08-07},
  eprint = {2308.03526},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2308.03526},
  urldate = {2024-01-22},
  abstract = {StarCraft II is one of the most challenging simulated reinforcement learning environments; it is partially observable, stochastic, multi-agent, and mastering StarCraft II requires strategic planning over long time horizons with real-time low-level execution. It also has an active professional competitive scene. StarCraft II is uniquely suited for advancing offline RL algorithms, both because of its challenging nature and because Blizzard has released a massive dataset of millions of StarCraft II games played by human players. This paper leverages that and establishes a benchmark, called AlphaStar Unplugged, introducing unprecedented challenges for offline reinforcement learning. We define a dataset (a subset of Blizzard's release), tools standardizing an API for machine learning methods, and an evaluation protocol. We also present baseline agents, including behavior cloning, offline variants of actor-critic and MuZero. We improve the state of the art of agents using only offline data, and we achieve 90\% win rate against previously published AlphaStar behavior cloning agent.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/brandonhosley/Zotero/storage/8YZYHT7G/Mathieu et al_2023_AlphaStar Unplugged.pdf;/Users/brandonhosley/Zotero/storage/YT9XV348/2308.html}
}

@online{mohan2023,
  title = {Structure in {{Reinforcement Learning}}: {{A Survey}} and {{Open Problems}}},
  shorttitle = {Structure in {{Reinforcement Learning}}},
  author = {Mohan, Aditya and Zhang, Amy and Lindauer, Marius},
  date = {2023-08-09},
  eprint = {2306.16021},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.16021},
  url = {http://arxiv.org/abs/2306.16021},
  urldate = {2024-01-26},
  abstract = {Reinforcement Learning (RL), bolstered by the expressive capabilities of Deep Neural Networks (DNNs) for function approximation, has demonstrated considerable success in numerous applications. However, its practicality in addressing various real-world scenarios, characterized by diverse and unpredictable dynamics, noisy signals, and large state and action spaces, remains limited. This limitation stems from issues such as poor data efficiency, limited generalization capabilities, a lack of safety guarantees, and the absence of interpretability, among other factors. To overcome these challenges and improve performance across these crucial metrics, one promising avenue is to incorporate additional structural information about the problem into the RL learning process. Various sub-fields of RL have proposed methods for incorporating such inductive biases. We amalgamate these diverse methodologies under a unified framework, shedding light on the role of structure in the learning problem, and classify these methods into distinct patterns of incorporating structure. By leveraging this comprehensive framework, we provide valuable insights into the challenges of structured RL and lay the groundwork for a design pattern perspective on RL research. This novel perspective paves the way for future advancements and aids in developing more effective and efficient RL algorithms that can potentially handle real-world scenarios better.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/brandonhosley/Zotero/storage/58RITZNG/Mohan et al_2023_Structure in Reinforcement Learning.pdf;/Users/brandonhosley/Zotero/storage/NEWISZEB/2306.html}
}

@article{perolat2022,
  title = {Mastering the {{Game}} of {{Stratego}} with {{Model-Free Multiagent Reinforcement Learning}}},
  author = {Perolat, Julien and de Vylder, Bart and Hennes, Daniel and Tarassov, Eugene and Strub, Florian and de Boer, Vincent and Muller, Paul and Connor, Jerome T. and Burch, Neil and Anthony, Thomas and McAleer, Stephen and Elie, Romuald and Cen, Sarah H. and Wang, Zhe and Gruslys, Audrunas and Malysheva, Aleksandra and Khan, Mina and Ozair, Sherjil and Timbers, Finbarr and Pohlen, Toby and Eccles, Tom and Rowland, Mark and Lanctot, Marc and Lespiau, Jean-Baptiste and Piot, Bilal and Omidshafiei, Shayegan and Lockhart, Edward and Sifre, Laurent and Beauguerlange, Nathalie and Munos, Remi and Silver, David and Singh, Satinder and Hassabis, Demis and Tuyls, Karl},
  options = {useprefix=true},
  date = {2022-12-02},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {378},
  number = {6623},
  eprint = {2206.15378},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {990--996},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.add4679},
  url = {http://arxiv.org/abs/2206.15378},
  urldate = {2024-03-15},
  abstract = {We introduce DeepNash, an autonomous agent capable of learning to play the imperfect information game Stratego from scratch, up to a human expert level. Stratego is one of the few iconic board games that Artificial Intelligence (AI) has not yet mastered. This popular game has an enormous game tree on the order of \$10\textasciicircum\{535\}\$ nodes, i.e., \$10\textasciicircum\{175\}\$ times larger than that of Go. It has the additional complexity of requiring decision-making under imperfect information, similar to Texas hold'em poker, which has a significantly smaller game tree (on the order of \$10\textasciicircum\{164\}\$ nodes). Decisions in Stratego are made over a large number of discrete actions with no obvious link between action and outcome. Episodes are long, with often hundreds of moves before a player wins, and situations in Stratego can not easily be broken down into manageably-sized sub-problems as in poker. For these reasons, Stratego has been a grand challenge for the field of AI for decades, and existing AI methods barely reach an amateur level of play. DeepNash uses a game-theoretic, model-free deep reinforcement learning method, without search, that learns to master Stratego via self-play. The Regularised Nash Dynamics (R-NaD) algorithm, a key component of DeepNash, converges to an approximate Nash equilibrium, instead of 'cycling' around it, by directly modifying the underlying multi-agent learning dynamics. DeepNash beats existing state-of-the-art AI methods in Stratego and achieved a yearly (2022) and all-time top-3 rank on the Gravon games platform, competing with human expert players.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Multiagent Systems},
  file = {/Users/brandonhosley/Zotero/storage/FFI4KSNZ/Perolat et al. - 2022 - Mastering the Game of Stratego with Model-Free Mul.pdf;/Users/brandonhosley/Zotero/storage/7P9M6VCA/2206.html}
}

@article{prudencio2024,
  title = {A {{Survey}} on {{Offline Reinforcement Learning}}: {{Taxonomy}}, {{Review}}, and {{Open Problems}}},
  shorttitle = {A {{Survey}} on {{Offline Reinforcement Learning}}},
  author = {Prudencio, Rafael Figueiredo and Maximo, Marcos R. O. A. and Colombini, Esther Luna},
  date = {2024},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  shortjournal = {IEEE Trans. Neural Netw. Learning Syst.},
  eprint = {2203.01387},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {1--0},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2023.3250269},
  url = {http://arxiv.org/abs/2203.01387},
  urldate = {2024-05-07},
  abstract = {With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Offline RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective offline RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications, such as education, healthcare, and robotics. In this work, we contribute with a unifying taxonomy to classify offline RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the field using a unified notation as well as a review of existing benchmarks' properties and shortcomings. Additionally, we provide a figure that summarizes the performance of each method and class of methods on different dataset properties, equipping researchers with the tools to decide which type of algorithm is best suited for the problem at hand and identify which classes of algorithms look the most promising. Finally, we provide our perspective on open problems and propose future research directions for this rapidly growing field.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/brandonhosley/Zotero/storage/RF2SDEJA/Prudencio et al. - 2024 - A Survey on Offline Reinforcement Learning Taxono.pdf;/Users/brandonhosley/Zotero/storage/8953YT2B/2203.html}
}

@software{romstad,
  title = {Stockfish},
  author = {Romstad, Tord and Castalba, Marco and Kiiski, Joona and Linscott, Gary},
  url = {https://stockfishchess.org/},
  abstract = {Please cite this software using the metadata from this file.}
}

@article{schrittwieser2020,
  title = {Mastering {{Atari}}, {{Go}}, {{Chess}} and {{Shogi}} by {{Planning}} with a {{Learned Model}}},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  date = {2020-12-24},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {588},
  number = {7839},
  eprint = {1911.08265},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {604--609},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-020-03051-4},
  url = {http://arxiv.org/abs/1911.08265},
  urldate = {2024-03-15},
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/brandonhosley/Zotero/storage/J44JGPSW/Schrittwieser et al. - 2020 - Mastering Atari, Go, Chess and Shogi by Planning w.pdf;/Users/brandonhosley/Zotero/storage/AT34P7RI/1911.html}
}

@inproceedings{shantia2011,
  title = {Connectionist Reinforcement Learning for Intelligent Unit Micro Management in {{StarCraft}}},
  booktitle = {The 2011 {{International Joint Conference}} on {{Neural Networks}}},
  author = {Shantia, Amirhosein and Begue, Eric and Wiering, Marco},
  date = {2011-07},
  pages = {1794--1801},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2011.6033442},
  url = {https://ieeexplore.ieee.org/document/6033442},
  urldate = {2023-12-06},
  abstract = {Real Time Strategy Games are one of the most popular game schemes in PC markets and offer a dynamic environment that involves several interacting agents. The core strategies that need to be developed in these games are unit micro management, building order, resource management, and the game main tactic. Unfortunately, current games only use scripted and fixed behaviors for their artificial intelligence (AI), and the player can easily learn the counter measures to defeat the AI. In this paper, we describe a system based on neural networks that controls a set of units of the same type in the popular game StarCraft. Using the neural networks, the units will either choose a unit to attack or evade from the battlefield. The system uses reinforcement learning combined with neural networks using online Sarsa and neural-fitted Sarsa, both with a short term memory reward function. We also present an incremental learning method for training the units for larger scenarios involving more units using trained neural networks on smaller scenarios. Additionally, we developed a novel sensing system to feed the environment data to the neural networks using separate vision grids. The simulation results show superior performance against the human-made AI scripts in StarCraft.},
  eventtitle = {The 2011 {{International Joint Conference}} on {{Neural Networks}}},
  file = {/Users/brandonhosley/Zotero/storage/HW4INV77/Shantia et al_2011_Connectionist reinforcement learning for intelligent unit micro management in.pdf;/Users/brandonhosley/Zotero/storage/PQZVQS9T/6033442.html}
}

@article{silver2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  options = {useprefix=true},
  date = {2016-01},
  journaltitle = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature16961},
  url = {https://www.nature.com/articles/nature16961},
  urldate = {2024-03-15},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  langid = {english},
  keywords = {Computational science,Computer science,Reward},
  file = {/Users/brandonhosley/Zotero/storage/F29T7VGP/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf}
}

@article{silver2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
  options = {useprefix=true},
  date = {2017-10},
  journaltitle = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature24270},
  url = {https://www.nature.com/articles/nature24270},
  urldate = {2024-03-15},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo. Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games. To beat world champions at the game of Go, the computer program AlphaGo has relied largely on supervised learning from millions of human expert moves. David Silver and colleagues have now produced a system called AlphaGo Zero, which is based purely on reinforcement learning and learns solely from self-play. Starting from random moves, it can reach superhuman level in just a couple of days of training and five million games of self-play, and can now beat all previous versions of AlphaGo. Because the machine independently discovers the same fundamental principles of the game that took humans millennia to conceptualize, the work suggests that such principles have some universal character, beyond human bias.},
  langid = {english},
  keywords = {Computational science,Computer science,Reward},
  file = {/Users/brandonhosley/Zotero/storage/RCPCCGV2/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf}
}

@online{silver2017a,
  title = {Mastering {{Chess}} and {{Shogi}} by {{Self-Play}} with a {{General Reinforcement Learning Algorithm}}},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  date = {2017-12-05},
  eprint = {1712.01815},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1712.01815},
  url = {http://arxiv.org/abs/1712.01815},
  urldate = {2024-03-15},
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/brandonhosley/Zotero/storage/PFGFAPGW/Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a Gene.pdf;/Users/brandonhosley/Zotero/storage/SR3IRUXR/1712.html}
}

@article{song2023,
  title = {Ensemble Reinforcement Learning: {{A}} Survey},
  shorttitle = {Ensemble Reinforcement Learning},
  author = {Song, Yanjie and Suganthan, Ponnuthurai Nagaratnam and Pedrycz, Witold and Ou, Junwei and He, Yongming and Chen, Yingwu and Wu, Yutong},
  date = {2023-12-01},
  journaltitle = {Applied Soft Computing},
  shortjournal = {Applied Soft Computing},
  volume = {149},
  pages = {110975},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2023.110975},
  url = {https://www.sciencedirect.com/science/article/pii/S1568494623009936},
  urldate = {2023-11-25},
  abstract = {Reinforcement Learning (RL) has emerged as a highly effective technique for addressing various scientific and applied problems. Despite its success, certain complex tasks remain challenging to be addressed solely with a single model and algorithm. In response, ensemble reinforcement learning (ERL), a promising approach that combines the benefits of both RL and ensemble learning (EL), has gained widespread popularity. ERL leverages multiple models or training algorithms to comprehensively explore the problem space and possesses strong generalization capabilities. In this study, we present a comprehensive survey on ERL to provide readers with an overview of recent advances and challenges in the field. Firstly, we provide an introduction to the background and motivation for ERL. Secondly, we conduct a detailed analysis of strategies such as model selection and combination that have been successfully implemented in ERL. Subsequently, we explore the application of ERL, summarize the datasets, and analyze the algorithms employed. Finally, we outline several open questions and discuss future research directions of ERL. By offering guidance for future scientific research and engineering applications, this survey significantly contributes to the advancement of ERL.},
  keywords = {Artificial neural network,Ensemble learning,Ensemble reinforcement learning,Ensemble strategy,Reinforcement learning},
  file = {/Users/brandonhosley/Zotero/storage/CK4PM6HW/Song et al_2023_Ensemble reinforcement learning.pdf;/Users/brandonhosley/Zotero/storage/TCX78WLC/S1568494623009936.html}
}

@article{sridharan2019,
  title = {Towards a {{Theory}} of {{Explanations}} for {{Human}}–{{Robot Collaboration}}},
  author = {Sridharan, Mohan and Meadows, Ben},
  date = {2019-12-01},
  journaltitle = {KI - Künstliche Intelligenz},
  shortjournal = {Künstl Intell},
  volume = {33},
  number = {4},
  pages = {331--342},
  issn = {1610-1987},
  doi = {10.1007/s13218-019-00616-y},
  url = {https://doi.org/10.1007/s13218-019-00616-y},
  urldate = {2023-11-02},
  abstract = {This paper makes two contributions towards enabling a robot to provide explanatory descriptions of its decisions, the underlying knowledge and beliefs, and the experiences that informed these beliefs. First, we present a theory of explanations comprising (i) claims about representing, reasoning with, and learning domain knowledge to support the construction of explanations; (ii) three fundamental axes to characterize explanations; and (iii) a methodology for constructing these explanations. Second, we describe an architecture for robots that implements this theory and supports scalability to complex domains and explanations. We demonstrate the architecture’s capabilities in the context of a simulated robot (a) moving target objects to desired locations or people; or (b) following recipes to bake biscuits.},
  langid = {english},
  keywords = {Explanations,Human–robot collaboration,Non-monotonic logical reasoning,Probabilistic planning},
  file = {/Users/brandonhosley/Zotero/storage/I899MPDI/Sridharan_Meadows_2019_Towards a Theory of Explanations for Human–Robot Collaboration.pdf}
}

@article{vinyals2019,
  title = {Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning},
  author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and Wünsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  date = {2019-11-14},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {575},
  number = {7782},
  pages = {350--354},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1724-z},
  url = {https://www.nature.com/articles/s41586-019-1724-z},
  urldate = {2023-12-24},
  langid = {english},
  file = {/Users/brandonhosley/Zotero/storage/97RK6RVS/Vinyals et al. - 2019 - Grandmaster level in StarCraft II using multi-agen.pdf}
}

@inproceedings{wang2021,
  title = {{{SCC}}: An Efficient Deep Reinforcement Learning Agent Mastering the Game of {{StarCraft II}}},
  shorttitle = {{{SCC}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Wang, Xiangjun and Song, Junxiao and Qi, Penghui and Peng, Peng and Tang, Zhenkun and Zhang, Wei and Li, Weimin and Pi, Xiongjun and He, Jujie and Gao, Chao and Long, Haitao and Yuan, Quan},
  date = {2021-07-01},
  pages = {10905--10915},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/wang21v.html},
  urldate = {2023-12-20},
  abstract = {AlphaStar, the AI that reaches GrandMaster level in StarCraft II, is a remarkable milestone demonstrating what deep reinforcement learning can achieve in complex Real-Time Strategy (RTS) games. However, the complexities of the game, algorithms and systems, and especially the tremendous amount of computation needed are big obstacles for the community to conduct further research in this direction. We propose a deep reinforcement learning agent, StarCraft Commander (SCC). With order of magnitude less computation, it demonstrates top human performance defeating GrandMaster players in test matches and top professional players in a live event. Moreover, it shows strong robustness to various human strategies and discovers novel strategies unseen from human plays. In this paper, we’ll share the key insights and optimizations on efficient imitation learning and reinforcement learning for StarCraft II full game.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/brandonhosley/Zotero/storage/KFLKSCJ6/Wang et al. - 2021 - SCC an efficient deep reinforcement learning agen.pdf}
}

@article{wells2021,
  title = {Explainable {{AI}} and {{Reinforcement Learning}}—{{A Systematic Review}} of {{Current Approaches}} and {{Trends}}},
  author = {Wells, Lindsay and Bednarz, Tomasz},
  date = {2021},
  journaltitle = {Frontiers in Artificial Intelligence},
  volume = {4},
  issn = {2624-8212},
  url = {https://www.frontiersin.org/articles/10.3389/frai.2021.550030},
  urldate = {2023-11-02},
  abstract = {Research into Explainable Artificial Intelligence (XAI) has been increasing in recent years as a response to the need for increased transparency and trust in AI. This is particularly important as AI is used in sensitive domains with societal, ethical, and safety implications. Work in XAI has primarily focused on Machine Learning (ML) for classification, decision, or action, with detailed systematic reviews already undertaken. This review looks to explore current approaches and limitations for XAI in the area of Reinforcement Learning (RL). From 520 search results, 25 studies (including 5 snowball sampled) are reviewed, highlighting visualization, query-based explanations, policy summarization, human-in-the-loop collaboration, and verification as trends in this area. Limitations in the studies are presented, particularly a lack of user studies, and the prevalence of toy-examples and difficulties providing understandable explanations. Areas for future study are identified, including immersive visualization, and symbolic representation.},
  file = {/Users/brandonhosley/Zotero/storage/HBIJ5FGS/Wells_Bednarz_2021_Explainable AI and Reinforcement Learning—A Systematic Review of Current.pdf}
}

@article{wiering2008,
  title = {Ensemble {{Algorithms}} in {{Reinforcement Learning}}},
  author = {Wiering, Marco A. and van Hasselt, Hado},
  options = {useprefix=true},
  date = {2008-08},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  volume = {38},
  number = {4},
  pages = {930--936},
  issn = {1941-0492},
  doi = {10.1109/TSMCB.2008.920231},
  url = {https://ieeexplore.ieee.org/document/4509588},
  urldate = {2023-11-25},
  abstract = {This paper describes several ensemble methods that combine multiple different reinforcement learning (RL) algorithms in a single agent. The aim is to enhance learning speed and final performance by combining the chosen actions or action probabilities of different RL algorithms. We designed and implemented four different ensemble methods combining the following five different RL algorithms: Q-learning, Sarsa, actor-critic (AC), QV-learning, and AC learning automaton. The intuitively designed ensemble methods, namely, majority voting (MV), rank voting, Boltzmann multiplication (BM), and Boltzmann addition, combine the policies derived from the value functions of the different RL algorithms, in contrast to previous work where ensemble methods have been used in RL for representing and learning a single value function. We show experiments on five maze problems of varying complexity; the first problem is simple, but the other four maze tasks are of a dynamic or partially observable nature. The results indicate that the BM and MV ensembles significantly outperform the single RL algorithms.},
  eventtitle = {{{IEEE Transactions}} on {{Systems}}, {{Man}}, and {{Cybernetics}}, {{Part B}} ({{Cybernetics}})},
  file = {/Users/brandonhosley/Zotero/storage/ZJUJRV7U/Wiering_van Hasselt_2008_Ensemble Algorithms in Reinforcement Learning.pdf;/Users/brandonhosley/Zotero/storage/U92LPSII/4509588.html}
}

@online{zotero-2222,
  title = {Deciphering {{AlphaStar}} on {{StarCraft II}} | {{Yekun}}'s {{Note}}},
  url = {https://ychai.uk/notes/2019/07/21/RL/DRL/Decipher-AlphaStar-on-StarCraft-II/},
  urldate = {2024-01-24},
  file = {/Users/brandonhosley/Zotero/storage/2GL72AQU/Decipher-AlphaStar-on-StarCraft-II.html}
}

@online{zotero-2247,
  title = {Lichess.Org Open Database},
  url = {https://database.lichess.org/},
  urldate = {2024-03-15},
  file = {/Users/brandonhosley/Zotero/storage/J2U69X67/database.lichess.org.html}
}

@online{zotero-2272,
  title = {{{FICS Games Database}} - {{Download}}},
  url = {https://www.ficsgames.org/download.html},
  urldate = {2024-03-15},
  file = {/Users/brandonhosley/Zotero/storage/R8BPMP88/download.html}
}
