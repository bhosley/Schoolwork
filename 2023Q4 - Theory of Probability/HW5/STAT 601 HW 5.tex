\documentclass[12pt,letterpaper]{exam}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[width=8.50in, height=11.00in, left=0.50in, right=0.50in, top=0.50in, bottom=0.50in]{geometry}

\usepackage{libertine}
\usepackage{multicol}
\usepackage[shortlabels]{enumitem}

\usepackage{booktabs}
\usepackage[table]{xcolor}

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bbm}

\usepackage{hyperref}
\usepackage{graphicx}
%\usepackage{wrapfig}
%\usepackage{capt-of}
%\usepackage{tikz}
%\usepackage{pgfplots}
%\usetikzlibrary{shapes,arrows,positioning,patterns}
%\usepackage{pythonhighlight}

\newcommand\chapter{5}
\renewcommand{\thequestion}{\textbf{\chapter.\arabic{question}}}
\renewcommand{\questionlabel}{\thequestion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\class}{STAT 601} % This is the name of the course 
\newcommand{\assignmentname}{Homework \# \chapter} % 
\newcommand{\authorname}{Hosley, Brandon} % 
\newcommand{\workdate}{\today} % 
\printanswers % this includes the solutions sections
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}
\pagestyle{plain}
\thispagestyle{empty}
\noindent
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
\begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} r @{\extracolsep{10pt}} l}
	\textbf{\class} & \textbf{\authorname}  &\\ %Your name here instead, obviously 
	\textbf{\assignmentname } & \textbf{\workdate} & \\
\end{tabular*}\\ 
\rule{\textwidth}{2pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% HEADER %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{questions}
	
	%5.3
	\setcounter{question}{2}
	\question 
	Let \(X_1,\ldots, X_n\) be iid random variables with continuous cdf \(F_X\), 
	and suppose \(E\,X_i = \mu\).
	Define the random variables \(Y_1,\ldots, Y_n\) by
	
	\[ 
	Y_i = 
	\begin{cases}
		1 & \text{if } X_i > \mu \\
		0 & \text{if } X_i \leq\mu.
	\end{cases}
	\]
	
	Find the distribution of \(\sum^{n}_{i=1}Y_i\).
	
	\begin{solution}
		The distribution can be calculated if \(F_X\) is known; the relationship between the distributions is,
		\[\sum^{n}_{i=1}Y_i \sim \text{binom}(n,p = P(X>\mu)).\]
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	%5.6
	\setcounter{question}{5}
	\question 
	If \(X\) has pdf \(f_X(x)\) and \(Y\), independent of \(X\), has pdf \(f_Y(y)\), 
	establish formulas, similar to (5.2.3), for the random variable \(Z\) 
	in each of the following situations.
	\begin{parts}
		\part \(Z = X - Y\)
		\part \(Z = XY\)
		\part \(Z = X/Y\)
	\end{parts}
	
	\begin{solution}
		\begin{parts}
			\part
			Let
			\[ Z = X - Y \quad\text{ and }\quad W = X \]
			then,
			\[ \det J = \begin{vmatrix} 0&1 \\ -1&1 \end{vmatrix} = 1 \]
			and,
			\[ f_{W,Z}(w,z) = f_X(g^{-1}(y))f_Y(g^{-1}(y)) = f_X(w)f_Y(z-w). \]
			We can then use this to find the marginal distribution of \(Z\),
			\[\int_{-\infty}^{\infty} f_X(w)f_Y(z-w) \,dw \]
			
			\part
			Let
			\[ Z = XY \quad\text{ and }\quad W = X \]
			then,
			\[ \det J = \begin{vmatrix} 0&1 \\ \frac{1}{w}&-\frac{z}{w^2} \end{vmatrix} = -\frac{1}{w} \]
			and,
			\[ f_{W,Z}(w,z) = f_X(g^{-1}(y))f_Y(g^{-1}(y)) = f_X(w)f_Y(z/w)\left|-\frac{1}{w}\right|. \]
			We can then use this to find the marginal distribution of \(Z\),
			\[\int_{-\infty}^{\infty} f_X(w)f_Y(z/w)\frac{1}{w} \,dw \]
			
			\part
			Let
			\[ Z = X/Y \quad\text{ and }\quad W = X \]
			then,
			\[ \det J = \begin{vmatrix} 0&1 \\ -\frac{w}{z^2}&\frac{1}{z} \end{vmatrix} = \frac{w}{z^2}\]
			and,
			
			\[ f_{W,Z}(w,z) = f_X(g^{-1}(y))f_Y(g^{-1}(y)) = f_X(w)f_Y(w/z)\left|\frac{w}{z^2}\right|. \]
			We can then use this to find the marginal distribution of \(Z\),
			\[\int_{-\infty}^{\infty} f_X(w)f_Y(w/z)\frac{1}{w} \frac{w}{z^2} \,dw \]
			
		\end{parts}
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	%5.11
	\setcounter{question}{10}
	\question 
	Suppose \(\bar{X}\) and \(S^2\) are calculated from a random sample \(X_1,\ldots, X_n\) 
	drawn from a population with finite variance \(\sigma^2\). We know that \(E\,S^2 = \sigma^2\). 
	Prove that \(E\,S \leq \sigma\), and
	if \(\sigma^2 > 0\), then \(E\,S < \sigma\).
	
	\begin{solution}
		We start with
		\[E[S^2] = \sigma^2.\]
		Jensen's inequality states that 
		\[E[S^2] \geq E[S]^2\]
		and transitively that
		\[\sigma^2 \geq E[S]^2,\]
		thus,
		\[\sigma \geq E[S].\]
		
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	%5.15
	\setcounter{question}{14}
	\question 
	Establish the following recursion relations for means and variances. 
	Let \(\bar{X}_n\) and \(S_n^2\) be the mean and variance, respectively, of \(X_1,\ldots,X_n\). 
	Then suppose another observation, \(X_{n+1}\), becomes available. Show that
	
	\begin{parts}
		\part \(\bar{X}_{n+1} = \frac{X_{n+1}  + n\bar{X}_n}{n + 1} \).
		\part \(nS_{n+1}^2 = (n-1)S_n^2 + \left( \frac{n}{n+1} \right) (X_{n+1}-\bar{X}_n)^2\).
	\end{parts}
	
	\begin{solution}
		\begin{parts}
			\part
			\[
			\bar{X}_{n+1} = \frac{\sum_{i=1}^{n+1}X_i}{n+1} = \frac{X_{n+1} + \sum_{i=1}^{n}X_i}{n+1} = \frac{X_{n+1} + n\bar X_n}{n+1}
			\]
			
			\part
			\begin{align*}
				nS_{n+1}^2 &= n\sum_{i=1}^{n+1} \frac{\left(X_i - \bar{X}_{n+1} \right)^2}{(n+1)-1} \\
				&= \sum_{i=1}^{n+1} \left(X_i - \bar{X}_{n+1} \right)^2 \\
				&= \sum_{i=1}^{n+1} \left(X_i - \frac{X_{n+1}+n\bar{X}_{n}}{n+1} \right)^2 \\
				&= \sum_{i=1}^{n+1} \left(X_i - \frac{X_{n+1}}{n+1} - \frac{n\bar{X}_{n}}{n+1} \right)^2 \\
				&= \sum_{i=1}^{n+1} \left(X_i - \frac{X_{n+1}}{n+1} - \frac{n\bar{X}_{n}+\bar{X}_{n}-\bar{X}_{n}}{n+1} \right)^2 \\
				&= \sum_{i=1}^{n+1} \left(X_i - \frac{X_{n+1}}{n+1} - \frac{(n+1)\bar{X}_{n}}{n+1} - \frac{\bar{X}_{n}}{n+1} \right)^2 \\
				&= \sum_{i=1}^{n+1} \left((X_i - \bar{X}_{n}) - \frac{ X_{n+1} - \bar{X}_{n} }{n+1} \right)^2 \\
				&= \sum_{i=1}^{n+1} (X_i - \bar{X}_{n})^2 + \left(\frac{ X_{n+1}-\bar{X}_{n} }{n+1} \right)^2 -2(X_i - \bar{X}_{n})	\frac{ X_{n+1}+\bar{X}_{n} }{n+1} \\
				&= \frac{\left( X_{n+1}-\bar{X}_{n}\right)^2 }{n+1}+ (X_{n+1}-\bar{X}_{n})^2  + \sum_{i=1}^{n} (X_i-\bar{X}_{n})^2 
					- \sum_{i=1}^{n+1} 2(X_i - \bar{X}_{n})\frac{ X_{n+1}-\bar{X}_{n} }{n+1} \\
				&= \frac{\left( X_{n+1}-\bar{X}_{n}\right)^2 }{n+1}+ (X_{n+1}-\bar{X}_{n})^2  + \sum_{i=1}^{n} (X_i-\bar{X}_{n})^2 
				-2\frac{ X_{n+1}-\bar{X}_{n} }{n+1} \sum_{i=1}^{n+1} (X_i - \bar{X}_{n}) \\
				&= \frac{\left(X_{n+1}-\bar{X}_{n}\right)^2 }{n+1}+ (X_{n+1}-\bar{X}_{n})^2 -2\frac{ (X_{n+1}-\bar{X}_{n})^2 }{n+1} +\sum_{i=1}^{n} (X_i-\bar{X}_{n})^2 \\
				&= \frac{\left( X_{n+1}-\bar{X}_{n}\right)^2 }{n+1}+ (X_{n+1}-\bar{X}_{n})^2 -2\frac{ (X_{n+1}-\bar{X}_{n})^2 }{n+1}  +(n+1)\sum_{i=1}^{n} \frac{(X_i-\bar{X}_{n})^2}{(n+1)} \\
				&= (n+1)S_n^2 + \frac{\left( X_{n+1}-\bar{X}_{n}\right)^2 -2(X_{n+1}-\bar{X}_{n})^2}{n+1}+ (X_{n-1}-\bar{X}_{n})^2 \\
				&= (n+1)S_n^2 + \frac{\left( X_{n+1}-\bar{X}_{n}\right)^2 -2(X_{n+1}-\bar{X}_{n})^2 +  (n+1)(X_{n-1}-\bar{X}_{n})^2}{n+1} \\
				&= (n+1)S_n^2 + \left( X_{n+1}-\bar{X}_{n}\right)^2 \frac{1 -2 + (n+1)}{n+1} \\
				&= (n+1)S_n^2 + \frac{n}{n+1}\left( X_{n+1}-\bar{X}_{n}\right)^2  \\
			\end{align*}			
		\end{parts}
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\clearpage
	
	%5.16
	\question 
	Let \(X_i,i = 1, 2, 3\), be independent with n\((i,i^2)\) distributions. 
	For each of the following situations, use the \(X_is\) to construct a statistic with the indicated distribution.
	
	\begin{parts}
	\part chi squared with 3 degrees of freedom
	\part \(t\) distribution with 2 degrees of freedom
	\part \(F\) distribution with 1 and 2 degrees of freedom
	\end{parts}
	
	
	\begin{solution}
		\begin{parts}
			\part
			Normalizing and using Lemma 5.3.2 parts 1 and 2;
			\[\sum_{i=1}^{3} \left(\frac{X_i-i}{i}\right)^2 \sim \chi^2_{(3)}\]
			
			\part
			Using theorem 3 in class notes 5.3; The distribution we seek to replicate is
			\[ T = \frac{Z}{\sqrt{\frac{V}{r=2}}} \sim t(r=2). \]
			First, finding a \(V\) equivalent,
			\[ N(0,1) \sim Z \equiv \frac{X_j-j}{j} \quad j\in \{i\} \]
			then finding a \(V\),
			\[ \chi^2_{(2)} \sim V \equiv \sum_{\{i\}/j} \left(\frac{X_i-i}{i}\right)^2 .\]
			
			\part
			The \(F\) distribution described in the class notes as,
			\[ F(r_1=1,r_2=2) \sim \frac{\frac{Z^2}{r_1=1}}{\frac{V}{r=2}}\]
			can use the same \(Z\) and \(V\) as described in part (b). Or collected as,
			
			\[ \frac{2\left(\frac{X_j-j}{j}\right)^2}{\sum_{\{i\}/j} \left(\frac{X_i-i}{i}\right)^2} \sim  F(1,2), \quad j\in\{i\}\]
			
		\end{parts}
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\clearpage
	
	%5.17
	\question 
	Let \(X\) be a random variable with an \(F_{p,q}\) distribution.
	
	\begin{parts}
		\part Derive the pdf of \(X\).
		\part Derive the mean and variance of \(X\).
		\part Show that \(1/X\) has an \(F_{q,p}\) distribution.
		\part Show that \((p/q)X/[1 + (p/q)X]\) has a beta distribution with parameters \(p/2\) and \(q/2\).
	\end{parts}
	
	\begin{solution}
		\begin{parts}
			\part
			Let \(V_1\sim\chi^2_{(p)}\) and \(V_2\sim\chi^2_{(q)}\)
			and let
			\[Z = \frac{V_1/p}{V_2/q} \quad\text{ and }\quad Y = V_2.\]
			Then,
			\[ J = \begin{vmatrix}\frac{yp}{q}&\frac{zp}{q}\\0&1\end{vmatrix} y\frac{p}{q} \]
			and
			\[X = f_{Z,Y}(z,y)= f_{v_1}(zy\frac{p}{q}) f_{v_2}(y) y\frac{p}{q} \]
			
			\part
			
			
			\part
			
			
			\part
			
			
		\end{parts}
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	%5.22
	\setcounter{question}{21}
	\question 
	Let \(X\) and \(Y\) be iid n\((0, 1)\) random variables, 
	and define \(Z = \min(X,Y)\). Prove that \(Z^2 \sim \chi^2_1 \).
	
	\begin{solution}
		\begin{align*}
			F_{Z^2}(z) 
			&= P(\min(x,y)^2\leq z) \\
			&= P(\min(x,y)\leq \sqrt{z}) - P(\min(x,y)\leq -\sqrt{z}) \\
			&= P(\min(x,y)\geq -\sqrt{z}) - P(\min(x,y)\geq \sqrt{z}) \\
			&= P(x\geq -\sqrt{z})P(y\geq -\sqrt{z}) - P(x\geq \sqrt{z})P(y\geq \sqrt{z}) \\
			\intertext{since \(X\) and \(Y\) are iid. let \(X,Y\in W\) where \(W\sim\) N\((0,1)\)}
			&\equiv P(w\geq -\sqrt{z})^2 - P(w\geq \sqrt{z})^2 \\
			&= P(w^2\geq -z) - P(w^2\geq z) \\
			&= P(w^2\leq z) - P(w^2\leq -z) \\
			\intertext{we know that a normal distribution squared is a chi-squared distribution with \(1\) degree of freedom,}
			&=\frac{1}{2} \int_{0}^{z} \frac{1}{2^{1/2}\Gamma(1/2)}w^{(2)1/2-1}e^{-w^2/2} \,dw
			- \frac{1}{2}\int_{0}^{-z} \frac{1}{2^{1/2}\Gamma(1/2)}w^{(2)1/2-1}e^{-w^2/2} \,dw \\
			&= \frac{1}{\pi\sqrt{2}}\left( \frac{1}{2}\int_{0}^{z} w^{(2)1/2-1}e^{-w^2/2} \,dw
			- \frac{1}{2}\int_{0}^{-z} w^{(2)1/2-1}e^{-w^2/2} \,dw \right) \\
			&= \frac{1}{2}\frac{1}{\pi\sqrt{2}}\left( \int_{0}^{z} w^{-1}e^{-w^2/2} \,dw - \int_{0}^{-z} w^{-1}e^{-w^2/2} \,dw \right) \\
			\intertext{graphing the function shows that it is odd, which allows,}
			&= \frac{1}{2}\frac{1}{\pi\sqrt{2}}\left( \int_{0}^{z} w^{-1}e^{-w^2/2} \,dw + \int_{0}^{z} w^{-1}e^{-w^2/2} \,dw \right) \\
			\intertext{thus,}
			&= \frac{1}{2}\frac{1}{\pi\sqrt{2}}\left( 2\int_{0}^{z} w^{-1}e^{-w^2/2} \,dw  \right) \\
			\intertext{which simplifies to a Chi-squared distribution,}
			&= \int_{0}^{z} \frac{1}{\pi\sqrt{2}} w^{-1}e^{-w^2/2} \,dw.  \\
		\end{align*}
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	%5.24
	\setcounter{question}{23}
	\question 
	Let \(X_1,\ldots, X_n\) be a random sample from a population with pdf
	
	\[
	f_X(x)=
	\begin{cases}
		1/\theta & \text{if } 0 < x < \theta \\
		0 	& \text{otherwise}.
	\end{cases}
	\]
	
	Let \(X_{(1)} < \ldots < X_{(n)}\) be the order statistics. 
	Show that \(X_{(1)}/X_{(n)}\) and \(X_{(n)}\) are independent random variables.
	
	\begin{solution}
		Where \(f_X(x) = 1/\theta\), \(F_X(x) = x/\theta\).
		The joint PDF
		\[f(y_1,y_n) = \frac{n!}{(n-2)!}\left(\frac{1}{\theta}\right)^2 \left(\frac{y_1}{\theta}-\frac{y_{n}}{\theta}\right)^{n-2}
		= n(n-1)\left(\frac{1}{\theta}\right)^n \left({y_1}-y_{n}\right)^{n-2} .\]
		
		Let
		\(Z = X_{(1)}/X_{(n)}.\)
		
		The jacobian for a \(z, y_n\) is
		\[\det J = \begin{vmatrix}1 &0 \\ y_1 &y_n \end{vmatrix} = y_n.\]
		Thus,
		\[
		f(z,y_n) = n(n-1)\left(\frac{1}{\theta}\right)^n \left({zy_{n}}-y_{n}\right)^{n-2} \,y_n
		= n(n-1)\left(\frac{1}{\theta}\right)^n \left(z-1\right)^{n-2} \left(y_{n}\right)^{n-1}.
		\]
		Independence can be seen in that the joint probability can be refactored into two independent functions of each variable.

		The support is \(0<Y_n<\theta\) and \(0<z<(\frac{\theta}{\theta} =1)\).
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	%5.31
	\setcounter{question}{30}
	\question
	Suppose \(\bar{X}\) is the mean of 100 observations from a population with mean \(\mu\) and variance \(\sigma^2 = 9\). 
	Find limits between which \(\bar{X}-\mu\) will lie with probability at least \(.90\). 
	Use both Chebychev’s Inequality and the Central Limit Theorem, and comment on each. 
	
	\begin{solution}
		From \(\sigma^2=9\) we can determine that \(\sigma_{\bar X} = 3/10\). 
		Using Chebyshev's inequality,
		\[ P(|\bar{X}-\mu|\geq k\sigma_{\bar X}) \leq \frac{1}{k^2}\]
		we can calculate,
		\[ P(|\bar{X}-\mu|\leq 3/\sqrt{10}) \geq 0.9.\]
		Thus, using Chebyshev's inequality we determine that with a probability of at least 90\%, \(\bar{X} \leq \mu\pm0.949\). \\
		
		Using a standard normal table we can look up that \(0.95\) (two-tailed .9) corresponds to \(Z=1.645\).	
		\[ \left| \frac{ \bar{X}_n-\mu }{ \sigma_{\bar{X}} } \right| \leq 1.645\]
		\[ \left| \bar{X}_n-\mu \right| \leq 0.4935\]
		\[ P(\bar{X}_n \leq \mu \pm 0.4935) = 0.9.\]
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		

\end{questions}
\end{document}
