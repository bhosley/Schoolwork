\documentclass[12pt,letterpaper]{exam}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[width=8.50in, height=11.00in, left=0.50in, right=0.50in, top=0.50in, bottom=0.50in]{geometry}

\usepackage{libertine}
\usepackage{multicol}
\usepackage[shortlabels]{enumitem}

\usepackage{booktabs}
\usepackage[table]{xcolor}

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bbm}

\usepackage{hyperref}
\usepackage{graphicx}
%\usepackage{wrapfig}
%\usepackage{capt-of}
\usepackage{tikz}
%\usepackage{pgfplots}
%\usetikzlibrary{shapes,arrows,positioning,patterns}
%\usepackage{pythonhighlight}

\newcommand\chapter{4}
\renewcommand{\thequestion}{\textbf{\chapter.\arabic{question}}}
\renewcommand{\questionlabel}{\thequestion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\class}{STAT 601} % This is the name of the course 
\newcommand{\assignmentname}{Homework \# \chapter} % 
\newcommand{\authorname}{Hosley, Brandon} % 
\newcommand{\workdate}{\today} % 
\printanswers % this includes the solutions sections
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}
\pagestyle{plain}
\thispagestyle{empty}
\noindent
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
\begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} r @{\extracolsep{10pt}} l}
	\textbf{\class} & \textbf{\authorname}  &\\ %Your name here instead, obviously 
	\textbf{\assignmentname } & \textbf{\workdate} & \\
\end{tabular*}\\ 
\rule{\textwidth}{2pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% HEADER %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{questions}

	%4.4
	\setcounter{question}{3}
	\question 
	A pdf is defined by
	
	\[
	f(x,y)=
	\begin{cases}
		C(x + 2y) & \text{if } 0 < y < 1 \text{ and } 0 < x < 2 \\
		0 & \text{otherwise}.
	\end{cases}
	\]
	
	\begin{parts}
		\part Find the value of \(C\).
		\part Find the marginal distribution of \(X\).
		\part Find the joint cdf of \(X\) and \(Y\).
		\part Find the pdf of the random variable \(Z = 9/(X + 1)^2\).
	\end{parts}
	
	\begin{solution}
		\begin{parts}
			\part
			If \(f(x,y)\) is a valid pdf then,
			\begin{align*}
				\int f(x,y) = 1
				&= \int_{0}^{2} \int_{0}^{1} C(x+2y) \,dy\,dx \\
				&= \int_{0}^{2} \bigg[Cxy+Cy^2\bigg]_0^1 \,dx \\
				&= \int_{0}^{2} Cx+C \,dx \\
				&= \left. Cx+\frac{Cx^2}{2} \right|_{0}^{2} \\
				&= 4C, \\
			\end{align*}
			thus we conclude that \(C = \frac{1}{4}\).
			
			\part
			We obtain the marginal by integrating out all other variables,
			\begin{align*}
				f_X(x)
				=\ \int_{0}^{1} \frac{1}{4} (x+2y) \,dy
				=\ \left. \frac{1}{4}xy + \frac{1}{4}y^2 \right|_0^1
				=\ \frac{1}{4}(x+1).
			\end{align*}

			\part
			For \(0<x<2\) and \(0<y<1\),
			\begin{align*}
				F(x,y)
				&= \int_{0}^{x} \int_{0}^{y} \frac{1}{4}x + \frac{1}{2}y \,dy \,dx \\
				&= \int_{0}^{x} \frac{1}{4}xy + \frac{1}{4}y^2 \,dx \\
				&= \frac{1}{8}x^2y + \frac{1}{4}xy^2 \\
				&= \frac{1}{4}xy \left(\frac{1}{2}x + y \right). \\
			\end{align*}
			
			The unintuitive extension of the distribution beyond joint feasibility,
			
			\(\frac{1}{8}x^2 + \frac{1}{4}x\ \) for \(1 \leq y\) and \(0<x<2\), and
			
			\(\frac{1}{4}y^2 + \frac{1}{8}y\ \) for \(2 \leq x\) and \(0<y<1\).
			
			\part
			
			\(g^{-1}(x) = \sqrt{\frac{9}{z}} - 1\)
			
			\begin{align*}
				f_Z(z)
				&= \frac{1}{4} \left( \sqrt{\frac{9}{z}} \right) \left| \frac{d}{dz} g^{-1}(x) \right| \\
				&= \frac{1}{4} \left( \frac{3}{\sqrt{z}} \right) \left( \frac{3}{2} z^{-\frac{3}{2}} \right) \\
				&= \frac{9}{8} z^{-2}
			\end{align*}
			
			Using \(g^{-1}(x)\) we calculate the support to be over \(1<z<9\).
			
		\end{parts}		
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	%4.5
	\question 
	\begin{parts}
		\part Find \(P(X>\sqrt{Y})\) if \(X\) and \(Y\) are jointly distributed with pdf
			\[f(x,y) = x + y, \quad 0 \leq x \leq 1, \quad 0 \leq y \leq 1.\]
		\part Find \(P(X^2 < Y < X)\) if \(X\) and \(Y\) are jointly distributed with pdf
			\[f(x,y) = 2x, \quad 0 \leq x \leq 1, \quad 0 \leq y \leq 1.\]
	\end{parts}
	
	\begin{solution}
		\begin{parts}
			\part
			
			\begin{align*}
				P(X > \sqrt{Y})
				&= \int_{0}^{1} \int_{\sqrt{y}}^{1} x + y \,dx \,dy \\
				&= \int_{0}^{1} \left[ \frac{1}{2}x^2 + xy \right]_{\sqrt{y}}^{1} \,dy \\
				&= \int_{0}^{1} \left[ \left(\frac{1}{2} + y\right) - \left(\frac{1}{2}y + y^{3/2}\right) \right] \,dy \\
				&= \int_{0}^{1} \frac{1}{2} + \frac{1}{2}y - y^{3/2} \,dy \\
				&= \left. \frac{1}{2}y + \frac{1}{4}y^2 - \frac{2}{5}y^{5/2} \right|_{0}^{1} \\
				&= \frac{1}{2} + \frac{1}{4} - \frac{2}{5} \\
				&= \frac{7}{20} \\
			\end{align*}
			
			\part
			
			\begin{align*}
				P(X^2 < Y < X)
				&= \int_{0}^{1} \int_{X^2}^{X} 2x \,dy \,dx \\
				&= \int_{0}^{1} \left[2xy\right]_{X^2}^{X} \,dx \\
				&= \int_{0}^{1} 2x^2 - 2x^3 \,dx \\
				&= \left. \frac{2}{3}x^3 - \frac{1}{2}x^4 \right|_{0}^{1} \\
				&= \frac{2}{3} - \frac{1}{2} \\
				&= \frac{1}{6} \\
			\end{align*}			
			
		\end{parts}
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	%4.7
	\setcounter{question}{6}
	\question 
	A woman leaves for work between 8 AM and 8:30 AM and takes between 40 and 50 
	minutes to get there. Let the random variable \(X\) denote her time of departure, and
	the random variable \(Y\) the travel time. Assuming that these variables are independent
	and uniformly distributed, find the probability that the woman arrives at work before
	9 AM.
	
	\begin{solution}
		Support is over \(0<x<30\), \(40<y<50\). The marginal probabilities for each part is		
		\(f(x) = \frac{1}{30} \mathbbm{1}_{(0,30)}(x)\) 
		and
		\(f(y) = \frac{1}{10} \mathbbm{1}_{(40,50)}(y)\).
		\begin{align*}
			P(X+Y<60)
			&= P(X<60-Y) \\
			&= \int_{40}^{50} \int_{0}^{60-y} \frac{1}{30} \mathbbm{1}(x) \frac{1}{10} \mathbbm{1}(y) \,dx \,dy \\
			&= \int_{40}^{50} \left. \frac{1}{30}x \right|_{0}^{60-y} \frac{1}{10} \mathbbm{1}(y) \,dy \\
			&= \int_{40}^{50} \frac{1}{5} - \frac{y}{300} \,dy \\
			&= \left. \frac{y}{5} - \frac{y^2}{300} \right|_{40}^{50} \\
			&= \left(10 - \frac{25}{6}\right) - \left(8 - \frac{16}{6}\right) \\
			&= 2 - \frac{9}{2} \\
			&= \frac{1}{2} .
		\end{align*}
		
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\clearpage
	
	%4.9
	\setcounter{question}{8}
	\question 
	Prove that if the joint cdf of \(X\) and \(Y\) satisfies
		\[F_{X,Y} (x,y) = F_X(x)F_Y(y),\]
	then for any pair of intervals \((a, b)\), and \((c, d)\),
		\[P(a \leq X \leq b, c \leq Y \leq d) = P(a \leq X \leq b)P(c \leq Y \leq d).\]
	
	\begin{solution}
		\begin{align*}
			P(a \leq X \leq b, c \leq Y \leq d) 
			&=\int_{a}^{b} \int_{c}^{d} F_{X,Y} (x,y) \,dy \,dx \\
			&=\int_{a}^{b} \int_{c}^{d} F_X(x)F_Y(y) \,dy \,dx \\
			&=\int_{a}^{b} F_X(x) \,dx \int_{c}^{d} F_Y(y) \,dy \\
			&= P(a \leq X \leq b)P(c \leq Y \leq d).
		\end{align*}
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	%4.10
	\question 
	The random pair \((X,Y)\) has the distribution
	
	\begin{center}
		\def\arraystretch{1.25}
		\begin{tabular}{cc|ccc}
			& & & \(X\) & \\
			& & 1 & 2 & 3 \\
			\hline
			& 2 & \(\frac{1}{12}\) & \(\frac{1}{6}\) & \(\frac{1}{12}\) \\
			\(Y\) & 3 & \(\frac{1}{6}\)  & \(0\) & \(\frac{1}{6}\) \\
			& 4 & \(0\) & \(\frac{1}{3}\) &  \(0\)
		\end{tabular}
	\end{center}
	
	
	\begin{parts}
		\part Show that \(X\) and \(Y\) are dependent.
		\part Give a probability table for random variables \(U\) and \(V\) 
			that have the same marginals as \(X\) and \(Y\) but are independent.
	\end{parts}
	
	\begin{solution}
		\begin{parts}
			\part
			If the variables are independent then \(P(Y|X) = P(Y)\).
			
			However, \(P(Y=4|X=1) = 0 \neq P(Y=4) = \frac{1}{3}\).
			
			\part \phantom{text}
			
			\begin{center}
				\def\arraystretch{1.25}
				\begin{tabular}{cc|ccc}
					& & & \(U\) & \\
					& & 1 & 2 & 3 \\
					\hline
					& 2 & \(\frac{1}{12}\) & \(\frac{1}{6}\) & \(\frac{1}{12}\) \\
					\(V\) & 3 & \(\frac{1}{12}\)  & \(\frac{1}{6}\) & \(\frac{1}{12}\) \\
					& 4 &\(\frac{1}{12}\) & \(\frac{1}{6}\) &  \(\frac{1}{12}\)
				\end{tabular}
			\end{center}
						
		\end{parts}
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	%4.12
	\setcounter{question}{11}
	\question 
	If a stick is broken at random into three pieces, what is the probability that the pieces
	can be put together in a triangle? (See Gardner 1961 for a complete discussion of this
	problem.)
	
	\begin{solution}
		With \(0\) being the left end and \(1\) being the right end of the stick,
		let \(x_1,x_2\) be the relative positions of the left and right breaks on the stick respectively.
		In order to form a triangle it is necessarily true that
		\[x_1, x_2-x_1, 1-x_2 <0.5.\]
		
		From this we can extrapolate that the two breaks must be on opposite sides of the \(0.5\) (half-way) of the stick.
		This ensures that neither resulting end piece is greater than \(0.5\).
		Now for the middle we will see that in order to prevent this piece being too long we will have to further 
		divide each side in half, where each break will have to be on the same half of their respective halves.
 		
 		\begin{center}
			\begin{tikzpicture}
				% Draw rectangle segments and label them
				\draw[fill=red,  opacity=0.2] (0,0) rectangle (2,0.5) node[midway] {A};
				\draw[fill=blue, opacity=0.2] (2,0) rectangle (4,0.5) node[midway] {B};
				\draw[fill=green,  opacity=0.2] (4,0) rectangle (6,0.5) node[midway] {C};
				\draw[fill=yellow,  opacity=0.2] (6,0) rectangle (8,0.5) node[midway] {D};
			\end{tikzpicture}
 		\end{center}
		
		The necessary positioning can be shown in this diagram. 
		If \(x_1\) is in quadrant A then it is necessary for \(x_2\) to be in quadrant C.
		If \(x_1\) is in quadrant B then it is necessary for \(x_2\) to be in quadrant D.
		Calculating the probability from this will require multiplying by 2
		to account for all of the situations in which the first break is \(x_2\) rather than \(x_1\).
		
		Thus we can calculate the probability of being able to form a triangle as
		\[ 2\left(\frac{1}{4} \cdot \frac{1}{4} + \frac{1}{4} \cdot \frac{1}{4}\right) = \frac{1}{4} \]
		
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	%4.19
	\setcounter{question}{18}
	\question 
	
	\begin{parts}
		\part Let \(X_1\) and \(X_2\) be independent n\((0, 1)\) random variables. 
			Find the pdf of \((X_1 - X_2)^2/2\).
		\part If \(X_i, i = 1, 2,\) are independent gamma\((\alpha_i, 1)\) random variables, 
			find the marginal distributions of \(X_1/(X_1 + X_2)\) and \(X_2/(X_1 + X_2)\).
	\end{parts}
	
	\begin{solution}
		\begin{parts}
			\part
			\begin{align*}
			\intertext{We are given that}
				X_1,X_2 &\overset{iid}{\sim} N(0,1).
			\intertext{Which implies}
				X_1 - X_2 &\sim N(\mu_1 + \mu_2, \sigma^2_1 + \sigma^2_2).		
			\intertext{So via substitution, let}
				Y &= X_1 - X_2 \sim N(0,2).
			\intertext{Then with a further substitution, let}
				Z &= \frac{Y}{\sqrt{2}} \sim N(0,1).
			\intertext{Thus $Z$ can be shown as a Chi-squared distribution as,}
				Z^2 \sim \chi^2_1& = \frac{1}{\Gamma(\frac{p}{2}) 2^{p/2}} x^{(p/2)-1} e^{-x/2} \\
				&= \frac{1}{\Gamma(\frac{1}{2}) 2^{1/2}} z^{2(1/2-1)} e^{-z^2/2} \\
				&= \frac{1}{z\sqrt{2}} e^{-z^2/2}.
			\end{align*}
			
			There is support \(-\infty<z<\infty\) for a chi distribution. 
			However, this distribution has the support \(0<z<\infty\) because 
			\(z\) was calculated using a square-root/square during substitution.
			
			
			\part
			Let
			\begin{align*}
				U &= \frac{X_1}{X_1 + X_2} \qquad q^{-1}_1(u,v) = UV \qquad\ \ \ (=x_1) \\
				V &= {X_1 + X_2} \qquad q^{-1}_2(u,v) = V-UV \quad (=x_2)
			\end{align*}
			
			The Jacobian for the transformation will be,
			\[|J| = \begin{vmatrix} v & u \\ -v & 1-u \end{vmatrix} = (1-u)v + uv = v.\]
			
			Then the transformed joint distribution is,
			\begin{align*}
				f_{UV}(u,v) &= f_{X_1}\left(g^{-1}_1(u,v) g^{-1}_2(u,v)\right) |v| \\
				&= \frac{1}{\Gamma(\alpha_1)\Gamma(\alpha_2)} (uv)^{\alpha_1-1} (v-uv)^{\alpha_2-1} e^{-(uv+v-uv)} \\
				&= \frac{1}{\Gamma(\alpha_1)\Gamma(\alpha_2)} u^{\alpha_1-1} v^{\alpha_1} v^{\alpha_2-1} (1-u)^{\alpha_2-1} e^{-v}.
			\end{align*}
			Which allows us to calculate the marginal distribution,			
			\begin{align*}
				f_U(u)
				&= \int_{0}^{\infty}\frac{1}{\Gamma(\alpha_1)\Gamma(\alpha_2)} 
					u^{\alpha_1-1} v^{\alpha_1} v^{\alpha_2-1} (1-u)^{\alpha_2-1} e^{-v} \,dv \\
				&= \underset{\sim\beta(\alpha_1,\beta_2) \rightarrow 1}{
					\underbrace{  \frac{1}{\Gamma(\alpha_1)\Gamma(\alpha_2)} u^{\alpha_1-1} (1-u)^{\alpha_2-1}  }}\ \
					\underset{\sim Gam(\alpha_1+\alpha_2,1)}{
						\underbrace{ \int_{0}^{\infty} v^{\alpha_1+\alpha_2-1}  e^{-v} \,dv }}.
			\end{align*}
			Thus we can see that for both  \(i\in\{1,2\}\) 
			the marginal distributions are \(Gam(\alpha_1+\alpha_2,1)\).
		\end{parts}
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\clearpage
	
	%4.23
	\setcounter{question}{22}
	\question 
	For \(X\) and \(Y\) as in Example 4.3.3, find the distribution of \(XY\) by making the transformations given in (a) and (b) and integrating out \(V\).
	
	\begin{parts}
		\part \(U = XY ,\ V = Y\)
		\part \(U = XY ,\ V = X/Y\)
	\end{parts}
	
	\begin{solution}
		\begin{parts}
			\part
			Let \(Y = V\) and \(X = U/V\)
			
			The Jacobian
			\[|J| = \begin{vmatrix} \frac{1}{v} & \frac{-u}{v^2} \\ 0 & 1 \end{vmatrix} = \frac{1}{v}\]
			
			\begin{align*}
				f_{UV}(u,v) 
				&= f_{XY}(g^{-1}_1(u,v)g^{-1}_2(u,v)) |J| \\
				&= \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \left(\frac{u}{v}\right)^{\alpha-1} \left(1-\frac{u}{v}\right)^{\beta-1}
				\frac{\Gamma(\alpha+\beta+\gamma)}{\Gamma(\alpha+\beta)\Gamma(\gamma)} \left(v\right)^{\alpha+\beta-1} \left(1-v\right)^{\gamma-1} \left(\frac{1}{v}\right) \\
				&= \frac{\Gamma(\alpha+\beta+\gamma)}{\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)}
					u^{\alpha-1} v^{\beta-1} (1 - u/v)^{\beta-1} (1-v)^{\gamma-1} \\
				&= \frac{\Gamma(\alpha+\beta+\gamma)}{\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)}
					u^{\alpha-1} v^{\beta-1} (v - u)^{\beta-1} (1-v)^{\gamma-1} \\
				&= \frac{\Gamma(\alpha+\beta+\gamma)}{\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)} u^{\alpha-1}
					\int_{u}^{1}  v^{\beta-1} (v - u)^{\beta-1} (1-v)^{\gamma-1} \,dv \\
				\intertext{substituting \(z = \frac{v-u}{1-u}\)}
				&= \frac{\Gamma(\alpha+\beta+\gamma)}{\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)} u^{\alpha-1}(1 - u)^{\beta+\gamma-1}
					\int_{0}^{1}  z^{\beta-1} (1-z)^{\gamma-1} \,dv \\
				\intertext{removing the Beta distribution by substituting in the reciprocal of the missing part,}
				&= \frac{\Gamma(\alpha+\beta+\gamma)}{\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)} u^{\alpha-1}(1 - u)^{\beta+\gamma-1}
					\frac{\Gamma(\beta)\Gamma(\gamma)}{\Gamma(\beta+\gamma)} \\
				&= \frac{\Gamma(\alpha+\beta+\gamma)}{\Gamma(\alpha)\Gamma(\beta+\gamma)} u^{\alpha-1}(1 - u)^{\beta+\gamma-1}
			\end{align*}
			Thus the distribution of \(U\) is a \(Gam(\alpha,\beta+\gamma)\)
			
			
			\part
			
			
		\end{parts}
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\clearpage

	%4.24
	\question
	Let \(X\) and \(Y\) be independent random variables with 
	\(X\sim\)gamma\((r,1)\) and \(Y\sim\)gamma\((s,1)\). 
	Show that \(Z_1 = X + Y\) and \(Z_2 = X/(X + Y )\) are independent, 
	and find the distribution of each. (\(Z_1\) is gamma and \(Z_2\) is beta.) 
	
	\begin{solution}
		Note that \(x = z_1z_2\) and \(y=z_1(1-z_2)\).
		The Jacobian of the \(Z\)'s is
		\[ |J| = \begin{vmatrix} z_2 & z_1 \\ 1 - z_2 & -z_1 \end{vmatrix} = z_1. \]
		The the \(X\) and \(Y\) joint distribution can be refactored into the joint \(Z\) distribution
		\begin{align*}
			f_{Z_1Z_2}(z_1,z_2)
			&= \frac{1}{\Gamma(r)} (z_1z_2)^{r-1} e^{-z_1z_2} \frac{1}{\Gamma(s)} z_1(1-z_2)^{s-1} e^{-z_1(1-z_2)} z_1\\
			&= \frac{\Gamma(r+s)}{\Gamma(r+s)} \frac{1}{\Gamma(r)} z_1^{r-1} z_1^{s-1}
			 \frac{1}{\Gamma(s)} z_2^{r} (1-z_2)^{s-1} e^{-z_1z_2}e^{-z_1+-z_1z_2} \\
			&= \frac{1}{\Gamma(r+s)} z_1^{s+r-1} e^{-z_1} \frac{\Gamma(r+s)}{\Gamma(r)\Gamma(s)} z_2^{r-1} (1-z_2)^{s-1} \\
		\end{align*}
		The last line shows that the two \(Z\) distributions are independent.
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	%4.30
	\setcounter{question}{29}
	\question 
	Suppose the distribution of \(Y\), conditional on \(X = x\), is n\((x, x^2)\) 
	and that the marginal distribution of \(X\) is uniform\((0, 1)\).
	
	\begin{parts}
		\part Find \(E\,Y\) , Var \(Y\) , and Cov\((X,Y)\).
		\part Prove that \(Y/X\) and \(X\) are independent.
	\end{parts}
	
	\begin{solution}
		\begin{parts}
			\part
			\[E[Y] = E[E[Y|X]] = E[X] = \frac{1}{2}.\]
			\[\text{Var}[Y] = \text{Var}[E[Y|X]] + E[\text{Var}[Y|X]] = \text{Var}[X] + E[X^2] 
				= 2\text{Var}[X] + E[X]^2 = 2\left(\frac{1}{12}\right) + \frac{1}{4} = \frac{5}{12}\]
			We will need \(E[XY]\) to calculate covariance
			\[E[XY] = E[E[XY|X]] = E[X]E[Y|X] = E[X^2] = \frac{1}{12} + \frac{1}{4} = \frac{1}{3}\]
			\[\text{Cov}(X,Y) = E[XY] - E[X]E[Y] = \frac{1}{3} - \frac{1}{4} = \frac{1}{12}\]
			
			\part
			
			
		\end{parts}
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\clearpage

	%4.31
	\question 
	Suppose that the random variable \(Y\) has a binomial distribution with \(n\) trials and success probability \(X\), where \(n\) is a given constant and \(X\) is a uniform\((0, 1)\) random variable.
	
	\begin{parts}
		\part Find \(E\,Y\) and Var \(Y\).
		\part Find the joint distribution of \(X\) and \(Y\).
	\end{parts}
	
	\begin{solution}
		\begin{parts}
			\part
			Borrowing a lot of work from 4.30;
			\[E[Y] = E[E[Y|X]] = E[nX] = \frac{n}{2}.\]
			\[\text{Var}[Y] = \frac{n^2}{12} + \frac{n}{6}\]
			
			\part
			\begin{align*}
				P(Y=y,X=x) = \binom{n}{y} x^y (1-x)^{n-y}
			\end{align*}
			
		\end{parts}
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	%4.41
	\setcounter{question}{40}
	\question 
	Show that any random variable is uncorrelated with a constant.
	
	\begin{solution}
		Let \(Y\) be a random variable of arbitrary distribution,
		and let \(k\) be an arbitrary constant.
		
		Then correlation is calculated by \[\rho_{Y,k} = \frac{\text{Cov}(Y,k)}{\sigma_Y\sigma_k}.\]
		However, note that \(\sigma_k =0\).
		Thus \(\rho_{Y,k}\) is undefined by virtue of division by 0.
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	%4.45
	\setcounter{question}{44}
	\question 
	Show that if \((X,Y)\sim\) bivariate normal
	\((\mu_X, \mu_Y , \sigma_X^2 , \sigma_Y^2 , \rho)\), then the following are true.
	
	\begin{parts}
		\part The marginal distribution of \(X\) is n\((\mu_X, \sigma_X^2 )\) 
		and the marginal distribution of \(Y\) is n\((\mu_Y , \sigma_Y^2 )\).
		\part The conditional distribution of Y given \(X = x\) is
			\[n(\mu_Y + \rho(\sigma_Y /\sigma_X)(x-\mu_X), \sigma_Y^2 (1-\rho^2)).\]
		%\part For any constants \(a\) and \(b\), the distribution of \(aX + bY\) is
		%	\[n(a\mu_X + b\mu_Y , a^2\sigma_X^2 + b^2\sigma_Y^2 + 2ab\rho\sigma_X\sigma_Y ).\]
	\end{parts}
	
	\begin{solution}
		\begin{align}
			f_{XY} 
			&= \frac{1}{2\pi\sigma_x\sigma_y\sqrt{1-\rho^2}} \exp \left\{- \frac{1}{2(1-\rho^2)} \left[\frac{(x-\mu_x)^2}{\sigma_x} 
				-2\rho\frac{(x-\mu_x)(y-\mu_y)}{\sigma_x\sigma_y} + \frac{(y-\mu_y)^2}{\sigma_y} \right]\right\} \\
			&= \frac{1}{\sigma_x\sqrt{2\pi}} \frac{1}{\sigma_y\sqrt{2\pi}} \frac{1}{\sqrt{1-\rho^2}}
			\exp \left\{- \frac{1}{2(1-\rho^2)} \left[\frac{(x-\mu_x)^2}{\sigma_x} - \rho \right] \left[
			\frac{(y-\mu_y)^2}{\sigma_y} - \rho \right]\right\} \\
		\end{align}
		\textcolor{red}{incomplete}
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	%4.52	
	\setcounter{question}{51}
	\question 
	Bullets are fired at the origin of an \((x, y)\) coordinate system, and the point hit, say \((X, Y )\), is a random variable. The variables \(X\) and \(Y\) are taken to be independent n\((0, 1)\) random variables. If two bullets are fired independently, what is the distribution of the distance between them?
	
	\begin{solution}
		\(X,Y \overset{iid}{\sim} N(0,1)\)
		
		Thus the distance between the two is
		\(Z = \sqrt{(X^2_1+X^2_2)+(Y^2_1+Y^2_2)}.\)
		
		We know that 
		\(X+Y \sim N(0+0,1^2+1^2)\)
		and this can be re-normalized as
		\(\frac{X+Y}{\sqrt{2}} \sim N(0,1)\).
		
		Jointly this is also a chi-squared distribution,
		\[\left(\frac{X+Y}{\sqrt{2}}\right)^2 \sim \chi^2_1.\]
		
		Which can be further manipulated to be closer to the desired \(Z\) above, 
		\(\frac{(X_1+Y_1)^2}{2} + \frac{(X_2+Y_2)^2}{2} \sim \chi^2_2\)
		
		Using that distribution and a substitution,
		\begin{align*}
			f_U(u) 
			&= \frac{1}{2^{k/2}\gamma(k/2)} u^{k/2-1} e^{-u/2} \\
			&= \frac{1}{2\Gamma(1)} u^0 e^{-u/2} \\
			&= \frac{1}{2} e^{-u/2}.
		\end{align*}
		
		This substitution relates to the desired distribution as, 
		\(g(u) = z = \sqrt{2u} \Rightarrow g^{-1}(z) = \frac{z^2}{2}. \)
		
		Thus the distribution of the Euclidean distance between the two shots is,		
		\begin{align*}
			f_Z(z)
			 &= f_u(g^{-1}(z) z) \\
			 &= \frac{1}{2}ze^{-(z^2/2)/2} \\
			 &= \frac{ze^{-z^2/4}}{2}.
		\end{align*}
		
		The support can be calculated by
		\begin{align*}
			0 \leq&\ u < \infty \\
			0 \leq&\ 2u < \infty \\
			-\infty <&\ \sqrt{2u} < \infty \\
			-\infty <&\ z < \infty.
		\end{align*}
		
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{questions}
\end{document}
