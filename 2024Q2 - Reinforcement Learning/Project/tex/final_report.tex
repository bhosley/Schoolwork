\documentclass[journal]{IEEEtran}

% Packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\graphicspath{./images/}
\usepackage[table,dvipsnames]{xcolor}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.7}

\usepackage{wrapfig}
\usepackage{float}
\usepackage{url}
\usepackage[backend=biber,style=ieee]{biblatex}
\addbibresource{project.bib}

\usepackage{comment}
\usepackage{hyperref}
\usepackage{csquotes}

\usepackage{tcolorbox}
\newtcbox{\inlinecode}{%
    on line, boxrule=0pt, boxsep=0pt, top=2pt, left=2pt, bottom=2pt, right=2pt, colback=gray!15,%
    colframe=white, fontupper={\ttfamily \footnotesize}%
}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{minted}

% Title
\title{DSOR 646 Survey of Applications of Reinforcement Learning}

\author{Brandon Hosley, Capt, \textit{AFIT}%
    \thanks{Manuscript received \today%
    %; revised Month DD, YYYY.
}}
%\keywords{reinforcement learning}

% Document
\begin{document}
\maketitle

\begin{abstract}
This report demonstrates the application of reinforcement learning techniques to the Lunar Lander 
problem using the Semi-Gradient \emph{n}-step SARSA, SARSA($\lambda$), and 
Least-Squares Policy Iteration (LSPI) algorithms. We implemented these algorithms with 
enhancements such as Fourier Cosine Basis and Boltzmann exploration. 
Our results show that the SARSA algorithms, particularly when combined with Boltzmann exploration,
achieve robust performance with stable learning curves. We also explored the parameter sensitivity 
and found the algorithms to be resilient to a range of parameter values. 
The results highlight the effectiveness of these algorithms in controlling the lunar lander module,
paving the way for more complex implementations using neural networks in future work.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
To demonstrate proficiency in the material presented in \emph{DSOR 646 - Reinforcement Learning}, 
this report will describe a project applying the course knowledge objectives to solving the 
Lunar Lander problem presented by Oleg Klimov and maintained by the Farama Foundation. 
The objective is to apply reinforcement learning techniques, specifically 
Semi-Gradient \emph{n}-step SARSA and SARSA($\lambda$), to solve the Lunar Lander problem and 
to evaluate the performance of these algorithms.

\section{Background}

\subsection{Problem Description}

The Lunar Lander problem is described on the Farama Foundation's webpage as
\begin{displayquote}
    This environment is a classic rocket trajectory optimization problem. According to Pontryagin's 
    maximum principle, it is optimal to fire the engine at full throttle or turn it off. 
    This is the reason why this environment has discrete actions: engine on or off
    ~\cite{farama}.
\end{displayquote}
The principle objective is to use three thrusters, located on the left, right, and bottom of the
lunar lander module to guide the module onto a landing pad.
It is further intended that the module will descend at a speed deemed reasonable
and maintain a generally upright position.
Both of these are described more explicitly in the following section under the reward function.

\subsection{MDP Model}
\label{sec:MDP Model}

For this project the actual Markov Decision Process (MDP) model established in the Lunar Lander
environment remains unchanged. In this section we will describe the constituent parts of its 
construction.

\subsubsection{Agent}
The agent in this problem is the lander module. It is modeled in a Python implementation of
the Box2D physics environment. A rendering of the module can be seen in figure \ref{fig:lander}.

\begin{figure}
    \center
    \includegraphics[width=0.7\linewidth]{images/lander_screen.png}
    \caption{Lunar Lander Module with left and right thruster particles visible.}
    \label{fig:lander}
\end{figure}

\subsubsection{State Space}
The state-space for Lunar Lander is presented as a tuple with 8 elements.
These elements are the \(x\) and \(y\) components of position and velocity of the module within
the 2D space, the angle and angular velocity of the module, and a pair of boolean values that 
correspond to each of the module's legs and take the value \(1\) when the leg is in contact with
with the lunar surface, and \(0\) otherwise.

For a given time-step \(t\), the state \(s_t \in S\) represents,
\[s_t = \begin{cases}
    x_t \in [-1.5,1.5]   & \text{Position in } x \\
    y_t \in [-1.5,1.5]   & \text{Position in } y \\
    \vec{x}_t \in [-5,5] & \text{Velocity in } x \\
    \vec{y}_t \in [-5,5] & \text{Velocity in } y \\
    \omega_t \in [-\pi,\pi] & \text{Angle}\\
    \vec{\omega}_t \in [-5,5] & \text{Angular Velocity}\\
    \mathbb{I}_{t}(\text{leg 1}) \in\{0,1\} & \text{Leg on ground}\\
    \mathbb{I}_{t}(\text{leg 2}) \in\{0,1\} & \text{Leg on ground}
\end{cases}\]

\subsubsection{Action Space}
The action-space available to the lander is a discrete space with four values.
The action associated with each value is enumerated below. 
Consequently, not only does the lander fire a thruster at full whenever the action is chosen 
(per Pontryagin maximum principle~\cite{kaufman1964}),
the lander may only fire one thruster at a time.
\[a_t \in A = 
    \begin{cases}
        0 & \text{No-Op} \\
        1 & \text{Fire Left Engine} \\
        2 & \text{Fire Main Engine} \\
        3 & \text{Fire Right Engine}
    \end{cases}
\]

\subsubsection{Transition Probabilities}
Unlike previous environments used in DSOR 646, Lunar Lander does not progress deterministically.
There are several features that can modify the stochasticity of the transition process.
Dispersion is a value sampled from a uniform distribution between \(-1\) and \(1\).
This value perturbs the effect of the thrusters on the velocity of the lander module.

While dispersion is always represented in the model, wind effects are optional and can make the
transitions more dynamic and might be considered to represent some orbital effects on the lander.
There are two values associated with the `wind' effect, \inlinecode{wind\_power}
and \inlinecode{turbulence\_power} the former controlling the lateral effect and the latter
the rotational effect on the lander module. By default \(k=0.1\).
\begin{equation*}
    \resizebox{\linewidth}{!}{
        \(P(s_{t+1}|s_t,a_t) \cong 
        \begin{cases}
            \text{Dispersion} \sim U(-1,1) \\
            \text{Wind} = \tanh(\sin(2 k x) + \sin(\pi k x)) \\
            \text{Turbulence} = \tanh(\sin(2 k x) + \sin(\pi k x))
        \end{cases}\) 
    }
\end{equation*}

\subsubsection{Reward Function}
The reward function for this problem has two principle parts.
The first applies only to the final state. The episode is terminated 
if the lander module leaves the viewport (regardless of if it is rendered),
the lander body contacts the lunar surface, or the lander has ceased to move.
If the body of the module contacts the the surface, 
it is determined to be a crash and a \(-100\) reward is returned. 
If the module comes to rest and is between the landing pad flags a \(+100\) reward is returned.

Per step rewards are calculated based on several factors corresponding to desired behavior.
A positive reward is given for each leg that contacts the ground. Negative rewards are given
for firing each thruster, \(0.03\) for lateral, \(0.3\) for the main; the Euclidean 
distance from the landing pad; the velocity of the module; and the tilt of the module.
\begin{align*}
    r_t = \\
    & +10 (s_{t,6} + s_{t,7})   & \text{leg(s) on ground} \\
    & - \{0,0.03,0.3,0.03\}[a_t] & \text{thruster cost} \\
    & - 100\sqrt{a_{t,0}^2+a_{t,1}^2} & \text{Distance} \\
    & - 100\sqrt{a_{t,2}^2+a_{t,3}^2} & \text{Velocity} \\
    & - 100|\omega_t| & \text{Tilt}
\end{align*}

\subsubsection{Objective}
Here we restate the objective of the simulation with these MDP components in mind.

The expectation is that the solution policy will move at a velocity proportional to 
its distance from the objective, attempting to remain as upright as possible given the lateral
distance that needs to be covered, and get at least one leg on the ground.
To continue receiving rewards it may be ideal to continue moving with one or both legs resting on 
the ground, but instability and the risk of crashing should prevent this and cause the lander 
to come to rest and receive a successful landing reward.

\section{Methodology}
\label{sec:methodology}
In this section we will discuss the solution methodology and approaches.
For this project two algorithms that were discussed in class will be implemented.


\subsection{Semi-Gradient \emph{n}-step SARSA}

\begin{algorithm}
    \caption{Episodic semi-gradient n-step SARSA~\cite{sutton2018}}
    \label{alg:semi-grad_sarsa}
    \begin{algorithmic}\small
      \State \textbf{Input:} a differentiable action-value function parameterization 
      \(\hat{q}:\mathcal{S} \times \mathcal{A} \times \mathbb{R}^d \rightarrow \mathbb{R}\)
      \State \textbf{Input:} \(a\) policy \(\pi\) (if estimating \(q_\pi\)) 
      \State \textbf{Algorithm parameters:} step size \(\alpha>0\), small \(\epsilon>0\), 
          a positive integer \(n\)
      \State Initialize value-function weights \(\mathbf{w} \in \mathbb{R}^d\) arbitrarily 
          (e.g., \(\mathbf{w} = \mathbf{0}\)) 
      \State All store and access operations (\(S_t\), \(A_t\), and \(R_t\)) 
          can take their index \(\mod n+1\)

      \For{each episode}
        \State Initialize and store \(S_0\neq\) terminal
        \State Select and store an action \(A_0\sim\pi(\cdot|S_0)\) 
        \State \quad or \(\epsilon\)-greedy wrt \(\hat{q}(S_0,\cdot,\mathbf{w})\)
        \State \(T\leftarrow\infty\)

        \For{\(t=0,1,2,\ldots\)}
          \If{\(t<T\)}
            \State Take action \(A_t\) 
            \State Observe and store the next reward as \(R_{t+1}\) 
            \State \quad and the next state as \(S_{t+1}\)
            \If{\(S_{t+1}\) is terminal}
              \State \(T \leftarrow t+1\)
            \Else
              \State Select and store an action \(A_{t+1}\sim\pi(\cdot|S_{t+1})\) or 
              \State \quad \(\epsilon\)-greedy wrt \(\hat{q}(S_{t+1},\cdot,\mathbf{w})\)
            \EndIf
            \State \(\tau \leftarrow t-n+1\)\qquad (\(\tau\) is the time estimate being updated)
          \EndIf
          \If{\(\tau\geq0\)}
            \State \(G \leftarrow \sum^{min(\tau+n,T)}_{i=\tau+1}\gamma^{i-\tau-1}R_i \)
            \If{\(\tau+n<T\)} 
              \State\(G\leftarrow G+\gamma^n \hat{q}(S_{\tau+n},A_{\tau+n},\mathbf{w})\)
              \Comment \(G_{\tau:\tau+n}\)
            \EndIf
            \State \(\mathbf{w}\leftarrow\mathbf{w}+\alpha[G-\hat{q}(S_\tau,A_\tau,\mathbf{w})] 
                \nabla\hat{q}(S_\tau,A_\tau,\mathbf{w})\)
          \EndIf
        \EndFor
        \State \textbf{Until} \(\tau = T-1\)
      \EndFor
    \end{algorithmic}
\end{algorithm}

The first algorithm is the Semi-Gradient \emph{n}-step SARSA as presented by Sutton and Barto
in~\cite{sutton2018}. 
The baseline for the algorithm was tested based on a script provided in class.
Additional implementations using a Fourier Cosine Basis and Boltzmann exploration were 
also implemented. The specifics will be discussed later in this section.

\subsection[SARSA(Lambda)]{SARSA($\lambda$)}

\begin{algorithm}
    \caption{True Online SARSA(\(\lambda\))~\cite{sutton2018}}
    \label{alg:lambda_sarsa}
    \begin{algorithmic}\small
      \State \textbf{Input:} a feature function \(\mathbf{x}:\mathcal{S}^+\times\mathcal{A}
        \rightarrow\mathbb{R}^d\) such that \(\mathbf{x}(terminal,\cdot)=\mathbf{0}\)
      \State \textbf{Input:} \(a\) policy \(\pi\) (if estimating \(q_\pi\)) 
      \State \textbf{Algorithm parameters:} step size \(\alpha>0\),
      \State \quad trace decay rate \(\lambda\in[0,1]\), small \(\epsilon>0\)
      \State Initialize value-function weights \(\mathbf{w} \in \mathbb{R}^d\) arbitrarily 
          (e.g., \(\mathbf{w} = \mathbf{0}\)) 
      \For{each episode}
        \State Initialize \(S\)
        \State Choose \(A\sim\pi(\cdot|S)\) or \(\epsilon\)-greedy according to 
            \(\hat{q}(S,\cdot,\mathbf{w})\)
        \State \(\mathbf{x} \leftarrow \mathbf{x}(S, A) \)
        \State \(\mathbf{z} \leftarrow 0\)
        \State \(Q_{old} \leftarrow 0\)
        \For{Each Episode}
          \State Take action \(A\), observe \(R,S^\prime\)
          \State Choose \(A^\prime\sim\pi(\cdot|S^\prime)\) or \(\epsilon\)-greedy according to 
            \(\hat{q}(S^\prime,\cdot,\mathbf{w}\))
          \State \(\mathbf{x}^\prime \leftarrow \mathbf{x}(S^\prime,A^\prime)\)
          \State \(Q \leftarrow \mathbf{w}^\top\mathbf{x}\)
          \State \(Q^\prime \leftarrow \mathbf{w}^\top\mathbf{x}^\prime\)
          \State \(\delta \leftarrow R + \gamma Q^\prime - Q\)
          \State \(\mathbf{z} \leftarrow \gamma\lambda\mathbf{z} + (1-\alpha\gamma\lambda\mathbf{z}^\top\mathbf{x})\mathbf{x}\)
          \State \(\mathbf{w} \leftarrow \mathbf{w} + \alpha(\delta + Q - Q_{old})\mathbf{z} -\alpha(Q-Q_{old})\mathbf{x}\)
          \State \(Q_{old} \leftarrow Q^\prime\)
          \State \(\mathbf{x} \leftarrow \mathbf{x}^\prime\)
          \State \(A \leftarrow A^\prime\)
        \EndFor
        \State until \(S^\prime\) is terminal
      \EndFor
    \end{algorithmic}
\end{algorithm}

The second algorithm is the SARSA(\(\lambda\)) presented by Sutton and Barto in~\cite{sutton2018}.
The baseline script for this was also provided during the period of normal coursework.
As with algorithm \ref{alg:semi-grad_sarsa} we implemented versions with a Fourier Cosine Basis
and Boltzmann exploration, discussed in the next two sections.

\subsection{Fourier Cosine Basis}
A Fourier basis represents the state using a series of cosine functions. 
The basis functions are defined as:
\[ \phi_i(s)=\cos(\pi\mathbf{c}_i\cdot\mathbf{s}) \]
where \(\mathbf{c}_i\) is a vector of coefficients (frequencies), 
\(\mathbf{s}\) is the state vector, and \(i\) is the index of the basis function.

For each coefficient vector, \(\mathbf{c}_i = [c_{i1},c_{i2},\ldots,c_{in}]\), where 
\(c_{ij}\) can take integer values from \(0\) to \(m\), where \(m\) is the basis order.

\subsection{Boltzmann Exploration}
Boltzmann exploration is a strategy used to balance exploration and exploitation. 
This method allows the agent to explore less frequently as it becomes more confident about its 
action-value estimates. This implementation uses a Boltzmann probability \(P(a|s)\) based
on action-value estimates \(Q(s,a)\) with a temperature parameter \(\tau\),
\[ P(a|s) = \frac{e^{Q(s,a)/\tau}}{\sum_{\alpha\in A} e^{Q(s,\alpha)/\tau}} .\]
We further tune this effect by reducing the value of \(\tau\) over time using a cooling parameter.

\subsection{Tile Coding}
Both the Semi-Gradient \emph{n}-step SARSA and SARSA($\lambda$) utilized tile coding,
a type of coarse encoding, to translate the continuous state-space into a discrete one.
The coarse coding, despite the name, allows for a more refined encoding of the space
than basic discretization. In particular, the coding scheme implemented in the
\inlinecode{tiles3.py} from Richard S. Sutton's personal website
implements a series of overlapping tiles covering the state-space, where each set of tiles
is offset from each other and a continuous space state is represented by its membership to each
of the subsets of tiles.

\section{Results and Analysis}
\label{sec:results}
Two algorithms were discussed in the previous section. 
The third option that was presented was Least-Squares Policy Iteration (LSPI).
The LSPI algorithm was implemented as well, leveraging code provided during the course.

\begin{figure}[H]
  \includegraphics[width=0.8\linewidth]{images/Importance_SemiGrad.png}
  \caption{Parameter Importance and Correlation for Semi-Gradient \emph{n}-step SARSA}
  \label{fig:SemiGrad_SARSA_Importance}
\end{figure}
\begin{figure}[H]
  \includegraphics[width=0.8\linewidth]{images/Importance_LambdaSarsa.png}
  \caption{Parameter Importance and Correlation for SARSA(\(\lambda\))}
  \label{fig:SARSA_Lambda_Importance}
\end{figure}
\begin{figure}[H]
  \includegraphics[width=0.8\linewidth]{images/Importance_SGLam_Boltzmann.png}
  \caption{Parameter Importance and Correlation for Semi-Gradient \emph{n}-step 
  SARSA with Boltzmann Exploration}
  \label{fig:SemiGrad_SARSA_Boltz_Importance}
\end{figure}
\begin{figure}[H]
  \includegraphics[width=0.75\linewidth]{images/Importance_Lambda_Boltzmann.png}
  \caption{Parameter Importance and Correlation for SARSA(\(\lambda\)) with Boltzmann Exploration}
  \label{fig:SARSA_Lambda_Boltz_Importance}
\end{figure}

Parameter sweeps were performed on each of the implementations listed in the 
section~\ref{sec:methodology}. Figures \ref{fig:SemiGrad_SARSA_Sweep}, \ref{fig:SARSA_Lambda_Sweep},
\ref{fig:SemiGrad_SARSA_Boltzmann_Sweep}, and \ref{fig:SARSA_Lambda_Boltzmann_Sweep} 
show simple coordinate comparison of the parameters with the algorithm score.

\begin{figure*}
  \includegraphics[width=0.8\linewidth]{images/Sweep_SemiGradSarsa.png}
  \caption{Parameter Sweep for Semi-Gradient \emph{n}-step SARSA}
  \label{fig:SemiGrad_SARSA_Sweep}
\end{figure*}
\begin{figure*}
  \includegraphics[width=0.8\linewidth]{images/Sweep_LambdaSarsa.png}
  \caption{Parameter Sweep for SARSA(\(\lambda\))}
  \label{fig:SARSA_Lambda_Sweep}
\end{figure*}
\begin{figure*}
  \includegraphics[width=0.8\linewidth]{images/Sweep_SemiGrad_Boltzmann.png}
  \caption{Parameter Sweep for Semi-Gradient \emph{n}-step SARSA with Boltzmann Exploration}
  \label{fig:SemiGrad_SARSA_Boltzmann_Sweep}
\end{figure*}
\begin{figure*}
  \includegraphics[width=0.8\linewidth]{images/Sweep_LambdaSarsa_Boltzmann.png}
  \caption{Parameter Sweep for SARSA(\(\lambda\)) with Boltzmann Exploration}
  \label{fig:SARSA_Lambda_Boltzmann_Sweep}
\end{figure*}

Figures \ref{fig:SemiGrad_SARSA_Importance}, \ref{fig:SARSA_Lambda_Importance},
\ref{fig:SemiGrad_SARSA_Boltz_Importance}, \ref{fig:SARSA_Lambda_Boltz_Importance}
show importance and correlation of the parameters.
Importance is determined by a Random Forest Regression algorithm~\cite{probst2019}.

The combination of these figures demonstrate that the algorithms are not overly sensitive to
the parameter values. Further, we may see that there are consistent clusters of resulting models
in the Semi-Gradient \emph{n}-step SARSA and SARSA(\(\lambda\)) with Boltzmann Exploration.

It may be noted that none of these figures show results for algorithms 
that utilize theFourier basis transformation. 
This is because the change incurred a substantial increase in the training time for the algorithm, 
further, the few training iterations that did complete did not demonstrate any improvement
over the lower bounds of the baseline implementations.

Example learning curves of superlative performing policies from each are presented in figures
\ref{fig:curve_sgsarsa}, \ref{fig:curve_sgsarsa_boltz}, \ref{fig:curve_sarsalam}, and
\ref{fig:curve_sarsalam_boltz}.

\begin{figure}[H]
  \includegraphics[width=0.9\linewidth]{images/curve_sarsa_boltz.png}
  \caption{Semi-Gradient \emph{n}-step SARSA with Boltzmann Exploration} 
  \label{fig:curve_sgsarsa}
\end{figure}
\begin{figure}[H]
  \includegraphics[width=0.9\linewidth]{images/curve_sg_sarsa.png}
  \caption{Semi-Gradient \emph{n}-step SARSA} 
  \label{fig:curve_sgsarsa_boltz}
\end{figure}
\begin{figure}[H]
  \includegraphics[width=0.9\linewidth]{images/curve_sarsa_lam.png}
  \caption{SARSA(\(\lambda\))} 
  \label{fig:curve_sarsalam}
\end{figure}
\begin{figure}[H]
  \includegraphics[width=\linewidth]{images/curve_sarsa_lam_boltz.png}
  \caption{SARSA(\(\lambda\)) with Boltzmann Exploration} 
  \label{fig:curve_sarsalam_boltz}
\end{figure}

% Conclusion
\section{Conclusion}
\label{sec:conclusion}
The assignment provided a limitation to three algorithms. Between the three, the two SARSA 
algorithms proved to be most expedient in their implementation for this problem. 
Our experiments demonstrate that both Semi-Gradient \emph{n}-step SARSA and SARSA($\lambda$) with 
Boltzmann exploration achieve stable and effective control of the lunar lander module. 

Parameter sweeps indicate that the performance of these algorithms is robust to a range of 
parameter settings, and the Boltzmann exploration strategy effectively balances exploration and 
exploitation. While the Fourier Cosine Basis did not yield significant improvements, 
it highlights the potential for more sophisticated feature representations in future work.

Future work would include the usage of neural networks, which we believe would substantially 
improve the ability of the algorithm to model the complexity of the environment. 
Integrating neural networks could enable the learning of more intricate patterns 
and lead to further performance enhancements. Additionally, exploring other advanced 
reinforcement learning techniques, such as deep Q-networks (DQN) or 
proximal policy optimization (PPO), could provide deeper insights and potentially better 
solutions for the Lunar Lander problem.

% References
\label{sec:references}
\printbibliography

\end{document}
