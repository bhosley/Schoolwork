\documentclass{beamer}
\usepackage[english]{babel}
\usepackage{lmodern}
\usepackage{libertinus}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage[backend=biber,style=ieee]{biblatex}
\addbibresource{survey.bib}


\usepackage{comment}


\title{DSOR 646~-~Reinforcement Learning Survey}
\author{Capt. Brandon Hosley\inst{1}}
\institute[ENS]{
    \inst{1}
    Department of Operational Sciences\\
    Air Force Institute of Technology}
\date{\today}

\AtBeginSection[]{
  \begin{frame}
    \tableofcontents[currentsection]
  \end{frame}
}


\usetheme{Madrid}
\usecolortheme{beaver}
\mode<presentation>

\begin{document}
\frame{\titlepage}
\begin{frame}
    \tableofcontents
\end{frame}

\section{Methodological Paper: A3C}

\begin{frame}
    \frametitle{A3C~-~Overview}
    \begin{description}
        \item[Title:] Asynchronous Methods for Deep Reinforcement Learning
        \item[Authors:]
        Volodymyr Mnih,
        Adrià Puigdomènech Badia,
        Mehdi Mirza,
        Alex Graves,
        Tim Harley,
        Timothy P. Lillicrap,
        David Silver,
        Koray Kavukcuoglu
        \item[Source:] arXiv:1602.01783~\footfullcite{mnih2016}
    \end{description}
    
    \vspace*{1em}
    
    \begin{block}{Objective}
        Introduce a lightweight framework for deep reinforcement learning (DRL).
        Use asynchronous gradient descent for optimizing deep neural network controllers.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{A3C~-~Key Contributions}
    \begin{description}
        \item[Asynchronous Variants:] \phantom{}\\
            \begin{itemize}
                \item Developed asynchronous versions of four standard RL algorithms.
                \item Demonstrated stabilizing effect of parallel actor-learners on training.
            \end{itemize}
        \item[State-of-the-Art Performance:] \phantom{}\\
            \begin{itemize}
                \item Asynchronous actor-critic method surpassed current 
                state-of-the-art on Atari domain.
                \item Achieved this while training on a single multi-core CPU 
                instead of a GPU.%
            \end{itemize}
        \item[Wide Applicability:] \phantom{}\\
            \begin{itemize}
                \item Success on continuous motor control problems.
                \item Navigation in random 3D mazes using visual input.
            \end{itemize}
    \end{description}
\end{frame}

\begin{frame}
    \frametitle{Asynchronous Variants}
    \begin{itemize}
        \item Algorithms Implemented,
        Asynchronous variants of:
        \begin{itemize}
            \item One-step Q-learning
            \item N-step Q-learning
            \item SARSA
            \item \textbf{Actor-Critic}
        \end{itemize} 
        \item Change to utilizes asynchronous gradient descent.
        \item Multiple actor-learners run in parallel.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Actor Critic}
    \begin{multicols}{2}
        \begin{description}
            \item[Actor] Updates the policy distribution $\pi(s)$
            \item[Critic] Estimates the value function; 
            action-value $Q(s,a)$ or state-value $V(s)$
        \end{description}
        \begin{figure}
            \includegraphics[width=\linewidth]{images/Actor-Critic_Structure.png}
            \caption{AC Structure\footfullcite{giang2020}}
        \end{figure}
    \end{multicols}
\end{frame}

\begin{frame}
    \frametitle{Advantage Actor Critic}
    Advantage Value:
    \begin{align*}
        A(s,a) &= Q(s,a) - V(s)
        \intertext{Substituting for $Q$,}
        Q(s_t,a_t) &= \mathbb{E}\left[r_{t+1}+\gamma V(s_{t+1})\right] \\
        A(s_t,a_t) &=\quad r_{t+1}+\gamma V(s_{t+1}) - V(s_t) \\
    \end{align*}
    Why? \\
    Advantage supplies context to $Q$ values relative to the state value from which they originate.
\end{frame}

\begin{frame}
    \frametitle{Asynchronous Gradient Descent}
    \begin{figure}
        \includegraphics[width=0.75\linewidth]{images/batchGD.png}
        \caption{Batch Gradient Descent\footfullcite{dabbura2022}}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Asynchronous Gradient Descent}
    \begin{figure}
        \includegraphics[width=0.5\linewidth]{images/a3c.jpg}
        \caption{Asynchronous Batching\footfullcite{sarkar2018}}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{A3C - Results}
    \begin{figure}
        \includegraphics[width=0.9\linewidth]{images/a3c results.png}
        \caption{Results in Atari 2600 environments\footcite{mnih2016}}
    \end{figure}
    \begin{itemize}
        \item Blue: single threaded DQN
        \item Yellow: A3C
        \item AC takes the advantages of $Q$ but reduces bias
    \end{itemize}
\end{frame}

\section{Survey}
\newcounter{papercounter}

\begin{frame}
    \stepcounter{papercounter}
    \frametitle{Paper \thepapercounter - Hybrid NOMA/OMA-Based Dynamic Power Allocation Scheme Using 
        Deep Reinforcement Learning in 5G Networks\footfullcite{giang2020}}
    \begin{description}
        \item[Model:]
        \begin{itemize}
            \item Agent: Transmission Tower
            \item $S$: Users
            \item $A$: Channel and Power Allocation
            \item $P$: Deterministic
            \item $R$: Data Throughput
        \end{itemize}
        \item[Contribution:] Applications of RL
        \item[Weakness:] POMDP v. Deep RL 
        \begin{itemize}
            \item Compared to random, uniform, myopic-OMA
            \item Probably should have compared to the MDP they mention
        \end{itemize}
        % Authors mention that the latter might not be appropriate.
        % They fail to state why they chose it vs. some other implementation
        % they argue as a matter of state space size, but never address SS 
        % reduction techniques.
    \end{description}
\end{frame}

\begin{frame} %% TRPO(schulman2017)
    \stepcounter{papercounter}
    \frametitle{Paper \thepapercounter - Trust Region Policy Optimization (TRPO)%
    \footfullcite{schulman2017}}
    \begin{description}
        \item[Environments:] MuJoCo and Atari
        \item[Model:]
        \begin{itemize}
            \item Agent: Simulated robot / Atari player
            \item $S$: continuous, positioning / screen input
            \item $A$: continuous, generally torque on joints / discrete, controller input
            \item $P$: deterministic / game dependent
            \item $R$: +Time alive +for moving -control costs / game dependent
        \end{itemize}
        \item[Algorithm:]
        \begin{itemize}
            \item Iterative
            \item Approximate: Trajectory sampling
            \item Monotonic: Constrained policy gradient
        \end{itemize}
    \end{description}
\end{frame}

\begin{frame} %PPO(schulman2017a)
    \stepcounter{papercounter}
    \frametitle{Paper \thepapercounter - Proximal Policy Optimization Algorithms (PPO)%
    \footfullcite{schulman2017a}}
    \begin{description}
        \item[Environment:] MuJoCo 
        \item[Model:]
        \begin{itemize}
            \item Agent: Simulated robot
            \item $S$: continuous, positioning and angles of joints
            \item $A$: continuous, generally torque on joints
            \item $P$: deterministic
            \item $R$: +Time alive +for moving -control costs
        \end{itemize}
        \item[Algorithm:] 
        \begin{itemize}
            \item Fixed-length trajectory sampling
            \item Advantage values
            \item Batch updates
        \end{itemize}
        \item[Weakness:] Same group as TRPO but did not test on the same environments.
    \end{description}
\end{frame}

\begin{frame}% QMIX(Rashid2018)
    \stepcounter{papercounter}
    \frametitle{Paper \thepapercounter - QMIX: Monotonic Value Function Factorisation for Deep 
    Multi-Agent Reinforcement Learning\footfullcite{rashid2018}}
    \begin{description}
        \item[Environment:] StarCraft II Learning Environment\cite{vinyals2019}
        \item[Model:]
        \begin{itemize}
            \item Agent: Individual units
            \item $S$: Local observations and global observations
            \item $A$: Local actions, joint action-values
            \item $P$: Non-deterministic
            \item $R$: Reward shaping, sparse rewards
        \end{itemize}
        \item[Algorithm:] QMIX is a mixture of centralized and individual Q-learning
        \begin{itemize}
            \item Centralized training
            \item Decentralized policy
            \item Off-policy
        \end{itemize}
    \end{description}
\end{frame}

\begin{frame}% QMIX(Rashid2018)
    \frametitle{Paper \thepapercounter - QMIX: Monotonic Value Function Factorisation for Deep 
    Multi-Agent Reinforcement Learning\cite{rashid2018}}
    \begin{description}
        \item[Algorithm:] QMIX is a mixture of centralized and individual Q-learning
    \end{description}
    \begin{figure}\small
        \includegraphics[width=0.9\linewidth]{images/qmix.png}
        \caption{Mixing network structure\cite{rashid2018}}
    \end{figure}
\end{frame}

\begin{frame} % DDPG(lillicrap2019) 
    \stepcounter{papercounter}
    \frametitle{Paper \thepapercounter - Continuous Control with Deep Reinforcement Learning%
    \footfullcite{lillicrap2019}}
    \begin{description}
        \item[Environment:] MuJuCo
        \item[Model:]
        \begin{itemize}
            \item Agent: Simulated robot
            \item $S$: Continuous, positioning
            \item $A$: Continuous, generally torque on joints
            \item $P$: Deterministic
            \item $R$: +Time alive +for moving(sometimes) -control costs
        \end{itemize}
        \item[Algorithm:] DPG\cite{silver2014} $\rightarrow$ DDPG 
            (Deep Deterministic Policy Gradient)
        \begin{itemize}
            \item Utilizes actor-critic framework
            \item \textbf{Extends DQN to be continuous}
        \end{itemize}
    \end{description}
\end{frame}

\begin{frame} % TD3(Fujimoto2018)
    \stepcounter{papercounter}
    \frametitle{Paper \thepapercounter - Addressing Function Approximation Error in Actor-Critic 
    Methods\footfullcite{fujimoto2018}}
    \begin{description}
        \item[Environment:] MuJuCo
        \item[Model:]
        \begin{itemize}
            \item Agent: Simulated robot
            \item $S$: Continuous, positioning
            \item $A$: Continuous, generally torque on joints
            \item $P$: Deterministic
            \item $R$: +Time alive +for moving(sometimes) -control costs
        \end{itemize}
        \item[Algorithm:] DDPG $\rightarrow$ TD3 
        (Twin Delayed Deep Deterministic policy gradient algorithm)
        \begin{itemize}
            \item Adds secondary critic, and takes min value
            \item Correct overestimation attributed to DDPG % also mentioned in MATD3
        \end{itemize}
    \end{description}
\end{frame}

\begin{frame}
    \stepcounter{papercounter}
    \frametitle{Paper \thepapercounter - Multi-Agent Actor-Critic for Mixed 
    Cooperative-Competitive Environments\footfullcite{lowe2020}}
    \begin{description}
        \item[Environment:] MPE
        \item[Model:]
        \begin{itemize} \small
            \item Agent: Particles
            \item $S$: Position of self, others, obstacles
            \item $A$: Move, speak, listen
            \item $P$: Deterministic
            \item $R$: Dependent on game-mode. 
        \end{itemize}
        \item[Algorithm:] DDPG $\rightarrow$ MADDPG
    \end{description}
\end{frame}

\begin{frame}
    \frametitle{Paper \thepapercounter - Multi-Agent Actor-Critic for Mixed 
    Cooperative-Competitive Environments\cite{lowe2020}}
    \begin{figure}\small
        \includegraphics[width=0.4\linewidth]{images/maddpg.png}
        \caption{Multi-agent Actor Critic\cite{lowe2020}}
    \end{figure}
\end{frame}

\begin{frame} %MATD3(Ackerman2019)
    \stepcounter{papercounter}
    \frametitle{Paper \thepapercounter - Reducing Overestimation Bias in Multi-Agent Domains 
    Using Double Centralized Critics\footfullcite{ackermann2019}}
    \begin{description}
        \item[Environments:] MPE and MuJoCo 
        \item[Model:]
        \begin{itemize}
            \item Agent:  Particles / Ant legs
            \item $S$: Position of self, others, obstacles / position and angle of limbs and body
            \item $A$: Move, speak, listen / torque on joints
            \item $P$: Deterministic
            \item $R$: Game mode dependent / +Time alive +forward movement -control costs
        \end{itemize}
        \item[Algorithm:] TD3 $\rightarrow$ MATD3
        \begin{itemize}
            \item Takes twin critic and extends it to be centralized critic
        \end{itemize}
    \end{description}
\end{frame}

\begin{frame} 
    \stepcounter{papercounter}
    \frametitle{Paper \thepapercounter - The Surprising Effectiveness of PPO in Cooperative, 
    Multi-Agent Games\footfullcite{yu2022}}
    \begin{description}\small
        \item[Environments:] 
    \end{description}
    \begin{figure}\small\vspace*{-1em}
        \includegraphics[width=0.75\linewidth]{images/yu2020_envs.png}
    \end{figure}
\end{frame}

\begin{frame} 
    \frametitle{Paper \thepapercounter - The Surprising Effectiveness of PPO in Cooperative, 
    Multi-Agent Games\footfullcite{yu2022}}
    \begin{description}\small
        \item[Model:] 
        \begin{itemize} 
            \item Agent: SMAC, GRF, MPE and Hanabi players
            \item $S,A$: According to game, disc. for Hanabi, cont. for others
            \item $P$: Deterministic for Hanabi and MPE, stochastic for SMAC and GRF
            \item $R$: Reward sharing, end of game rewards
        \end{itemize}
        \item[Algorithm:] PPO\cite{schulman2017a} $\rightarrow$ MAPPO
        \begin{itemize}
            \item Kept fixed-length trajectory sampling and advantage values
            \item Allowed multiple agents to contribute to batch updates for central policy
        \end{itemize}
        \item[Strengths:]
        \begin{itemize}
            \item Ablative study of their changes
            \item Added to and tested previously used environments
        \end{itemize}
    \end{description}
\end{frame}

\begin{frame}
    \stepcounter{papercounter}
    \frametitle{Paper \thepapercounter - Heterogeneous-Agent Reinforcement Learning%
    \footfullcite{zhong2024}}
    \begin{description} \small
        \item[Environments:]
    \end{description}
        \includegraphics[width=\linewidth]{images/MARL_envs.png}
    \begin{description}\small
        \item[Model:]
        \begin{itemize} \small
            \item Agent: Multiple agents with distinct capabilities and roles.
            \item $S_m$, $A_m$, $R_m$: Dependent on the testing environment.
            Potentially distinct sets between each Agent $m\in M$.
        \end{itemize}
        \item[Algorithms:] Introduced updates to previous MARL Algorithms, \vspace*{-1em}
        \begin{multicols*}{2}
            \begin{itemize} \small
                \item TRPO\cite{schulman2017} $\rightarrow$ HATRPO
                \item MAPPO\cite{yu2022} $\rightarrow$ HAPPO
                \item A3C\cite{mnih2016} $\rightarrow$ HAA2C
                \item MADDPG\cite{lowe2020} $\rightarrow$ HADDPG
                \item MATD3\cite{ackermann2019} $\rightarrow$ HATD3
            \end{itemize}
        \end{multicols*}
    \end{description}
\end{frame}

\section{References}

\renewcommand*{\bibfont}{\tiny}
\frame[allowframebreaks]{\printbibliography}

\begin{frame}
    Questions?
\end{frame}

\end{document}