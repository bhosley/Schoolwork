@online{dabbura2022,
  title = {Gradient {{Descent Algorithm}} and {{Its Variants}}},
  author = {Dabbura, Imad},
  date = {2022-09-27T12:24:38},
  url = {https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3},
  urldate = {2024-05-21},
  abstract = {Optimization refers to the task of minimizing/maximizing an objective function f(x) parameterized by x. In machine/deep learning…},
  langid = {english},
  organization = {Medium},
  file = {/Users/brandonhosley/Zotero/storage/4WHREMZZ/gradient-descent-algorithm-and-its-variants-10f652806a3.html}
}

@article{giang2020,
  title = {Hybrid {{NOMA}}/{{OMA-Based Dynamic Power Allocation Scheme Using Deep Reinforcement Learning}} in {{5G Networks}}},
  author = {Giang, Hoang and Hoan, Tran and Thanh, Pham and Koo, Insoo},
  date = {2020-06-20},
  journaltitle = {Applied Sciences},
  shortjournal = {Applied Sciences},
  volume = {10},
  pages = {4236},
  doi = {10.3390/app10124236},
  abstract = {Non-orthogonal multiple access (NOMA) is considered a potential technique in fifth-generation (5G). Nevertheless, it is relatively complex when applying NOMA to a massive access scenario. Thus, in this paper, a hybrid NOMA/OMA scheme is considered for uplink wireless transmission systems where multiple cognitive users (CUs) can simultaneously transmit their data to a cognitive base station (CBS). We adopt a user-pairing algorithm in which the CUs are grouped into multiple pairs, and each group is assigned to an orthogonal sub-channel such that each user in a pair applies NOMA to transmit data to the CBS without causing interference with other groups. Subsequently, the signal transmitted by the CUs of each NOMA group can be independently retrieved by using successive interference cancellation (SIC). The CUs are assumed to harvest solar energy to maintain operations. Moreover, joint power and bandwidth allocation is taken into account at the CBS to optimize energy and spectrum efficiency in order to obtain the maximum long-term data rate for the system. To this end, we propose a deep actor-critic reinforcement learning (DACRL) algorithm to respectively model the policy function and value function for the actor and critic of the agent (i.e., the CBS), in which the actor can learn about system dynamics by interacting with the environment. Meanwhile, the critic can evaluate the action taken such that the CBS can optimally assign power and bandwidth to the CUs when the training phase finishes. Numerical results validate the superior performance of the proposed scheme, compared with other conventional schemes.},
  file = {/Users/brandonhosley/Zotero/storage/2UHZBJ64/Giang et al_2020_Hybrid NOMA-OMA-Based Dynamic Power Allocation Scheme Using Deep Reinforcement.pdf}
}

@online{mnih2016,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  date = {2016-06-16},
  eprint = {1602.01783},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1602.01783},
  url = {http://arxiv.org/abs/1602.01783},
  urldate = {2024-05-05},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/brandonhosley/Zotero/storage/ZNCS528Y/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf;/Users/brandonhosley/Zotero/storage/MJV4AMZK/1602.html}
}

@online{sarkar2018,
  title = {A {{Brandom-ian}} View of {{Reinforcement Learning}} towards Strong-{{AI}}},
  author = {Sarkar, Atrisha},
  date = {2018-03-07},
  eprint = {1803.02912},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1803.02912},
  urldate = {2024-05-24},
  abstract = {The analytic philosophy of Robert Brandom, based on the ideas of pragmatism, paints a picture of sapience, through inferentialism. In this paper, we present a theory, that utilizes essential elements of Brandom’s philosophy, towards the objective of achieving strong-AI. We do this by connecting the constitutive elements of reinforcement learning and the Game Of Giving and Asking For Reasons. Further, following Brandom’s prescriptive thoughts, we restructure the popular reinforcement learning algorithm A3C, and show that RL algorithms can be tuned towards the objective of strong-AI.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/brandonhosley/Zotero/storage/24L6V564/Sarkar - 2018 - A Brandom-ian view of Reinforcement Learning towar.pdf}
}
