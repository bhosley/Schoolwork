\documentclass[journal]{IEEEtran}

% Packages
\usepackage{amsmath}
\usepackage{graphicx}
\graphicspath{./images/}
\usepackage[table,dvipsnames]{xcolor}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{pgfplotstable}

\usepackage{wrapfig}
\usepackage{float}
\usepackage{url}
\usepackage[backend=biber,style=ieee]{biblatex}
\addbibresource{survey.bib}

% Drafting Utility Package
\usepackage{comment}
\usepackage{blindtext}
\usepackage{hyperref}
%\usepackage[toc,page]{appendix}

% Title
\title{DSOR 646 Survey of Applications of Reinforcement Learning}

\author{Brandon Hosley, Capt, \textit{AFIT}%
    \thanks{Manuscript received \today%
    %; revised Month DD, YYYY.
}}
%\keywords{reinforcement learning}


% Document
\begin{document}
\maketitle
% Abstract
\begin{abstract}
\end{abstract}
% Introduction
\section{Introduction}
\label{sec:introduction}

Reinforcement Learning (RL) is a rapidly evolving field within artificial intelligence, 
characterized by its unique approach to solving decision-making problems where an agent 
learns to make sequences of decisions by interacting with an environment. 
Over the past decade, RL has seen significant advancements, 
both in terms of theoretical developments and practical applications. 
This survey will explore the application of RL in various domains by examining 10 
selected papers published in the last 10 years in high-ranking journals. 
The primary objective is to assess the modeling components, solution procedures, and results 
of these applications, providing insights into the strengths and weaknesses of each study.

This survey is structured to provide an overview of RL applications, focusing on how 
different problems are formulated using Markov Decision Processes (MDPs), 
the solution approaches employed, and the outcomes achieved. Each paper is analyzed to 
highlight key contributions and to evaluate applications over a diverse range of areas; 
demonstrating the versatility and impact of RL in solving complex problems across various fields.

\section{Overview}
\label{sec:overview}

For each of the analyzed papers we will examine core MDP components;

\begin{itemize}
    \item \textbf{Environments:} The external system with which agents interact.
    \item \textbf{Agents:} Entities that make decisions.
    \item \textbf{States:} Representations of the environment at a given time.
    \item \textbf{Actions:} Possible decisions or moves the agent can make.
    \item \textbf{Rewards:} Feedback from the environment based on the actions taken.
\end{itemize}


(b) Summarize results, creating a table summarizing key information with one row for each paper 
i. Authors, topic or application area
iii. Solution approaches, algorithmic features, novel extensions
iv. tuning and/or testing adequate?



% Analysis of Selected Papers
\section{Analysis of Selected Papers}
\label{sec:selected_papers}

\begin{table*}
    \begin{tabularx}{\textwidth}{cX X ccc}
        Citation & Short-Title & MDP Characteristics & Solution Approaches & Algorithm &
        Novel Extensions\\
        \midrule
        \cite{giang2020} & Hybrid NOMA/OMA Power Allocation Using Deep RL in 5G Network & 
        $S$: \# Users and transmission requests \newline
        $A$: Channel and Power Allocation \newline
        $R$: Data Throughput &
        & A3C\cite{mnih2016} & Application
    \end{tabularx}
\end{table*}

\newcolumntype{C}{>{\centering\arraybackslash}p{60mm}}%
\newcolumntype{L}{>{\raggedright\arraybackslash}p{60mm}}%
\begin{table*}
    \pgfplotstabletypeset[
    %columns={citekey,Short-Title,MDP Characteristics,Extended Algorithm,Novel Extensions},
    col sep=comma,
    string type,
    every head row/.style={after row=\midrule},
    every last row/.style={after row=\bottomrule},
    columns/citekey/.style={column name=Citation, column type=c},
    columns/Short-Title/.style={column name=Short-Title, column type=L},
    ]{lit_review_table.csv}
\end{table*}


\subsection{Hybrid NOMA/OMA-Based Dynamic Power Allocation Scheme 
Using Deep Reinforcement Learning in 5G Networks}

In their paper\cite{giang2020}, Giang et al discuss the evolution of wireless communication systems
from 4G to 5G, emphasizing the need for efficient spectrum utilization due to the explosive growth 
in devices and data volumes. Non-orthogonal multiple access (NOMA) is identified as a promising 
candidate for 5G due to its ability to allow multiple users to share time and spectrum resources.
NOMA uses power-domain multiplexing, allowing multiple users to share the same spatial layer, 
contrasting with traditional orthogonal multiple access (OMA) techniques like FDMA and TDMA. 
This results in higher spectral efficiency (SE).

The paper addresses the lack of research on resource allocation using deep reinforcement learning 
(DRL) under non-RF energy-harvesting scenarios in uplink cognitive radio networks (CRNs).
They use a deep actor-critic reinforcement learning (DACRL) framework for efficient joint power and
bandwidth allocation in hybrid NOMA/OMA CRNs, where solar-powered users are assigned optimal 
transmission power and bandwidth to maximize long-term data transmission rates.

\subsection{Trust Region Policy Optimization}
In \cite{schulman2017} Schulman et. al. introduce Trust Region Policy Optimization (TRPO), 
an iterative method for optimizing policies with guaranteed monotonic improvement. 
TRPO combines theoretical rigor with practical approximations to develop a robust algorithm 
suitable for large nonlinear policies, such as neural networks. 
The algorithm is evaluated on various tasks, including simulated robotic control in MuJoCo
(swimming, hopping, walking) and Atari games using raw pixel inputs. 
TRPO consistently shows monotonic improvement and robust performance with minimal hyperparameter 
tuning, making it a reliable choice for policy optimization in diverse environments.






\subsection{Proximal Policy Optimization Algorithms}
This paper proposes Proximal Policy Optimization (PPO), a new family of policy gradient methods for reinforcement learning. PPO alternates between sampling data from the environment and optimizing a surrogate objective function using stochastic gradient ascent. Unlike standard policy gradient methods, PPO's objective function allows multiple epochs of minibatch updates, improving sample efficiency. The algorithm retains the benefits of Trust Region Policy Optimization (TRPO) but is simpler to implement and more general. Empirical results show that PPO outperforms other online policy gradient methods on benchmark tasks, including simulated robotic locomotion and Atari games, striking a favorable balance between sample complexity, simplicity, and performance.
\cite{schulman2017a}

\subsection{QMIX: Monotonic Value Function Factorization for Deep Multi-Agent Reinforcement Learning}
This paper introduces QMIX, a value-based multi-agent reinforcement learning (MARL) method that facilitates centralized training and decentralized execution. QMIX estimates joint action-values as a non-linear combination of per-agent values conditioned on local observations, with a structural monotonicity constraint ensuring consistency between centralized and decentralized policies. The method is evaluated on StarCraft II micromanagement tasks, showing significant performance improvements over existing value-based MARL methods. QMIX effectively leverages centralized training to derive robust decentralized policies, making it a strong approach for complex coordination tasks in multi-agent settings.
\cite{rashid2018}

\subsection{Continuous Control with Deep Reinforcement Learning}
This paper presents an adaptation of Deep Q-Learning (DQN) for continuous action domains through an actor-critic, model-free algorithm based on deterministic policy gradients (DPG). The proposed algorithm effectively solves over 20 simulated physics tasks, such as cartpole swing-up, dexterous manipulation, legged locomotion, and car driving, using the same learning algorithm, network architecture, and hyperparameters. It achieves performance competitive with planning algorithms that have full access to the domain dynamics and derivatives. Additionally, the algorithm can learn policies directly from raw pixel inputs, demonstrating robustness and flexibility across various tasks.
\cite{lillicrap2019}

\subsection{Addressing Function Approximation Error in Actor-Critic Methods}
This paper tackles the issue of function approximation errors in actor-critic methods, which can lead to overestimated value estimates and suboptimal policies. The authors propose mechanisms to minimize these errors on both the actor and the critic. The algorithm builds on Double Q-learning by taking the minimum value between a pair of critics to limit overestimation. Additionally, delaying policy updates is suggested to reduce per-update error. Evaluations on the OpenAI gym tasks show that the proposed method outperforms state-of-the-art approaches in every tested environment, demonstrating its robustness and effectiveness in addressing approximation errors.
\cite{fujimoto2018}

\subsection{MA2C for Mixed Environments}
%\subsection{Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments}
This paper explores deep reinforcement learning methods for multi-agent domains, specifically focusing on mixed cooperative-competitive environments. Traditional algorithms like Q-learning and policy gradient face challenges in multi-agent settings due to non-stationarity and high variance. The authors propose an adaptation of actor-critic methods that takes into account the action policies of other agents, enabling the learning of complex coordination strategies. Additionally, an ensemble of policies for each agent is introduced, resulting in more robust multi-agent policies. The approach is validated in various cooperative and competitive scenarios, demonstrating superior performance and robustness compared to existing methods.
\cite{lowe2020}

\subsection{Reducing Overestimation Bias in Multi-Agent Domains Using Double Centralized Critics}
This paper addresses the issue of value function overestimation bias in multi-agent reinforcement learning (MARL). The authors propose using double centralized critics to mitigate this bias, drawing inspiration from Double Q-Learning in single-agent RL. The proposed method is evaluated on six mixed cooperative-competitive tasks, showing significant improvements over current methods. Additionally, the approach is tested on high-dimensional robotic tasks, demonstrating its capability to learn decentralized policies in complex environments. The results highlight the effectiveness of reducing overestimation bias and the potential for broader applications in MARL.
\cite{ackermann2019}

\subsection{The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games}
This paper investigates the performance of Proximal Policy Optimization (PPO) in cooperative multi-agent reinforcement learning (MARL) settings. Despite being less utilized than off-policy algorithms due to perceived sample inefficiency, PPO demonstrates strong performance in four multi-agent testbeds: particle-world environments, StarCraft multi-agent challenge, Google Research Football, and the Hanabi challenge. With minimal hyperparameter tuning and without domain-specific modifications, PPO often achieves competitive or superior results compared to off-policy methods. The study includes ablation analyses to identify key implementation factors, providing practical suggestions for optimizing PPO's performance. These findings suggest that PPO can serve as a strong baseline in cooperative MARL.
\cite{yu2022}

\subsection{Heterogeneous-Agent Reinforcement Learning}
This paper addresses the challenges in cooperative multi-agent reinforcement learning (MARL) with heterogeneous agents. Traditional approaches often rely on parameter sharing among agents, limiting their applicability to homogeneous-agent settings and resulting in training instability and lack of convergence guarantees. The authors propose Heterogeneous-Agent Reinforcement Learning (HARL) algorithms to overcome these issues. Key contributions include the multi-agent advantage decomposition lemma and the sequential update scheme. The paper introduces Heterogeneous-Agent Trust Region Learning (HATRL) and its approximations, HATRPO and HAPPO, which provide stability and convergence guarantees. Additionally, the Heterogeneous-Agent Mirror Learning (HAML) framework strengthens theoretical guarantees and facilitates the design of new cooperative MARL algorithms. The proposed algorithms are comprehensively tested on six challenging benchmarks, demonstrating superior effectiveness and stability compared to existing methods like MAPPO and QMIX.
\cite{zhong2024}



% Conclusion
\section{Conclusion}
\label{sec:conclusion}

\subsection{Summary of Key Insights}


\subsection{Overall Impact on the Field}
How the reviewed literature has shaped the current state of RL


% References
\label{sec:references}
\printbibliography

%\clearpage
%\begin{appendices}
%\end{appendices}

\end{document}
