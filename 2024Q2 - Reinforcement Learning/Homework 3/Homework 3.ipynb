{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<h1><center>\n",
    "    DSOR 646 - Reinforcement Learning </br>\n",
    "    Brandon Hosley\n",
    "</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-policy Monte Carlo Control (every-visit):\n",
    "\n",
    "- [ ] Tune for $Q_1$, $\\varepsilon_a$, and $\\varepsilon_b$.\n",
    "- [ ] Execute at least 40 runs\n",
    "- [ ] At least 10 reps per run\n",
    "- [ ] Report Mean Time-Avg EETDR and Mean Max EETDR\n",
    "- [ ] When milestone testing a policy, use at least 30 episodes\n",
    "- [ ] Use the lower bound of the confidence interval, i.e., 95CILB, to identify a superlative policy\n",
    "- [ ] Show a scatter plot (See the Lesson 9 Handout for an example of the desired scatter plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Parameters\n",
    "num_episodes = 5000\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.999\n",
    "\n",
    "# Action-value function Q(s, a) and policy Ï€(s)\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "# Tracking returns\n",
    "returns = defaultdict(list)\n",
    "\n",
    "def discretize_state(state, bins):\n",
    "    # Discretize each state component separately\n",
    "    state_bins = [np.digitize(state[i], bins[i]) for i in range(len(bins))]\n",
    "    return tuple(state_bins)\n",
    "\n",
    "# Define bins for discretization of the continuous state space\n",
    "state_bins = [\n",
    "    np.linspace(-4.8, 4.8, 10),  # Cart position\n",
    "    np.linspace(-5, 5, 10),      # Cart velocity\n",
    "    np.linspace(-0.418, 0.418, 10),  # Pole angle\n",
    "    np.linspace(-5, 5, 10)       # Pole angular velocity\n",
    "]\n",
    "\n",
    "# Epsilon-greedy policy\n",
    "def epsilon_greedy_policy(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(env.action_space.n)\n",
    "    return np.argmax(Q[state])\n",
    "\n",
    "# Monte Carlo Control Loop\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = discretize_state(state, state_bins)\n",
    "    episode_data = []\n",
    "\n",
    "    # Generate an episode following the current policy\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = epsilon_greedy_policy(state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = discretize_state(next_state, state_bins)\n",
    "        episode_data.append((state, action, reward))\n",
    "        state = next_state\n",
    "\n",
    "    # Calculate returns for the episode\n",
    "    G = 0\n",
    "    visited_pairs = set()\n",
    "    for state, action, reward in reversed(episode_data):\n",
    "        G = reward + gamma * G\n",
    "        if (state, action) not in visited_pairs:\n",
    "            visited_pairs.add((state, action))\n",
    "            returns[(state, action)].append(G)\n",
    "            Q[state][action] = np.mean(returns[(state, action)])\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(min_epsilon, epsilon * decay_rate)\n",
    "\n",
    "# Demonstrate the learned policy\n",
    "num_demo_episodes = 10\n",
    "for _ in range(num_demo_episodes):\n",
    "    state = env.reset()\n",
    "    state = discretize_state(state, state_bins)\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = np.argmax(Q[state])\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        state = discretize_state(next_state, state_bins)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-policy Monte Carlo Control (every-visit, weighted importance sampling):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from scipy.stats.qmc import LatinHypercube\n",
    "from joblib import Parallel, delayed\n",
    "from typing import Tuple\n",
    "from datetime import datetime\n",
    "#from CPv1_MCC_onpoloicy_algorithm_for_parDOE import MCC_onpolicy_DOE\n",
    "\n",
    "def run_experiment (run_index:int, factors:np.ndarray) -> Tuple[int,float,float,float]:\n",
    "    run_start_time = time.time()\n",
    "    maxETDR, maxETDRhw, meanMaxTestEETDR, maxTestHW, meanAULC, hwAULC, secs_taken =\\\n",
    "        MCC_onpolicy_DOE(factors[0],factors[1],factors[2])\n",
    "    alg_score = 0.6*(meanMaxTestEETDR-maxTestHW) + 0.4*(meanAULC-hwAULC)\n",
    "    print(f\"Complete experiment run {run_index} with a score of {alg_score:.2f} ({time.time() - run_start_time:.1f}s)\")\n",
    "    return run_index, maxETDR, maxETDRhw, meanMaxTestEETDR, maxTestHW, meanAULC, hwAULC, secs_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CPU_CORE_PROCS = 6\n",
    "num_runs = 30\n",
    "\n",
    "num_alg_feats = 3\n",
    "rng_seed = 0\n",
    "sampler = LatinHypercube(num_alg_feats, scramble=False, optimization=\"lloyd\", seed=rng_seed)\n",
    "factor_table = sampler.random(n=num_runs)\n",
    "\n",
    "print(f\"\\nInitializing LHS experiment with {num_runs} runs...\")\n",
    "experiment_start_time = time.time()\n",
    "# create an instance of the Parallel object to manage execution of our processes\n",
    "parallel_manager = Parallel(n_jobs = NUM_CPU_CORE_PROCS)\n",
    "# generate a list of function calls to run_experiment () for each row of the factor table \n",
    "# each row of the factor table is an algorithm design run \n",
    "# delayed () creates the list without actually executing run_experiment ()\n",
    "run_list = (delayed (run_experiment)(run_index, factor_table[run_index]) for run_index in range (num_runs) )\n",
    "#execute the list of run_experiment() calls in parallel\n",
    "print (\"\\nExecuting experiment...\")\n",
    "results_table = parallel_manager (run_list)\n",
    "results_table = np.array(results_table)\n",
    "print (f\"\\n\\nCompleted experiment ({time.time () - experiment_start_time:.3f}s)\")\n",
    "\n",
    "# combine the factor table with the results table, add column headers, and save the date to a CSV file\n",
    "# compute algorithm run score, the average of the 95% CI lowerbounds for maximum and mean performance\n",
    "maxEETDR_95CI_LB = results_table[:,3] - results_table[:,4]\n",
    "meanEETDR_95CI_LB = results_table[:,5] - results_table[:,6]\n",
    "score = 0.6*maxEETDR_95CI_LB + 0.4*meanEETDR_95CI_LB\n",
    "results_table = np.column_stack((results_table[:,0], factor_table, results_table[:,1:], score))\n",
    "# grab data for performance scatter plot\n",
    "x = results_table[:,6]\n",
    "y = results_table[:,8]\n",
    "column_names = [\"Run Index\", \"eps_a\", \"eps_b\", \"Init Qbar\", \"Sup EETDR\", \"Sup EETDR hw\", \"Mean Max EETDR\", \n",
    "                \"Mean Max EETDR hw\", \"Time-Avg EETDR\", \"Time-Avg EETDR hw\", \"Secs per run\", \"Score\"]\n",
    "results_table = np.row_stack((column_names, results_table))\n",
    "filename_DOE = \"MCC_onpolicy_results_DOE_\" + datetime.now().strftime('%Y%m%d_%H%M%S') + \".csv\"\n",
    "np. savetxt (filename_DOE, results_table, delimiter = \",\", fmt = \"&s\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Plot Performance Results\n",
    "\n",
    "For example, we can compare different algorithms or same algorithm with large, \n",
    "nominal structural differences using these visual aids\n",
    "\"\"\"\n",
    "\n",
    "#import necessary libraries\n",
    "import matplotlib. pyplot as plt\n",
    "\n",
    "# create scatter plot\n",
    "plt. scatter(x, y, label=\"MCC (on-policy) -- 40 reps per run, 10k episodes per rep\")\n",
    "\n",
    "# setting axes boundaries\n",
    "plt.xlim(0,1) \n",
    "plt.ylim(0,1)\n",
    "# setting title and labels\n",
    "plt.title(\"MCC (on-policy) LHS DOE Performance Results\")\n",
    "plt.xlabel(\"Mean Maximum EETDR\")\n",
    "plt.ylabel(\"Mean Time-Average EETDR\")\n",
    "# grid on\n",
    "plt.grid()\n",
    "# legend on\n",
    "plt.legend(loc='upper left', fontsize=7)\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "X = factor_table\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "input_features = column_names[1:num_alg_feats+1]\n",
    "feature_names = [\n",
    "    name.replace(' ','_').replace('^','_pow_').replace('*','_times_')\n",
    "    for name in poly.get_feature_names_out(input_features=input_features)]\n",
    "df = pd.DataFrame(X_poly, columns=feature_names)\n",
    "\n",
    "# define response variable\n",
    "df['AlgScore'] = score\n",
    "\n",
    "# Create the formula string for the OLS model\n",
    "# Exclude the first column (the constant term) from the predictors\n",
    "predictors = '+'.join(df. columns [1: -1]) # Exclude '1' and 'y'\n",
    "formula = f'AlgScore ~ {predictors}'\n",
    "\n",
    "# Create and fit the OLS model\n",
    "model = ols(formula, data=df)\n",
    "results = model. fit ()\n",
    "\n",
    "# Display the summary\n",
    "print (\"\\n\\n\" )\n",
    "print (results.summary())\n",
    "\n",
    "# Perform ANOVA and display the table\n",
    "anova_results = sm.stats.anova_lm(results, typ=2)\n",
    "print(anova_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
