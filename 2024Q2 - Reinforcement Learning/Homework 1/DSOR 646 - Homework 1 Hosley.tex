\documentclass[12pt,letterpaper]{exam}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[width=8.50in, height=11.00in, left=0.50in, right=0.50in, top=0.50in, bottom=0.50in]{geometry}

\usepackage{libertine}
\usepackage{multicol}
\usepackage[shortlabels]{enumitem}

\usepackage{booktabs}
\usepackage[table]{xcolor}

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bbm}

\usepackage{hyperref}
\usepackage{graphicx}
%\usepackage{wrapfig}
%\usepackage{capt-of}
%\usepackage{tikz}
%\usepackage{pgfplots}
%\usetikzlibrary{shapes,arrows,positioning,patterns}
%\usepackage{pythonhighlight}

\newcommand\chapter{1}
\renewcommand{\thequestion}{\textbf{\chapter.\arabic{question}}}
\renewcommand{\questionlabel}{\thequestion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\class}{ DSOR 646 $-$ Reinforcement Learning } % This is the name of the course 
\newcommand{\assignmentname}{Homework \# \chapter} % 
\newcommand{\authorname}{Hosley, Brandon} % 
\newcommand{\workdate}{\today} % 
\printanswers% this includes the solutions sections
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
\pagestyle{plain}
\thispagestyle{empty}
\noindent
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
\begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} r @{\extracolsep{10pt}} l}
	\textbf{\class} & \textbf{\authorname}  &\\ %Your name here instead, obviously 
	\textbf{\assignmentname} & \textbf{\workdate} & \\
\end{tabular*}\\ 
\rule{\textwidth}{2pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% HEADER %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{questions}

	\setcounter{question}{1-1}
	\question%
	Self-play (p.12)

	\emph{Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, 
	with both sides learning. What do you think would happen in this case? Would it learn a different policy for selecting moves?}
	
	\begin{solution}
		If we may assume that the agent making the first move is not fixed,
		then the two agents will probably develop distinct polices during the early episodes of training.
		Provided that exploration occurs sufficiently, the two agents policies will likely converge to the same policy.
		This is only the case as the result of the relative simplicity of tic-tac-toe.

		If exploration is insufficient, or loss is treated as a lower value than a tie,
		then the two agent's policies will converge on a pair of strategies that are optimized to force the opponent
		into a tied game. The pair of strategiesmay not necessarily by the same.

		If the agent making the first move is fixed, then the two agents would necessarily develop different
		policies despite using the same algorithm.
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\question%
	Symmetries (p.12)
	
	\emph{Many tic-tac-toe positions appear different but are really the same because of symmetries. 
	How might we amend the learning process described above to take advantage of this? 
	In what ways would this change improve the learning process? Now think again. 
	Suppose the opponent did not take advantage of symmetries. In that case, should we? 
	Is it true, then, that symmetrically equivalent positions should necessarily have the same value?}

	\begin{solution}
		Any function on the state space that returns a rotation and reflection invariant result
		can be used to address the equivalence of symmetric states.
		This effectively reduces the functional state-space, which in turn allows training to occur at a faster rate.

		If the opponent does not recongize this state symmetry, then the probability of selection between
		symmetric responses may not necessarily be the same values. Thus it would be beneficial for our own policy
		to discard the symmetry as it attempts to optimize its own response to the opponent.

		As a result, the value gained (especially in the long term) depends on mutual recognition of symmetry.
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\question%
	Greedy play (p.12)
	
	\emph{Suppose the reinforcement learning player was greedy, that is, 
	it always played the move that brought it to the position that it rated the best. 
	Might it learn to play better, or worse, than a nongreedy player? What problems might occur?}

	\begin{solution}
		While the initial performance of the greedy player iwll likely be higher than that of 
		the less greedy player, the tendency for the less greedy player to be able to explore
		will allow them to learn over a larger ranger of possibilities.
		It follows that this will increase the probability that the less greedy player is able to 
		find a less obvious but more optimal policy.
		
		As for the results; the greedy player will likely have higher returns in the short term,
		but the player more willing to explore may have an opportunity to catch up.
		How fast this occurs is dependent on how sub-optimal the greedy player is behaving,
		how much better the actions that the less greedy player finds are,
		and how frequently these actions are chosen in leui of exploration.
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\question%
	Learning from exploration (p.13)
	
	\emph{Suppose learning updates occurred after all moves, including exploratory moves. 
	If the step-size parameter is appropriately reduced over time (but not the tendency to explore), 
	then the state values would converge to a different set of probabilities. 
	What (conceptually) are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? 
	Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?}

	\begin{solution}
		Updates performed after every move will operate akin to a reward signal as described
		by Sutton and Barto; that is, resulting probabilities will be conceptually related to 
		a state reward function.

		If instead we delay the updates and perform updates for multiple states and/or 
		multiple time steps at once, we develop probabilities conceptually similar to value functions.
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	

	\renewcommand\chapter{2}


	\question%
	$\varepsilon$-greedy action selection (p.28)
	
	\emph{In $\varepsilon$-greedy action selection, for the case of two actions and $\varepsilon$=0.5, 
	what is the probability that the greedy action is selected?}

	\begin{solution}
		Under the chapter 2 paradigm the probability of selecting the current greedy 
		action for an arbitrary action space is
		\[
			P\left(A_t= \arg\max_a Q_t(a) \right) = 
			1 - \varepsilon\left(\frac{n-1}{n}\right)
		\]
		where \(n=\) the number of actions available to the agent at the time \(t\).
		
		In the case of \(\varepsilon=0.5\) and \(n=2\)
		the probabilityof selecting the greedy option is \(0.75\).
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\question%
	Bandit example (p.30)
	
	\emph{Consider a $k$-armed bandit problem with $k = 4$ actions, denoted 1, 2, 3, and 4. 
	Consider applying to this problem a bandit algorithm using $\varepsilon$-greedy action selection, 
	sample-average action-value estimates, and initial estimates of $Q_1(a) = 0$, for all $a$. 
	Suppose the initial sequence of actions and rewards is 
	\(A_1 = 1, R_1 = 1, 
	A_2 = 2, R_2 = 1, 
	A_3 = 2, R_3 = 2, 
	A_4 = 2, R_4 = 2, 
	A_5 = 3, R_5 = 0\). 
	On some of these time steps the $\varepsilon$ case may have occurred, causing an action to be selected at random. 
	On which time steps did this definitely occur? On which time steps could this possibly have occurred?}

	\begin{solution}
		Assuming that this implementation does not utilize a \"warm-start\" early exploration
		schema then we know that \(A_2\) is an \(\varepsilon\) case.
		Additionally, we know that \(A_5\) is also an \(\varepsilon\) case.

		Without insight into the RNG of the actor it is not possible to know if \(A_3,A_4\)
		are also \(\varepsilon\) cases with certainty, just that they may not have been.

		If this scenario is an episodic task, then \(A_1\) may also have been an \(\varepsilon\) case,
		otherwise, it may be unusual to consider it in terms of \(\varepsilon\).
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\question%
	Method comparison (p.30)
	
	\emph{In the comparison shown in Figure 2.2, which method will perform best in the long run in terms of 
	cumulative reward and probability of selecting the best action? How much better will it be? 
	Express your answer quantitatively.}

	\begin{solution}
		The \(\varepsilon=0.1\) outperform the other two in the the examined term.
		It is probable that \(\varepsilon=0.01\) will converge to a similar action profile
		as \(\varepsilon=0.1\) if training were allowed to continue.

		There are a number of ways that one might compare the effectiveness of each method quantitatively.
		In terms of closeness to optimality, the difference of the optimality of each method's choices
		at the end of the evaluation period may provide valuable comparisons.
		Utilizing the average of the optimality would provide a more stable comparison;
		but using a moving average would provide that same stability but over time reduce the effect 
		of the initial behavior.
		%
		We would want this comparison if we desire the action-value calculations after the episode.

		If, rather than final behavior, we are concerned with the performance of each methods as they
		accumulate rewards over the episode we would instead want to integrate or sum the rewards across each time step. 
		The difference between these sums would inform us of the performance of each method during the episode.
		%
		We want this comparison if we wish to emphasize episodic performance.
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\renewcommand\chapter{ }


	\question%
	Encode a Python implementation of the simple bandit algorithm defined in the pseudocode block on page 32. 
	Use your implementation to recreate Figure 2.2 (p.29) by performing tests on the 10-armed testbed, 
	described in the first paragraph of Section 2.3. Include your code with your homework submission.
	
	\begin{solution}
		Please see accompanying \colorbox{lightgray}{Homework 1.ipynb} file.
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\question%
	Encode a Python implementation of one of the following algorithms, and test it on the 10-armed testbed you created for the previous problem. 
	Test different parameter values and comment on your results. 
	Use tables and figures as appropriate. Include your code with your homework submission.
	\begin{itemize}
		\item Optimistic greedy
		\item Upper confidence bound 
		\item Gradient bandit
	\end{itemize}
	
	\begin{solution}
		Please see accompanying \colorbox{lightgray}{Homework 1.ipynb} file.
	\end{solution}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{questions}
\end{document}